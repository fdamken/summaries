\chapter{Introduction} % 1.1, 1.2, 1.3, 1.4, 1.5
    \todo{Content}

    \section{Recent and Not-So-Recent Successes} % 1.6, 1.7, 1.8, 1.9
        \todo{Content}
    % end

    \section{"Artificial Intelligence"} % 1.10, 1.11, 1.12, 1.13, 1.14, 1.15, 1.16, 1.17, 1.18
        \todo{Content}
    % end

    \section{Reinforcement Learning Formulation} % 1.19, 1.20, 1.21, 1.22, 1.23, 1.24, 1.25
        \todo{Content}

        \subsection{Flavors} % 1.26, 1.31
            \todo{Content}

            \paragraph{Full} % 1.27
                \todo{Content}
            % end

            \paragraph{Partially Observable Markov Decision Problem} % 1.28
                \todo{Content}
            % end

            \paragraph{Markov Decision Problem} % 1.29
                \todo{Content}
            % end

            \paragraph{Further Simplifications} % 1.30
                \todo{Content}
            % end
        % end

        \subsection{Components} % 1.33, 1.34, 1.35
            \todo{Content}
        % end
    % end

    \section{Wrap-Up} % 1.36, 1.37, 1.38
        \todo{Content}
    % end
% end

\chapter{Preliminaries} % N/A
    \todo{Content}

    \section{Functional Analysis} % 2.56
        \todo{Content}

        \subsection{Normed Vector Spaces} % 2.57
            \todo{Content}
        % end

        \subsection{Contractions} % 2.58
            \todo{Content}
        % end

        \subsection{Fixed Point (Theorem)} % 2.59, 2.60
            \todo{Content}
        % end
    % end

    \section{Statistics} % N/A
        \todo{Content}

        \subsection{Stochastic Processes} % 2.5, 2.6
            \todo{Content}
        % end

        \subsection{Monte-Carlo Estimation} % 3.38
            \todo{Content}
        % end

        \subsection{Bias-Variance Trade-Off} % 3.64, 3.65
            \todo{Content}
        % end

        \subsection{Important Sampling} % 4.36
            \todo{Content}
        % end

        \subsection{Linear Function Approximation} % 5.13
            \todo{Content}

            \subsubsection{Feature Construction} % 5.20, 5.21
                \todo{Content}

                \paragraph{Polynomial Features} % 5.22
                    \todo{Content}
                % end

                \paragraph{Fourier Basis} % 5.23, 5.24
                    \todo{Content}
                % end

                \paragraph{Coarse Coding} % 5.25
                    \todo{Content}
                % end

                \paragraph{Tile Coding} % 5.26, 5.27
                    \todo{Content}
                % end

                \paragraph{Radial Basis Functions} % 5.28
                    \todo{Content}
                % end

                \paragraph{Neural Networks} % 5.29, 5.30
                    \todo{Content}
                % end
            % end
        % end

        \subsection{Fisher Information Matrix} % 6.33, 6.34
            \todo{Content}
        % end

        \subsection{Entropy and Relative Entropy} % 8.2, 8.3, 8.4, 8.5
            \todo{Content}
        % end

        \subsection{Reparametrization Trick} % 8.41, 8.42
            \todo{Content}
        % end
    % end

    \section{Miscellaneous} % N/A
        \todo{Content}

        \subsection{Useful Integrals} % 6.55, 6.56
            \todo{Content}
        % end

        \subsection{Conjugate Gradient} % 8.24, 8.25, 8.26, 8.27
            \todo{Content}
        % end
    % end
% end

\chapter{Markov Decision Processes and Policies} % 2.1
    \todo{Content}

    \section{Markov Decision Processes} % 2.2, 2.3, 2.4, 2.7, 2.8, 2.9, 2.10, 2.14
        \todo{Content}

        \subsection{Continuous State-Action-Space} % 5.3, 5.4
            \todo{Content}
        % end

        \subsection{Example} % 2.11, 2.12, 2.13
            \todo{Content}
        % end
    % end

    \section{Markov Reward Processes} % 2.15, 2.16
        \todo{Content}

        \subsection{Return and Discount} % 2.18, 2.19, 2.20
            \todo{Content}
        % end

        \subsection{Value Function} % 2.21
            \todo{Content}

            \subsubsection{Bellman Equation} % 2.25, 2.27, 2.28
                \todo{Content}
            % end
        % end

        \subsection{Example} % 2.17, 2.22, 2.23, 2.24, 2.26
            \todo{Content}
        % end
    % end

    \section{Markov Decision Processes} % 2.29, 2.30, 2.31, 2.32
        \todo{Content}

        \subsection{Policies} % 2.33, 2.34
            \todo{Content}

            \subsubsection{Value Functions} % 2.35
                \todo{Content}

                \paragraph{Bellman Expectation Equation} % 2.37, 2.38
                    \todo{Content}
                % end

                \paragraph{Bellman Operator} % 2.39, 2.40, 2.50
                    \todo{Content}
                % end
            % end

            \subsubsection{Optimality} % 2.41, 2.42, 2.44
                \todo{Content}

                \paragraph{Bellman Optimality Equation} % 2.46, 2.47, 2.51
                    \todo{Content}
                % end

                \paragraph{Bellman Optimality Operator} % 2.49, 2.50
                    \todo{Content}
                % end
            % end
        % end

        \subsection{Example} % 2.31, 2.36, 2.43, 2.45, 2.48
            \todo{Content}
        % end
    % end

    \section{Wrap-Up} % 2.52, 2.53, 2.54
        \todo{Content}
    % end
% end

\chapter{Dynamic Programming} % 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7
    \todo{Content}

    \section{Finite Horizon DP} % 3.8, 3.9
        \todo{Content}
    % end

    \section{Policy Iteration} % 3.19
        \todo{Content}

        \subsection{Policy Evaluation} % 3.10, 3.11, 3.12
            \todo{Content}
        % end

        \subsection{Policy Improvement} % 3.16, 3.17, 3.18
            \todo{Content}
        % end

        \subsection{Using the Action-Value Function} % 4.9
            \todo{Content}
        % end

        \subsection{Examples} % 3.13, 3.14, 3.15; 3.20, 3.10
            \todo{Content}
        % end
    % end

    \section{Value Iteration} % 3.24, 3.25, 3.27
        \todo{Content}

        \subsection{Principle of Optimality} % 3.22, 3.23, 3.26
            \todo{Content}
        % end

        \subsection{Convergence} % 3.30
            \todo{Content}
        % end

        \subsection{Example} % 3.29
            \todo{Content}
        % end
    % end

    \section{Policy vs. Value Iteration} % 3.28
        \todo{Content}
    % end

    \section{Efficiency} % 3.31, 3.32
        \todo{Content}
    % end
% end

\chapter{Monte-Carlo Algorithms} % 3.33, 3.34, 3.35, 3.36, 3.37
    \todo{Content}

    \section{Policy Evaluation} % 3.39, 3.40, 3.41
        \todo{Content}
    % end

    \section{Example} % 3.42, 3.43, 3.44, 3.45, 3.46, 3.47, 3.48, 3.49, 3.50, 3.51, 3.52, 3.53, 3.54, 3.55, 3.56, 3.57
        \todo{Content}
    % end
% end

\chapter{Temporal Difference Learning} % 3.58, 3.59, 3.60
    \todo{Content}

    \section{Temporal Differences vs. Monte-Carlo} % 3.63
        \todo{Content}

        \subsection{Bias-Variance Trade-Off} % 3.66, 3.67
            \todo{Content}
        % end

        \subsection{Markov Property} % 3.68
            \todo{Content}
        % end

        \subsection{Backup} % 3.69, 3.70, 3.71
            \todo{Content}
        % end
    % end

    \section{Bootstrapping and Sampling} % 3.72
        \todo{Content}
    % end

    \section{\( \text{TD}(\lambda) \)} % 3.73
        \todo{Content}

        \subsection{\(n\)-Step Return} % 3.74, 3.75, 3.76
            \todo{Content}
        % end

        \subsection{\(\lambda\)-Return} % 3.77, 3.78, 3.79
            \todo{Content}
        % end

        \subsection{Eligibility Traces} % 3.80, 3.81, 3.82, 3.83, 3.84, 3.85, 3.86
            \todo{Content}
        % end
    % end

    \section{Example} % 3.61, 3.62
        \todo{Content}
    % end

    \section{Wrap-Up} % 3.87, 3.88, 3.89
        \todo{Content}
    % end
% end

\chapter{Tabular Reinforcement Learning} % 4.1, 4.2, 4.3, 4.4
    \todo{Content}

        \subsection{Monte-Carlo Methods} % 4.5
            \todo{Content}

            \subsubsection{Generalized Policy Iteration} % 4.6, 4.7, 4.8, 4.10
                \todo{Content}
            % end

            \subsubsection{Greediness and Exploration vs. Exploitation} % 4.11, 4.12, 4.13, 4.14
                \todo{Content}

                \paragraph{\(\epsilon\)-Greedy Exploration and Policy Improvement} % 4.15, 4.16
                    \todo{Content}
                % end

                \paragraph{Monte-Carlo Policy Iteration and Control} % 4.17, 4.18
                    \todo{Content}
                % end
            % end

            \subsubsection{GLIE Monte-Carlo Control} % 4.19, 4.20
                \todo{Content}
            % end
        % end

        \subsection{TD-Learning: SARSA} % 4.22, 4.23, 4.24, 4.25, 4.26
            \todo{Content}

            \subsubsection{Convergence} % 4.27
                \todo{Content}
            % end

            \subsubsection{\(n\)-Step} % 4.30
                \todo{Content}
            % end

            \subsubsection{Eligibility Traces and \( \text{SARSA}(\lambda) \)} % 4.31, 4.32
                \todo{Content}
            % end

            \subsubsection{Example} % 4.28, 4.29, 4.33
                \todo{Content}
            % end
        % end
    % end

    \section{Off-Policy Methods} % 4.34, 4.35
        \todo{Content}

        \subsection{Monte-Carlo} % 3.37, 3.38
            \todo{Content}
        % end

        \subsection{TD-Learning} % N/A
            \todo{Content}

            \subsubsection{Importance Sampling} % 3.39
                \todo{Content}
            % end

            \subsubsection{Q-Learning} % 3.40, 3.41, 3.43
                \todo{Content}

                \paragraph{Convergence} % 3.42
                    \todo{Content}
                % end

                \paragraph{Example} % 3.44
                    \todo{Content}
                % end
            % end
        % end
    % end

    \section{Remarks} % 3.45, 3.46
        \todo{Content}
    % end

    \section{Wrap-Up} % 3.47, 3.48
        \todo{Content}
    % end
% end

\chapter{Function Approximation} % 5.1
    \todo{Content}

    \section{On-Policy Methods} % 5.5, 5.6, 5.7, 5.13
        \todo{Content}

        \subsection{Stochastic Gradient Descent} % 5.8
            \todo{Content}
        % end

        \subsection{Gradient Monte-Carlo} % 5.9, 5.10
            \todo{Content}

            \subsubsection{\dots with Linear Function Approximation} % 5.14
                \todo{Content}
            % end
        % end

        \subsection{Semi-Gradient Methods} % 5.11, 5.12
            \todo{Content}

            \subsubsection{\dots with Linear Function Approximation} % 5.15
                \todo{Content}
            % end
        % end

        \subsection{Least-Squares TD} % 5.16
            \todo{Content}

            \subsubsection{Semi-Gradient SARSA} % 5.17, 5.18
                \todo{Content}
            % end
        % end
    % end

    \section{Off-Policy Methods} % 5.31
        \todo{Content}

        \subsection{Semi-Gradient TD} % 5.32
            \todo{Content}
        % end

        \subsection{Divergence} % 5.33, 5.34+, 5.35, 5.36
            \todo{Content}
        % end
    % end

    \section{The Deadly Triad} % 5.37
        \todo{Content}
    % end

    \section{Offline Methods} % N/A
        \todo{Content}

        \subsection{Batch Reinforcement Learning} % 5.38
            \todo{Content}
        % end

        \subsection{Least-Squares Policy Iteration} % 5.39
            \todo{Content}
        % end

        \subsection{Fitted Q-Iteration} % 5.40
            \todo{Content}
        % end
    % end

    \section{Wrap-Up} % 5.41, 5.42, 5.43
        \todo{Content}
    % end
% end

\chapter{Policy Search} % 6.1, 6.2, 6.3, 6.4, 6.5, 6.6, 6.7, 6.8
    \todo{Content}

    \section{Policy Gradient} % 6.9, 6.10, 6.11
        \todo{Content}

        \subsection{Computing the Gradient} % N/A
            \todo{Content}

            \subsubsection{Finite Differences} % 6.12
                \todo{Content}
            % end

            \subsubsection{Least-Squares-Based Finite Differences} % 6.13, 6.14
                \todo{Content}
            % end

            \subsubsection{Likelihood-Ratio Trick} % 6.15, 6.16, 6.17
                \todo{Content}
            % end
        % end

        \subsection{REINFORCE} % 6.18, 6.19
            \todo{Content}

            \subsubsection{Gradient Variance and Baselines} % 6.20, 6.21, 6.22+, 6.23
                \todo{Content}
            % end

            \subsubsection{Example} % 6.24, 6.25, 6.26, 6.27, 6.28
                \todo{Content}
            % end
        % end

        \subsection{GPOMDP} % 6.29, 6.30, 6.31
            \todo{Content}
        % end
    % end

    \section{Natural Policy Gradient} % 6.32, 6.35, 6.36; 6.57, 6.58+, 6.59
        \todo{Content}
    % end

    \section{The Policy Gradient Theorem} % 6.37, 6.38, 6.39, 6.40; 6.60, 6.61+
        \todo{Content}

        \subsection{Actor-Critic} % 6.41
            \todo{Content}
        % end

        \subsection{Compatible Function Approximation} % 6.42, 6.43, 6.44+
            \todo{Content}

            \subsubsection{Example} % 6.45
                \todo{Content}
            % end
        % end

        \subsection{Advantage Function} % 6.46
            \todo{Content}
        % end

        \subsection{Episodic Actor-Critic} % 6.47, 6.48, 6.49, 6.50
            \todo{Content}
        % end
    % end

    \section{Wrap-Up} % 6.51, 6.52, 6.53
        \todo{Content}
    % end
% end

\chapter{Deep Reinforcement Learning} % 7.1, 7.2, 7.3, 7.4, 7.5
    \todo{Content}

    \section{Deep Q-Learning: DQN} % 7.6, 7.7, 7.8, 7.13
        \todo{Content}

        \subsection{Replay Buffer} % 7.9
            \todo{Content}
        % end

        \subsection{Target Network} % 7.10
            \todo{Content}
        % end

        \subsection{Minibatch Updates} % 7.11
            \todo{Content}
        % end

        \subsection{Reward- and Target-Clipping} % 7.12
            \todo{Content}
        % end

        \subsection{Examples} % 7.14, 7.15, 7.16, 7.17, 7.18
            \todo{Content}
        % end
    % end

    \section{DQN Enhancements} % 7.19
        \todo{Content}

        \subsection{Overestimation and Double Deep Q-Learning} % 7.20, 7.21, 7.22, 7.23
            \todo{Content}

        \subsection{Prioritized Replay Buffer} % 7.24, 7.25, 7.26
            \todo{Content}
        % end

        \subsection{Dueling DQN} % 7.27
            \todo{Content}
        % end

        \subsection{Noisy DQN} % 7.28, 7.29
            \todo{Content}
        % end

        \subsection{Distributional DQN} % 7.30, 7.31, 7.32, 7.33, 7.34, 7.35
            \todo{Content}
        % end

        \subsection{Rainbow} % 7.36, 7.37
            \todo{Content}
        % end
    % end

    \section{Other DQN-Bases Methods} % 7.38
        \todo{Content}

        \subsection{Count-Based Exploration} % 7.39, 7.40, 7.41, 7.42
            \todo{Content}
        % end

        \subsection{Curiosity-Driven Exploration} % 7.43, 7.44
            \todo{Content}
        % end

        \subsection{Ensemble-Driven Exploration} % 7.45, 7.46, 7.47
            \todo{Content}
        % end
    % end

    \section{Wrap-Up} % 7.48, 7.49, 7.50
        \todo{Content}
    % end
% end

\chapter{Deep Actor-Critic} % 8.1, 8.6, 8.7, 8.8, 8.9; 8.15
    \todo{Content}

    \section{Surrogate Loss} % 8.10, 8.11, 8.12
        \todo{Content}

        \subsection{Kakade-Langford-Lemma} % 8.13+
            \todo{Content}
        % end

        \subsection{Practical Surrogate Loss} % 8.14
            \todo{Content}
        % end
    % end

    \section{Advantage Actor-Critic (A2C)} % 8.16, 8.17, 8.18
        \todo{Content}
    % end

    \section{On-Policy Methods} % 8.19
        \todo{Content}

        \subsection{Trust-Region Policy Optimization (TRPO)} % 8.20, 8.21, 8.28
            \todo{Content}

            \subsubsection{Practical Implementation} % 8.22, 8.23
                \todo{Content}
            % end
        % end

        \subsection{Proximal Policy Optimization (PPO)} % 8.29, 8.30
            \todo{Content}
        % end
    % end

    \section{Off-Policy Methods} % 8.31
        \todo{Content}

        \subsection{Deep Deterministic Policy Gradient (DDPG)} % 8.32, 8.33, 8.34, 8.35
            \todo{Content}
        % end

        \subsection{Twin Delayed DDPG (TD3)} % 8.36, 8.37
            \todo{Content}
        % end

        \subsection{Soft Actor-Critic (SAC)} % 8.38, 8.39, 8.40, 8.43, 8.44, 8.45
            \todo{Content}
        % end
    % end

    \section{Wrap-Up} % 8.46, 8.47, 8.48
        \todo{Content}
    % end
% end

\chapter{Frontiers} % 9.1
    \todo{Content}

    \section{Partial Observability} % 9.2, 9.3, 9.4, 9.5, 9.6, 9.7
        \todo{Content}
    % end

    \section{Hierarchical Control} % 9.8, 9.9, 9.10, 9.17
        \todo{Content}

        \subsection{The Options Framework} % 9.11, 9.12, 9.13, 9.14, 9.15, 9.16
            \todo{Content}
        % end
    % end

    \section{Markov Decision Processed Without Reward} % 9.18, 9.19
        \todo{Content}

        \subsection{Intrinsic Motivation} % 9.20, 9.21, 9.22
            \todo{Content}
        % end

        \subsection{Inverse Reinforcement Learning} % 9.23, 9.24, 9.25, 9.26, 9.27
            \todo{Content}
        % end
    % end

    \section{Model-Based Reinforcement Learning} % 9.28, 9.29, 9.30, 9.31, 9.32, 9.33, 9.34
        \todo{Content}
    % end

    \section{Wrap-Up} % 9.35, 9.36
        \todo{Content}
    % end
% end

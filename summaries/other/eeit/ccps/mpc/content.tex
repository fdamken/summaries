\renewcommand{\mat}[1]{\bm{\mathrm{#1}}}
\newcommand{\transposed}{{\!\top}}

\chapter{Introduction}
	This summary covers \emph{model predictive control} (MPC) combined with \emph{machine learning} (ML). In MPC, a model of a dynamical system is used to find inputs that steer the system optimally (in some sense). ML, on the other hand, can be used to build such models from data. This document focuses primarily on the MPC part, featuring nominal, robust, and stochastic MPC. Subsequently, connections to and applications of ML are drawn as these fields get more and more interconnected. The key topics are understanding MPC basics, identifying benefits and drawbacks of MPC, understanding the role of ML in control, understanding the basic concepts of ML-supported MPC as well as its benefits and drawbacks.

	\section{What is Model Predictive Control?}
		In general, \emph{control} is concerned with influencing a dynamical system such that it exhibits a wanted behavior. Usually, this involves incorporating feedback (e.g., the actual state of the system) into the control law (feedback control). In \emph{optimal} control, the inputs shall be optimal in some sense (e.g., minimal energy consumption, avoidance of states, \dots).

		In \emph{model} predictive control, a model of the system is used to predict the influence of inputs. This has the advantage of the controller actually understanding what it is doing, potentially increasing the performance and yielding a structured design process of the controller. On the other hand, MPC needs a model that can be hard to obtain\footnote{A common way to obtain a model aside from deriving it using first principles are gathering data of the system, fixing a model structure, and fitting the parameters.}. Also, it is computationally expensive and its  performance is highly influenced by the model quality and accuracy.

		An MPC controller performs \emph{prediction} using the model to assess the influence of certain actions. This can be used for finding the optimal inputs by minimizing a cost function on the predicted states. This optimal input can then be applied to the system. Hence, MPC is \emph{optimization-based} control: the optimal input is retrieved by minimizing a cost function with the dynamical system as a constraint on the states. This allows to incorporate additional constraints (e.g., min/max actions or unsafe states) directly into the optimizer. To incorporate feedback from the actual system, this optimization problem is solved repeatedly during execution (see \autoref{fig:mpcCycle}).

		Model predictive control is covered in detail in \autoref{c:mpcNominal}, \ref{c:mpcRobust}, and \ref{c:mpcStochastic}.

		\begin{figure}
			\centering
			\begin{tikzpicture}[->, every node/.style = { draw, rectangle, minimum width = 3cm, minimum height = 1cm }]
				\node (a) {Obtain State};
				\node [right = 1.5 of a] (b) {Predict System};
				\draw (a) to coordinate(d) (b);
				\node [below = 1 of d] (c) {Apply Input};
				\draw (b) |- (c);
				\draw (c) -| (a);
			\end{tikzpicture}
			\caption{Illustration of the model predictive control cycle.}
			\label{fig:mpcCycle}
		\end{figure}
	% end

	\section{What is Machine Learning?}
	   	While there is no unified definition of machine learning, it is often used to describe systems that use a bunch of data to find relations/patterns/connections/\dots in them and to extract general rules. Typical methods are neural networks, Gaussian processes, support vector machines, and many more. In control, the applications of machine learning are twofold: first, it can be used to find a model and use the model in a model-based controller (supervised learning and regression); second, this step can also be skipped and ML can be applied directly as the controller (usually covered in reinforcement learning).

	   	Machine learning for MPC is covered in detail in \autoref{c:ml}.
	% end
% end

\chapter{Preliminaries}
	This chapter covers some preliminaries required to understand the upcoming chapters.

	\section{System Theory}
		\emph{System theory} describes the study of all kinds of dynamical systems, their stability, controllability, and various other properties. This section introduces the most important concepts like the different kinds of representations and stability. In MPC, system theory is both used to study the behavior of the actual system as well as the model.

		\subsection{Types of Dynamical Systems}
			On a high level, dynamical systems separate into two classes: time-continuous and time-discrete. By Shannon's sampling theorem, it is always possible to turn a continuous model into a discrete one with an appropriate sample rate.

			\subsubsection{Time-Continuous}
				A time-continuous nonlinear system is represented by an (ordinary) initial value problem
				\begin{align}
					\dot{\vec{x}} &= \vec{f}(\vec{x}, \vec{u}; t) &
					\vec{y} &= \vec{h}(\vec{x}, \vec{u}; t) &
					\vec{x}(t_0) &= \vec{x}_0
				\end{align}
				where \(\vec{x}\) are the states, \(\vec{u}\) is the control input, \(\vec{y}\) are the observations, and \(t\) is the time. If \(\vec{f}\) or \(\vec{h}\) is \(t\)-dependent, the system is called \emph{time-variant}, otherwise it is called \emph{time-invariant}. If \(\vec{f}\) and \(\vec{h}\) are linear functions \( \vec{f}(\vec{x}, \vec{u}; t) = \mat{A}(t) \vec{x} + \mat{B}(t) \vec{u} \) and \( \vec{h}(\vec{x}, \vec{u}; t) = \mat{C}(t) \vec{x} + \mat{D}(t) \vec{u} \), the system is called \emph{linear} with state dynamics matrix \(\mat{A}\), control matrix \(\mat{B}\), output/observation matrix \(\mat{C}\), and control influence matrix \(\mat{D}\).

				If the system matrices \(\mat{A}\), \(\mat{B}\), \(\mat{C}\), and \(\mat{D}\) are time-independent, a major advantage of linear systems is that they exhibit an analytical solution:
				\begin{equation}
					\vec{x}(t) = \exp\bigl\{ \mat{A}(t - t_0) \bigr\} \vec{x}_0 + \int_{t_0}^{t}\! \exp\bigl\{ \mat{A}(t - \tau) \bigr\} \mat{B} \vec{u}(\tau) \dd{\tau}.
				\end{equation}
				Note that here, \( \mat{A}(t - t_0) \) does \emph{not} correspond to an invocation and time-dependence, is is simply a multiplication with \(t - t_0\). However, the vast majority of dynamical systems are not linear! Hence, these models are often approximated locally (around an operation point \( (\bar{\vec{x}}, \bar{\vec{u}}) \)) using the Taylor series of \(\vec{f}\):
				\begin{equation}
					\vec{f}(\vec{x}, \vec{u}; t) \approx \vec{f}(\bar{\vec{x}}, \bar{\vec{u}}; t) + \Biggl( \pdv{\vec{f}}{\vec{x}}\bigg\vert_{\vec{x} = \bar{\vec{x}}} \Biggr) (\vec{x} - \bar{\vec{x}}) + \Biggl( \pdv{\vec{f}}{\vec{u}}\bigg\vert_{\vec{u} = \bar{\vec{u}}} \Biggr) (\vec{u} - \bar{\vec{u}})
				\end{equation}
				By cutting off the higher-order terms, this yields a linear approximation.
			% end

			\subsubsection{Time-Discrete}
				A \emph{time-discrete} nonlinear system is represented by a dynamics equation
				\begin{align}
					\vec{x}_{k + 1} &= \vec{f}(\vec{x}_k, \vec{u}_k; k) &
					\vec{y}_k &= \vec{h}(\vec{x}_k, \vec{u}_k; k)
				\end{align}
				with an initial value \(\vec{x}_0\). Time-variant and -invariant systems as well as linear models are defined analogous to time-continuous systems. Again, linear time-invariant models can be solved in closed form:
				\begin{equation}
					\vec{x}_k = \mat{A}^k\, \vec{x}_0 + \sum_{j = 0}^{k - 1} \mat{A}^{k - j - 1}\, \mat{B} \vec{u}_j  \label{eqn:discreteClosedForm}
				\end{equation}
				However, while discrete systems are easier to handle than continuous systems (e.g., computers work discretely), the world is inherently continuous. Hence, systems are often \emph{discretized} by using discrete indices \(\cdot_k\) corresponding to the value at time \(t_k = k h\), where \(h\) is the \emph{sampling time}. To apply a discrete control signal \( \vec{u}_k \) to a continuous system, it is usually applied using a step function, i.e., \( u(t) = u(t_k) \) for \( t \in [t_k, t_{k + 1}) \). To compute a discrete system from a continuous system, the difference quotient can be used:
				\begin{equation}
					\dot{x} \approx \frac{\vec{x}_{k + 1} - \vec{x}_k}{h}
					\quad\implies\quad
					\vec{x}_{k + 1} \approx \vec{x}_k + h \dot{x}
				\end{equation}
				This is, in fact, equivalent to Euler's method for solving an initial value problem.
			% end
		% end

		\subsection{Stability} % 2.29, 12.4
			One of the fundamental properties studied in dynamical systems theory is \emph{stability}. Stability describes the asymptotic behavior of a system: a system is either asymptotically stable, instable, or marginally stable. All of these variants can also occur in a \emph{ringing} configuration where the system oscillates between different values (see \autoref{fig:stability}). Usually, the goal of control is to stabilize an unstable system.

			\begin{definition}[Global Asymptotic Stability]
				A dynamical system with state \( \vec{x}(t) \) is \emph{globally asymptotically stable} in an equilibrium point \( \bar{\vec{x}} \) iff \( \lim_{t \to \infty} \vec{x}(t) = \bar{\vec{x}} \) for all \( \vec{x}_0 \in \R^n \).
			\end{definition}

			\begin{theorem}[Global Asymptotic Stability of Linear, Discrete-Time, Time-Invariant Systems]
				The system \( \vec{x}_{k + 1} = \mat{A} \vec{x}_k \) is globally asymptotically stable for \( \bar{\vec{x}} = \vec{0} \) iff \( \lvert \lambda_i \rvert < 1 \) for all \( i = 1, 2, \dots, n \), where \( \lambda_i \) is the \(i\)-th eigenvalue of \(\mat{A}\).
			\end{theorem}
			\begin{theorem}[Global Asymptotic Stability of Linear, Continuous-Time, Time-Invariant Systems]
				The system \( \dot{\vec{x}} = \mat{A} \vec{x} \) is globally asymptotically stable for \( \bar{\vec{x}} = \vec{0} \) iff \( \Re(\lambda_i) < 0 \) for all \( i = 1, 2, \dots, n \), where \( \lambda_i \) is the \(i\)-th eigenvalue of \(\mat{A}\).
			\end{theorem}

			\begin{figure}
				\centering
				\includegraphics[width=\linewidth]{tmp-stability}
				\caption{Stability Characteristics}
				\label{fig:stability}
			\end{figure}

			\subsubsection{State-Feedback Controllers}
				By introducing feedback-control \( \vec{u}_k = -\mat{K} \vec{x} \) with a \emph{gain matrix} \(\mat{K}\), the eigenvalues of a dynamical systems are characterized by
				\begin{equation}
					\vec{x}_{k + 1}
						= \mat{A} \vec{x}_k + \mat{B} \vec{u}_k
						= \mat{A} \vec{x}_k - \mat{B} \mat{K} \vec{x}_k
						= \underbrace{(\mat{A} - \mat{B} \mat{K})}_{\eqqcolon\, \tilde{\mat{A}}} \vec{x}_k.
				\end{equation}
				Hence, the eigenvalues (also called \emph{poles}) can be placed arbitrarily by modifying \(\mat{K}\) and henceforth \(\tilde{\mat{A}}\) which defines stability.
			% end

			\subsubsection{Lyapunov Stability and Lyapunov Function}
				For nonlinear systems, multiple or even infinite or no equilibrium points might exist, making characterization of stability difficult. One option is \emph{Lyapunov stability}:
				\begin{definition}[Lyapunov Stability]
					An equilibrium point \(\bar{\vec{x}}\) is \emph{Lyapunov stable} iff for all \(t\) and  for all \(\epsilon > 0\) there exists a \(\delta(\epsilon) > 0\) such that \( \lVert \vec{x}(t) - \bar{\vec{x}} \rVert < \epsilon \) if \( \lVert \vec{x}(0) - \bar{\vec{x}} \rVert > \delta(\epsilon) \).
				\end{definition}

				This builds on the intuition that stability causes the system to stay close to an equilibrium point of the system starts close to it. Lyapunov stability can be further extended to asymptotic stability of nonlinear systems by requiring that the equilibrium is attractive:
				\begin{definition}[(Global) Asymptotic Lyapunov Stability]
					An equilibrium point \(\bar{\vec{x}} \in D\) is \emph{asymptotically stable} in \( D \subseteq \R^n \) if it is Lyapunov stable and attractive, i.e., \( \lim_{t \to \infty} \lVert \vec{x}(t) - \bar{\vec{x}} \rVert = 0 \) for all \( \vec{x}_0 \in D \). If additionally \( D = \R^n \), it is called \emph{globally} asymptotically stable.
				\end{definition}

				However, while these definitions are quote straightforward, checking stability for an arbitrary nonlinear system is still an open challenge. One option is to linearize the system around an equilibrium point and subsequently analyze the stability of the linear system. Another option is the usage of \emph{Lyapunov functions}:
				\begin{definition}[Discrete Lyapunov Functions]
					A continuous function \( V : D \to \R \), \( D \subseteq \R^n \) is a \emph{Lyapunov function} for a system \( \vec{x}_{k + 1} = \vec{f}(\vec{x}_k) \) if all of the following hold:
					\begin{enumerate}
						\item \( V(\vec{0}) = 0 \)
						\item \( V(\vec{x}) > 0 \) for all \( \vec{x} \in D \setminus \{ \vec{0} \} \)
						\item \( V\bigl( \vec{f}(\vec{x}) \bigr) - V(\vec{x}) \leq 0 \) for all \( \vec{x} \in D \)
					\end{enumerate}
					If additionally \( V\bigl( \vec{f}(\vec{x}) \bigr) - V(\vec{x}) > 0 \) holds for all \( \vec{x} \in D \setminus \{ \vec{0} \} \), \(V\) is a \emph{strict} Lyapunov function.
				\end{definition}
				\begin{definition}[Continuous Lyapunov Functions]
					A continuously differentiable function \( V : D \to \R \), \( D \subseteq \R^n \) is a \emph{Lyapunov function} for a system \( \dot{\vec{x}} = \vec{f}(\vec{x}) \) if all of the following hold:
					\begin{enumerate}
						\item \( V(\vec{0}) = 0 \)
						\item \( V(\vec{x}) > 0 \) for all \( \vec{x} \in D \setminus \{ \vec{0} \} \)
						\item \( \dot{V}(\vec{x}) \leq 0 \) for all \( \vec{x} \in D \)
					\end{enumerate}
					If additionally \( \dot{V}(\vec{x}) > 0 \) holds for all \( \vec{x} \in D \setminus \{ \vec{0} \} \), \(V\) is a \emph{strict} Lyapunov function.
				\end{definition}
				\begin{theorem}[Lyapunov Functions for Stability]
					If a Lyapunov functions exists for a dynamical system, it is \emph{locally stable} in \( \vec{x} = \vec{0} \). If the Lyapunov function is strict, the equilibrium is locally \emph{asymptotically} stable.
				\end{theorem}

				However, finding these Lyapunov functions is generally hard. For systems derived from first order principles, the energy of the system is generally a good candidate for a Lyapunov function worth checking. Other methods for checking stability are, for example, Nyquist and Routh-Hurwitz stability.
			% end
		% end

		\subsection{Detectability, Observability, Controllability, and Stabilizability}
			As seen before, the eigenvalues of linear systems can be placed using a linear control law. However, being able to control a system like this has some requirements: first, the system must be \emph{observable} (i.e., the states must be known, either by observing them directly of by reconstructing them from the measurements). Second, the system must be \emph{controllable} (i.e., the states must be directly or indirectly influenced by the control inputs). As milder condition to stabilize a system is stabilizability requiring that at least the unstable states must be influenceable\footnote{Note that stabilizability is milder as it does not allow to steer the system to arbitrary states.}.

			\begin{definition}[Observability]
				A system is \emph{observable} iff there exists an \(N\) such that for every initial state \(\vec{x}_0\), the measurements \( \vec{y}_0, \vec{y}_1, \dots, \vec{y}_{N - 1} \) uniquely determine \(\vec{x}_0\).
			\end{definition}
			\begin{definition}[Controllability]
				A system is \emph{controllable} iff for every initial state \(\vec{x}_0\) and desired state \(\vec{x}_d\) there exists an input sequence \( \vec{u}_0, \vec{u}_1, \dots, \vec{u}_{N - 1} \) such that \( \vec{x}_N = \vec{x}_d \).
			\end{definition}

			\begin{theorem}[Observability of Linear Time-Invariant Systems]
				A system \( \vec{x}_{k + 1} = \mat{A} \vec{x}_k \) is \emph{observable} iff
				\begin{equation}
					\rank
					\begin{bmatrix}
						\mat{C} \\
						\mat{C} \mat{A} \\
						\mat{C} \mat{A}^2 \\
						\vdots \\
						\mat{C} \mat{A}^{n - 2} \\
						\mat{C} \mat{A}^{n - 1}
					\end{bmatrix}
					= n.
				\end{equation}
				The system is \emph{detectable} iff \( \rank \begin{bmatrix} \lambda \mat{I} - \mat{A} & \mat{C} \end{bmatrix} = n \).
			\end{theorem}
			\begin{theorem}[Controllability of Linear Time-Invariant Systems]
				A system \( \vec{x}_{k + 1} = \mat{A} \vec{x}_k \) is \emph{controllable} iff
				\begin{equation}
					\rank
					\begin{bmatrix}
						\mat{B} &
						\mat{A} \mat{B} &
						\mat{A}^2\, \mat{B} &
						\cdots &
						\mat{A}^{n - 2}\, \mat{B} &
						\mat{A}^{n - 1}\, \mat{B}
					\end{bmatrix}
					= n.
				\end{equation}
				The system is stabilizable iff \( \rank \begin{bmatrix} \lambda \mat{I} - \mat{A} & \mat{b} \end{bmatrix} = n \).
			\end{theorem}
		% end
	% end

	\section{Linear Quadratic Regulator}
		This section introduces the linear quadratic regulator (LQR), an unconstrained optimal control method for simple linear systems. Of course, to perform optimal control, a notion of \emph{optimality} has to be defined. This definition also defines the overall objective of the controller. Some notions are:
		\begin{itemize}
			\item \eqmakebox[optimalControl][l]{\emph{Terminal Control Problem:}} the system shall be as close to a given terminal state as possible within a given period of time
			\item \eqmakebox[optimalControl][l]{\emph{Minimum Time:}} reach the terminal state in minimum time
			\item \eqmakebox[optimalControl][l]{\emph{Minimum Energy:}} reach the terminal state with minimum expenditure
		\end{itemize}
		All of these goals are subsumed in the \emph{cost function} \(J\) of an optimal control problem.

		\subsection{Cost Functions}
			As seen already, the cost function is the core component of an optimal control problem defining \emph{optimality}. Some examples for cost functions are
			\begin{align}
				J &= E(\vec{x}(t_e)) &
				J &= t_e,\, \vec{x}(t_e) = \vec{x}_d &
				J &= \sum_{k = 1}^{N - 1} L\bigl( \vec{u}_k \bigr) \simeq \int_{t_0}^{t_e}\! L\bigl( \vec{u}(\tau) \bigr) \dd{\tau}
			\end{align}
			encoding a terminal cost, minimum time, and minimum energy, respectively (from left to right). The last cost has to be augmented for continuous problems by replacing the sum with an integral, indicated by \( \simeq \). Note that these functions can be combined, e.g., by defining an energy cost and a terminal cost. Also note that \(L\) in the energy cost is pretty general and might even represent quantities aside from energy.

			In a \emph{regulation}, the desired setpoint \(\vec{x}_d\) is constant (and usually zero by shifting the coordinates appropriately) and does not depend on time. The goal is therefore to stabilize the system at the desired state. With \emph{tracking}, the setpoint is time-variant and the goal is to steer the system to follow the trajectory (trajectory tracking).
		% end

		\subsection{LQR Formulation}
			In the LQR setting, a linear time-discrete system along with a quadratic cost function
			\begin{equation}
				J = \frac{1}{2} \sum_{k = 1}^{N - 1} \vec{x}_k^\transposed \mat{Q} \vec{x}_k + \vec{u}_k^\transposed \mat{R} \vec{u}_k + \vec{x}_N^\transposed \mat{S} \vec{x}_N.  \label{eqn:lqrCost}
			\end{equation}
			with\footnote{For a matrix \(\mat{M}\), \( \mat{M} \succeq 0\) and \( \mat{M} \succ 0 \) mean that \(\mat{M}\) is (semi) positive definite.} \( \mat{Q} \succeq 0 \), \( \mat{R} \succ 0 \), \( \mat{S} \succ 0 \). Besides the dynamics \( \vec{x}_{k + 1} = \mat{A}_k \vec{x}_k + \mat{B}_k \vec{u} \), no further constraints are considered. The optimal control problem is now framed as follows (with \( \tilde{\vec{u}} = \vec{u}_{1:N - 1} = \{ \vec{u}_1, \vec{u}_2, \dots, \vec{u}_{N - 1} \} \)):
			\begin{align}
				\min_{\tilde{\vec{u}}}\; & \frac{1}{2} \sum_{k = 1}^{N - 1} \vec{x}_k^\transposed \mat{Q} \vec{x}_k + \vec{u}_k^\transposed \mat{R} \vec{u}_k + \vec{x}_N^\transposed \mat{S} \vec{x}_N \\
				\text{s.t.}\qquad&
					\begin{aligned}
						\vec{x}_{k + 1} &= \mat{A}_k \vec{x}_k + \mat{B}_k \vec{u}_k
					\end{aligned}
				\label{eqn:lqr}
			\end{align}
			In the upcoming sections, this problem is solved in two different fashions.
		% end

		\subsection{Batch Optimization}
			\label{subsec:batchOpt}

			The straightforward approach for solving \eqref{eqn:lqr} in the time-invariant case is to use batch optimization over the variables \(\tilde{\vec{u}}\) by treating them as a function of the initial state \(\vec{x}_0\) by exploiting the closed form solution \eqref{eqn:discreteClosedForm} and writing it in vector form:
			\begin{equation}
				\underbrace{\begin{bmatrix}
					\vec{x}_0 \\
					\vec{x}_1 \\
					\vec{x}_2 \\
					\vdots \\
					\vec{x}_{N - 1} \\
					\vec{x}_N
				\end{bmatrix}}_{\tilde{\vec{x}} \,\coloneqq}
				=
				\underbrace{\begin{bmatrix}
					\mat{I} \\
					\mat{A} \\
					\mat{A}^2 \\
					\vdots \\
					\mat{A}^{N - 1} \\
					\mat{A}^N
				\end{bmatrix}}_{\tilde{\mat{A}} \,\coloneqq}
				\vec{x}_0
				+
				\underbrace{\begin{bmatrix}
					\mat{O}                   & \cdots                    & \cdots                    & \cdots  & \mat{O} \\
					\mat{B}                   & \mat{O}                   & \cdots                    & \cdots  & \mat{O} \\
					\mat{A} \mat{B}           & \mat{B}                   & \mat{O}                   & \cdots  & \mat{O} \\
					\vdots                    & \ddots                    & \ddots                    & \ddots  & \vdots  \\
					\mat{A}^{N - 2}\, \mat{B} & \mat{A}^{N - 3}\, \mat{B} & \ddots                    & \mat{B} & \mat{O} \\
					\mat{A}^{N - 1}\, \mat{B} & \mat{A}^{N - 2}\, \mat{B} & \mat{A}^{N - 3}\, \mat{B} & \cdots  & \mat{B}
				\end{bmatrix}}_{\tilde{\mat{B}} \,\coloneqq}
				\underbrace{\begin{bmatrix}
					\vec{u}_0 \\
					\vec{u}_1 \\
					\vec{u}_2 \\
					\vdots \\
					\vec{u}_{N - 1}
				\end{bmatrix}}_{\tilde{\vec{u}} \,\coloneqq}
			\end{equation}
			Using the matrices defined above, this can be written shortly as \( \tilde{\vec{x}} = \tilde{\mat{A}} \vec{x}_0 + \tilde{\mat{B}} \tilde{\vec{u}} \). Similarly, the cost function \eqref{eqn:lqrCost} can be reformulated as \( J(\vec{x}_0, \tilde{\vec{u}}) \propto \tilde{\vec{x}}^\transposed \tilde{\mat{Q}} \tilde{\vec{x}} + \tilde{\vec{u}}^\transposed \tilde{\mat{R}} \tilde{\vec{u}} \) with
			\begin{align}
				\tilde{\mat{Q}} &= \diag(\underbrace{\mat{Q}, \mat{Q}, \dots, \mat{Q}}_{N\text{ times}}, \mat{S}) &
				\tilde{\mat{R}} &= \diag(\underbrace{\mat{R}, \mat{R}, \cdots, \mat{R}}_{N\text{ times}}).
			\end{align}
			By plugging \(\tilde{\vec{x}}\) into this cost function, it can be solved in closed form, yielding the optimal control inputs
			\begin{equation}
				\tilde{\vec{u}}^\ast = -\mat{H}^{-1}\, \mat{F}^\transposed \vec{x}_0,\qquad
			\end{equation}
			with \( \mat{H} = \tilde{\mat{B}}^\transposed \tilde{\mat{Q}} \tilde{\mat{B}} + \tilde{\mat{R}} \) and \( \mat{F} = \tilde{\mat{A}}^\transposed \tilde{\mat{Q}} \tilde{\mat{B}} \).

			However, while this approach is simplistic, it a major caveat: no feedback is involved (the control signals just depend on the initial value, i.e., it is an open-look controller). Hence, if the real system deviates from the model, the control inputs might be suboptimal or even harmful. This problem is addressed by the next solution method which also exploits the special structure of the problem.
		% end

		\subsection{Dynamic Programming}
			As seen before, applying batch optimization---while being straightforward---is not ideal as the solution does not incorporate the system's feedback. An alternative approach is to use \emph{dynamic programming}. The underlying idea of dynamic programming is that partial trajectories of trajectories are optimal, too. In other words: a trajectory composed of partial optimal ones is optimal. A relevant quantity for solving LQR with this principle is the optimal \emph{cost to go}, also called the \emph{value function}:
			\begin{equation}
				J_j^\ast(\vec{x}_j) = \min_{\vec{u}_{j:N - 1}}\; \frac{1}{2} \sum_{k = j}^{N - 1} \vec{x}_k^\transposed \mat{Q} \vec{x}_k + \vec{u}_k^\transposed \mat{R} \vec{u}_k + \vec{x}_N^\transposed \mat{S} \vec{x}_N.
			\end{equation}
			This function quantifies the optimal cost when starting from state \(\vec{x}_j\) at time \(j\). By solving this problem\footnote{See "Robot Learning" (\url{https://fabian.damken.net/summaries/cs/elective/ce/role/}) for a more thorough (stochastic) treatment.} recursively starting from \(j = N\), the optimal control input at time step \(k\) is found to be
			\begin{equation}
				\vec{u}_k^\ast = \mat{K}_k \vec{x}_k
			\end{equation}
			with \( \mat{K}_k = -\bigl( \mat{B}^\transposed \mat{P}_{k + 1} \mat{B} + \mat{R} \bigr)^{-1\,} \mat{B}^\transposed \mat{P}_{k + 1} \mat{A} \) and optimal cost \( J_k^\ast(\vec{x}_j) = \vec{x}_k^\transposed \mat{P}_k \vec{x}_k / 2 \). Here, \( \mat{P}_k \) is given by the \emph{discrete time-variant algebraic Riccati equation}
			\begin{equation}
				\mat{P}_k = \mat{Q} + \mat{A}^\transposed \mat{P}_{k + 1} \mat{A} - \mat{A}^\transposed \mat{P}_{k + 1} \mat{B} \bigl( \mat{R} + \mat{B}^\transposed \mat{P}_{k + 1} \mat{B} \bigr)^{-1\,} \mat{B}^\transposed \mat{P}_{k + 1} \mat{A}
			\end{equation}
			which has to be calculated from \( k = N, N - 1, \dots, 1 \), starting with \( \mat{P}_N = \mat{S} \). With \(N \to \infty\), \(\mat{P}_k\) becomes \(k\)-independent and the Riccati equation becomes an implicit algebraic equation, the \emph{discrete time algebraic Riccati equation} (DARE):
			\begin{equation}
				\mat{P} = \mat{Q} + \mat{A}^\transposed \mat{P} \mat{A} - \mat{A}^\transposed \mat{P} \mat{B} \bigl( \mat{R} + \mat{B}^\transposed \mat{P} \mat{B} \bigr)^{-1\,} \mat{B}^\transposed \mat{P} \mat{A}.  \tag{DARE}
			\end{equation}
			Thus, also \(\mat{K}\) become time-invariant and the feedback control law becomes time-invariant, too. Note that the Riccati equation only converges to the above value if \( (\mat{A}, \mat{B}) \) is stabilizable and \( (\mat{Q}^{1/2}, \mat{A}) \) is detectable.

			For continuous dynamics and cost, the optimal input is given as \( \vec{u}^\ast(t) = \mat{K}(t) \vec{x}(t) \) with \( \mat{K}(t) = -\mat{R}^{-1} \mat{B}^\transposed \mat{P}(t)\) and the solution of the \emph{continuous time Riccati equation} (CARE):
			\begin{equation}
				-\dot{\mat{P}}(t) = \mat{P}(t) \mat{A} + \mat{A}^\transposed \mat{P}(t) - \mat{P}(t) \mat{B} \mat{R}^{-1\,} \mat{B}^\transposed \mat{P}(t) + \mat{Q}  \tag{CARE}\label{eqn:care}
			\end{equation}
			The optimal cost is again \( J_t^\ast\bigl(\vec{x}(t)\bigr) = \vec{x}^\transposed(t) \mat{P}(t) \vec{x}(t) / 2 \). For an infinite horizon, all of this becomes again time-invariant with \( \dot{\mat{P}} = \mat{O} \), turning \eqref{eqn:care} again into al algebraic equation.
		% end

		\subsection{Stability}
			\begin{theorem}[Stability of the Time-Discrete LQR with Infinite Horizon]
				Consider a time-discrete linear system with quadratic cost where \( (\mat{A}, \mat{B}) \) is stabilizable and \( (\mat{A}, \mat{Q}^{1/2}) \) is detectable. Then the solution
				\begin{align}
					\vec{u}_k^\ast &= \mat{K} \vec{x}_k \\
					\mat{K} &= -\bigl( \mat{B}^\transposed \mat{P} \mat{B} + \mat{R} \bigr)^{-1\,} \mat{B}^\transposed \mat{P} \mat{A} \\
					\mat{P} &= \mat{Q} + \mat{A}^\transposed \mat{P} \mat{A} - \mat{A}^\transposed \mat{P} \mat{B} \bigl( \mat{R} + \mat{B}^\transposed \mat{P} \mat{B} \bigr)^{-1\,} \mat{B}^\transposed \mat{P} \mat{A}
				\end{align}
				of the optimal control problem asymptotically stabilizes the system \( \vec{x}_{k + 1} = \mat{A} \vec{x}_k + \mat{B} \vec{u}_k \).
			\end{theorem}
			\begin{proof}[Proof Sketch]
				Showing asymptotic stability can be done by proofing that the infinite horizon cost \( J^\ast(\vec{x}_k) = \vec{x}_k^\transposed \mat{P} \vec{x}_k \) is a strict Lyapunov function of the (controlled) system.
			\end{proof}

			Note that for finite-horizon LQR, the optimal input is not necessarily stabilizing!
		% end
	% end

	\section{Constrained Static Optimization}
		\label{sec:staticOpt}

		So far, optimal control has been considered without any constraints. However, most real systems have constraints (e.g., a car shall stay on the road, forces are limited, \dots). This section focuses on optimization with constraints in a \emph{static} setting, i.e., where the system does not evolve (it is not a \emph{dynamic} system). The general form of such an optimization problem is
		\begin{align}
			\min_{\vec{u} \,\in\, \R^n}\; & F(\vec{u}) \\
			\text{s.t.}\qquad&
			\begin{aligned}[t]
				\vec{G}(\vec{u}) &= \vec{0} \\
				\vec{H}(\vec{u}) &\leq \vec{0}
			\end{aligned}
		\end{align}
		where \( F : \R^n \to \R \), \( \vec{G} : \R^n \to \R^m \), and \( \vec{H} : \R^n \to \R^p \) are the objective function, equality, and inequality constraints, respectively. The \(=\) and \(\leq\) in the constraints are element-wise (with a slight abuse of notation). A constraint is called \emph{active} if the solution \(\vec{u}^\ast\) causes the constraint to vanish (thus, equality constraints are always active and inequality constraints may be inactive).

		The optimization problem is \emph{feasible} if the feasible set \( \mathcal{U} = \{ \vec{u} : \vec{G}(\vec{u}) = \vec{0}, \vec{H}(\vec{u}) \leq \vec{0} \} \) is non-empty. With this set, the following notions of optimality can be defined:
		\begin{definition}[Local Optimality]
			A point \( \vec{u}^\ast \) is \emph{locally optimal} if \( F(\vec{u}) \geq F(\vec{u}^\ast) \) for all \( \vec{u} \in \mathcal{U} \) with \( \lVert \vec{u} - \vec{u}^\ast \rVert \leq R \) for some \( R > 0 \), i.e., for all points in a vicinity of \(\vec{u}^\ast\).
		\end{definition}
		\begin{definition}[Global Optimality]
			A point \( \vec{u}^\ast \) is \emph{globally optimal} if \( F(\vec{u}) \geq F(\vec{u}^\ast) \) for all \( \vec{u} \in \mathcal{U} \).
		\end{definition}
		In both cases, the optimal cost is \( p^\ast = F(\vec{u}^\ast) \). If the problem is infeasible, it is said to have optimal cost \( p^\ast = \infty \).

		\subsection{Convexity}
			It turns out that \emph{convex} optimization problems are especially easy: every local optimum is a global optimum. To discuss this further, first convexity and a few other nomenclature have to be defined:
			\begin{definition}[Convex Sets]
				A set \(\mathcal{U}\) is \emph{convex} iff \( \lambda \vec{u} + (1 - \lambda) \vec{v} \in \mathcal{U} \) holds for all \(\vec{u}, \vec{v} \in \mathcal{U}\) and \( \lambda \in [0, 1] \). That is, all points on a line between two arbitrary points lie in the set, too.
			\end{definition}
			\begin{definition}[Convex Functions]
				A function \( F : \mathcal{U} \to \R \) is \emph{convex} iff \( F\bigl( \lambda \vec{u} + (1 - \lambda) \vec{v} \bigr) \leq \lambda F(\vec{u}) + (1 - \lambda) F(\vec{v}) \) holds for all \( \vec{u}, \vec{v} \in \mathcal{U} \) and \( \lambda \in [0, 1] \). A function is \emph{strictly} convex iff the relaxed inequality is never equal.
			\end{definition}

			For convex functions, it is possible to use the derivatives of the function (if it is differentiable) to check its convexity:
			\begin{theorem}[First Order Condition for Convexity]
				A differentiable function \( F : \mathcal{U} \to \R \) is \emph{convex} iff \( F(\vec{v}) \geq F(\vec{u}) + \grad{F}^\transposed(\vec{u}) (\vec{v} - \vec{u}) \) holds for all \( \vec{u}, \vec{v} \in \mathcal{U} \) where \( \grad{F} \) is the gradient of \(F\).
			\end{theorem}
			\begin{theorem}[Second Order Condition for Convexity]
				A twice differentiable function \( F : \mathcal{U} \to \R \) is \emph{convex} iff \( \grad^2 F(\vec{u}) \succeq 0 \) holds for all \( \vec{u} \in \mathcal{U} \) where \( \grad^2 F \) is the Hessian of \(F\).
			\end{theorem}

			Additionally, the further discussion needs the notion of \emph{(sub-)level sets:}
			\begin{definition}[Level Sets]
				The \emph{level set} \(L_c\) of a function \( F : \mathcal{U} \to \R \) is the set of values for which \(F\) takes on a constant value \(c\), i.e., \( L_c \coloneqq \{ \vec{u} : F(\vec{u}) = c \} \).
			\end{definition}
			\begin{definition}[Sublevel Sets]
				The \emph{sublevel set} \(L_c^-\) of a function \( F : \mathcal{U} \to \R \) is the set of values for which \(F\) takes on a value equal or less than a constant \(c\), i.e., \( L_c^- \coloneqq \{ \vec{u} : F(\vec{u}) \leq c \} \). Note that if \(F\) is convex or concave, \(L_c^-\) is convex.
			\end{definition}
			\begin{theorem}[Convexity of Level Sets]
				Let \( F : \mathcal{U} \to \R \) be a convex function. Then the level set \(L_c\) of \(F\) for some constant \(c\) is convex.
			\end{theorem}
			\begin{proof}
				Let \(L_c^-\) and \(L_{-c}^-\) be the sublevel sets of \(F\) and \(-F\), respectively. As \(F\) is convex, \(-F\) is concave and hence both \(L_c^-\) and \(L_{-c}^-\) are convex. By definition, \( L_c^- \cap L_c^+ = \{ \vec{u} : F(\vec{u}) \leq c \} \cap \{ \vec{u} : -F(\vec{u}) \leq -c \} = \{ \vec{u} : F(\vec{u}) \leq c \} \cap \{ \vec{u} : F(\vec{u}) \geq c \} = \{ \vec{u} : F(\vec{u}) = c \} = L_c \) is the level set of \(F\) for \(c\). Hence, as the intersection of convex sets is convex again, \(L_c\) is convex.
			\end{proof}

			\subsubsection{Convex Optimization Problems and Optimality}
				\begin{theorem}[Global Optimality of Convex Optimization Problems]
					For an optimization problem with a convex feasibility set \( \mathcal{U} \) and a convex objective function, a \emph{convex} optimization problem, every locally optimal solution is globally optimal.
				\end{theorem}
				\todo{Proof of global optimality of convex optimization problems.}

				To check convexity of an optimization problem, it usually suffices to look at the objective and constraints separately due to the properties of convex sets, namely that the intersection of two convex sets is still convex. If all constraints are convex, their respective (sub-) level sets are convex too, and hence the feasibility set is convex.
			% end
		% end

		\subsection{Quadratic Programming}
			If the objective function is a quadratic function \( J(\vec{u}) = \vec{u}^\transposed \mat{Q} \vec{u} + \vec{q}^\transposed \vec{u} + r \) with \( \mat{Q} \in \R^{n \times n} \), \( \mat{Q} \succeq 0 \), \( \vec{q} \in \R^n \), and \( r \in \R \) and the constraints are affine, the problem is called q \emph{quadratic program} (QP). For QPs, the optimal \(\vec{u}^\ast\) is either inside the feasible set or at its boundary. If the inequality constraints are quadratic and convex, too, the problem is called a \emph{quadratically constrained quadratic program} (QCQP) and the feasible set is an intersection of the ellipsoids defined by the constraints. Quadratic programs are a friendly class of problems for which many efficient solvers exist.
		% end

		\subsection{Optimality Conditions for Constrained Optimization Problems} % 4.22
			The optimality conditions for constrained optimization problems use the (generalized) Lagrangian:
			\begin{definition}[(Generalized) Lagrangian]
				For an optimization problem with objective \(F(\vec{u})\), equality constraints \( \vec{G}(\vec{u}) = \vec{0} \), and inequality constraints \( \vec{H}(\vec{u}) \leq \vec{0} \), the \emph{generalized Lagrangian} is
				\begin{equation}
					\mathcal{L}(\vec{u}, \vec{\lambda}, \vec{\mu}) = F(\vec{u}) + \vec{\lambda}^\transposed \vec{G}(\vec{u}) + \vec{\mu}^\transposed \vec{H}(\vec{u})
				\end{equation}
				with the \emph{Lagrange multipliers} \(\vec{\lambda}\) and \(\vec{\mu}\). If no inequality constraints are present, this is called the \emph{Lagrangian} and with inequality constraints it is the \emph{generalized} Lagrangian.
			\end{definition}
			The Lagrangian is a weighted sum of the objective and the constraints. It can now be used to characterize the optimality conditions for the optimization problem:
			\begin{theorem}[Necessary First Order Conditions (Karush-Kuhn-Tucker)]
				Let \(\vec{u}^\ast\) be a (local) extrema of \(F\) subject to \( \vec{G}(\vec{u}^\ast) = \vec{0} \) and \( \vec{H}(\vec{u}^\ast) \leq \vec{0} \), then there exist Lagrangian multipliers \( \vec{\lambda}^\ast \) and \( \vec{\mu}^\ast \) such that all of the following hold:
				\begin{equation}
					\begin{aligned}
						\grad_{\vec{u}} \mathcal{L}(\vec{u}^\ast, \vec{\lambda}^\ast, \vec{\mu}^\ast) &= \vec{0} \\
						\grad_{\vec{\lambda}} \mathcal{L}(\vec{u}^\ast, \vec{\lambda}^\ast, \vec{\mu}^\ast) &= \vec{0} \\
						\grad_{\vec{\mu}} \mathcal{L}(\vec{u}^\ast, \vec{\lambda}^\ast, \vec{\mu}^\ast) &\leq \vec{0} \\
						\bigl(\vec{\mu}^\ast\bigr)^\transposed \vec{H}(\vec{u}^\ast) &= \vec{0} \\
						\vec{\mu}^\ast &\geq \vec{0}
					\end{aligned}  \tag{KKT}\label{eqn:kkt}
				\end{equation}
				For problems with strong duality, these conditions are also \emph{sufficient}.
			\end{theorem}
			\begin{theorem}[Necessary Second Oder Conditions]
				Let \(\vec{u}^\ast\) be such that the \ref{eqn:kkt}-conditions hold. If \(\vec{u}^\ast\) is a minimum, the following holds for all \(\vec{p}\) with \( \vec{p}^\transposed \grad_{\vec{u}} \vec{G}(\vec{u}^\ast) = 0 \):
				\begin{equation}
					\vec{p}^\transposed \bigl( \grad_{\vec{u}}^2 \mathcal{L}(\vec{u}^\ast, \vec{\lambda}^\ast, \vec{\mu}^\ast) \bigr) \vec{p} \geq 0
				\end{equation}
				If this inequality is strict, this condition is also \emph{sufficient.}
			\end{theorem}
		% end

		\subsection{Numerical Solvers}
			A variety of numerical solvers exist for constrained optimization problems, and especially fast solvers exist for QPs (e.g., Matlab's \texttt{quadprog} and \texttt{fmincon}). Some approaches are interior point, trust region, and active set, and the class of sequential quadratic programming (SQP) methods\footnote{See "Optimization of Static and Dynamic Systems" (\url{https://fabian.damken.net/summaries/cs/elective/ce/opt/}) for a more thorough study of numerical optimization techniques}.

			SQP methods use a Taylor approximation of the objective (quadratic) and constraints (linear) to solve the optimization problem. By applying this method multiple times, it usually converges to a good local minimum. Another method for dealing with constraints is to reformulate them and integrate them into the objective by using barrier functions and penalty terms: penalty methods add a term \(\epsilon(\vec{u})\) to the objective that has a high values once a constraint is violated, forcing the optimizer to look elsewhere. Common choices for such a barrier term are logarithmic barrier functions and exact penalty functions. An alternative method is to allow the constraints to be violated. This is done by introducing a slack variable \( \vec{H}(\vec{u}) \leq \epsilon \) and penalize it in the objective with a function \( \Theta(\epsilon) \), e.g., a quadratic function in \(\epsilon\)

			\todo{Numerical Solvers; 6.23, 6.24, ..., 6.32}
		% end
	% end
% end

\chapter{Nominal Model Predictive Control}
	\label{c:mpcNominal}

	So far, optimal control without constraints (LQR) and constrained optimization without dynamics (KKT) has been covered. However, in MPC, an optimal control problem should be solved while taking constraints into account. This chapter deals with the simplest case of MPC, \emph{nominal} MPC. In nominal MPC, it is assumed that the model at hand is perfect, i.e., that the real system evolves exactly as the model predicts. Subsequently, after introducing nominal MPN and receding horizon control, certain properties regarding stability as discussed.

	The general MPC problem has the form
	\begin{align}
		\min_{\tilde{\vec{u}}}\; & \sum_{k = 0}^{\infty} F(\vec{x}_k, \vec{u}_k) \\
		\text{s.t.}\quad&
			\begin{aligned}[t]
				\vec{x}_{k + 1} &= \vec{f}(\vec{x}_k, \vec{u}_k),\, \vec{x}_0 \\
				\vec{x}_k &\in \mathcal{X} \\
				\vec{u}_k &\in \mathcal{U}
			\end{aligned}
	\end{align}
	with stage cost \(F\), the state dynamics \(\vec{f}\) with initial state \(\vec{x}_0\), and state and input constraints \(\mathcal{X}\) and \(\mathcal{U}\), respectively. With LQR-conformant cost, the optimal controller would be stabilizable in general. However, solving this problem with optimal control is impossible due to the constraints. Also, due to the infinite time horizon, it is not possible to apply batch normalization as the number of variables are infinite. The underlying idea of MPC now is to approximate the infinite horizon problem by a finite horizon problem
	\begin{equation}
		\begin{aligned}
			\min_{\tilde{\vec{u}}}\, & E(\vec{x}_n) + \sum_{k = 0}^{N - 1} F(\vec{x}_k, \vec{u}_k) \\
			\text{s.t.}\quad&
				\begin{aligned}[t]
					\vec{x}_{k + 1} &= \vec{f}(\vec{x}_k, \vec{u}_k),\, \vec{x}_0 \\
					\vec{x}_k &\in \mathcal{X} \\
					\vec{u}_k &\in \mathcal{U} \\
					\vec{x}_N &\in \mathcal{E}
				\end{aligned}
		\end{aligned}
		\label{eqn:recedingHorizon}
	\end{equation}
	with a terminal cost \(E\) and terminal constraints \(\mathcal{E}\). These can be interpreted as follows: the terminal cost approximates the "remainder" of the cost behind the horizon and the terminal constraints approximate the "remainder" of the constraints. However, even this finite-horizon MPC cannot be solved using dynamic programming due to the constraints; but it is possible to apply batch optimization! By repeatedly solving the batch optimization problem, feedback is incorporated by re-computing the batch solution at every time step and applying only a single input. That is, the feedback is given by repeatedly measuring the state and incorporating it as the initial value of the dynamical systems. This method is also called \emph{receding horizon control}.

	Then the problem of MPC boils down to two pressing questions:
	\begin{itemize}
		\item How to guarantee that the optimal control problem is feasible in every time step?
		\item How to guarantee that the optimal solution of the finite horizon approximate actually stabilizes the system?
	\end{itemize}
	The remainder of this chapter mostly deals with these two questions in \emph{nominal} settings. As already said, this means that the system model is a perfect representation and the plant will behave exactly as predicted. Note that this is not true in reality (due to modeling errors, noise, etc.) but is a purely theoretical assumption\footnote{Although it turns out that even nominal MPC is (to some extend) robust towards modeling errors.}. Incorporating uncertainties into the model and control is covered in \autoref{c:mpcRobust} and \ref{c:mpcStochastic}.

	\section{Linear MPC}
		\emph{Linear} MPC assumes a linear system model and a quadratic stage cost, leaving the following optimization problem (at time instance \(i\)):
		\begin{equation}
			\begin{aligned}
				\min_{\tilde{\vec{u}}}\; & \vec{x}_{i + N}^\transposed \mat{S} \vec{x}_{i + N} + \sum_{k = i}^{i + N - 1} \vec{x}_k^\transposed \mat{Q} \vec{x}_k + \vec{u}_k^\transposed \mat{R} \vec{u}_k \\
				\text{s.t.}\quad &
					\begin{aligned}[t]
						\vec{x}_{k + 1} &= \mat{A} \vec{x}_k + \mat{B} \vec{u}_k,\, \vec{x}_i \\
						\vec{x}_k &\in \mathcal{X} \\
						\vec{u}_k &\in \mathcal{U} \\
						\vec{x}_{i + N} &\in \mathcal{E}
					\end{aligned}
			\end{aligned}
			\label{eqn:linearRecedingHorizon}
		\end{equation}
		where \(\mathcal{X}\), \(\mathcal{U}\), and \(\mathcal{E}\) are convex sets. As the dynamics, constraints, and cost are time-invariant, the notation can be simplified by assuming \(i = 0\); cf. \eqref{eqn:recedingHorizon} with the initial state \(\vec{x}_0\) being equal to the \(i\)-th measured state.

		\subsection{Feasibility and Stability}
			Recall from \autoref{sec:staticOpt} that a static optimization problem is feasible iff the feasible set is non-empty. In dynamic optimization, an optimal control problem is feasible iff the constraints on the states and controls, \(\mathcal{X}\) and \(\mathcal{U}\), the terminal constraints \(\mathcal{E}\), and the dynamics are satisfied. Hence, checking feasibility is more tedious as it has to be checked whether there exists an input sequence satisfying all constraints (a \emph{admissible} input sequence).

			\begin{definition}[Admissible Input Sequence]
				An input sequence \( U(\vec{x}_0) \coloneqq (\vec{u}_0, \vec{u}_1, \dots, \vec{u}_{N - 1}) \) for an initial state \(\vec{x}_0\) is \emph{admissible} iff all constraints are satisfied:
				\begin{align}
					\forall k = 0, 1, \dots, N - 1 : \vec{u}_k &\in \mathcal{U} &
					\forall k = 0, 1, \dots, N - 2 : \vec{f}(\vec{x}_k, \vec{u}_k) &\in \mathcal{X} &
					\vec{f}(\vec{x}_{N - 1}, \vec{u}_{N - 1}) &\in \mathcal{E}
				\end{align}
			\end{definition}
			\begin{definition}[Feasible Initial Conditions]
				Let \( \mathcal{X}_0 \coloneqq \bigl\{ \vec{x}_0 : \text{exists an admissible }U(\vec{x}_0) \bigr\} \) be the \emph{set of feasible initial conditions}.
			\end{definition}

			With these two definitions, proofing feasibility of linear MPC means to show that \(\mathcal{X}_0\) is non-empty for all consecutive time steps, i.e., the states need to stay in \(\mathcal{X}_0\) for all time (also called \emph{recursive} feasibility). However, even for nominal MPC, this can (in general) not be guaranteed due to the finite-horizon problem being "shortsighted": the controller might drive the system towards the boundary of the constraints in the long run which it cannot account for as the horizon is too short. Once MPC "sees" the problem, it may be impossible to countersteer due to input bounds.

			While recursive feasibility is impossible to achieve for general linear MPC, it is achievable for some (important) special cases using the terminal constraints and cost. These are studied in the upcoming sections.

			\subsubsection{Terminal Equality}
				By fixing the terminal state to zero, i.e., \( \mathcal{E} = \{ \vec{0} \} \), the terminal cost vanishes. Then recursive feasibility and stability can be shown using the terminal set and Lyapunov functions, respectively. However, requiring that \( \vec{x}_N = \vec{0} \) is restrictive and makes the set of feasible initial conditions (for the first control problem) small. This is due to the requirement that the origin is reachable in \(N\) steps. While this can be tackled by increasing \(N\), this increases the computational effort as the number of optimization variables increases. Also, it just pushes the problem away a bit.

				\begin{theorem}[Recursively Feasibility of Linear MPC With Terminal Equality Constraint]
					Linear MPC with the receding horizon optimal control problem \eqref{eqn:linearRecedingHorizon} is recursively feasible if the initial problem is feasible and \( \mathcal{E} = \{ \vec{0} \} \).
				\end{theorem}
				\begin{proof}
					Let the linear MPC be feasible for the first instantiation with \(i = 0\) and the initial condition \(\vec{x}_0\). Let \( (\vec{u}_0^\ast, \vec{u}_1^\ast, \dots, \vec{u}_{N - 1}^\ast) \) be the optimal admissible input sequence. The corresponding state sequence end, by the terminal constraints, with \( \vec{x}_N^\ast = \vec{0} \). As the MPC is nominal, the system evolves to \( \vec{x}_1 = \vec{x}_1^\ast = \mat{A} \vec{x}_0 + \mat{B} \vec{u}_0^\ast \). For \(i = 1\), \(\vec{x}_1^\ast\) is used as the input sequence. Therefore, the previous input sequence \( (\vec{u}_1^\ast, \dots, \vec{u}_{N - 1}^\ast) \) is still admissible and optimal, but one entry too short. This can be fixed by appending \( \vec{u}_N^\ast = \vec{0} \), yielding \( \vec{x}_{N + 1}^\ast = \mat{A} \vec{x}_N^\ast + \mat{B} \vec{u}_N^\ast = \mat{A} \vec{0} + \mat{B} \vec{0} = \vec{0} \). This next terminal state still fulfills the state constraints. Hence, by repeating this procedure, linear MPC with zero terminal state is recursively feasible.
				\end{proof}

				\begin{theorem}[Asymptotic Stability of Linear MPC With Terminal Equality Constraint]
					Linear MPC with the receding horizon optimal control problem \eqref{eqn:linearRecedingHorizon} is asymptotically stable if the initial problem is feasible and \( \mathcal{E} = \{ \vec{0} \} \).
				\end{theorem}
				\begin{proof}
					\todo{Proof of stability of linear MPC; 5.25, 5.26, 5.27}
				\end{proof}
			% end

			\subsubsection{Terminal Inequality}
				To fix the problems introduced by a terminal equality constraint (namely that ht feasible set of initial conditions is small), this section extends the terminal set to have more than a single value. For discusses which properties the terminal set has to fulfill, a few definitions are needed beforehand:
				\begin{definition}[Invariant Set]
					A set \(\mathcal{S}\) is \emph{positively invariant} for a system \( \vec{x}_{k + 1} = \vec{f}(\vec{x}_k) \) if \( \vec{x}_k \in \mathcal{S} \) holds for all \( k \in \N_+ \) given that \( \vec{x}_0 \in \mathcal{S} \). In other words: trajectories starting in an invariant set, stay in the invariant set. The unique set that contains all positively invariant sets is called the \emph{maximum positively invariant set}.
				\end{definition}
				\begin{definition}[Control Invariant Set]
					A set \(\mathcal{S}\) is \emph{control invariant} for a system \( \vec{x}_{k + 1} = \vec{f}(\vec{x}_k, \vec{u}_k) \) if for all \( \vec{x}_k \in \mathcal{S} \) there exists an input \( \vec{u}_k \in \mathcal{U} \) such that \( \vec{f}(\vec{x}_k, \vec{u}_k) \in \mathcal{S} \) holds. In other words: every state in a control invariant set can be steered such that the successor is still in the set. The set containing all other control invariant sets is called the \emph{maximum control invariant set} and is denoted \(\mathcal{S}_\infty\). Let \( \vec{u}_k = \kappa_{\vec{f}}^{\mathcal{S}}(\vec{x}_k) \) be the control law ensuring control invariance.
				\end{definition}

				Using these definitions, it can be shown that linear MPC with a control invariant terminal set is recursively feasible and stabilizing. However, finding the terminal constraint set is rather hard and there is no general "recipe" to do it. For linear systems, one approach is to use unconstrained, infinite-horizon LQR, and finding the maximum invariant set under the closed-loop control law.
				\todo{5.37: What's a pre-set?}

				\begin{theorem}[Recursively Feasibility of Linear MPC With Terminal Inequality Constraint]
					Linear MPC with the receding horizon optimal control problem \eqref{eqn:linearRecedingHorizon} is recursively feasible (i.e., \(\mathcal{X}_0\) is positively invariant) if the initial problem \(\mathcal{E}\) is control invariant.
				\end{theorem}
				\begin{proof}
					\todo{Proof of recursive feasibility of linear MPC; 5.33, 5.34}
				\end{proof}

				\begin{theorem}[Asymptotic Stability of Linear MPC With Terminal Inequality Constraint]
					Linear MPC with the receding horizon optimal control problem \eqref{eqn:linearRecedingHorizon} is asymptotically stable if the initial problem \(\mathcal{E}\) is control invariant.
				\end{theorem}
				\begin{proof}
					\todo{Proof of stability of linear MPC; 5.32, 5.35, 5.36}
				\end{proof}
			% end
		% end
	% end

	\section{Solving MPC Problem}
		Until now, (nominal) MPC was studied from a rather theoretical perspective without actually solving it. This sections deals with two methods for reformulating a (linear) MPC problem in a quadratic program that can be solved efficiently. For nonlinear MPC, the prevalent approach is to approximate the problem locally to get quadratic optimization problems. Reformulating the optimal control problem as a QP can be done in two versions: with and without substituting the dynamics.

		\subsection{Option A: Substituting the Dynamics}
			This first idea is directly borrowed from batch optimization in LQR (\autoref{subsec:batchOpt}). To incorporate the linear constraints
			\begin{align}
				\mathcal{U} &= \{ \vec{u} : \mat{A}_u \vec{u} \leq \vec{b}_u \} &
				\mathcal{X} &= \{ \vec{x} : \mat{A}_x \vec{x} \leq \vec{b}_x \} &
				\mathcal{E} &= \{ \vec{x} : \mat{A}_f \vec{x} \leq \vec{b}_f \}
			\end{align}
			they are reformulated into a single constraint \( \tilde{\mat{H}} \vec{u} \leq \vec{w} + \mat{E} \vec{x}_0 \). In a first step, the constraints
			\begin{align}
				\mat{A}_u \vec{u}_k &\leq \vec{b}_u &
				\mat{A}_x \vec{x}_k &\leq \vec{b}_x &
				\mat{A}_f \vec{x}_N &\leq \vec{b}_f
			\end{align}
			for \( k = 0, 1, \dots, N - 1 \) are batched using the matrices and vectors
			\begin{align}
				\tilde{\mat{A}}_u &= \diag(\underbrace{\mat{A}_u, \mat{A}_u, \dots, \mat{A}_u}_{N\text{ times}}) &
				\tilde{\vec{b}}_u &= \bigl(\underbrace{\vec{b}_u^\transposed, \vec{b}_u^\transposed, \dots, \vec{b}_u^\transposed}_{N\text{ times}}\bigr)^\transposed \\
				\tilde{\mat{A}}_x &= \diag(\underbrace{\mat{A}_x, \mat{A}_x, \dots, \mat{A}_x}_{N\text{ times}}, \mat{A}_f) &
				\tilde{\vec{b}}_x &= \bigl(\underbrace{\vec{b}_x^\transposed, \vec{b}_x^\transposed, \dots, \vec{b}_x^\transposed}_{N\text{ times}}, \vec{b}_f^\transposed\bigr)^\transposed
			\end{align}
			yielding \( \tilde{\mat{A}}_u \tilde{\vec{u}} \leq \tilde{\vec{b}}_u \) and \( \tilde{\mat{A}}_x \tilde{\vec{x}} \leq \tilde{\vec{b}}_x \) with the usual abuse of notation. Note that the control constraints are already in the wanted form (with \( \vec{w} \triangleq \tilde{\vec{b}}_u \)). Reformulating the state constraint is simply by plugging in the (batched) state dynamics:
			\begin{equation}
				\tilde{\mat{A}}_x \tilde{\vec{x}} \leq \tilde{\vec{b}}_x
				\qquad\iff\qquad
				\tilde{\mat{A}}_x (\tilde{\mat{A}} \vec{x}_0 + \tilde{\mat{B}} \tilde{\vec{u}}) \leq \tilde{\vec{b}}_x
				\qquad\iff\qquad
				\tilde{\mat{A}}_x \tilde{\mat{B}} \tilde{\vec{u}} \leq \tilde{\vec{b}}_x - \tilde{\mat{A}}_x \tilde{\mat{A}} \vec{x}_0.
			\end{equation}
			Plugging it all together, the constraints are compactly represented as \( \tilde{\mat{H}} \vec{u} \leq \vec{w} + \mat{E} \vec{x}_0 \) with
			\begin{align}
				\tilde{\mat{H}} &= \begin{bmatrix} \tilde{\mat{A}}_u \\ \tilde{\mat{A}}_x \end{bmatrix} &
				\vec{w} &= \begin{bmatrix} \tilde{\vec{b}}_u \\ \tilde{\vec{b}}_x \end{bmatrix} &
				\mat{E} &= \begin{bmatrix} \mat{O} \\ \tilde{\mat{A}}_x \tilde{\mat{A}} \end{bmatrix}
			\end{align}
			where \(\mat{O}\) are zeros such that \(\mat{E}\) has the appropriate size. After all of this, the QP to solve is as simple as
			\begin{equation}
				\begin{aligned}
					\min_{\tilde{\vec{u}}}\; & \tilde{\vec{u}}^\transposed \mat{H} \tilde{\vec{u}} + 2 \vec{x}_0^\transposed \mat{F} \vec{u} + \vec{x}_0^\transposed \mat{G} \vec{x}_0 \\
					\text{s.t.}\quad &
						\begin{aligned}
							\tilde{\mat{H}} \tilde{\vec{u}} &\leq \vec{w} + \mat{E} \vec{x}_0
						\end{aligned}
				\end{aligned}
				\label{eqn:qpA}
			\end{equation}
			with \( \mat{H} = \tilde{\mat{B}}^\transposed \tilde{\mat{Q}} \tilde{\mat{B}} + \tilde{\mat{R}} \), \( \mat{F} = \tilde{\mat{A}}^\transposed \tilde{\mat{Q}} \tilde{\mat{B}} \), and \( \mat{G} = \tilde{\mat{A}}^\transposed \tilde{\mat{Q}} \tilde{\mat{A}} \).
		% end

		\subsection{Option B: Adding the Dynamics as Constraints}
			The second approach is to extend the optimization variables by the states,
			\begin{equation}
				\vec{z} \coloneqq \begin{bmatrix} \vec{x}_1^\transposed & \vec{x}_2^\transposed & \cdots & \vec{x}_N^\transposed & \vec{u}_0^\transposed & \vec{u}_1^\transposed & \vec{u}_{N - 1}^\transposed \end{bmatrix}^\transposed
			\end{equation}
			and adding the dynamics as an equality constraint. With
			\begin{align}
				\mat{H}_\mathrm{eq} &=
					\begin{bmatrix}
						\mat{I}  &          &         &          &         & -\mat{B} &          &          &        &          \\
						-\mat{A} & \mat{I}  &         &          &         &          & -\mat{B} &          &        &          \\
						         & -\mat{A} & \mat{I} &          &         &          &          & -\mat{B} &        &          \\
						         &          & \ddots  & \ddots   &         &          &          &          & \ddots &          \\
						         &          &         & -\mat{A} & \mat{I} &          &          &          &        & -\mat{B}
					\end{bmatrix} &
				\mat{E}_\mathrm{eq} &=
					\begin{bmatrix}
						\mat{A} \\
						\mat{O} \\
						\vdots \\
						\mat{O}
					\end{bmatrix}
			\end{align}
			the dynamics can be expressed as \( \mat{H}_\mathrm{eq} \vec{z} = \mat{E}_\mathrm{eq} \vec{x}_0 \) in term of the extended optimization variables \(\vec{z}\). Similarly, the state, terminal, and control constraints can be expressed as \( \mat{H}_\mathrm{in} \vec{z} \leq \vec{w} + \mat{E}_\mathrm{in} \vec{x}_0 \) with
			\begin{align}
				\mat{H}_\mathrm{in} &=
					\begin{bmatrix}
						\mat{O}           & \mat{O}           \\
						\tilde{\mat{A}}_x & \mat{O}           \\
						\mat{O}           & \tilde{\mat{A}}_u
					\end{bmatrix} &
				\vec{w}_\mathrm{in} &=
					\begin{bmatrix}
						\tilde{\vec{b}}_x \\
						\tilde{\vec{b}}_u
					\end{bmatrix} &
				\mat{E}_\mathrm{in} &=
					\begin{bmatrix}
						-\mat{A}_x \\
						\mat{O}    \\
						\vdots     \\
						\mat{O}
					\end{bmatrix}
			\end{align}
			and \(\tilde{\mat{A}}_x\) and \(\tilde{\mat{A}}_u\) defined as in option A. Finally, with \( \bar{\mat{H}} = \diag(\tilde{\mat{Q}}, \tilde{\mat{R}}) \), the QP is as follows:
			\begin{equation}
				\begin{aligned}
					\min_{\vec{z}}\; & \vec{z}^\transposed \bar{\mat{H}} \vec{z} \\
					\text{s.t.}\quad &
						\begin{aligned}[t]
							\mat{H}_\mathrm{eq} \vec{z} &= \mat{E}_\mathrm{eq} \vec{x}_0 \\
							\mat{H}_\mathrm{in} \vec{z} &\leq \mat{E}_\mathrm{in} \vec{x}_0
						\end{aligned}
				\end{aligned}
				\label{eqn:qpB}
			\end{equation}
			A comparison of the two methods is given in the next section.
		% end

		\subsection{Comparison of A and B}
			Both presented versions, \eqref{eqn:qpA} and \eqref{eqn:qpB}, can be efficiently solved by QP solvers. The main difference is that option A has \(Nm\) variables (with \(m\) input dimensionality) and option B has \(N (n + m)\) variables (with \(n\) being the state dimensionality). While it seems that option A must be more efficient, option B has sparse cost and constraint functions while option A is dense. Note also that option B has the advantage that it can easily be applied to nonlinear MPC.
		% end
	% end
% end

\chapter{Robust Model Predictive Control}
	\label{c:mpcRobust}

	So far, in nominal MPC, it was assumed that the predictive model is perfect. However, this is never the case in practice due to invalid modeling assumptions, incorrect parameter, noise, and unmodeled disturbances. When these errors are "small enough", nominal MPC still often works in practice: is is inherently robust by repeatedly measuring the states. Hence, in practice often simply the nominal method is used even though stability cannot be proven. If it does not, it is necessary to incorporate uncertainty into the system's model and MPC solution. Abstractly, this deviation can be incorporated by adding uncertainties \(\vec{w}_k\) to the model, i.e., \( \vec{x}_{k + 1} = \vec{f}(\vec{x}_k, \vec{u}_k, \vec{w}_k) \). The two common types of uncertain models are parametric uncertainties (here shown for the linear case)
	\begin{equation}
		\vec{f}(\vec{x}_k, \vec{u}_k, \vec{w}_k) = \mat{A}_k(\vec{w}_k) \vec{x}_k + \mat{B}_k(\vec{w}_k) \vec{u}_k
	\end{equation}
	and additive uncertainty
	\begin{equation}
		\vec{f}(\vec{x}_k, \vec{u}_k, \vec{w}_k) = \vec{f}(\vec{x}_k, \vec{u}_k) + \vec{w}_k.  \label{eqn:additiveUncertainty}
	\end{equation}
	In both cases, the uncertainty can be time-dependent (as in the above equations) or time-independent (by dropping the \(\cdot_k\)). This section deals with the controller design for additive time-dependent uncertainties \eqref{eqn:additiveUncertainty}. The goal for the control law is to satisfy the state and control constraints \(\mathcal{X}\) and \(\mathcal{U}\) for all uncertainties (at least with a given probability), to optimize the objective (e.g., expected or worst-case cost), and to stabilize the closed-loop system.

	For additive uncertainty, two types are covered in this summary: bounded and stochastic uncertainty (left and right, respectively):
	\begin{align}
		\vec{w}_k &\in \mathcal{W} &
		\vec{w}_k &\sim \mathcal{N}(\vec{\mu}, \mat{\Sigma})
	\end{align}
	In the former case, the noise lies within a bounded set \(\mathcal{W}\) and it is possible to consider worst case scenarios as the disturbances have a maximum/minimum. The state predictions are then sets of states. For stochastic uncertainties, this is not possible as the Gaussian distribution is unbounded (even though very small/high deviations from the mean have low probability). Hence, predictions are distributions over values. This chapter covers bounded uncertainty as \emph{robust} MPC and \autoref{c:mpcStochastic} covers stochastic uncertainty as \emph{stochastic} MPC.

	For a linear time-independent system with time-dependent additive noise, the dynamics equation is
	\begin{equation}
		\vec{x}_{k + 1} = \mat{A} \vec{x}_k + \mat{B} \vec{x}_k + \vec{w}_k  \label{eqn:discreteAdditiveUncertainty}
	\end{equation}
	with the solution
	\begin{equation}
		\vec{x}_k
			= \underbrace{\mat{A}^{k\,} \vec{x}_0 + \sum_{j = 0}^{k - 1} \mat{A}^{k - j - 1\,} \mat{B} \vec{u}_j}_{\vec{z}_k \,\coloneqq} + \sum_{j = 0}^{k - 1} \mat{A}^{k - j - 1\,} \vec{w}_j
			= \vec{z}_k + \sum_{j = 0}^{k - 1} \mat{A}^{k - j - 1\,} \vec{w}_j.
		\label{eqn:discreteAdditiveUncertaintyClosedForm}
	\end{equation}
	\begin{proof}
		Showing \eqref{eqn:discreteAdditiveUncertaintyClosedForm} is done by induction. The base case, \( k = 0 \), holds trivially. Similarly, \( k = 1 \) holds:
		\begin{equation}
			\vec{x}_1
				= \mat{A}^{1\,} \vec{x}_0 + \sum_{j = 0}^{0} \mat{A}^{-j\,} \mat{B} \vec{u}_j + \sum_{j = 0}^{0} \mat{A}^{-j\,} \vec{w}_j
				= \mat{A} \vec{x}_0 + \mat{A}^{0\,} \mat{B} \vec{u}_j + \mat{A}^{0\,} \vec{w}_j
				= \mat{A} \vec{x}_0 + \mat{B} \vec{u}_j + \vec{w}_j
		\end{equation}
		For the induction set, assume that \eqref{eqn:discreteAdditiveUncertaintyClosedForm} holds for some \(k\). It then follows that it also holds for \(k + 1\):
		\begin{align}
			\vec{x}_{k + 1}
				&= \mat{A} \vec{x}_k + \mat{B} \vec{u}_k + \vec{w}_k \\
				&= \mat{A} \biggl( \mat{A}^{k\,} \vec{x}_0 + \sum_{j = 0}^{k - 1} \mat{A}^{k - j - 1\,} \mat{B} \vec{u}_j + \sum_{j = 0}^{k - 1} \mat{A}^{k - j - 1\,} \vec{w}_j \biggr) + \mat{B} \vec{u}_k + \vec{w}_k \\
				&= \mat{A}^{k + 1\,} \vec{x}_0 + \sum_{j = 0}^{k - 1} \mat{A}^{(k + 1) - j - 1\,} \mat{B} \vec{u}_j + \sum_{j = 0}^{k - 1} \mat{A}^{(k + 1) - j - 1\,} \vec{w}_j + \mat{B} \vec{u}_k + \vec{w}_k \\
				&= \mat{A}^{k + 1\,} \vec{x}_0 + \sum_{j = 0}^{k - 1} \mat{A}^{(k + 1) - j - 1\,} \mat{B} \vec{u}_j + \sum_{j = 0}^{k - 1} \mat{A}^{(k + 1) - j - 1\,} \vec{w}_j + \mat{A}^{(k + 1) - k - 1} \vec{u}_k + \mat{A}^{(k + 1) - k - 1} \vec{w}_k \\
				&= \mat{A}^{k + 1\,} \vec{x}_0 + \sum_{j = 0}^{k} \mat{A}^{(k + 1) - j - 1\,} \mat{B} \vec{u}_j + \sum_{j = 0}^{k} \mat{A}^{(k + 1) - j - 1\,} \vec{w}_j \\
		\end{align}
		Hence, \eqref{eqn:discreteAdditiveUncertaintyClosedForm} is the solution of \eqref{eqn:discreteAdditiveUncertainty}.
	\end{proof}

	With the solution \eqref{eqn:discreteAdditiveUncertaintyClosedForm} the question of how to compute the cost function remains as the uncertainties are unknown. Let \( \tilde{J}(\vec{x}_k, \tilde{\vec{u}}, \tilde{\vec{w}}) \) be the \emph{uncertain} cost-to-go (where \( \tilde{\vec{w}} \) contains all future uncertainties similar), there are three fundamental methods to form a certain cost-to-go:
	\begin{align}
		J(\vec{x}^i, \tilde{\vec{u}}) &= \tilde{J}(\vec{x}^i, \tilde{\vec{u}}, \vec{0}) &
		J(\vec{x}^i, \tilde{\vec{u}}) &= \max_{\tilde{\vec{w}} \,\in\, \mathcal{W}}\, \tilde{J}(\vec{x}^i, \tilde{\vec{u}}, \tilde{\vec{w}}) &
		J(\vec{x}^i, \tilde{\vec{u}}) &= \E_{\tilde{\vec{w}}}\bigl[ \tilde{J}(\vec{x}^i, \tilde{\vec{u}}, \tilde{\vec{w}}) \bigr]
		\label{eqn:certainCostFunctions}
	\end{align}
	From left to right, these are called \emph{nominal}, \emph{expected}, and \emph{worst case} cost. Note that the worst case and expected cost are only applicable in bounded and stochastic settings, respectively.

	\section{Minimax MPC}
		In \emph{minimax} MPC, the worst-case cost function \eqref{eqn:certainCostFunctions} is simply plugged into the general formulation, yielding the following nested optimization problem:
		\begin{equation}
			\begin{aligned}
				\min_{\tilde{\vec{u}}} \max_{\tilde{\vec{w}}}\; & \tilde{J}(\vec{x}^i, \tilde{\vec{u}}, \tilde{\vec{w}}) \\
				\text{s.t.}\quad &
					\begin{aligned}[t]
						\vec{x}_{k + 1} &= \mat{A} \vec{x}_k + \mat{B} \vec{u}_k + \vec{w}_k,\, \vec{x}_0 = \vec{x}^i \\
						\vec{x}_k &\in \mathcal{X} \\
						\vec{u}_k &\in \mathcal{U} \\
						\vec{x}_N &\in \mathcal{E} \\
						\vec{w}_k &\in \mathcal{W}
					\end{aligned}
			\end{aligned}
		\end{equation}
		While this formulation is straightforward and easy to implement, it is extremely hard to solve due to the nested optimizations. Hence, it is slow and hardly applicable in real-time and thus has very few practical applications. However, current research on approximations on bilevel optimization might enhance applicability in the future.
	% end

	\section{Robust Open-Loop MPC}
		\label{sec:robustOpenLoopMpc}

		In robust MPC, the nominal cost function is used (opposed to minimax MPC), and the uncertainties are incorporated by tightening the constraints. This is based on the idea that if the nominal systems stays withing the tightened constraints, the uncertain system will satisfy the regular constraints. To discuss this further, notions of set additions and subtractions are needed:
		\begin{definition}[Minkowski/Set Sum]
			Let \( A, B \subseteq \R^n \). The \emph{Minkowski sum} is then
			\begin{equation}
				A \oplus B \coloneqq \{ \vec{a} + \vec{b} \forwhich \vec{a} \in A, \vec{b} \in B \}.
			\end{equation}
			With a slight abuse of notation, let \( \vec{v} \oplus A \equiv \{ \vec{v} \} \oplus A \) for a \( \vec{v} \in \R^n \). Note that set addition is symmetric.
		\end{definition}
		\begin{definition}[Pontryagin/Set Difference]
			Let \( A, B \subseteq \R^n \). The \emph{Pontryagin difference} is then
			\begin{equation}
				A \ominus B \coloneqq \{ a \in A \forwhich \forall b \in B : a + b \in A \}.
			\end{equation}
			Note that set subtraction is \emph{not} symmetric.
		\end{definition}
		These definitions of set summation and subtractions can now be used to describe the system's predictions. With the uncertain system evolution \( \vec{x}_k = \vec{z}_k + \sum_{j = 0}^{k - 1} \mat{A}^{k - j - 1\,} \vec{w}_j \) the state constraints are tightened as
		\begin{equation}
			\vec{x}_k \in \vec{z}_k \oplus \bigl( \mathcal{W} \oplus \mat{A} \mathcal{W} \oplus \mat{A}^{2\,} \mathcal{W} \oplus \cdots \oplus \mat{A}^{k - 1\,} \mathcal{W} \bigr) \subseteq \mathcal{X}.
		\end{equation}
		Ensuring that this constraint is satisfied can be done by enforcing the following constraint on the nominal states:
		\begin{equation}
			\vec{z}_k \in \mathcal{X} \ominus \bigl( \mathcal{W} \oplus \mat{A} \mathcal{W} \oplus \mat{A}^{2\,} \mathcal{W} \oplus \cdots \oplus \mat{A}^{k - 1\,} \mathcal{W} \bigr).
		\end{equation}
		Applying the same tightening on the terminal constraints yields the following optimization problem:
		\begin{equation}
			\begin{aligned}
				\min_{\tilde{\vec{u}}}\; & E(\vec{z}_N) + \sum_{k = 0}^{N - 1} F(\vec{z}_k, \vec{u}_k) \\
				\text{s.t.}\quad&
					\begin{aligned}[t]
						\vec{z}_{k + 1} &= \mat{A} \vec{z}_k + \mat{B} \vec{u}_k,\, \vec{z}_0 = \vec{x}^i \\
						\vec{z}_k &\in \mathcal{X} \ominus \bigl( \mathcal{W} \oplus \mat{A} \mathcal{W} \oplus \mat{A}^{2\,} \mathcal{W} \oplus \cdots \oplus \mat{A}^{k - 1\,} \mathcal{W} \bigr) \\
						\vec{z}_N &\in \mathcal{E} \ominus \bigl( \mathcal{W} \oplus \mat{A} \mathcal{W} \oplus \mat{A}^{2\,} \mathcal{W} \oplus \cdots \oplus \mat{A}^{N - 1\,} \mathcal{W} \bigr) \\
						\vec{u}_k &\in \mathcal{U}
					\end{aligned}
			\end{aligned}
		\end{equation}
		This problem is numerically tractable, incorporates the constraints, and does not have nested optimizations. Going even further, the tightened constraints can be calculated offline before the optimization! To study recursive feasibility, again some definitions have to be imposed:
		\begin{definition}[Robust Invariant Set]
			A set \(S\) is \emph{robustly positively invariant} for a system \( \vec{x}_{k + 1} = \vec{f}(\vec{x}_k, \vec{w}_k) \) if \( \vec{f}(\vec{x}, \vec{w}) \in \mathcal{S} \) holds for all \( \vec{x} \in \mathcal{S} \) and \( \vec{w} \in \mathcal{W} \).
		\end{definition}
		By choose the terminal set to be robustly positively invariant, feasibility can be shown: the computed trajectory is feasible for any disturbances, and hence it is feasible for the actual observation. However, the primary problem of robust open-loop is that the feasible initial set becomes very small or even empty as the uncertainty prediction becomes very wide. Hence, the "tube" of feasible values becomes small.
	% end

	\section{Tube MPC}
		\label{sec:tubeMpc}

		\emph{Tube MPC} tackles the primary problem of robust open-loop MPC, namely the wide uncertainty prediction, by using additional feedback controlling the size of the tube. The controller is then composed of two parts, i.e., \( \vec{u}^i = \vec{v}^i + \vec{\mu}^i \), where the first part steers the nominal system to the origin and the second parts steers the uncertain trajectory to the nominal one. The first part is given by the nominal MPC law and the second part is chosen to have a linear feedback gain, \( \vec{\mu}^i = \mat{K} (\vec{x}^i - \vec{z}^i) \), as linear systems are considered here. To design a tube MPC controller, the following basic steps have to be done:
		\begin{enumerate}
			\item compute the error \( \vec{e} \) between the nominal and uncertain states with linear feedback
			\item calculate the set \(\mathcal{R}\) the error remains in
			\item setup a nominal MPC with tightened constraints using \(\mathcal{E}\)
			\item apply the combined control law
		\end{enumerate}
		To compute the error, its dynamics have to be considered. Let the error be \( \vec{e}_k \coloneqq \vec{x}_k - \vec{z}_k \). Then its dynamics are given through the state dynamics (with, as defined above, \( \vec{u}_k = \vec{v}_k + \vec{\mu}_k \) and \( \vec{\mu}_k = \mat{K} \vec{e}_k \)):
		\begin{align}
			\vec{e}_{k + 1}
				&= \vec{x}_{k + 1} - \vec{z}_{k + 1}
				 = \mat{A} \vec{x}_k + \mat{B} \vec{u}_k + \vec{w}_k - \mat{A} \vec{z}_k - \mat{B} \vec{v}_k
				 = \mat{A} \vec{e}_k + \mat{B} \vec{u}_k + \vec{w}_k - \mat{B} \vec{v}_k \\
				&= \mat{A} \vec{e}_k + \mat{B} (\vec{v}_k + \vec{\mu}_k) + \vec{w}_k - \mat{B} \vec{v}_k
				 = \mat{A} \vec{e}_k + \mat{B} \vec{\mu}_k + \vec{w}_k
				 = \mat{A} \vec{e}_k + \mat{B} \mat{K} \vec{e}_k + \vec{w}_k
				 = (\mat{A} + \mat{B} \mat{K}) \vec{e}_k + \vec{w}_k.
		\end{align}
		Now \(\mat{K}\) is chosen such that \( (\mat{A} + \mat{B} \mat{K}) \) is stable, i.e., that the error does not grow indefinitely. Since \( (\mat{A} + \mat{B} \mat{K}) \) is stable and the noise is bounded, the set \( \mat{R}_k \) with \( \vec{e}_k \in \mat{R}_k \) can be computed as
		\begin{equation}
			\mathcal{R}_k
				= \mat{W} \oplus (\mat{A} + \mat{B} \mat{K}) \mathcal{W} \oplus (\mat{A} + \mat{B} \mat{K})^{2\,} \mat{W} \oplus \cdots \oplus (\mat{A} + \mat{B} \mat{K})^{k - 1\,} \mat{W}
				= \bigoplus_{j = 0}^{k - 1}\, (\mat{A} + \mat{B} \mat{K})^{j\,} \mathcal{W}
		\end{equation}
		assuming that \( \vec{e}_0 = \vec{0} \). Instead of using the time-variant set \(\mathcal{R}_k\), it is also possible to calculate the outer hull of all \(\mathcal{R}_k\), the \emph{minimum} robust invariant set, via
		\begin{equation}
			\mathcal{R} = \bigoplus_{j = 0}^{\infty}\, (\mat{A} + \mat{B} \mat{K})^{j\,} \mathcal{W}.
		\end{equation}
		Note that sometimes this set converges after a finite amount of steps; if it does not, a large number instead of infinity can be used to approximate \(\mathcal{R}\).

		With all these ingredients, the optimal control problem
		\begin{equation}
			\begin{aligned}
				\min_{\tilde{\vec{v}}}\; & E(\vec{z}_N) + \sum_{k = 0}^{N - 1} F(\vec{z}_k, \vec{v}_k) \\
				\text{s.t.}\quad&
					\begin{aligned}[t]
						\vec{z}_{k + 1} &= \mat{A} \vec{z}_k + \mat{B} \vec{v}_k,\, \vec{z}_0 = \vec{x}^i \\
						\vec{z}_k &\in \mathcal{X} \ominus \mathcal{R} \\
						\vec{z}_N &\in \mathcal{E} \\
						\vec{u}_k &\in \mathcal{U} \ominus \mat{K} \mathcal{R}
					\end{aligned}
			\end{aligned}
		\end{equation}
		is solved in every instance \(i\), and subsequently the control input \( \vec{u}^i = \vec{v}_0^\ast + \mat{K} (\vec{x}^i - \vec{z}^i) \) is applied to the system. Note that the gain \(\mat{K}\) and all constraint sets can be computed beforehand as they are independent of \(\vec{x}^i\) and \(i\) itself. Additionally, the terminal cost and constraints have to be adjusted to obtain stability which is not covered here.

		By adding the additional feedback controller, the uncertainty tubes are kept small and therefore the feasible sets are bigger.
	% end
% end

\chapter{Stochastic Model Predictive Control}
	\label{c:mpcStochastic}

	So far, bounded noise \( \vec{w} \in \mathcal{W} \) has been considered. In this chapter, the noise is additive and distributed with some Gaussian \( \mathcal{N}(\vec{\mu}, \mat{\Sigma}) \). The Gaussian is a reasonable choice for the noise usually justified with the distribution being somewhat natural with heights/weights/etc. and sensor/measurement noise being normally distributed. Also, it is often the result of a sum of many random variables even if these are not Gaussian distributed (central limit theorem). However, the usual reason for assuming Gaussians is that they are easy to work with: the complete distribution is described by the mean and (co-) variance and linear transformations of Gaussians result in Gaussians again. For this chapter, assume that while the noise \(\vec{w}_k\) is time-dependent, the distribution is not, i.e., mean and covariance are constant.

	If the noise has zero mean and all entries are independent (i.e., the covariance matrix is diagonal), the nominal and expected cost function are equivalent (for linear systems with a quadratic cost function). Hence, simply taking the expected value might not incorporate the constraints well and robust/tube MPC that was introduced before is not applicable due to the Gaussian being unbounded (it is impossible to compute the worst-case cost).

	\section{Chance Constraints}
		\emph{Chance constraints} reduce the stochastic MPC problem to a robust MPC problem by demanding constraint satisfaction only with a certain probability. The constraints can then be written as
		\begin{equation}
			p(\vec{x}_k \in \mathcal{X}) \leq p_x
		\end{equation}
		where \(p_x\) is a given desired probability \( p_x \in (0, 1) \) and the set \(\mathcal{X}\) is chosen such that the probability of a state lying in it is greater than \(p_x\). The constraint is therefore not valid for all \(\vec{w}\), but the most probable ones. For Gaussian noise, the chance constraints can be reformulated as deterministic constraints on the mean:
		\begin{equation}
			p(\vec{x}_k \in \mathcal{X}) \leq p_x
			\qquad\iff\qquad
			\E[\vec{x}] \in \tilde{\mathcal{X}}
		\end{equation}
		with \( \tilde{\mathcal{X}} = \mathcal{X} \ominus \mathcal{R} \) where \(\mathcal{R}\) is constructed based on the variance\footnote{For instance, with \( n = 1 \) and \( p_x = 0.95 \), the set is \( \mathcal{R} = [-2 \sigma_x, 2 \sigma_x] \) with \( w \sim \mathcal{N}(0, \sigma_x^2) \).}.
	% end

	\section{Stochastic Tube MPC}
		Similar to robust open-loop MPC (\autoref{sec:robustOpenLoopMpc}), stochastic MPC with chance constraints suffers from the uncertainty tubes getting very wide and the feasibility set shrinks. \emph{Stochastic tube MPC} is analogous to tube MPC (\autoref{sec:tubeMpc}) and used an additional linear feedback law for controlling the tube size. As before, the state tightening is chosen such that the constraints are satisfied with a given probability. The input constraints, on the other hand, are still deterministic and shall be fulfilled in a deterministic sense, i.e., without chance constraints.
	% end

	\section{Outlook}
		Stochastic MPC is a broad field and this chapter only touched the tip of the iceberg. For example: in a nonlinear system model, the states will in general not be normally distributed anymore even if the noise is Gaussian. Propagating uncertainties through such a nonlinear system is still open research. Some ideas are:
		\begin{itemize}
			\item approximating the non-Gaussian distribution by a Gaussian distribution (e.g., moment matching, Taylor series expansion, mean predictions, etc.)
			\item using Monte Carlo simulations
			\item polynomial chaos expansion
			\item and many more\dots
		\end{itemize}
		Also, stability and recursive feasibility are an open end, too: due to the probabilistic nature, also recursive feasibility may be lost. Discussing these properties requires extensions to probabilistic convergence.
	% end
% end

\chapter{Machine Learning in Model Predictive Control}
	\label{c:ml}

	The previous sections covered the first part of this document: model predictive control. In this section, machine learning and its applications in control theory are discussed\footnote{Note that this chapter is \emph{not} and introduction to machine learning. See one of "Statistical Machine Learning" (\url{https://fabian.damken.net/summaries/cs/elective/vc/statml/}) or "Data Mining and Machine Learning" (\url{https://fabian.damken.net/summaries/cs/elective/iws/dmml/}) for a more thorough treatment.}. An extremely related concept to MPC is \emph{reinforcement learning} where an agent autonomously learns to steer a system by maximizing a \emph{reward} (which is equivalent to minimizing a cost by flipping the sign). Compared to MPC, it leverages data instead of a given model although some approaches build an internal model and some approaches in MPC use data, too. Hence, the boundary is rather fluid. This chapter first discusses applications of ML in MPC and subsequently covers two regression models; Gaussian processes and (artificial) neural networks.

	\section{Machine Learning for MPC}
		In ML-supported MPC, one or more of the components (system model, desired reference, uncertainties/disturbances, constraints, cost function, or even the control inputs directly) as learned from data with the system model being component replaced most often.

		\label{10.3, 10.4}

		\subsection{Learning State Dynamics}
			Modeling the dynamics of a system using ML, two basic approaches exist: pure ML models using a black box and a combination of first principles and ML models (gray box):
			\begin{align}
				\vec{x}_{k + 1} &= \vec{f}_\mathrm{ml}(\vec{x}_k, \vec{u}_k; k) &
				\vec{x}_{k + 1} &= \vec{f}_\mathrm{fp}(\vec{x}_k, \vec{u}_k; k) + \vec{f}_\mathrm{ml}(\vec{x}_k, \vec{u}_k; k)
			\end{align}
			In the former, the accounts for the residual error (e.g., friction). A usual choice is to use a linear first-principle model and a nonlinear ML model. If the ML model only depends on \(k\), it models the noise. This can be useful to improve the quality of robust MPC by extrapolating the noise (which might be misleading and is often used in repetitive scenarios). For training these models, three common approaches are \emph{offline}, \emph{online}, and \emph{iterative} learning (see \autoref{fig:offlineOnlineIterative}) which are explained in more detail in the following sections.

			\begin{figure}
				\centering
				\begin{subfigure}[b]{0.3\linewidth}
					\centering
					\begin{tikzpicture}[every node/.style = { draw, rectangle, minimum width = 2cm, minimum height = 0.75cm }]
						\node (learning) {Learning};
						\node [below = 1 of learning] (control) {Control};
						\draw [->] (learning) to (control);
					\end{tikzpicture}
					\caption{Offline Learning}
				\end{subfigure}
				\hfill
				\begin{subfigure}[b]{0.3\linewidth}
					\centering
					\begin{tikzpicture}[every node/.style = { draw, rectangle, minimum width = 2cm, minimum height = 0.75cm }]
						\node (learning) {Learning};
						\node [below = 1 of learning] (control) {Control};
						\draw [->] (learning) to (control);
						\coordinate [left = 0.75 of control] (a);
						\coordinate [right = 0.75 of control] (b);  % For symmetry.
						\draw [->] (control) to (a) |- (learning);
						\draw [->, opacity = 0] (control) to (b) |- (learning);
					\end{tikzpicture}
					\caption{Offline Learning}
				\end{subfigure}
				\hfill
				\begin{subfigure}[b]{0.3\linewidth}
					\centering
					\begin{tikzpicture}[abc/.style = { draw, rectangle, minimum width = 2cm, minimum height = 0.75cm }]
						\node [abc] (control) {Control};
						\node [abc, below = 0.5 of control] (learning) {Learning};
						\draw [->] (control) to (learning);
						\node [left = 0.25 of control] {1st Iteration};
						\node [right = 0.25 of control] {\phantom{1st Iteration}};  % For symmetry.
						\node [abc, below = 0.5 of learning] (control) {Control};
						\draw [->] (learning) to (control);
						\node [abc, below = 0.5 of control] (learning) {Learning};
						\draw [->] (control) to (learning);
						\node [left = 0.25 of control] {2nd Iteration};
						\node [right = 0.25 of control] {\phantom{2nd Iteration}};  % For symmetry.
					\end{tikzpicture}
					\caption{Offline Learning}
				\end{subfigure}
				\caption{Different types of training a machine learning model combined with control.}
				\label{fig:offlineOnlineIterative}
			\end{figure}

			\subsubsection{Offline Learning}
				When training offline, i.e., before the control law is deployed, the training can be time-consuming as it does not have to be executed in real time. However, MPC has to deal with nonlinear dynamics as powerful ML models are usually nonlinear. Also, during training, it has be be made sure that the model is continuously differentiable, a proper MPC-ML-model combination is chosen (e.g., if the model is good, use nominal MPC; if it is bad, use robust or stochastic MPC), and real-time feasibility of the prediction is ensured.

				Overall, this is the most boring method of combining MPC and ML.
			% end

			\subsubsection{Online Learning}
				In online learning, the training data is collected while running the controller (cf. adaptive control and adaptive MPC) making it possible to adapt the model fast and easy when the situation changes. However, it is quite resource remanding as the training phase is done online. Also, the model accuracy is not known beforehand, making it hard to guarantee stability or robustness. One idea of guaranteeing stability or robustness is to have a fallback controller that is not learning-based which can take over if learning-based MPC fails. Another approach is to directly design a robust MPC that can handle changing models.

				\paragraph{Safe Sets}
					\emph{Safe sets} implement the first approach with an alternative "safe" controller: if the state is inside a \emph{safe set} \(\mathcal{S}\), learning-supported MPC is used. If it is not, a \emph{safe controller} \( \pi_\mathcal{S}(\vec{x}) \) that guarantees invariance of \(\mathcal{S}\) takes over. To find such a set, the sublevel set \( L_\alpha^- \) of a Lyapunov function \(V(\vec{x})\) of the closed-loop system \( \dot{\vec{x}} = \vec{f}\bigl( \vec{x}, \pi_\mathcal{S}(\vec{x}) \bigr) \) can be used.
				% end

				\paragraph{Learning-Supported Tube MPC}
					For robust learning-supported tube MPC, two models are used: one which is never updated, incorporates (additive) uncertainty, and is used to ensure constraint satisfaction and one which is updated online and is used for increase performance:
					\begin{equation}
						\begin{aligned}
							\min_{\tilde{\vec{v}}}\; & E(\vec{\zeta}_N) + \sum_{k = 0}^{N - 1} F(\vec{\zeta}_k, \vec{v}_k) \\
							\text{s.t.}\quad&
								\begin{aligned}[t]
									\vec{z}_{k + 1} &= \mat{A} \vec{z}_k + \mat{B} \vec{v}_k,\, \vec{z}_0 = \vec{x}^i \\
									\vec{\zeta}_{k + 1} &= \mat{A} \vec{\zeta}_k + \mat{B} \vec{v}_k + \vec{f}_\mathrm{ml}(\vec{\zeta}_k, \vec{v}_k),\, \vec{\zeta}_0 = \vec{x}^i \\
									\vec{z}_k &\in \mathcal{X} \ominus \mat{K} \mathcal{R} \\
									\vec{z}_N &\in \mathcal{E} \\
									\vec{v}_k &\in \mathcal{U} \ominus \mat{K} \mathcal{R}
								\end{aligned}
						\end{aligned}
					\end{equation}
				% end
			% end

			\subsubsection{Iterative Learning}
				In iterative learning, the same task is performed repeatedly and the model is updated between the batches by training with the data collected in the previous batch. This has the benefit of no real-time requirements and being able to evaluate the model before deploying it. Iterative learning can also be used to update different parts of the optimal control problem (e.g., reference and constraints).
			% end
		% end

		\subsection{Learning Constraints}
			Usually, the constraints are known (e.g., actuator limitations, safety regulations, desired operating range). But the constraints can be learned, too. For example, the tightening of the constraints can be learned by modeling the model uncertainties. Another special case is learning the terminal regions by incorporating previous trajectories.
		% end

		\subsection{Learning Cost Functions}
			Like constraints, the cost function is usually known as it is a pure design decision, does not reflect a physical property, and can be chosen freely. Hence, for simplicity, it is usually chosen to be a quadratic cost. In some special cases, it might be beneficial to learn the costs from data. Some methods are approximating the infinite horizon cost with reinforcement learning, updating the cost in between batches, and training a neural network to learn the cost from demonstrations.
		% end

		\subsection{Learning Control Input: Replacing MPC}
			When the control input is learned directly, this basically completely replaces MPC with ML. This faces the main drawback of MPC that it is time-consuming. One way of replacing MPC completely is by running MPC offline and training a ML model (e.g., a neural network) to directly map states to the optimal input. This model can then be deployed online allowing fast control.
		% end
	% end

	\section{Gaussian Processes}
		This section introduces \emph{Gaussian processes} (GPs), a ML model with great applicability and outstanding capabilities of uncertainty estimation\footnote{Note that the treatment of GPs is quite limited here. See Rasmussen and Williams: "Gaussian Processes for Machine Learning" for a very thorough treatment.}. A GP works with a dataset \( \mathcal{D} \coloneqq \{ (\vec{\xi}_i, \gamma_i)_{i = 1, 2, \dots, n} \} \) which contains features \(\vec{\xi}_i \in \R^d\) and corresponding labels \(\gamma_i \in \R\) generated from an unknown function \( \vec{f} : \R^d \to \R \) with Gaussian noise \( \epsilon \sim \mathcal{N}(0, \sigma_n^2) \), i.e., \( \gamma_i = \mathcal{N}\bigl(f(\vec{\xi}_i), \sigma_n^2\bigr) \). To simplify the further discussion, the data is organized into matrices and vectors:
		\begin{align}
			\mat{\Xi} &= \begin{bmatrix} \vec{\xi}_1^\transposed \\ \vec{\xi}_2^\transposed \\ \vdots \\ \vec{\xi}_k^\transposed \end{bmatrix} \in \R^{n \times d} &
			\mat{\Xi}_\ast &= \begin{bmatrix} \vec{\xi}_{\ast 1}^\transposed \\ \vec{\xi}_{\ast 2}^\transposed \\ \vdots \\ \vec{\xi}_{\ast k}^\transposed \end{bmatrix} \in \R^{k \times d}
		\end{align}
		The starred data represents the test data, i.e., the data unobserved during training.

		Formally, a GP is a stochastic process for which any finite subset of random variables is jointly Gaussian. This can be understand as a GP being a Gaussian distribution over functions. To represent the GP, a \emph{mean function} \( m : \R^d \to \R \) and a covariance function \( k : \R^d \times \R^d \to \R \). These functions encode the belief of how the prior evolves (usually, a zero-mean prior is chosen to encode no prior knowledge). The prior covariance function \(k\) then represents the influence of the data points on each other (usually such that close points influence each other more than points far away from each other). The data points \( f(\mat{\Xi}_\ast) \) are then distributed according to the GP, i.e.,
		\begin{equation}
			f(\mat{\Xi}_\ast) \sim \mathcal{N}\bigl( m(\mat{\Xi}_\ast), k(\mat{\Xi}_\ast, \mat{\Xi}_\ast) \bigr).
		\end{equation}
		Note that the notation \( m(\mat{\Xi}_\ast) \) is a slight abuse of notation and corresponds to invoking \(m\) for each \(\vec{\xi}_\ast\) in \(\mat{\Xi}_\ast\). The same goes for \( k(\cdot, \cdot) \), however, this returns a symmetric covariance values. The prior mean and covariance function usually depend on hyper-parameters \(\theta\) which can be estimated from data or incorporate actual prior knowledge.

		Some example of prior mean functions are constant, linear, or quadratic in \(\vec{\xi}\). Usually, a zero mean is used. Note that the prior mean can be an arbitrary function. The covariance function, however, has to fulfill some criteria summarized in the following definition:
		\begin{definition}[Covariance Function]
			A function \( k : \R^d \times \R^d \to \R : (\vec{\xi}, \vec{\xi}') \mapsto k(\vec{\xi}, \vec{\xi}') \) is a \emph{covariance function} (or \emph{positive definite kernel}) if for all \( \vec{\xi}_1, \vec{\xi}_2, \dots, \vec{\xi}_n \in \R^d \), the matrix \( (\mat{K})_{ij} = k(\vec{\xi}_i, \vec{\xi}_j) \) is positive semi-definite and symmetric (for all \(n \in \N_+\)).
		\end{definition}
		Only with these constraints the result of a covariance matrix can be used as a covariance matrix for a Gaussian distribution. Note that the kernel is written as a function of the inputs, but gives the covariance of the outputs. A covariance function that only depends on the difference \( \vec{\xi} - \vec{\xi}' \) is called \emph{stationary} and a covariance function that only depends on the magnitude of the difference \( \lvert \vec{\xi} - \vec{\xi}' \rvert \) is called \emph{isotropic} or \emph{radial}. The next section lists some commonly used covariance functions and some of their properties.

		\subsection{Covariance Functions}
			As training a GP well usually boils down to selecting the right covariance function, it is useful to know a few\dots

			\paragraph{Squared Exponential}
				One of the most used kernels is the \emph{squared exponential} (SE) kernel
				\begin{equation}
					k(\vec{\xi}, \vec{\xi}') = \sigma_f^2 \exp\biggl\{ -\frac{\lvert \vec{\xi} - \vec{\xi}' \rvert^2}{2 \ell^2} \biggr\}
				\end{equation}
				with \emph{signal variance} \( \sigma_f^2 \) and \emph{length scale} \( \ell^2 \). Due to its infinite differentiability, it yields very smooth functions. The signal variance just scales the covariance and therefore the vertical output scale without changing the mean and the length scale control smoothness, i.e., a short length scale exhibits rough functions.
			% end

			\paragraph{Squared Exponential with Automatic Relevance Determination}
				By assigning a separate length scale to each input,
				\begin{equation}
					k(\vec{\xi}, \vec{\xi}') = \sigma_f^2 \exp\biggl\{ -\frac{1}{2} (\vec{\xi} - \vec{\xi}')^\transposed \mat{\Lambda}^{-1} (\vec{\xi} - \vec{\xi}') \biggr\},
				\end{equation}
				with \( \mat{\Lambda} = \diag(\ell_1^2, \ell_2^2, \dots, \ell_d^2) \), learning the length scale affects the influence of the separate input dimensions on the output. That is, the inverse length scale \( \ell_i^{-2} \) can be understood as weighting factors of the corresponding dimensions.
			% end

			\paragraph{Periodic Kernel}
				The periodic kernel
				\begin{equation}
					k(\vec{\xi}, \vec{\xi}') = \sigma_f^2 \exp\biggl\{ -\frac{2 \sin^2\bigl( (\vec{\xi} - \vec{\xi}') / p \bigr)}{\ell^2} \biggr\}
				\end{equation}
				introduces periodicity and is thus useful for modeling periodic functions. Like the SE kernel, it yields very smooth functions.
			% end

			\paragraph{Linear Kernel}
				The linear kernel
				\begin{equation}
					k(\vec{\xi}, \vec{\xi}') = \sigma_b^2 + \sigma_f^2 \frac{(\vec{\xi} - c)^\transposed (\vec{\xi}' - c)}{\ell^2}
				\end{equation}
				yields affine functions with an offset \(c\) where \( \sigma_b^2 \) introduces a bias in \(c\).
			% end

			\paragraph{Many, Many More}
				There are much more kernels useful for different things, e.g., Matrn, dot product, rational quadratic, etc.
			% end
		% end

		\subsection{GP Prediction}
			To predict new values with a GP, the distribution is condition on the training data. That is, first a joint distribution
			\begin{equation}
				\begin{bmatrix}
					f(\mat{\Xi}) \\
					f(\mat{\Xi}_\ast)
				\end{bmatrix}
				\sim
				\mathcal{N}\Biggl(
					\begin{bmatrix}
						m(\mat{\Xi}) \\
						m(\mat{\Xi}_\ast)
					\end{bmatrix},\,
					\begin{bmatrix}
						k(\mat{\Xi}, \mat{\Xi}) & k(\mat{\Xi}, \mat{\Xi}_\ast) \\
						k(\mat{\Xi}_\ast, \mat{\Xi}) & k(\mat{\Xi}_\ast, \mat{\Xi}_\ast)
					\end{bmatrix}
				\Biggr)
			\end{equation}
			is formed and then conditioned on \( f(\mat{\Xi}) \) using basic properties of the Gaussian. The posterior is therefore given by \( f(\mat{\Xi}_\ast) \sim \mathcal{N}(\mu_\ast, \sigma_\ast^2) \) with
			\begin{align}
				\mu_\ast &= m(\mat{\Xi}_\ast) + k(\mat{\Xi}_\ast, \mat{\Xi}) k^{-1}(\mat{\Xi}, \mat{\Xi}) \bigl( f(\mat{\Xi}) - m(\mat{\Xi}) \bigr) \\
				\sigma_\ast^2 &= k(\mat{\Xi}_\ast, \mat{\Xi}_\ast) - k(\mat{\Xi}_\ast, \mat{\Xi}) k^{-1}(\mat{\Xi}, \mat{\Xi}) k(\mat{\Xi}, \mat{\Xi}_\ast)
			\end{align}
			For no observation noise, the predictions now perfectly match the training data, i.e., the conditioning points.
		% end

		\subsection{Hyper-Parameter Optimization}
			To find the optimal hyper-parameters, gradient ascent is performed on the log-likelihood:
			\begin{equation}
				\log p(\vec{y} \given \mat{\Xi}, \theta)
					= \underbrace{-\frac{1}{2} \bigl( \vec{y} - m(\mat{\Xi}) \bigr)^\transposed k^{-1}(\mat{\Xi}, \mat{\Xi}) \bigl( \vec{y} - m(\mat{\Xi}) \bigr)}_\text{Data Fit}
					  \underbrace{-\frac{1}{2} \log \lvert \mat{K} \rvert}_\text{Complexity Penalty}
					  \underbrace{-\frac{n}{2} \log(2\pi)}_\text{Normalization}
			\end{equation}
			This is based on the assumption that the posterior over the parameters is sharply peaked around the optimum and hence the posterior can be approximated by the exact value (called \emph{empirical Bayes}).
		% end

		\subsection{Advantages and Drawbacks}
			A major drawback of GPs is the computationally expensive matrix inversion in the posterior computation (which is cubic in the number of data points). This large resource demand limits the online applicability of GPs. But sometimes, the various advantages outweigh the drawbacks:
			\begin{itemize}
				\item posterior distribution provides uncertainty quantification (aleatoric and epistemic)
				\item any function can be learned by using the approximate kernels
				\item good results even with few data
				\item prior knowledge can be incorporated via the mean (e.g., a first-principle model)
				\item Gaussian processes are somewhat robust towards overfitting
				\item automatic relevance determination can eliminate input dimensions that are irrelevant
			\end{itemize}
		% end

		\subsection{Dynamic Process Models}
			So far, only GPs with a one-dimensional output have been considered. But in a state-space model, often multiple dimensions are needed. Often, the simple approach of training as many independent GPs as the problem has dimensions is used. This ignores the correlation between outputs, but reduces the computational effort that would be required for multi-dimensional GPs.

			In MPC, GPs are usually used to incorporate the uncertainty estimation into stochastic MPC to tighten the constraints. However, propagating the uncertainty more than one time step is challenging and an open research topic. Hence, usually only the mean is propagated (mean prediction) and the uncertainty estimation is taken from a single step only.
		% end
	% end

	\section{(Artificial) Neural Networks}
		Artificial neural networks are---in their pure form---a relatively simple yet powerful way to approximate any function\footnote{Like before, this is not a thorough study. See "Deep Learning: Architectures and Methods" (\url{https://fabian.damken.net/summaries/cs/elective/iws/dlam/}) for a deeper (haha) treatment.}. This chapter just lists some advantages and drawbacks and leaves the detailed discussion to the aforementioned document.

		Advantages:
		\begin{itemize}
			\item efficiently trainable using parallel compute units (e.g., graphics cards)
			\item once the network is trained, the training data can be discarded (cf. Gaussian processes)
			\item neural networks can learn complicated behavior that is hard to model using first principles
			\item in theory, by the universal function approximation theorem, every function can be approximated
		\end{itemize}
		Drawbacks:
		\begin{itemize}
			\item large networks are prone to overfit
			\item training of large networks is hard (local minima, vanishing gradient, exploding gradient, \dots)
			\item interpretation is hard; hence, neural networks have to be tested on unseen data
		\end{itemize}
	% end

	\section{Reinforcement Learning vs. MPC}
		In reinforcement learning, the past data is used to improve the future behavior of an agent. Compared to MPC, reinforcement learning usually works with discrete states and transition probabilities, i.e., incorporates noise in the system by inherently assuming that state transitions are probabilistic. Also, it usually does not assume a model (model-free reinforcement learning). A big problem, however, is to generate the data that can be used for trial-and-error as a robot shall not perform dangerous actions during exploration but needs to explore them to see that they are bad.

		Some years ago, the trend went towards replacing everything physical with machine learning. Nowadays, incorporating prior knowledge from physics and first principles into control and combining control theory with ML gets a lot more attention. In the future, both fields should be considered unified opposed to clearly separated.
	% end
% end

\lstset{language = Python}



\chapter{Introduction} % 1.27, 1.28, 1.30, 1.38, 1.39
	Most of the content in this summary, the ideas, the underlying structure and the image ideas are taken from the lecture \href{https://www.ias.informatik.tu-darmstadt.de/Teaching/MachineLearning1Lecture}{"Statistical Machine Learning"} by \href{https://www.ias.informatik.tu-darmstadt.de/Team/JanPeters}{Prof. Jan Peters}. It is really just a \emph{summary} of the contents of the lecture.

	\todo{Intro}

	\section{Examples} % 1.32, 1.33, 1.34, 1.35, 1.36, 1.37
		\todo{Intro: Examples}
	% end

	\section{Classification vs. Regression} % 1.40, 1.41
		\todo{Intro: Classification vs. Regression}
	% end

	\section{Paradigm} % 1.42, 1.43
		\todo{Intro: Paradigm}
	% end

	\section{Key Challenges} % 1.44, 1.48, 1.49, 1.50, 1.51, 1.52
		\todo{Intro: Challenges}

		\subsection{Generalization} % 1.45, 1.46
			\todo{Intro: Generalization}
		% end
	% end
% end

\chapter{Fundamentals: Linear Algebra}
	\section{Vectors}
		A \emph{vector} is an ordered list of numbers that can be interpreted as an \emph{arrow} in multidimensional spaces:
		\begin{equation}
			\vec{v} \in \R^n \quad\rightarrow\quad v =
				\begin{bmatrix}
					v_1 \\
					\vdots \\
					v_n
				\end{bmatrix}
		\end{equation}

		\paragraph{Scalar Multiplication}
			Multiplying a vector by a scalar is defined as multiplying each component by that scalar (let \( \vec{v} \in \R^n \), \( \lambda \in \R \)):
			\begin{equation}
				\lambda \vec{v} = \lambda
					\begin{bmatrix}
						v_1 \\
						\vdots \\
						v_n
					\end{bmatrix}
				=
					\begin{bmatrix}
						\lambda v_1 \\
						\vdots \\
						\lambda v_n
					\end{bmatrix}
			\end{equation}
			Per component:
			\begin{equation}
				(\lambda v)_i = \lambda v_i
			\end{equation}

			\info{Scalar multiplication is a linear operation.}
		% end

		\paragraph{Addition}
			Adding two vectors is defined by adding the component of both vectors (thus, both vectors must have the same size; let \( v, w \in \R^n \)):
			\begin{equation}
				\vec{v} + \vec{w} =
					\begin{bmatrix}
						v_1 + w_1 \\
						\vdots \\
						v_n + w_n
					\end{bmatrix}
			\end{equation}
			Per component:
			\begin{equation}
				(v + w)_i = v_i + w_i
			\end{equation}

			\info{Vector addition is both associative and commutative.}
		% end

		\paragraph{Vector Transpose}
			The \emph{transposed} version \( \vec{v}^T \) of a vector \( \vec{v} \in \R^n \) is a vector that was flipped around its main axis (thus, the new vector is a row/column vector if the initial vector as a column/row vector):
			\begin{align}
				\begin{bmatrix}
					v_1 \\
					\vdots \\
					v_n
				\end{bmatrix}^T &=
				\begin{bmatrix}
					v_1 & \cdots & v_n
				\end{bmatrix} \\
				\begin{bmatrix}
					v_1 & \cdots & v_n
				\end{bmatrix}^T &=
				\begin{bmatrix}
					v_1 \\
					\vdots \\
					v_n
				\end{bmatrix}
			\end{align}

			\info{Transposing a vector twice returns the initial vector \[ \vec{v} = \big(\vec{v}^T\big)^T \]}
		% end

		\paragraph{Linear Combination}
			A \emph{linear combination} of multiple vectors \( \vec{v}_1, \cdots, \vec{v}_n \in \R^m \) is the addition of the scaled versions of them (scaled by scalars \( \lambda_1, \cdots, \lambda_n \in \R \)):
			\begin{equation}
				u = \lambda_1 v_1 + \cdots + \lambda_n v_n
			\end{equation}
			If in a group of vectors \( \vec{v}_1, \cdots, \vec{v}_n \in \R^m \), no vector can be represented as a linear combination of the others, they are called \emph{linearly independent}.
		% end

		\paragraph{Inner and Outer Product and Length}
			The \emph{inner product} of two vectors \( \vec{v}, \vec{w} \in \R^n \) is the sum of the product of the components and is denoted by a single dot (\(\vec{v} \cdot \vec{w}\)):
			\begin{equation}
				\vec{v} \cdot \vec{w} = \vec{v}^T \vec{w} = (v_1 w_1) + \cdots + (v_n w_n)
			\end{equation}
			Therefore, the inner product gives a scalar value. In Cartesian coordinates, this is also called the \emph{scalar product}.

			The length \( \norm{\vec{v}} \) of a vector \( \vec{v} \in \R^n \) is given as the euclidean norm:
			\begin{equation*}
				\norm{\vec{v}} = \big(\vec{v} \cdot \vec{v}\big)^\frac{1}{2} = \sqrt{v_1^2 + \cdots + v_n^2}
			\end{equation*}

			The \emph{outer product} of two vectors \( \vec{v}, \vec{w} \in \R^n \) is defined analogue to the inner product, but with the transpose switched and is denoted by a cross in a circle (\( \vec{v} \otimes \vec{w} \)):
			\begin{equation}
				\vec{v} \otimes \vec{w} = \vec{v} \otimes \vec{w}^T
					\begin{bmatrix}
						v_1 \\
						\vdots \\
						v_n
					\end{bmatrix}
				\otimes
					\begin{bmatrix}
						w_1 & \cdots & w_n
					\end{bmatrix}
				=
					\begin{bmatrix}
						v_1 w_1 & \cdots & v_1 w_n \\
						\vdots  & \ddots & \vdots  \\
						v_n w_1 & \cdots & v_n w_n
					\end{bmatrix}
			\end{equation}
			This is equivalent to matrix multiplication with two vectors and thus produces a matrix \( \vec{v} \otimes \vec{w} \in \R^{n \times n} \).
		% end

		\paragraph{Angles between Vectors}
			The \emph{angle} \(\theta\) between two vectors \( \vec{v}, \vec{w} \in \R \) is given by:
			\begin{equation}
				\cos \theta = \frac{\vec{v} \cdot \vec{w}}{\norm{\vec{v}} \, \norm{\vec{w}}} \quad\iff\quad \theta = \arccos \frac{\vec{v} \cdot \vec{w}}{\norm{\vec{v}} \, \norm{\vec{w}}}
			\end{equation}
		% end

		\paragraph{Projections of Vectors}
			A \emph{projection} of a vector \( \vec{v} \in \R^n \) onto a vector \( \vec{w} \in \R^n \) results in a scalar value \( x \in \R \) which equals the length of the adjacent w.r.t. to the angle between both vectors given that \( \vec{v} \) is the hypotenuse and the third line is orthogonal to \( \vec{w} \), see figure~\ref{fig:vectorprojection} for an illustration. Then this length is given as:
			\begin{equation}
				x = \norm{\vec{v}} \cos \theta = \frac{\vec{v} \cdot \vec{w}}{\norm{\vec{w}}}
			\end{equation}

			\begin{figure}
				\centering
				\begin{tikzpicture}
					\coordinate (a) at (0, 0);
					\coordinate (b) at (10, 0);
					\coordinate (c) at (7, 3);
					\coordinate (d) at (7, 0);
					\coordinate [below = 0.7 of a] (A);
					\coordinate [below = 0.7 of d] (D);
					\coordinate [above = 0.2 of A] (A1);
					\coordinate [below = 0.2 of A] (A2);
					\coordinate [above = 0.2 of D] (D1);
					\coordinate [below = 0.2 of D] (D2);

					\draw [->] (a) -- node[below]{\( \vec{w} \)} (b);
					\draw [->] (a) -- node[above]{\( \vec{v} \)} (c);
					\draw [dashed] (c) -- (d);
					\draw (A) -- node[below]{\( x \)} (D);
					\draw (A1) -- (A2);
					\draw (D1) -- (D2);
				\end{tikzpicture}
				\caption{Illustration of Vector Projection}
				\label{fig:vectorprojection}
			\end{figure}
		% end
	% end

	\section{Matrices}
		A \emph{matrix} is an ordered group of numbers that are ordered in two dimensions:
		\begin{equation}
			A \in \R^{n \times m} \quad\rightarrow\quad A =
				\begin{bmatrix}
					a_{11} & \cdots & a_{1m} \\
					\vdots & \ddots & \vdots \\
					a_{n1} & \cdots & a_{nm}
				\end{bmatrix}
		\end{equation}

		\paragraph{Scalar Multiplication}
			Multiplying a matrix by a scalar is defined as multiplying each component by that scalar (let \( A \in \R^{n \times m} \), \( \lambda \in \R \)):
			\begin{equation}
				\lambda \, A = \lambda
					\begin{bmatrix}
						a_{11} & \cdots & a_{1m} \\
						\vdots & \ddots & \vdots \\
						a_{n1} & \cdots & a_{nm}
					\end{bmatrix}
				=
					\begin{bmatrix}
						\lambda a_{11} & \cdots & \lambda a_{1m} \\
						\vdots		 & \ddots & \vdots		 \\
						\lambda a_{n1} & \cdots & \lambda a_{nm}
					\end{bmatrix}
			\end{equation}
			Per component:
			\begin{equation}
				(\lambda \, A)_{ij} = \lambda a_{ij}
			\end{equation}

			\info{Scalar multiplication is a linear operation.}
		% end

		\paragraph{Addition}
			Adding two matrices is defined by adding the component of both matrices (thus, both matrices must have the same size; let \( A, B \in \R^{n \times m} \)):
			\begin{equation}
				A + B =
					\begin{bmatrix}
						a_{11} + b_{11} & \cdots & a_{1m} + b_{1m} \\
						\vdots & \ddots & \vdots \\
						a_{n1} + b_{n1} & \cdots & a_{nm} + b_{nm}
					\end{bmatrix}
			\end{equation}
			Per component:
			\begin{equation}
				(A + B)_{ij} = a_{ij} + b_{ij}
			\end{equation}

			\info{Matrix addition is both associative and commutative.}
		% end

		\paragraph{Transpose}
			The \emph{transposed} version \( A^T \) of a matrix \( A \in \R^{n \times m} \) is a matrix \( A^T \in \R^{m \times n} \) that was flipped around its main axis:
			\begin{align}
				\begin{bmatrix}
					a_{11} & \cdots & a_{1m} \\
					\vdots & \ddots & \vdots \\
					a_{n1} & \cdots & a_{nm}
				\end{bmatrix}^T
				=
				\begin{bmatrix}
					a_{11} & \cdots & a_{n1} \\
					\vdots & \ddots & \vdots \\
					a_{1m} & \cdots & a_{nm}
				\end{bmatrix}
			\end{align}

			\info{Transposing a matrix twice returns the initial matrix \[ A = \big(A^T\big)^T \]}
		% end

		\paragraph{Matrix Multiplication}
			The \emph{matrix multiplication} of two matrices is only possible of the number of columns of the first matrix equals the number of rows of the second matrix (i.e. \( A \in \R^{n \times m} \), \( B \in \R^{m \times o} \)). The resulting matrix then has the dimensions \( AB \in \R^{n \times o} \). Matrix multiplication is defined as follows:
			\begin{equation}
				AB =
					\begin{bmatrix}
						a_{11} & \cdots & a_{1m} \\
						\vdots & \ddots & \vdots \\
						a_{n1} & \cdots & a_{nm}
					\end{bmatrix}
				\cdot
					\begin{bmatrix}
						b_{11} & \cdots & b_{1o} \\
						\vdots & \ddots & \vdots \\
						b_{m1} & \cdots & b_{mo}
					\end{bmatrix}
				=
					\begin{bmatrix}
						a_{11} b_{11} + \cdots + a_{1m} b_{m1} & \cdots & a_{11} b_{1o} + \cdots + a_{1m} b_{mo} \\
						\vdots & \ddots & \vdots \\
						a_{n1} b_{11} + \cdots + a_{nm} b_{m1} & \cdots & a_{n1} b_{1o} + \cdots + a_{nm} b_{mo}
					\end{bmatrix}
			\end{equation}
			Per component:
			\begin{equation}
				(AB)_{ij} = \sum_{k = 1}^{n} A_{ik} B_{kj}
			\end{equation}

			\info{Matrix multiplication is only associative and distributive w.r.t. matrix addition.}
		% end

		\paragraph{Inverse}
			Let \( I_n \in \R^{n \times n} \) be the identity matrix with all ones on the main diagonal and the rest zeros. If the dimension is clear, the \(n\) can be left out.

			Using this definition, the \emph{inverse} of a matrix \( A \in \R^{n \times n} \) is defined as matrix \( A^{-1} \in \R^{n \times n} \) that holds the following equation:
			\begin{equation}
				AA^{-1} = A^{-1}A = I_n
			\end{equation}
			If such a matrix exists, the matrix \(A\) is called \emph{regular} or \emph{nonsingular}.
		% end

		\paragraph{Pseudoinverse}
			If a matrix \( A \in \R^{n \times m} \) is not squared (i.e. \( n \neq m \)), there exists no inverse matrix. Instead, \emph{Pseudoinverse Matrices} can be used to "invert" such a matrix. Left and right pseudoinverse are mutually exclusive, meaning that the one can only exist if the other does not (whilst neither have to exist).

			The \emph{left pseudoinverse} does only exist of the matrix has full column rank and is defined as:
			\begin{equation}
				A^{\#} = \big( A^T A \big)^{-1} A^T \quad\implies\quad A^{\#}A = I_m
			\end{equation}

			The \emph{right pseudoinverse} does only exist if the matrix has full row rank and is defined as:
			\begin{equation}
				A^{\#} = J^T \big(JJ^T\big)^{-1} \quad\implies\quad AA^{\#} = I_n
			\end{equation}
		% end

		\paragraph{Properties}
			\subparagraph{Symmetricity} A squared matrix \( A \in \R^{n \times n} \) is \emph{symmetric} iff \( A^T = A \). This implies that:
			\begin{itemize}
				\item The inverse matrix \(A^{-1}\) is also symmetric.
				\item \(A\) can be decomposed into \( A = QDQ^T \), where \(D\) is a diagonal matrix with all eigenvalues of \(A\) and \(Q\) is a matrix with all columns as the eigenvectors of \(A\).
			\end{itemize}

			\subparagraph{Definite Quadratic Form} Let \(A\in\R^{n \times n}\) be a squared matrix and let \( \sigma(A) \) be its spectral. Then the matrix \(A\) is
			\begin{equation}
				\begin{cases*}
					\textrm{positive definite} & \(\forall \lambda \in \sigma : \lambda > 0 \quad\iff\quad \vec{x}^TA\vec{x} > 0\) \\
					\textrm{negative definite} & \(\forall \lambda \in \sigma : \lambda < 0 \quad\iff\quad \vec{x}^TA\vec{x} < 0\) \\
					\textrm{positive semi-definite} & \(\forall \lambda \in \sigma : \lambda \geq 0 \quad\iff\quad \vec{x}^TA\vec{x} \geq 0\) \\
					\textrm{positive semi-definite} & \(\forall \lambda \in \sigma : \lambda \leq 0 \quad\iff\quad \vec{x}^TA\vec{x} \leq 0\) \\
					\textrm{indefinite} & else
				\end{cases*}
			\end{equation}
			for all vectors \(\vec{x}\in\R^n\).

			\subparagraph{Regularity/Nonsingularity}
				All of the following are equivalent w.r.t. a matrix \( A \in \R^{n \times n} \):
				\begin{itemize}
					\item The matrix is regular.
					\item The matrix is nonsingular (or not singular).
					\item There exists a matrix \( A^{-1} \in \R^{n \times n} \) with \( AA^{-1} = A^{-1}A = I_n \).
					\item The determinant of the matrix is nonzero: \( \det A \neq 0 \).
					\item The matrix has full row rank.
					\item The matrix has full column rank.
				\end{itemize}
			% end
		% end
	% end

	\section{Operations and Linear Transformations} % 2.23
		\todo{LA: Ops. and Linear Trans.}

		\paragraph{Change of Basis} % 2.24, 2.25
			\todo{LA: Change of Basis}
		% end

		\paragraph{Linear Transformations} % 2.26
			\todo{LA: Linear Transformations}
		% end
	% end

	\section{Eigenvalues and -vectors} % 2.27
		\todo{LA: Eigenvalues and -vectors}

		\paragraph{Basis} % 2.28, 2.29
			\todo{LA: Eigenbasis}
		% end

		\paragraph{Linear Transformations} % 2.30
			\todo{LA: Spectral Linear Transformations}
		% end
	% end
	
	\section{Wrap-Up}
		\begin{itemize}
			\item Vectors and matrices
			\item Operations on vectors and matrices
			\item Eigenvectors and -values
			\item Linear transformations
		\end{itemize}
	% end
% end

\chapter{Fundamentals: Statistics}
	For a much more detailed introduction into the basic concepts (e.g. random variables), please lookup the summary of "Math 3: Stochastic and Statistics" (see \href{https://www.dmken.com/cs}{https://www.dmken.com/cs}), but notice that it is in German.

	\section{Random Variables}
		A \emph{random variable} is a number that is determined by chance, draw according to a probability distribution.
	% end

	\section{Distributions}
		A probability distribution describes the probability that a random variable will equal a certain value (or lie in a certain range).

		\subsection{Uniform Distribution}
			All data/all values are equally likely within a bounded region \(R\) with size \(R\).
			\begin{equation}
				p(x) = \frac{1}{R}
			\end{equation}
			The distribution is plotted in figure~\ref{fig:uniformDistribution}.

			\begin{figure}
				\centering
				\begin{tikzpicture}
					\draw [->] (-5, 0) -- (5, 0) node[right]{\(x\)};
					\draw [->] (0, -0.5) -- (0, 3);
					\draw [density] (-4.5, 0) -- (-3, 0) -- (-3, 2) -- (3, 2) -- (3, 0) -- (4.5, 0);
				\end{tikzpicture}
				\caption{Uniform Distribution}
				\label{fig:uniformDistribution}
			\end{figure}
		% end

		\subsection{Discrete Distributions}
			The random variables take \emph{discrete values} (can be infinite, but countably infinity) and their probabilities sum up to \(1\):
			\begin{equation}
				\sum_i p(x_i) = 1
			\end{equation}
			A discrete distribution is described by a \emph{probability mass function} which is a normalized histogram.

			\subsubsection{Bernoulli Distribution}
				A \emph{Bernoulli random variable} only takes on two values, e.g. \(0\) or \(1\).

				\subparagraph{Parameters}
				\begin{description}
					\item[\(\mu\)] The probability that the variable equals \(1\).
				\end{description}

				\subparagraph{Properties}
				\begin{align}
					x                           & \in \{ 0, 1 \}        \\
					p(x = 1 \given \mu)         & = \mu                 \\
					\textrm{Bern}(x \given \mu) & = y^x (1 - \mu)^{1-x} \\
					\E(x)                       & = \mu                 \\
					\Var(x)                     & = \mu(1 - \mu)
				\end{align}
			% end

			\subsubsection{Binomial Distribution}
				\emph{Binomial variables} are a sequence of \(N\) Bernoulli variables.

				\subparagraph{Parameters}
				\begin{description}
					\item [\(\mu\)] The probability that one variable equals \(1\).
					\item [\(N\)] The number of trials/samples.
				\end{description}

				\subparagraph{Properties}
				\begin{align}
					\textrm{Bin}(m \given N, \mu) & = { N \choose m } \mu^m (1 - \mu)^{N - m} \\
					\E(m)                         & = N\mu                                \\
					\Var(m)                       & = N \mu (1 - \mu)
				\end{align}

				See figure~\ref{fig:binomialDistribution} for a visualization of \( \textrm{Bin}(m \given 10, 0.25) \).

				\begin{figure}
					\centering
					\begin{tikzpicture}[
								declare function={binom(\k,\n,\p)=\n!/(\k!*(\n-\k)!)*\p^\k*(1-\p)^(\n-\k);}
							]
						\begin{axis}[
									samples at = { 0, ..., 10 },
									ybar = 0pt,
									bar width = 1,
									xlabel = \(m\),
									ymin = 0
								]
							\addplot [fill = colorDensity] { binom(x, 10, 0.25) };
						\end{axis}
					\end{tikzpicture}
					\caption{Binomial Distribution \( \textrm{Bin}(m \given 10, 0.25) \)}
					\label{fig:binomialDistribution}
				\end{figure}
			% end

			\subsubsection{Multinoulli Distribution}
				\emph{Multinoulli variables} (also called \emph{categorical variables}) are a generalization of Bernoulli variables where each variable can have multiple (namely \(K\)) outputs. The random variables is a vector with one-hot-encoding.

				\subparagraph{Parameters}
				\begin{description}
					\item[\(\vec{\mu}\)] The entry \(\mu_i\) defines the probability that the entry \(x_i\) equals \(1\). \\ All entries must be \( \mu_i \geq 0 \) and \( \sum_{k = 1}^{K} \mu_k = 1 \).
					\item [\(K\)] The number of classes/outcomes.
				\end{description}

				\subparagraph{Properties}
				\begin{align}
					\vec{x}                     & = [ 0, 0, 1, 0, 0, 0 ]^T                                           \\
					p(x_i \given \vec{\mu})     & = \mu_i                                                            \\
					p(\vec{x} \given \vec{\mu}) & = \prod_{k = 1}^{K} \mu_k^{x_k}                                    \\
					\E(\vec{x} \given \mu)      & = \sum_{\vec{x}} p(\vec{x} \given \vec{\mu}) \vec{x} = \vec{\mu}^T
				\end{align}
			% end

			\subsubsection{Multinomial Distribution}
				\emph{Multinomial variables} are a sequence of \(N\) Multinoulli variables.

				\subparagraph{Parameters}
				\begin{description}
					\item [\(\vec{\mu}\)] The entry \(\mu_i\) defines the probability that, for one variable, the entry \(x_i\) equals \(1\). \\ All entries must be \( \mu_i \geq 0 \) and \( \sum_{k = 1}^{K} \mu_k = 1 \).
					\item [\(K\)] The number of classes/outcomes.
					\item [\(N\)] The number of trials/samples.
				\end{description}

				\subparagraph{Properties}
				\begin{align}
					\textrm{Mult}(m_1, m_2, \cdots, m_K \given \vec{\mu}, N) & = { N \choose m_1, m_2, \cdots, m_K } \prod_{k = 1}^{K} \mu_k^{m_k} \\
					\E(m_k)                                                  & = N\mu_k                                                            \\
					\Var(m_k)                                                & = N\mu_k(1 - \mu_k)                                                 \\
					\Cov(m_j, m_k)                                           & = -N\mu_j\mu_k
				\end{align}
			% end

			\subsubsection{Poisson Distribution}
				A \emph{Poisson distribution} is a binomial distribution where the number of trials goes to infinity \( N \to \infty \) and the success of each trial goes to zero \( \mu \to 0 \), s.t. \( N\mu = \lambda \) is constant.

				\subparagraph{Parameters}
				\begin{description}
					\item[\(\lambda\)] Defines the expectation value and the variance at once.
				\end{description}

				\subparagraph{Properties}
				\begin{align}
					p(m \given \lambda) & = \frac{\lambda^m}{m!} e^{-\lambda} \\
					\E(m)               & = \lambda                           \\
					\Var(m)             & = \lambda
				\end{align}

				See figure~\ref{fig:poissonDistribution} for a visualization of \( p(m \given 5) \).

				\begin{figure}
					\centering
					\begin{tikzpicture}[
								declare function={poisson(\n,\p)=(\p^\n)/\n!*e^(-\n);}
							]
						\begin{axis}[
									samples at = { -10, ..., 10 },
									ybar = 0pt,
									bar width = 1,
									xlabel = \(m\),
									ymin = 0
								]
							\addplot [fill = colorDensity] { poisson(x, 5) };
						\end{axis}
					\end{tikzpicture}
					\caption{Poisson Distribution \( p(m \given 5) \)}
					\label{fig:poissonDistribution}
				\end{figure}
			% end
		% end

		\subsection{Continuous Distributions}
			The random variables take \emph{discrete values} (infinite, can be uncountable) and their probability density function integrates to \(1\):
			\begin{equation}
				\int\limits_{-\infty}^{+\infty} p(x) \dd{x} = 1
			\end{equation}
			A continuous distribution is described by a \emph{probability density function} \(p(x)\).

			The probability that a random variable \( x \) falls into the interval \( (a, b) \) is
			\begin{equation}
				P(a < x < b) = \int\limits_a^b p(x) \dd{x}
			\end{equation}

			\subsubsection{Gaussian Distribution}
				\subparagraph{Parameters}
				\begin{description}
					\item[\(\mu\)] The expectation value.
					\item[\(\sigma^2\)] The variance.
				\end{description}

				\subparagraph{Properties}
				\begin{align}
					p(x) = \mathcal{N}\,(x \given \mu, \sigma^2) &= \frac{1}{\sqrt{2\pi\sigma^2}} \exp\Bigg\{ -\frac{1}{2\sigma^2} (x - \mu)^2 \Bigg\} \\
					\E(x) &= \mu \\
					\Var(x) &= \sigma^2
				\end{align}
				A Gaussian distribution has more really useful properties:
				\begin{itemize}
					\item A Gaussian has soft tails, i.e. they fade away smoothly.
					\item Gaussians are often good models for data and provide analytical solutions.
				\end{itemize}

				See figure~\ref{fig:gaussianDistribution} for a visualization of \( \mathcal{N}\,(x \given 0, 1) \) (the standard Gaussian distribution).

				\begin{figure}
					\centering
					\begin{tikzpicture}
						\begin{axis}[
							ymin = 0,
							xlabel = \(x\),
							ymax = 0.5
						]
						\addplot [density, smooth] { gaussian(x, 0, 1) };
						\end{axis}
					\end{tikzpicture}
					\caption{Standard Gaussian Distribution \( \mathcal{N}\,(x \given 0, 1) \)}
					\label{fig:gaussianDistribution}
				\end{figure}
			% end
		% end

		\subsection{Multivariate Gaussian Distribution}
			Gaussians can be applied to \(D\)-dimensional data \(x_1, x_2, \cdots\) using multivariate Gaussian distributions.

			\subparagraph{Parameters}
			\begin{description}
				\item[\(\vec{\mu}\)] A vector \( \mu \in \R^D \) of the expectation values for each dimension.
				\item[\(\Sigma\)] The \emph{covariance matrix} containing the variance for each dimension on its main axis and the covariances on the other spaces. It is symmetric and defined as:
			\end{description}
			\begin{equation}
				\Sigma =
					\begin{bmatrix}
						\Var(x_1)      & \Cov(x_1, x_2) & \cdots & \Cov(x_1, x_D) \\
						\Cov(x_2, x_1) & \Var(x_2)      & \cdots & \Cov(x_2, x_D) \\
						\vdots         & \vdots         & \ddots & \vdots         \\
						\Cov(x_D, x_1) & \Cov(x_D, x_2) & \cdots & \Var(x_D)
					\end{bmatrix}
			\end{equation}

			\subparagraph{Properties}
			\begin{align}
				p(\vec{x}) = \mathcal{N}\,(\vec{x} \given \vec{\mu}, \Sigma) & = \frac{1}{\sqrt{2\pi}^D \sqrt{\det \Sigma}} \exp\Bigg\{ -\frac{1}{2} (\vec{x} - \vec{\mu})^T \Sigma^{-1} (\vec{x} - \vec{\mu}) \Bigg\} \\
				\E(\vec{x})                                                & = \vec{\mu}                                                                                                                             \\
				\Var(x_i)                                                  & = \Sigma_{ii}
			\end{align}

			\subsubsection{Geometry} % 3.27
				\todo{Stats: Multivariate Gaussian: Geometry}
			% end

			\subsubsection{Moments} % 3.44, 3.45
				\todo{Stats: Multivariate Gaussian: Moments}
			% end
		% end

		\subsection{Partitioned Gaussian Distributions} % 3.32, 3.33, 3.34
			\todo{Stats: Partitioned Gaussian}
		% end
	% end

	\section{Central Limit Theorem}
		The distribution of the sum of \(N\) i.i.d. random variables becomes increasingly Gaussian as \(N\) increases. That is, with \(N \to \infty\), it converges towards a Gaussian.
	% end

	\section{Probability Rules}
		\subparagraph{Joint Distribution}
		\begin{equation}
			p(x, y)
		\end{equation}

		\subparagraph{Marginal Distribution}
		\begin{equation}
			p(y) = \int p(x, y) \dd{x}
		\end{equation}

		\subparagraph{Conditional Distribution}
		\begin{equation}
			p(y \given x) = \frac{p(x, y)}{p(x)}
		\end{equation}

		\subparagraph{Probabilistic/Stochastic Independence}
		\begin{equation}
			p(x, y) = p(x) p(y)
		\end{equation}

		\subparagraph{Chain Rule of Probabilities}
		\begin{align}
			p(x_1, \cdots, x_n) &= p(x_1 \given x_2, \cdots, x_n) p(x_2, \cdots, x_n) \\
				&= p(x_1 \given x_2, \cdots, x_n) p(x_2 \given x_3, \cdots, x_n) \,\cdots\, p(x_{n - 1} \given x_n) p(x_n)
		\end{align}

		\subsubsection{Bayes Rule}
			\begin{equation}
				p(y \given x) = \frac{p(x \given y) p(y)}{p(x)}
			\end{equation}

			\begin{itemize}
				\item Posterior: \tabto{4cm} \( p(y \given x) \)
				\item Likelihood: \tabto{4cm} \( p(x \given y) \)
				\item Prior: \tabto{4cm} \( p(y) \)
				\item Normalization Factor: \tabto{4cm} \( p(x) = \int p(x, y) \dd{y} = \int p(x \given y) p(y) \dd{y} \)
			\end{itemize}
		% end
	% end

	\section{Expectation, Variance and Moments}
		\subsection{Expectation}
			The \emph{expectation} value of a random variable \(x\) with a distribution \(p(x)\) is defined as:
			\begin{equation}
				\E_{x \sim p(x)}\big(f(x)\big) = \E_x\big(f(x)\big) = \E\big(f(x)\big) =
					\begin{cases}
						\sum_x f(x) \, p(x) & \textrm{for discrete distributions} \\
						\int_x f(x) \, p(x) \dd{x} & \textrm{for continious distributions}
					\end{cases}
			\end{equation}
			This gives a similar formula for the \emph{conditional expectation}:
			\begin{equation}
				\E_{x \sim p(x \given y)}\big(f(x)\big) = \E_x\big(f(x)\big) = \E\big(f(x)\big) =
					\begin{cases}
						\sum_x f(x) \, p(x \given y) & \textrm{for discrete distributions} \\
						\int_x f(x) \, p(x \given y) \dd{x} & \textrm{for continious distributions}
					\end{cases}
			\end{equation}

			With enough samples, the expectation value can be approximated using the arithmetic mean:
			\begin{equation}
				\E\big(f(x)\big) \approx \frac{1}{N} \sum_{i = 1}^{N} f(x_i)
			\end{equation}

			\paragraph{Calculation Rules}
				Let \(x\), \(y\) be random variables and \(\alpha \in \R\).
				\begin{align}
					\E(\alpha x) &= \alpha \E(x) \\
					\E(x + y) &= \E(x) + \E(y) \\
					\E(xy) &= \E(x) \E(y) \label{eqn:expXYId}
				\end{align}
				Equation \ref{eqn:expXYId} only holds if \(x\) and \(y\) are statistically independent.
			% end
		% end

		\subsection{Variance and Covariance}
			The \emph{variance} measures the spread of the variable in relation to its mean:
			\begin{equation}
				\Var(x) = \E\Big(\!\big( x - \E(x) \big)^2 \Big) = \E\big(x^2\big) - \big(\E(x)\big)^2
			\end{equation}

			The \emph{covariance} measures the correlation between two variables (how much the variables change together):
			\begin{align}
				\Cov(x, y) &= \E_{x,y}(xy) - \E_x(x) \E_y(y) \\
				\Cov(\vec{x}, \vec{y}) &= \E_{\vec{x},\vec{y}}(\vec{x}\vec{y}^T) - \E_{\vec{x}}(\vec{x}) \E_{\vec{y}}(\vec{y}^T)
			\end{align}
			This gives the following very important rule (with \(\vec{\mu}\) and \(\Sigma\) from the Gaussian):
			\begin{equation}
				\E(\vec{x}\vec{x}^T) = \vec{\mu}\vec{\mu}^T + \Sigma
			\end{equation}
		% end

		\subsection{Moments}
			A \emph{moment} is defined as
			\begin{equation}
				m_n = E\big(x^n\big)
			\end{equation}
			The \emph{central moment} is defined as
			\begin{equation}
				\textit{cm}_n = \E\big( (x - \mu)^n \big)
			\end{equation}
			which leads to another definition of the variance, skewness and kurtosis:
			\begin{description}
				\item[\( \textit{\textrm{cm}}_2 \)] Variance (measure of spreading)
				\item[\( \textit{\textrm{cm}}_3 \)] Skewness (measure of asymmetry)
				\item[\( \textit{\textrm{cm}}_4 \)] Kurtosis (measure of heavy/light tailed-ness)
			\end{description}
		% end
	% end

	\section{Exponential Family}
		The \emph{exponential family} is a large class of distributions that are analytically interesting, because taking the log of them simplifies them a lot. All distributions of this family are unimodal with the following general form:
		\begin{equation}
			p(\vec{x} \given \vec{\eta}) = h(\vec{x}) g(\vec{\eta}) \exp \big\{ \vec{\eta}^T \vec{u}(\vec{x}) \big\}
		\end{equation}
		where \(\vec{\eta}\) is the natural parameter and
		\begin{equation}
			g(\vec{\eta}) \int\limits_{-\infty}^{+\infty} h(\vec{x}) \exp \big\{ \vec{\eta}^T \vec{u}(\vec{x}) \big\} = 1
		\end{equation}
		holds. \(g\) can be interpreted as a normalization to make this property hold true.

		\subsection{Example: Bernoulli Distribution}
			The Bernoulli distribution is part of the exponential family and decomposes as
			\begin{align}
				\textrm{Bern}(x \given \mu) &= \mu^x (1 - \mu)^{1 - x} \\
					&= \exp \big\{ x \ln(\mu) + (1 - x) \ln(1 - \mu) \big\} \\
					&= (1 - y) \exp \Bigg\{ \ln\bigg(\!\frac{\mu}{1 - \mu} \bigg) x \Bigg\}
			\end{align}
			with the logistic sigmoid
			\begin{equation}
				\sigma(\eta) = \frac{1}{1 + \exp(-\eta)}
			\end{equation}
			and
			\begin{equation}
				\eta = \ln\bigg(\!\frac{\mu}{1 - \mu} \bigg)
			\end{equation}
			we can write the Bernoulli distribution as
			\begin{equation}
				p(x \given \mu) = \sigma(-\eta) \exp(\eta x)
			\end{equation}
			which, in the exponential family form, gives:
			\begin{equation}
				h(x) = 1 \qquad g(\eta) = \sigma(-\eta) \qquad u(x) = x
			\end{equation}
		% end

		\subsection{Example: Gaussian Distribution}
			The Gaussian distribution is part of the exponential family and decomposes as
			\begin{align}
				\mathcal{N}\,(x \given \mu, \sigma^2) &= \frac{1}{\sqrt{2\pi\sigma^2}} \exp\Bigg\{ -\frac{1}{2\sigma^2} (x - \mu)^2 \Bigg\} \\
					&= \frac{1}{\sqrt{2\pi\sigma^2}} \exp\Bigg\{ -\frac{1}{2\sigma^2} x^2 + \frac{\mu}{\sigma^2} x - \frac{\mu^2}{2\sigma^2} \Bigg\} \\
					&= h(x) g(\vec{\eta}) \exp\big\{ \vec{\eta}^T \vec{u}(x) \big\}
			\end{align}
			with
			\begin{equation}
				\eta = \begin{bmatrix} -\frac{1}{2\sigma^2} & \frac{\mu}{\sigma^2} \end{bmatrix}^T \qquad h(x) = 1 \qquad g(\vec{\eta}) = \sqrt{-\frac{\eta_1}{\pi}} \exp \Bigg\{ \frac{\eta_2^2}{4\eta_1} \Bigg\} \qquad \vec{u}(x) = \begin{bmatrix} x^2 \\ x \end{bmatrix}
			\end{equation}
		% end
	% end

	\section{Information Theory and Entropy}
		\emph{Information theory} is about how to represent information compactly (as few bits as possible) and therefore about compression.

		This raises three questions:
		\begin{itemize}
			\item How to measure complexity?
			\item How to measure the "distance" between probability distributions?
			\item How to reconstruct data?
		\end{itemize}

		\subsection{Information and Entropy}
			\begin{itemize}
				\item Information is hiding in data.
				\item E.g. in the English alphabet, every letter has a different probability \(p_i\) of occurring.
				\item A lower probability indicates that the data point contains more information.
				\item The \emph{average information}, called \emph{entropy} can be calculated as
			\end{itemize}
			\begin{equation}
				H(p) = -\sum_{i} p_i \log_2(p_i)
			\end{equation}
		% end

		\subsection{Kullback-Leibler Divergence}
			The \emph{Kullback-Leibler Divergence} is a similarity measurement between probability distributions, defined by
			\begin{align}
				\textrm{KL}(p \,\Vert\, q) &= -\int p(x) \ln\big(q(x)\big) \dd{x} - \Bigg( -\int p(x) \ln\big(p(x)\big) \dd{x} \Bigg) \\
					&= -\int p(x) \ln\bigg(\frac{q(x)}{p(x)}\bigg) \dd{x}
			\end{align}
			The KL divergence represents the average additional number of bits required to specify a symbol \(x\), if the underlying probability distribution is the estimated \(q(x)\) and not the true one \(p(x)\).

			Some properties:
			\begin{itemize}
				\item \( \textrm{KL}(p \,\Vert\, q) \neq \textrm{KL}(q \,\Vert\, p) \) not a distance
				\item \( \textrm{KL}(p \,\Vert\, q) \geq 0 \) non-negative distance
				\item \( \big(\forall x : p(x) = q(x)\big) \implies \textrm{KL}(p \,\Vert\, q) = 0 \)
			\end{itemize}

			There exist other metrics than KL, but KL is deeply connected to maximum likelihood estimation. \todo{Stats: KL: Connect with MLE}
		% end
	% end
	
	\section{Wrap-Up}
		\begin{itemize}
			\item Random variables (both continuous and discrete)
			\item Probability distributions
			\item Basic rules of probability theory
			\item Expectation and variance
			\item Gaussian distribution and its importance
			\item Information and entropy
		\end{itemize}
	% end
% end

\chapter{Fundamentals: Optimization}
	\info{"All learning problems are essentially optimization problems on data" (Christopher G. Atkeson, Professor at CMU)}

	All machine learning problems are optimization problems in the form
	\begin{align}
		\min_\theta J(\theta, \mathcal{D}) & \\
		\textrm{s.t.} \qquad
		f(\theta, \mathcal{D}) &= 0 \\
		g(\theta, \mathcal{D}) &\geq 0
	\end{align}
	with parameters \(\theta\) to enable learning, a data set \(\mathcal{D}\) to learn from, a cost function \(J(\theta, \mathcal{D})\) to measure the performance and equality and inequality constraints \( f(\theta, \mathcal{D}) = 0 \), \( g(\theta, \mathcal{D}) \geq 0 \).

	\section{Convexity}
		A set \( C \subseteq \R^n \) is \emph{convex} iff for all \( \vec{x}, \vec{y} \in C \) and for all \( \alpha \in [0, 1] \) the following holds:
		\begin{equation}
			\alpha\vec{x} + (a - \alpha)\vec{y} \in C
		\end{equation}
		Intuition: Every point on a line drawn between two arbitrary points in space lie in the set itself. The set has no "bays".

		A function \( f : \R^n \to \R \) is \emph{convex} iff for all \( \vec{x}, \vec{y} \in \textrm{Domain}(f) \) and for all \( \alpha \in [0, 1] \) the following holds:
		\begin{equation}
			f\big( \alpha\vec{x} + (1 - \alpha)\vec{y} \big) \leq \alpha f(\vec{x}) + (1 - \alpha)f(\vec{y})
		\end{equation}
		Intuition: The drawn line between two arbitrary points on the function do not cross the function (they may touch, so linear functions are also convex).

		If \(f\) is differentiable, it is convex iff for all \( \vec{x}, \vec{y} \in \textrm{Domain}(f) \) the following holds:
		\begin{equation}
			f(\vec{y}) \geq f(\vec{x}) + \nabla_{\vec{x}}(\vec{y} - \vec{x})
		\end{equation}

		If \(f\) is twice differentiable, it is convex iff for all \( \vec{x} \in \textrm{Domain}(f) \) the following holds:
		\begin{equation}
			\nabla_{\vec{x}}^2 f(\vec{x}) \preceq 0
		\end{equation}

		\warning{Differentiability is not a condition for convexity!}
	% end

	\section{Cost Functions}
		An ideal cost function is convex. But most of the time, they are not\dots

		\subsection{Common Cost Functions}
			Table~\ref{tab:costFunctions} lists common cost functions for classification, regression, density estimation and clustering.

			\begin{table}
				\centering
				\begin{tabular}{lll}
					\textbf{Problem} & \textbf{Example Cost Functions} & \textbf{Resulting Method} \\ \hline
					\textbf{Classification} & \( \min_\theta \sum_{i = 1}^{n} \ln\Big(1 \! + \exp\big(-y_ix_i^T\theta\big)\Big) \) & Logistic Regression \\
					& \( \min_{\theta_1, \theta_2} \sum_{i = 1}^{n} \bigg( \! y_i - g\Big( \theta_2^T g\big(\theta_1^T x_i\big) \Big) \! \bigg)^2 \) & Neural Network Classification \\
					& \makecell[l]{\(\min_\theta \lVert\theta\rVert^2 + C \sum_{i = 1}^{n} \xi_i\) \\ \( \quad\textrm{s.t.}\quad \xi_i - \big(1 - y_ix_i^T\theta\big) \geq 0,\, \xi_i \geq 0 \)} & Support Vector Machines \\
					\textbf{Regression} & \( \min_\theta \sum_{i = 1}^{n} \big(y_i - \phi(x_i)^T \theta\big)^2 \) & Linear Regression \\
					& \( \min_{\theta_1, \theta_2, \theta_3} \sum_{i = 1}^{n} \bigg( \! y_i - \theta_3^T g\Big( \theta_2^T g\big( \theta_1^T x_i \big) \Big) \! \bigg)^2 \) & Neural Network Regression \\
					\textbf{Density Estimation} & \( \min_\theta \sum_{i = 1}^{n} \ln\big(p(x_i \given \theta)\big) \) & General Formulation \\
					\textbf{Clustering} & \( \min_{\mu_1, \cdots, \mu_k} \sum_{j = 1}^{k} \sum_{i \in C_j} \lVert x_i - \mu_i \rVert ^2 \)
				\end{tabular}
				\caption{Common Cost Functions}
				\label{tab:costFunctions}
			\end{table}
		% end
	% end

	\section{Constrained/Unconstrained Optimization}
		The general form of a constrained optimization problem is
		\begin{align}
			\max\limits_{\vec{\theta}} \, J(\vec{\theta}) & \\
			\textrm{s.t.} \qquad
			\vec{f}(\vec{\theta}) & = 0 \\
			\vec{g}(\vec{\theta}) &\geq 0
		\end{align}
		with a cost function \(J(\vec{\theta})\), some equality constraints \(\vec{f}(\vec{\theta})\) and inequality constraints \(\vec{g}(\vec{\theta})\).
	% end

	\section{Lagrange Multipliers}
		With a constrained optimization problem in the general form, the \emph{Lagrangian} is defined as
		\begin{equation}
			\mathcal{L}(\vec{\theta}, \vec{\lambda}, \vec{\mu}, \vec{\epsilon}) = J(\theta) + \vec{\lambda}^T\vec{f}(\vec{\theta}) + \vec{\mu}^T \big(\vec{g}(\vec{\theta} + \vec{\epsilon}^2)\big)
		\end{equation}
		The coefficients \(\vec{\lambda}\) and \( \vec{\mu} \) are called \emph{Lagrangian Multipliers}, the variables \( \vec{\epsilon} \) are called \emph{slack variables} and are used to convert the inequality constraints into equality constraints.

		To solve the optimization problem, take the derivatives w.r.t. \( \theta \), \(\lambda\) and \(\mu\) and set them to zero:
		\begin{equation}
			\nabla_{\vec{\theta}} \mathcal{L} = 0 \qquad \nabla_{\vec{\lambda}} \mathcal{L} = 0 \qquad \nabla_{\vec{\mu}} \mathcal{L} = 0
		\end{equation}
		If this results in any \( \epsilon_i = 0 \), the inequality constraint is called \emph{active} and the solution lies on the edge of that constraint. To check whether the result really is a minima/maxima, take the second derivative of the cost function w.r.t. \(\theta\), \( \nabla_{\vec{\theta}}^2 J(\vec{\theta}) \), and check whether the resulting Hessian is positive or negative definite, yielding that the found solution is a minima or maxima, respectively.

		\subsection{Dual Formulation}
			Given the so-called \emph{primal problem}
			\begin{align}
				\min\limits_{\vec{\theta}} \, J(\vec{\theta}) & \\
				\textrm{s.t.} \qquad
				\vec{f}(\vec{\theta}) & = 0 \\
				\vec{g}(\vec{\theta}) &\geq 0
			\end{align}
			with the Lagrangian
			\begin{equation}
				\mathcal{L}(\vec{\theta}, \vec{\lambda}, \vec{\mu}, \vec{\epsilon}) = J(\theta) + \vec{\lambda}^T\vec{f}(\vec{\theta}) + \vec{\mu}^T \big(\vec{g}(\vec{\theta} + \vec{\epsilon}^2)\big)
			\end{equation}
			the \emph{dual problem} is
			\begin{align}
				\max\limits_{\vec{\lambda}, \vec{\mu}} \, \hat{\mathcal{L}}(\vec{\lambda}, \vec{\mu}, \vec{\epsilon}) &= \min\limits_{\vec{\theta}} \, \mathcal{L}(\vec{\theta}, \vec{\lambda}, \vec{\mu}, \vec{\epsilon}^2) \\
				\textrm{s.t.} \quad
				\vec{\lambda} &\geq \vec{0} \\
				\vec{\mu} &\geq \vec{0}
			\end{align}
			\begin{itemize}
				\item If \( \vec{\lambda}^\ast \) is the solution for the dual problem, then \( \hat{\mathcal{L}}(\vec{\lambda}^\ast) \) is a \emph{lower bound} for the primal problem due to two concepts:
					\begin{itemize}
						\item \emph{Minimax inequality:} For any function with two arguments \( \phi(x, y) \), the maximin is less or equal to the minimax:
					\end{itemize}
			\end{itemize}
			\begin{equation}
				\max_y \min_x \, \phi(x, y) \leq \min_x \max_y \, \phi(x, y)
			\end{equation}
			\begin{itemize}
				\item[]
					\begin{itemize}
						\item \emph{Weak duality:} The primal values are always greater or equal to the dual values:
					\end{itemize}
			\end{itemize}
			\begin{equation}
				\min_{\vec{\theta}} \max_{\substack{\vec{\lambda} \,\geq\, \vec{0} \\ \vec{\mu} \,\geq\, \vec{0}}} \, \mathcal{L}(\vec{\theta}, \vec{\lambda}, \vec{\mu}) \geq \max_{\substack{\vec{\lambda} \,\geq\, \vec{0} \\ \vec{\mu} \,\geq\, \vec{0}}} \min_{\vec{\theta}} \, \mathcal{L}(\vec{\theta}, \vec{\lambda}, \vec{\mu})
			\end{equation}
			\begin{itemize}
				\item In machine learning, the dual is often far more useful than the primal.
				\item That is because \(\hat{\mathcal{L}}\) is a concave function and easy to optimize, even if \(J\) and the constraints may be nonconvex.
				\item Given some \(\vec{\lambda}\) and \(\vec{\mu}\), the dual is an unconstrained problem.
			\end{itemize}
		% end

		\subsection{Example}
			Given the following optimization problem (in the real numbers):
			\begin{align}
				\arg\max\limits_{x, y} \, J(x, y) &= x + y \\
				\textrm{s.t.} \quad
				x^2 + y^2 - 1 &= 0 \\
				2 - x &\geq 0
			\end{align}
			the Lagrangian is written as
			\begin{align}
				\mathcal{L}(x, y, \lambda, \mu, \epsilon) = x + y + \lambda (x^2 + y^2 - 1) + \mu (2 - x + \epsilon^2)
			\end{align}
			Take the derivatives:
			\begin{align}
				\nabla_x \mathcal{L}       & = 1 + 2 \lambda x - \mu \\
				\nabla_y \mathcal{L}       & = 1 + 2 \lambda y \\
				\nabla_\lambda \mathcal{L} & = x^2 + y^2 - 1 \\
				\nabla_\mu \mathcal{L}     & = 2 - x + \epsilon^2 \\
				\nabla_\epsilon \mathcal{L} &= 2\mu\epsilon
			\end{align}
			Settings them to zero gives the insight that either \( \mu = 0 \) or \( \epsilon = 0 \) must be true. This must be done per case.
			
			\paragraph{Case 1: \( \mu = 0 \)}
				This yields the following equation system:
				\begin{align}
					0 & = 1 + 2 \lambda x    \\
					0 & = 1 + 2 \lambda y    \\
					0 & = x^2 + y^2 - 1      \\
					0 & = 2 - x + \epsilon^2
				\end{align}
				with the solution \( x = y = \pm \frac{1}{\sqrt{2}} \) and \( \epsilon^2 = \frac{1}{\sqrt{2}} - 2 \), so the inequality constraint is not active as the solution fulfills the equation \( x < 2 \).
			% end
			
			\paragraph{Case 2: \( \epsilon = 0 \)}
				This yields the following equation system:
				\begin{align}
					0 & = 1 + 2 \lambda x - \mu \\
					0 & = 1 + 2 \lambda y       \\
					0 & = x^2 + y^2 - 1         \\
					0 & = 2 - x
				\end{align}
				Which yields the following solutions:
				\begin{align}
					x_1 & = 2          &  & x_2 = 2         \\
					y_1 & = -i\sqrt{3} &  & y_2 = i\sqrt{3}
				\end{align}
				As \( \epsilon = 0 \), the solution must lie on the edge of the inequality constraint, thus \( x = 2 \). As only real solutions where wanted, this solution can be discarded.
			% end
		% end
	% end

	\section{Numerical Optimization}
		For a lot of optimization problems, the solution cannot be computed analytically, so these have to be approximated using numerical optimization.

		The performance of numerical methods can be measured with the following questions:
		\begin{itemize}
			\item Does the algorithm converge to the optimal solution?
			\item How many steps does it take to converge?
			\item Is the convergence smooth or bumpy?
			\item Does it work for all types of functions or just a special type (e.g. convex)?
			\item \dots
		\end{itemize}
		Thus boils down to the following metrics that have to be taken into account:
		\begin{itemize}
			\item Number of iterations required
			\item Cost per iteration
			\item Memory footprint
			\item Region of convergence
			\item Is the cost function noisy?
		\end{itemize}

		The basic idea behind numerical optimization is to find a \( \delta\theta \) with
		\begin{equation}
			J(\theta + \alpha\delta\theta) < J(\theta)
		\end{equation}
		and to apply iterative updates rules like
		\begin{equation}
			\theta_{n+1} = \theta_n + \alpha\delta\theta
		\end{equation}
		The key question is: How to find a good direction \(\delta\theta\)?

		\subsection{Learning Rate}
			There are two basic methods for finding the learning rate \(\alpha\):
			\begin{itemize}
				\item \textbf{Line Search} \\ The learning rate is searched for each step with
			\end{itemize}
			\begin{equation}
				\alpha_n = \arg\min_\alpha J(\theta_n + \alpha\delta\theta_n)
			\end{equation}
			\begin{itemize}
				\item \textbf{Constant Learning Rate} \\ The learning rate \( \alpha = \const \) is just fixed and not dynamically determined.
				\item \textbf{Adaptive Learning Rate} \\ The learning rate \( \alpha \) is changed in each step according to some rules. Note that line search is kind of an adaptive learning rate that does not take previous learning rates into account. See \ref{sec:adaptiveLearningRate} for more information about adaptive learning rates.
			\end{itemize}
		% end

		\subsection{Test Functions}
			For testing the performance of the methods, well-known functions with interesting properties are used.

			\subsubsection{Quadratic Function}
				\begin{equation}
					J(\vec{\theta}) = (\theta_1 - 5)^2 + (\theta_1 - 5) (\theta_2 - 5) + (\theta_2 - 5)^2
				\end{equation}

				\begin{figure}
					\centering
					\begin{tikzpicture}
						\begin{axis}[
									axis equal,
									ymin = -5,
									ymax = 15,
									xmin = -10,
									xmax = 15,
									domain = -10:15,
									y domain = -5.5:15.5,
									xlabel = \(\theta_1\),
									ylabel = \(\theta_2\),
									colorbar,
									view = {0}{90}
								]
							\addplot3 [fill = colorDensity, surf, shader = interp] { (x-5)^2 + (x-5)*(y-5) + (y-5)^2 };
						\end{axis}
					\end{tikzpicture}
					\caption{Quadratic Function}
					\label{fig:quadraticFunction}
				\end{figure}

				The quadratic function is plotted in \ref{fig:quadraticFunction}.
			% end

			\subsubsection{Rosenbrock Function}
				\begin{equation}
					J(\vec{\theta}) = \big(\theta_2 - \theta_1^2\big)^2 + 0.01 (1 - \theta_1)^2
				\end{equation}

				\begin{figure}
					\centering
					\begin{tikzpicture}
						\begin{axis}[
								axis equal,
								domain = -3.75:3.75,
								xmin = -3.5,
								xmax = 3.5,
								ymin = -2,
								ymax = 4,
								y domain = -2:4,
								xlabel = \(\theta_1\),
								ylabel = \(\theta_2\),
								colorbar,
								view = {0}{90}
							]
						\addplot3 [fill = colorDensity, surf, shader = interp] { (y - x^2)^2 + 0.01 * (1 - x)^2 };
						\end{axis}
					\end{tikzpicture}
					\caption{Rosenbrock Function}
					\label{fig:rosenbrockFunction}
				\end{figure}

				The Rosenbrock function is plotted in \ref{fig:rosenbrockFunction}.
			% end
		% end

		\subsection{Axial Iteration}
			Alternate minimization for each axis.
		% end

		\subsection{Steepest Descent}
			\begin{itemize}
				\item Also called \emph{gradient descent}.
				\item Move in the direction of the gradient \( \nabla J(\vec{\theta}) \).
				\item The gradient is perpendicular to the contour lines and the next gradient is always orthogonal to the previous step direction after line minimization.
				\item As the gradient points into the direction of the maximum, the gradient has to be added for maximization and subtracted for minimization (with a positive step size).
				\item Problem: The gradient walks down in a zig-zag line that is very inefficient.
			\end{itemize}

			Algorithm~\ref{alg:steppestDescent} shows gradient descent in its basic version with a fixed learning rate \(\alpha\) and the initialization vector \(\vec{0}\). The algorithm terminates after \(n\) iterations. For maximization, the minus in line~\ref{alg:steepestDescent:min} has to be changed to a plus.

			\begin{algorithm}
				\(\vec{\theta}^{(1)} \gets \vec{0} \)

				\For{\( i = 1, \cdots, n \)}{
					\( \vec{\theta}^{(i + 1)} \gets \vec{\theta}^{(i)} - \alpha \nabla_{\vec{\theta}} J(\vec{\theta}) \) \label{alg:steepestDescent:min}
				}

				\Return \(\vec{\theta}^{(n + 1)}\)

				\caption{Steepest Descent (Minimization)}
				\label{alg:steppestDescent}
			\end{algorithm}

			\subsubsection{Test Functions}
				The plot of gradient descent working on the Rosenbrock function is plotted in figure~\ref{fig:steepestDescentExampleRosenbrock}, on the Quadratic function in figure~\ref{fig:steepestDescentExampleQuadratic}.

				\begin{figure}
					\centering
					\includegraphics{tmp-steepestDescent-rosenbrock.pdf}
					\caption{Steepest Descent on Rosenbrock}
					\label{fig:steepestDescentExampleRosenbrock}
				\end{figure}
				\begin{figure}
					\centering
					\includegraphics{tmp-steepestDescent-quadratic.pdf}
					\caption{Steepest Descent on Quadratic}
					\label{fig:steepestDescentExampleQuadratic}
				\end{figure}
			% end
		% end

		\subsection{Newtons Method}
			\emph{Newtons method} uses the first-order Taylor approximation
			\begin{align}
				J(\vec{\theta} + \delta\vec{\theta}) &\approx J(\vec{\theta}) + \nabla_{\vec{\theta}} J(\vec{\theta})^T \delta\vec{\theta} + \frac{1}{2} \delta\vec{\theta}^T \nabla_{\vec{\theta}}^2 J(\vec{\theta}) \delta\vec{\theta} \\
					&= c + \vec{g}^T \delta\vec{\theta} + \frac{1}{2} \delta\vec{\theta}^T H \delta\vec{\theta} \eqqcolon \tilde{J}(\delta\vec{\theta})
			\end{align}
			where \(\vec{g}\) is the Jacobian and \(H\) is the Hessian.

			Minimizing this approximation yields the solution
			\begin{equation}
				\delta\vec{\theta} = -H^{-1} \vec{g}
			\end{equation}

			\begin{itemize}
				\item Has quadratic convergence and finds the optimal solution for quadratic functions in one step (in the case of a learning rate \(\alpha = 1\)).
				\item If the Hessian is positive definite, \(\delta\vec{\theta}\) is guaranteed to point downhill.
				\item If the Hessian just equals the identity matrix \( H = I \), this method is equal to steepest descent.
				\item Problem: Computing the Hessian at every iteration is extremely expensive and often not feasible (the inversion can be removed by transforming it into a linear equation system).
			\end{itemize}

			Algorithm~\ref{alg:newtonMethod} shows Newtons method for minimization.

			\begin{algorithm}
				\(\vec{\theta}^{(1)} \gets \vec{0}\)

				\For{\( i = 1, \cdots, n \)}{
					\( \vec{\theta}^{(i + 1)} \gets \vec{\theta}^{(i)} - \alpha H^{-1}\big(\vec{\theta}^{(i)}\big) \vec{g}\big(\vec{\theta}^{(i)}\big) \)
				}

				\Return \(\vec{\theta}^{(n + 1)}\)

				\caption{Newtons Method (Minimization)}
				\label{alg:newtonMethod}
			\end{algorithm}

			\subsubsection{Test Functions}
				The plot of newtons method working on the Rosenbrock function is plotted in figure~\ref{fig:newtonsMethodExampleRosenbrock}, on the Quadratic function in figure~\ref{fig:newtonsMethodExampleQuadratic}.

				\begin{figure}
					\centering
					\includegraphics{tmp-newtonsMethod-rosenbrock.pdf}
					\caption{Newtons Method on Rosenbrock}
					\label{fig:newtonsMethodExampleRosenbrock}
				\end{figure}
				\begin{figure}
					\centering
					\includegraphics{tmp-newtonsMethod-quadratic.pdf}
					\caption{Newtons Method on Quadratic}
					\label{fig:newtonsMethodExampleQuadratic}
				\end{figure}
			% end
		% end

		\subsection{Quasi-Newton Method (BFGS)}
			\begin{itemize}
				\item Approximate the Hessian using
					\begin{itemize}
						\item Hessians change slowly,
						\item Hessians are symmetric and
						\item the derivatives interpolate.
					\end{itemize}
			\end{itemize}
			This gives the following optimization problem:
			\begin{align}
				\min \,&\, \lVert H - H_n \rVert \\
				\textrm{s.t.} \qquad
				H & = H^T \\
				H & \big(\vec{\theta}^{(n + 1)} - \vec{\theta}^{(n)}\big) = g\big(\vec{\theta}^{(n)}\big) - \vec{g}\big(\vec{\theta}^{(n)}\big)
			\end{align}
			Using this, the Hessian can be computer iteratively:
			\begin{align}
				H_{n + 1}^{-1} &= \Bigg( I - \frac{\vec{s}_n \vec{y}_n^T}{\vec{y}_n \cdot \vec{s}_n} \Bigg) H_n^{-1}
				                  \Bigg( I - \frac{\vec{y}_n \vec{s}_n^T}{\vec{y}_n \cdot \vec{s}_n} \Bigg)
				                + \frac{\vec{s}_n \vec{s}_n^T}{\vec{y}_n \cdot \vec{s}_n} \\
				\vec{y}_n &= \vec{g}\big(\vec{\theta}^{(n + 1)}\big) - \vec{g}\big(\vec{\theta}\big) \\
				\vec{s}_n &= \vec{\theta}^{(n + 1)} - \vec{\theta}^{(n)}
			\end{align}

			\begin{itemize}
				\item The first step in the algorithm can be slightly off due to initialization errors.
				\item For great dimensions, BFGS is preferred over the others as it does not require to compute the Hessian.
			\end{itemize}

			Algorithm~\ref{alg:bfgs} shows BFGS for minimization.

			\begin{algorithm}
				\(\vec{\theta}^{(1)} \gets \vec{0}\)

				\(H_{1}^{-1} \gets 0\)

				\For{\( i = 1, \cdots, n \)}{
					\( \vec{\theta}^{(i + 1)} \gets \vec{\theta}^{(i)} - \alpha H_i^{-1}\big(\vec{\theta}^{(i)}\big) \vec{g}\big(\vec{\theta}^{(i)}\big) \)

					\( \vec{y} \gets \vec{g}\big(\vec{\theta}^{(n + 1)}\big) - \vec{g}\big(\vec{\theta}\big) \)

					\( \vec{s} \gets \vec{\theta}^{(n + 1)} - \vec{\theta}^{(n)} \)

					\( H_{i + 1}^{-1} = \Big( I - \frac{\vec{s} \, \vec{y}^T}{\vec{y} \cdot \vec{s}} \Big) H_i^{-1}
									    \Big( I - \frac{\vec{y} \, \vec{s}^T}{\vec{y} \cdot \vec{s}} \Big)
									  + \frac{\vec{s} \, \vec{s}^T}{\vec{y} \cdot \vec{s}} \)
				}

				\Return \(\vec{\theta}^{(n + 1)}\)

				\caption{Quasi-Newton-Method, BFGS (Minimization)}
				\label{alg:bfgs}
			\end{algorithm}

			\subsubsection{Test Functions}
				The plot of BFGS working on the Rosenbrock function is plotted in figure~\ref{fig:bfgsExampleRosenbrock}, on the Quadratic function in figure~\ref{fig:bfgsExampleQuadratic}.

				\begin{figure}
					\centering
					\includegraphics{tmp-bfgs-rosenbrock.pdf}
					\caption{BFGS on Rosenbrock}
					\label{fig:bfgsExampleRosenbrock}
				\end{figure}
				\begin{figure}
					\centering
					\includegraphics{tmp-bfgs-quadratic.pdf}
					\caption{BFGS on Quadratic}
					\label{fig:bfgsExampleQuadratic}
				\end{figure}
			% end
		% end

		\subsection{Conjugate Gradient (CG)}
			\begin{itemize}
				\item \emph{Conjugate gradient} choose the descent direction \(\delta\vec{\theta}\) such that it is guaranteed to reach the minimum in a finite number of steps.
				\item Each \(\delta\vec{\theta}\) is chosen to conjugate all previous search directions w.r.t. the Hessian.
				\item The resulting search directions are mutually linearly independent.
				\item This avoid undoing previously done work.
				\item An \(N\)-dimensional quadratic function can be minimized in at most \(N\) CG steps.
				\item Also avoids computing the Hessian!
				\item \(\delta\vec{\theta}^{(n)}\) is calculated using only \( \delta\vec{\theta}^{(n - 1)} \), \( \nabla_{\vec{\theta}} \big(\vec{\theta}^{(n)}\big) \) and \( \nabla_{\vec{\theta}} \big(\vec{\theta}^{(n - 1)}\big) \):
			\end{itemize}
			\begin{equation}
				\delta\vec{\theta}^{(n)} = \nabla_{\vec{\theta}} J\big(\vec{\theta}^{(n)}\big) + \frac{\Big|\nabla_{\vec{\theta}} J\big(\vec{\theta}^{(n)}\big)\Big|^2}{\Big|\nabla_{\vec{\theta}} J\big(\vec{\theta}^{(n - 1)}\big)\Big|^2} \delta\vec{\theta}^{(n - 1)}
			\end{equation}

			Algorithm~\ref{alg:cg} shows CG for minimization.

			\begin{algorithm}
				\( \vec{\theta}^{(0)} \gets \vec{0} \)

				\( \vec{\theta}^{(1)} \gets \vec{0} \)

				\( \delta\vec{\theta}^{(1)} \gets \vec{0} \)

				\For{\( i = 1, \cdots, n \)}{
					\( \delta\vec{\theta}^{(i + 1)} \gets \nabla_{\vec{\theta}} J\big(\vec{\theta}^{(i)}\big) + \frac{\big\lvert\nabla_{\vec{\theta}} J\big(\vec{\theta}^{(i)}\big)\big\rvert^2}{\big\lvert\nabla_{\vec{\theta}} J\big(\vec{\theta}^{(i - 1)}\big)\big\rvert^2} \delta\vec{\theta}^{(i - 1)} \)

					\( \vec{\theta}^{(i + 1)} \gets \vec{\theta}^{(i)} - \alpha \, \delta\vec{\theta}^{(i + 1)} \)
				}

				\Return \(\vec{\theta}^{(n + 1)}\)

				\caption{Conjugate Gradients (Minimization)}
				\label{alg:cg}
			\end{algorithm}

			\subsubsection{Test Functions}
				The plot of CG working on the Rosenbrock function is plotted in figure~\ref{fig:cgExampleRosenbrock}, on the Quadratic function in figure~\ref{fig:cgExampleQuadratic}.

				\begin{figure} \todo{Opt: CG: Rosenbrock impl. wrong}
					\centering
					\includegraphics{tmp-cg-rosenbrock.pdf}
					\caption{CG on Rosenbrock}
					\label{fig:cgExampleRosenbrock}
				\end{figure}
				\begin{figure}
					\centering
					\includegraphics{tmp-cg-quadratic.pdf}
					\caption{CG on Quadratic}
					\label{fig:cgExampleQuadratic}
				\end{figure}
			% end
		% end

		\subsection{Conjugate Gradients vs. BFGS}
			\begin{itemize}
				\item BFGS is more costly per iteration than CG.
				\item BFGS converges in fewer steps.
				\item BFGS has less tendency to get stuck.
				\item BFGS requires algorithmic "hacks" to achieve a significant descent per iteration.
				\item Which one is better depends on the problem.
			\end{itemize}
		% end
	% end
	
	\section{Wrap-Up}
		\begin{itemize}
			\item Relation between machine learning and optimization
			\item Properties of good cost functions
			\item Convex sets and functions
			\item Importance of convex functions in machine learning
			\item Constrained and unconstrained optimization problems
			\item Lagrangian formulation
			\item Different numerical methods
		\end{itemize}
	% end
% end

\chapter{Bayesian Decision Theory}
	\label{c:bayesianDecisionTheory}

	\emph{Bayesian decision theory} is a statistical approach to make optimal decisions.

	\begin{itemize}
		\item All data in machine learning is generated by a stochastic process that is governed by the rules of probability.
		\item The data is understood as a set of samples from some underlying probability distribution.
	\end{itemize}

	\section{Character Recognition}
		Goal: Classify a new letter so that the probability of a wrong classification is minimized where the only possibilities are \(a\) and \(b\).

		\subsection{Class Conditional Probabilities}
			The \emph{class conditional probability} (likelihood) \( p(\vec{x} \given C_k) \) is the probability of making an observation \(\vec{x}\) knowing that it comes from some specific class \(C_k\). \(\vec{x}\) is often the \emph{feature vector}, e.g. the number of black pixels, the height of black pixels, etc.

			Let \(x\) be the height of black pixels and therefore a scalar value \(x\ in \R\).

			\begin{itemize}
				\item This yields some useful decision theory: Given some \(x\), decide for class \(a\) if \( p(x \given a) \geq p(x \given b) \).
				\item But: If \( p(x \given a) = p(x \given b) \), this yields no solution and class priors have to be taken into account.
			\end{itemize}

			\paragraph{Example}
				Figure~\ref{fig:bayesianCCP} shows some example conditional properties.

				\begin{itemize}
					\item For \( x = 5 \) or \( x = 11 \), the decision is clear (choose \(a\) or \(b\) respectively).
					\item But for \( x = 8 \), its completely unclear.
				\end{itemize}

				\begin{figure}
					\centering
					\begin{tikzpicture}
						\begin{axis}[
									domain = 0:15,
									xmin = 0,
									xmax = 15,
									ymin = 0,
									ymax = 0.5,
									xlabel = \(x\)
								]
							\addplot [density, TUDa-9b, smooth] { gaussian(x, 5, 2) }; \addlegendentry{\( p(x \given a) \)};
							\addplot [density, TUDa-1b, smooth] { gaussian(x, 11, 2) }; \addlegendentry{\( p(x \given b) \)};
						\end{axis}
					\end{tikzpicture}
					\caption{Class Conditional Probabilities}
					\label{fig:bayesianCCP}
				\end{figure}
			% end
		% end

		\subsection{Class Priors}
			A \emph{class prior} is a a-priori probability of a letter to occur (e.g. in the English alphabet, the probability of observing the letter \(e\) is much higher than observing a \(y\)).

			\begin{itemize}
				\item All class priors have to sum up to one (it must be anything).
			\end{itemize}

			\paragraph{Example}
				In the text \texttt{aaaaaaaabbabbaaaaaaa}, the class priors are:
				\begin{align}
					C_1 &= a \\
					C_2 &= b \\
					p(a) &= \frac{16}{20} = 80\% \\
					p(b) &= \frac{4}{20} = 20\% \\
					\sum_k p(C_k) &= p(a) + p(b) = 1
				\end{align}
				By using Bayes theorem, the posterior can be calculated and the likelihood can be scaled by the prior to give a better view on the problem. Scaling by the normalization factor visualized the decision boundary.

				Figure~\ref{fig:bayesianCCPrior} shows these plots.

				\begin{figure}
					\centering
					\begin{tikzpicture}
						\begin{axis}[
									name = plot1,
									domain = 0:15,
									xmin = 0,
									xmax = 15,
									ymin = 0,
									ymax = 0.5,
									xlabel = \(x\)
								]
							\addplot [density, TUDa-9b, smooth] { gaussian(x, 5, 2) * 0.8 }; \addlegendentry{\( p(x \given a) p(a) \)};
							\addplot [density, TUDa-1b, smooth] { gaussian(x, 11, 2) * 0.2 }; \addlegendentry{\( p(x \given b) p(b) \)};
						\end{axis}
						\begin{axis}[
									name = plot2,
									domain = 0:15,
									xmin = 0,
									xmax = 15,
									ymin = 0,
									ymax = 1,
									xlabel = \(x\),
									at = (plot1.right of south east),
									anchor = left of south west,
									xshift = 0.5cm,
									legend style = { yshift = -0.5cm }
								]
							\addplot [density, TUDa-9b, smooth] { (gaussian(x, 5, 2) * 0.8) / (gaussian(x, 5, 2) * 0.8 + gaussian(x, 11, 2) * 0.2) }; \addlegendentry{\( p(a \given x) \)};
							\addplot [density, TUDa-1b, smooth] { (gaussian(x, 11, 2) * 0.2) / (gaussian(x, 5, 2) * 0.8 + gaussian(x, 11, 2) * 0.2) }; \addlegendentry{\( p(b \given x) \)};
						\end{axis}
					\end{tikzpicture}
					\caption{Class Priors}
					\label{fig:bayesianCCPrior}
				\end{figure}
			% end
		% end
	% end

	\section{Bayesian Decision Theory}
		With the class conditional probability \( p(X_k \given \vec{x}) \) and the prior \( p(C_k) \), the class posterior can be calculated as
		\begin{equation}
			p(C_k \given \vec{x}) = \frac{p(\vec{x} \given C_K) p(C_k)}{p(\vec{x})} = \frac{p(\vec{x} \given C_K) p(C_k)}{\sum_j p(\vec{x} \given C_j) p(C_j)}
		\end{equation}
	% end

	\section{Bayesian Probabilities}
		\begin{itemize}
			\item With \emph{Bayesian probabilities}, probability is not just interpreted as a frequency of certain events, but as a degree of belief in an outcome.
			\item This allows to assert a prior belief in a data point coming from a certain class.
		\end{itemize}
	% end

	\section{Misclassification Rate}
		The goal of Bayesian decision theory is to minimize the \emph{misclassification rate}, the probability of making a wrong decision:
		\begin{align}
			p(\textrm{error}) = p(x \in R_1, C_2) + p(x \in R_2, C_1) = \int_{R_1} \! p(x \given C_2) p(C_2) \dd{x} + \int_{R_2} \! p(x \given C_1) p(C_1) \dd{x}
		\end{align}
	% end

	\section{Decision Rule, Optimal Classifier and Decision Boundary}
		\label{sec:bayesianDecisionRule}

		The basic \emph{decision rule} is to decide for \(C_1\) iff
		\begin{align}
			p(C_1 \given x) > p(C_2 \given x) \quad\iff\quad \frac{p(x \given C_1)}{p(x \given C_2)} > \frac{p(C_2)}{p(C_1)}
		\end{align}
		A classifier that obeys this rule is called \emph{Bayesian optimal classifier}.

		The \emph{decision boundary} is the point where \( \frac{p(x \given C_1)}{p(x \given C_2)} = \frac{p(C_2)}{p(C_1)} \). This line (or curve) can then be drawn into some graph and is the point where the classifier "switches" to the other class. This is most of the time only used for understanding what the classifier does than for real application (however, understanding what happens is really important).

		\subsection{Multiple Classes}
			Decide for class \(k\) iff it has the highest a-posteriori probability (\(\forall j \neq k\))
			\begin{align}
				p(C_k \given x) > p(C_j \given x) \quad\iff\quad \frac{p(x \given C_K)}{p(x \given C_j)} > \frac{p(C_j)}{p(C_k)}
			\end{align}
			This yields more decision regions and multiple decision boundaries.
		% end

		\subsection{High Dimensional Features}
			For a lot of problems, the feature vector must have more than one entry, so \( \vec{x} \in \R^n \) with \( n \geq 2 \). The derived decision boundaries still apply, but multivariate class conditional densities \( p(\vec{x} \given C_k) \) have to be taken into account.
		% end
	% end

	\section{Dummy Classes}
		In some applications, a \emph{dummy class} "don't know" or "don't care" must be present (also called \emph{reject option}).
	% end

	\section{Risk Minimization}
		\begin{itemize}
			\item Minimizing the misclassification rate may not alway be enough, as not every misclassification may be equally bad.
			\item The key idea is to construct a \emph{loss function} (or \emph{cost function}) that expresses which misclassifications are really bad and which are not so bad.
			\item This loss function is called \( \lambda(\alpha_i \given C_j) \), where \(C_j\) is the actual class and \(\alpha_i\) is the decision. Let \( \lambda_{ij} \coloneqq \lambda(\alpha_i \given C_j) \).
			\item The expected loss of making a decision \(\alpha_i\) (the \emph{overall risk}) then calculates as:
		\end{itemize}
		\begin{equation}
			R(\alpha_i \given x) = \E_{C_k \sim p(C_k \given x)} \big(\lambda(\alpha_i \given C_k)\big) = \sum_j \lambda(\alpha_i \given C_j) p(C_j \given x)
		\end{equation}
		\begin{itemize}
			\item So instead of minimizing the misclassification rate, minimize the overall risk.
		\end{itemize}

		\subsection{Decision Rule}
			Decide for class \(C_1\) iff
			\begin{equation}
				R(\alpha_2 \given x) > R(\alpha_1 \given x) \quad\iff\quad \frac{p(x \given X_1)}{p(x \given X_2)} > \frac{\lambda_{12} - \lambda_{22}}{\lambda_{21} - \lambda_{11}} \frac{p(C_2)}{p(C_1)}
			\end{equation}
			This rule can be generalized for multiple classes and high dimensional features just like before.

			Applying a 0-1 loss function
			\begin{equation}
				\lambda(\alpha_i \given C_j) =
					\begin{cases}
						0 & i = j \\
						1 & i \neq j
					\end{cases}
			\end{equation}
			yields the decision rule without an explicit loss function as seen in section~\ref{sec:bayesianDecisionRule}.
		% end
	% end
	
	\section{Wrap-Up}
		\begin{itemize}
			\item Class-conditional probabilities, class priors and class posteriors
			\item Bayesian decision theory
			\item Usage of Bayes theorem for classification
			\item Misclassification rate
			\item Bayes optimal classifier
			\item Generalization of decisions for more than two classes
			\item Risk and relation to misclassification
		\end{itemize}
	% end
% end

\chapter{Probability Density Estimation}
	\emph{Probability density estimation} (PDE) is about to estimate/learn the class conditional probability density \( p(x \given C_k) \).

	\begin{itemize}
		\item In supervised learning, both the input data points and their true labels/classes are known.
		\item The density is estimated separately for each class \(C_k\).
		\item Let \( p(x) \coloneqq p(x \given C_k) \) for simplicity.
		\item There exist three models for PDE:
			\begin{itemize}
				\item Parametric Models \\ A "small" number of parameters completely define the probability density.
				\item Non-Parametric Models \\ No explicit parameters are used, but every known data point is used as a parameter (so, non-parametric models have as much parameters as there is data).
				\item Mixture Models \\ Combination of both.
			\end{itemize}
	\end{itemize}

	\section{Parametric Models}
		A simple case for a \emph{parametric model} is the Gaussian distribution
		\begin{equation}
			p(x \given \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\Bigg\{ -\frac{1}{2\sigma^2} (x - \mu)^2 \Bigg\}
		\end{equation}
		that is governed by two parameters: mean \(\mu\) and variance \(\sigma^2\). If both are known, the probability distribution is fully described.

		The notation to say "variable \(x\) is defined by the parametric model \( p(x \given \theta) \) with parameters \(\theta\)", write \( x \sim p(x \given \theta) \). For a Gaussian, the parameters are \( \theta = (\mu, \sigma^2) \).

		\begin{itemize}
			\item \emph{Learning} means to estimate the parameters \(\theta\) given some training data \( X = \{ x_1, x_2, \cdots \} \).
			\item The \emph{likelihood} of \(\theta\) (the probability that the data \(X\) was generated from a probability density function with parameters \(\theta\)) is defined as
		\end{itemize}
		\begin{equation}
			L(\theta) = p(X \given \theta) \quad\overset{if i.i.d.}{=}\quad \prod_{i = 1}^{N} p(x_i \given \theta)
		\end{equation}

		\subsection{Maximum Likelihood}
			Assume that all data is i.i.d.!

			\begin{itemize}
				\item The parameters \(\theta\) can be estimated by maximizing the likelihood.
				\item If a model has more than one parameter, maximize it w.r.t. each parameter once to get multiple estimators for the different parameters.
				\item Instead of maximizing the normal likelihood \(L(\theta)\), it is mostly better to maximize the log-likelihood \( \mathcal{L}(\theta) = \ln L(\theta) \) because:
					\begin{itemize}
						\item it removes the product and turns it into a sum and
						\item for members of the exponential family, removes the exponential part and splits the products into additions.
					\end{itemize}
					This is possible because the logarithm is strictly increasing. In fact, every strictly increasing function can be used, but the logarithm is most of the time the best decision.
			\end{itemize}
			\begin{equation}
				\mathcal{L}(\theta) = \ln\big(L(\theta)\big) = \ln p(X \given \theta) = \sum_{i = 1}^{N} \ln p(x_n \given \theta)
			\end{equation}
			\begin{itemize}
				\item Then maximize the log-likelihood by taking the derivative w.r.t. \(\theta\) (or the parameter to be estimated) and set them to zero.
			\end{itemize}

			\paragraph{Example}
				Given the Gaussian probability density
				\begin{equation}
					p(x \given \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\Bigg\{ -\frac{1}{2\sigma^2} (x - \mu)^2 \Bigg\}
				\end{equation}
				the log-likelihood computes as
				\begin{align}
					\mathcal{L} &= \ln \prod_{i = 1}^{N} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\Bigg\{ -\frac{1}{2\sigma^2} (x_i - \mu)^2 \Bigg\} \\
						&= \sum_{i = 1}^{N} \ln \frac{1}{\sqrt{2\pi\sigma^2}} - \frac{1}{2\sigma^2} (x_i - \mu)^2
				\end{align}

				To estimate \(\mu\), take the derivative and set it so zero (this maximizes the likelihood w.r.t. the mean):
				\begin{align}
					&& \nabla_\mu \mathcal{L} &= \frac{1}{\sigma^2} \sum_{i = 1}^{N} (x_i - \mu) = -N\frac{1}{\sigma^2}\mu + \frac{1}{\sigma^2} \sum_{i = 1}^{N} x_i & \\
					\implies && 0 &= -\frac{1}{\sigma^2}N\mu + \frac{1}{\sigma^2} \sum_{i = 1}^{N} x_i & \\
					\iff && \mu &= \frac{1}{N} \sum_{i = 1}^{N} x_i  & \\
				\end{align}
				this yields the arithmetic mean as an estimator for the expectation value.

				To estimate \(\sigma^2\), take the derivative and set it so zero:
				\begin{align}
					&& \nabla_{\sigma} \mathcal{L} &= \sum_{i = 1}^{N} -\frac{1}{\sigma} + \frac{1}{\sigma^3} (x_i - \mu)^2 = -\frac{1}{\sigma}N + \frac{1}{\sigma^3} \sum_{i = 1}^{N} (x_i - \mu)^2 & \\
					\implies && 0 &= -\frac{1}{\sigma}N + \frac{1}{\sigma^3} \sum_{i = 1}^{N} (x_i - \mu)^2 & \\
					\iff && \sigma^2 &= \frac{1}{N} \sum_{i = 1}^{N} (x_i - \mu)^2 & \\
				\end{align}
				this yields an estimator for \(\sigma^2\). But this estimator is biased, so the maximum likelihood estimation not always yields an unbiased estimator.
			% end
		% end

		\subsection{Degenerate Case}
			If only one data point is available (\( N = 1 \), \( X = \{ x_1 \} \)), the resulting Gaussian stretches infinitely to the top on one point, so is the likelihood.

			To still get a useful estimate, a prior has to be put on the mean. This leads to Bayesian estimation.

			\paragraph{Explanation}
				A probability density function \( p(x) \) such as the Gaussian has to integrate to \(1\):
				\begin{equation}
					\int\limits_{-\infty}^{+\infty} p(x) \dd{x} = 1
				\end{equation}
				If the Gaussian has no variance \( \sigma^2 = 0 \), it does only have positive values on one exact point (\( p(\mu) > 0 \)). As the integral can be thought of as calculating the "area under the curve", the function has to be somewhat 2-dimensional to have an area. In small \( \Delta x \), the integral can be approximated with a square
				\begin{equation}
					\int\limits_{x}^{x + \Delta x} p(x) \dd{x} \approx p(x) \Delta x
				\end{equation}
				By just looking at the square "withing" the mean \( p(\mu) \Delta x \), the \(\Delta x\) shrinks to \(0\) as the variance is zero.

				But the density function has to integrate to \(1\)!

				This way, we get:
				\begin{align}
					& 1 = \int\limits_{-\infty}^{+\infty} p(x) \dd{x} = \int\limits_{-\infty}^\mu p(x) \dd{x} + \int\limits_{\mu}^\mu p(x) \dd{x} + \int\limits_\mu^{+\infty} p(x) \dd{x} = \int\limits_{\mu}^\mu p(x) \dd{x} = \lim\limits_{\Delta x \to 0} p(\mu) \Delta x  \\
					\implies & p(\mu) \to \infty
				\end{align}
				to match the requirement.

				This can also be calculated with \( \lim\limits_{x \to 0} \frac{1}{x} \to \infty \).
			% end
		% end

		\subsection{Bayesian Estimation}
			\begin{itemize}
				\item In Bayesian estimation/learning of parametric distributions, it is assumed that parameters are not fixed, but are random variables too.
				\item This allows the usage of prior knowledge about the parameters.
			\end{itemize}

			The dependence on a prior can be formulated as a \emph{conditional probability} \( p(x \given X) \):
			\begin{equation}
				p(x \given X) = \int p(x, \theta \given X) \dd{\theta} \qquad p(x, \theta \given X) = p(x \given \theta, X) p(\theta \given X)
			\end{equation}
			As \( p(x) \) is fully determined by \(\theta\) (it is a \emph{sufficient statistic}), \( p(x \given \theta, X) = p(x \given \theta) \) holds. This way, the above equation can be simplified:
			\begin{equation}
				p(x \given X) = \int p(x \given \theta) p(\theta \given X) \dd{\theta}
			\end{equation}

			The probability \( p(\theta \given X) \) makes it explicit how the parameters depend on the data and can be calculated using Bayes theorem
			\begin{equation}
				p(\theta \given X) = \frac{p(X \given \theta) p(\theta)}{p(X)}
			\end{equation}
			with the prior \( p(\theta) \).

			If \( p(\theta \given X) \) is small for most \(\theta\), but large for a specific \( \hat{\theta} \), the probability density \( p(x \given X) \) can be estimated as
			\begin{equation}
				p(x \given X) \approx p(x \given \theta)
			\end{equation}
			this is called \emph{Bayes point}. The more uncertain the estimator is about \( \hat{\theta} \), the more the density is averaged across multiple \(\theta\).

			Problem: Most of the time it is impossible to integrate over \(\theta\) (or just do so numerically). Analytical solutions are rare.

			\subsubsection{Gaussian Bayesian Estimation}
				For a Gaussian distribution, there exists a closed form solution to estimate the density
				\begin{equation}
					p(\mu \given X) = \frac{p(X \given \mu) p(\mu)}{p(X)}
				\end{equation}
				when the variance of the data distribution is known and fixed with prior \( p(\mu) = \mathcal{\mu_0, \sigma_0^2} \).

				Then, with the sample mean \( \bar{x} = \frac{1}{N} \sum_{i = 1}^{N} x_i \), the parameters of the distribution \( p(\mu \given X) \sim \mathcal{N}\,(\mu_N, \sigma_N^2= \) can be estimated as
				\begin{equation}
					\mu_n = \frac{N\sigma_0^2 + \sigma^2 \mu_0}{N\sigma_0^2 + \sigma^2} \qquad \frac{1}{\sigma_N^2} = \frac{N}{\sigma^2} + \frac{1}{\sigma_0^2}
				\end{equation}
				where \(\sigma^2\) is the variance of the data distribution, \( (\mu_0, \sigma_0^2) \) are the parameters of the prior and \( (\mu_N, \sigma_n^2) \) are the parameters to be estimated.
			% end
		% end

		\subsubsection{Conjugate Priors}
			\begin{itemize}
				\item \emph{Conjugate priors} are prior distributions that do not change the distribution family of the posterior distribution family, i.e. they both lie in the same distribution family.
				\item Gaussians are conjugate to themselves, which yields elegant closed form solutions.
				\item In general, this is not the case which makes everything more complicated.
			\end{itemize}
		% end
	% end

	\section{Non-Parametric Models}
		\begin{itemize}
			\item Non-parametric models are useful if the underlying probability density distribution family is unknown.
			\item They are directly estimated from data, without an explicit parametric model.
			\item Every data point is a parameter, so non-parametric models have an uncertain and possibly infinite number of parameters.
			\item The biggest problem with most estimation models is the "too smooth vs. not smooth enough" problem.
			\item Note: All of the following examples use a dataset of \(10000\) data points that was generated by a mixture of two Gaussians \(
			(5, 10) \) and \( \mathcal{N}\,(10, 5) \), both equally weighted. which is plotted in red as the actual distribution.
		\end{itemize}

		\subsection{Histograms}
			\begin{itemize}
				\item \emph{Histograms} discretize the continuous feature space into discrete bins of data.
				\item They can be used for nearly every problem and can approximate any probability density arbitrarily well with the right data set.
				\item But it is a brute-force method.
				\item In high dimensional feature spaces, histograms become impractical because of the exponential increase of bins. They require exponentially much data. This is known as the \emph{curse of dimensionality}.
				\item The size of the bins is somewhat arbitrary.
			\end{itemize}

			\paragraph{Formally}
				The probability that a data point \(\vec{x}\) falls into Region \(R\) is measured as
				\begin{equation}
					P(\vec{x} \in R) = \int_R p(\vec{x}) \dd{\vec{x}}
				\end{equation}
				If \(R\) is sufficiently small with volume \(V\), \(p(\vec{x})\) is almost constant:
				\begin{equation}
					P(\vec{x} \in R) \approx p(\vec{x}) V
				\end{equation}
				If \(R\) is sufficiently large with volume \(V\):
				\begin{equation}
					P(\vec{x} \in R) = \frac{K}{N} \quad\implies\quad p(\vec{x}) \approx \frac{K}{NV}
				\end{equation}
				where \(N\) is the total number of data points and \(K\) is the number of points that fall into region \(R\).
			% end

			\paragraph{Example}
				Figure~\ref{fig:histogram} shows three histograms that are not smooth enough (bin size \(0.5\)), just right (bin size \(3\)) and too smooth (bin size \(20\)).

				\begin{figure}
					\centering
					\includegraphics{tmp-nonParametricModels-histogram.pdf}
					\caption{Histogram}
					\label{fig:histogram}
				\end{figure}
			% end
		% end

		\subsection{Kernel Density Estimation (KDE)}
			\emph{Kernel density estimation} (KDE) is a variation of "histograms" where \(V\) gets fixed and \(K\) is determined (i.e. count the data points that fall in a fixed hypercube).

			\subsubsection{Parzen Window}
				These hypercube is called \emph{Parzen window} in \(d\) dimensions with edge length \(h\). It has the following equations:
				\begin{align}
					H(\vec{u}) &=
						\begin{cases}
							1 & \abs{u_j} \leq \frac{h}{2}, j = 1, \cdots, d \\
							0 & \textrm{otherwise}
						\end{cases} \\
					V = \int H(\vec{u}) \dd{\vec{u}} &= h^d \\
					K(\vec{x}) &= \sum_{i = 1}^{N} H\big(\vec{x} - \vec{x}^{(i)}\big) \\
					\implies\qquad p(\vec{x}) \approx \frac{K(\vec{x})}{NV} &= \frac{1}{Nh^d} \sum_{i = 1}^{N} H\big(\vec{x} - \vec{x}^{(i)}\big)
				\end{align}

				\paragraph{Example}
					Figure~\ref{fig:kdeParzen} shows the estimated density distribution using kernel density estimation with a Parzen window.

					\begin{figure} \todo{PDE: KDE: Parzen impl. wrong}
						\centering
						\includegraphics{tmp-nonParametricModels-kde-parzen.pdf}
						\caption{Kernel Density Estimation (Parzen Window)}
						\label{fig:kdeParzen}
					\end{figure}
				% end
			% end

			\subsubsection{Gaussian Kernel}
				The Gaussian kernel uses a "soft" window in \(d\) dimensions with parameter \(h\) and gives smoother results than the Parzen window. Problem: Has infinite support and requires a lot of computation. It has the following equations:
				\begin{align}
					H(\vec{u}) &= \frac{1}{\big(\sqrt{2 \pi h^2}\big)^d} \exp\Bigg\{ -\frac{\lVert \vec{u} \rVert^2}{2h^2} \Bigg\} \\
					V = \int H(\vec{u}) \dd{\vec{u}} &= 1 \\
					K(\vec{x}) &= \sum_{i = 1}^{N} H\big(\vec{x} - \vec{x}^{(i)}\big) \\
					\implies\qquad p(\vec{x}) \approx \frac{K(\vec{x})}{NV} &= \frac{1}{N\big(\sqrt{2 \pi h^2}\big)^d} \sum_{i = 1}^{N} \exp\Bigg\{ -\frac{\lVert \vec{x} - \vec{x}^{(i)} \rVert^2}{2h^2} \Bigg\}
				\end{align}

				\paragraph{Example}
					Figure~\ref{fig:kdeGaussian} shows the estimated density distribution using kernel density estimation with a Gaussian kernel. The parameter \( h = 5 \) seems to fit the density the best at the first view while not being too noisy.

					\begin{figure}
						\centering
						\includegraphics{tmp-nonParametricModels-kde-gaussian.pdf}
						\caption{Kernel Density Estimation (Gaussian Kernel)}
						\label{fig:kdeGaussian}
					\end{figure}
				% end
			% end

			\subsubsection{Arbitrary Kernel}
				An arbitrary kernel has the following form and the kernel function \( k(\vec{u}) \) (which must have the properties \( k(\vec{u}) \geq 0 \) and \( \int k(\vec{u}) \dd{\vec{u}} = 1 \)):
				\begin{align}
					V &= h^d \\
					K(\vec{x}) &= \sum_{i = 1}^{N} k\Bigg( \frac{\lVert \vec{x} - \vec{x}^{(i)} \rVert^2}{h} \Bigg) \\
					\implies\qquad p(\vec{x}) \approx \frac{K(\vec{x})}{NV} &= \frac{1}{Nh^d} \sum_{i = 1}^{N} k\Bigg( \frac{\lVert \vec{x} - \vec{x}^{(i)} \rVert^2}{h} \Bigg)
				\end{align}
			% end

			\subsubsection{Common Kernel Comparison}
				All kernel methods have one problem in common: The kernel bandwidth \(h\) has to be selected properly.

				\subparagraph{Parzen Window}
				\begin{equation}
					k(u) =
						\begin{cases}
							1 & \abs{u} \leq \frac{1}{2} \\
							0 & \textrm{otherwise}
						\end{cases}
				\end{equation}
				\begin{itemize}
					\item Not very smooth results.
				\end{itemize}

				\subparagraph{Gaussian Kernel}
				\begin{equation}
					k(u) = \frac{1}{\sqrt{2\pi}} \exp\Bigg\{ -\frac{1}{2} u^2 \Bigg\}
				\end{equation}
				\begin{itemize}
					\item Problem: Kernel has infinite support and requires a lot of computation.
					\item But gives much smoother results than the Parzen window.
				\end{itemize}

				\subparagraph{Epanechnikov Kernel}
				\begin{equation}
					k(u) = \max\Bigg\{ 0,\quad \frac{3}{4} (1 - u)^2 \Bigg\}
				\end{equation}
				\begin{itemize}
					\item Smoother and has finite support.
				\end{itemize}
			% end
		% end

		\subsection{K-Nearest Neighbors (KNN)}
			\emph{Kernel density estimation} (KDE) is a variation of "histograms" where \(K\) gets fixed and \(V\) is determined (i.e. increase the size of a sphere until \(K\) data points fall into it). In general, KNN produces a pretty noisy density estimation and is very sensitive to small changes.

			\paragraph{Example}
				Figure~\ref{fig:knn} shows the estimated density distribution using K-nearest neighbors. The parameter \( K = 75 \) seems to fit the density the best at the first view while not being too noisy (of course it is still pretty noise, because it is KNN).

				\begin{figure}
					\centering
					\includegraphics{tmp-nonParametricModels-knn.pdf}
					\caption{K-Nearest Neighbors}
					\label{fig:knn}
				\end{figure}
			% end

			\subsubsection{Classification}
				Assume a data set with \(N\) points, where \(N_j\) is the number of points in class \(C_j\) and \( \sum_j N_j = N \). To classify a new point \(x\), draw a sphere around the point that contains \(K\) points (regardless which class they belong to). Let \(V\) be the volume of the sphere that contains \(K_j\) points of class \(C_j\).

				With Bayesian classification
				\begin{equation}
					P(C_j \given x) = \frac{P(x \given C_j) P(C_j)}{P(X)}
				\end{equation}
				this yields the solution
				\begin{equation}
					P(X) \approx \frac{K}{NV} \qquad P(x \given C_j) \approx \frac{K_j}{N_jV} \qquad P(C_j) \approx \frac{N_j}{N} \\
					\implies\qquad P(C_j \given x) \approx \frac{K_j}{N_jV} \frac{N_j}{N} \frac{NV}{K} = \frac{K_j}{K}
				\end{equation}
				This, with KNN, the posterior probability can be computed without the knowledge about how many data points are available and without an explicit influence of the sphere size.
			% end
		% end
	% end

	\section{Mixture Models}
		\emph{Mixture models} combine parametric and non-parametric models.

		The probability density \(p(x)\) of a mixture model can be described as
		\begin{equation}
			p(x) = \sum_{j = 1}^{M} p(x \given j) p(j)
		\end{equation}
		where \(M\) is the number of mixture components and \(p(j)\) is the probability (or \emph{weight}) of mixture component \(j\). These probabilities have to sum up to one.

		\subsection{Mixture of Gaussians}
			A \emph{mixture of Gaussians} (MoG) is one of the basic mixture models. It has the following form (where \( p(x \given j) \) is just another notation for \( p(x \given C_j) \), similar for other probability densities):
			\begin{align}
				p(x) &= \sum_{j = 1}^{M} p(x \given j) p(j) \\
				p(x \given j) &= \mathcal{N}\,(x \given \mu_j, \sigma_j^2) = \frac{1}{\sqrt{2\pi\sigma_j^2}} \exp\Bigg\{ -\frac{1}{2\sigma_j^2} (x - \mu_j)^2 \Bigg\} \\
				p(j) &= \pi_j, \quad 0 \leq \pi_j \leq 1, \quad \sum_{j = 1}^{M} \pi_j = 1
			\end{align}
			with the mixture parameters \( \theta = \{ \mu_1, \sigma_1^2, \pi_1, \cdots, \mu_M, \sigma_M^2, \pi_M \} \).

			Figure~\ref{fig:mog} shows the mixture of Gaussians that was used for the examples in the previous section that has the following properties:
			\begin{align}
				p(x \given 1) &= \mathcal{N}\,(x \given 5, 10) \\
				p(x \given 2) &= \mathcal{N}\,(x \given 30, 5) \\
				\pi_1 &= \pi_2 = \frac{1}{2} \\
				\implies\qquad p(x \given 5, 10, 0.5, 30, 5, 0.5) &= \frac{1}{2} \mathcal{N}\,(x \given 5, 10) + \frac{1}{2} \mathcal{N}\,(x \given 30, 5)
			\end{align}
			This example will be used for all further examples with a dataset with \(10000\) data points. The real mixture distribution is plotted in red.

			\begin{figure}
				\centering
				\includegraphics{tmp-mixtureModels-mog.pdf}
				\caption{Mixture of Gaussians}
				\label{fig:mog}
			\end{figure}

			\subsubsection{Maximum Likelihood Estimation}
				Applying MLE to a mixture of Gaussians
				\begin{align}
					&& \mathcal{L} &= \ln L(\theta) = \sum_{i = 1}^{N} \ln p(x_i \given \theta) & \\
					\implies && \nabla_\theta \mathcal{L} &= 0 & \\
					\implies && \mu_j = \frac{\sum_{j = 1}^{N} p(j \given x_j) x_j}{\sum_{j = 1}^{N} p(j \given x_j)}
				\end{align}
				gives a circular dependency through all estimators. Therefore, no analytical solution exist!

				\paragraph{Gradient Ascent}
					\begin{itemize}
						\item \emph{Gradient ascent} can be used to maximize the log-likelihood numerically.
						\item But it typically has a complex (nonlinear, circular) gradient.
						\item So the optimization of one Gaussian depends on all other components.
						\item Hard to compute!
					\end{itemize}
				% end

				\paragraph{Different Strategy}
					Split the data set into \emph{observes} and \emph{unobserved} (\emph{latent}) variables. Typically, \(x\) is observed and \(p(j \given x)\) is unobserved (the component that has generated an \(x\) is latent).

					\begin{itemize}
						\item If both the observed and the latent dataset are known (the \emph{complete} dataset), the maximum likelihood solution can be computed via
					\end{itemize}
					\begin{equation}
						\mu_j = \frac{\sum_{j = 1}^{N} p(j \given x_j) x_j}{\sum_{j = 1}^{N} p(j \given x_j)}
					\end{equation}
					\begin{itemize}
						\item If the distributions are known, the unobserved data can be inferred using Bayes decision rule.
						\item But if neither the latent dataset nor the distribution is known, an estimation of \(j\) is needed. This can be done using clustering.
					\end{itemize}
				% end
			% end
		% end

		\subsection{Estimation using Clustering}
			\subsubsection{Hard Assignments}
				\begin{itemize}
					\item Every points gets assigned a mixture label.
					\item No points gets "multiple" labels with probabilities. It is a 0-1 labeling (\emph{hard assignments}).
					\item The mixture components are then estimated using only this data.
				\end{itemize}

				\paragraph{Gaussians}
					If a guess about the distribution is available, but the unobserved data is not, the probabilities can be calculated for each mixture component:
					\begin{equation}
						p(j \given x) = \frac{p(x \given j) \pi_j}{\sum_{j = 1}^{M} p(x \given j) \pi_j}
					\end{equation}
				% end
			% end

			\subsubsection{Expectation Maximization (EM)} % TODO: PDE: EM: Intuition; slides: 6A.1, 6A.2, 6A.3, 6A.4, 6A.5, 6A.6, 6A.7, 6A.8, 6A.9
				Let  be the observed data and let  be the latent data, so the complete data is \( Z = (X, Y) \).
				\begin{itemize}
					\item The \emph{expectation maximization} (EM) algorithm is used to perform maximum likelihood estimation even if the data is incomplete.
					\item Idea: Estimate the latent variables and use the estimations to estimate the distribution parameters.
					\item In case of Gaussian mixtures, associate every data point to one of the mixture components.
				\end{itemize}

				\paragraph{Properties and Definitions}
					With the observed data \( X = \{ x_1, \cdots, x_N \} \), the unobserved data \( Y = \{ y_1, \cdots, y_N \} \) and the joint density
					\begin{equation}
						p(Z) = p(X, Y) = p(Y \given X) p(X)
					\end{equation}
					with parameters
					\begin{equation}
						p(Z \given \theta) = p(X, Y \given \theta) = p(Y \given X, \theta) p(X \given \theta)
					\end{equation}
					the incomplete and complete likelihood can be defined as:
					\begin{itemize}
						\item Incomplete Likelihood
					\end{itemize}
					\begin{equation}
						L(\theta \given X) = p(X \given \theta) = \prod_{i = 1}^{N} p(x_i \given \theta)
					\end{equation}
					\begin{itemize}
						\item Complete Likelihood
					\end{itemize}
					\begin{equation}
						L(\theta \given Z) = p(Z \given \theta) = p(Y \given X, \theta) p(X \given \theta) = \prod_{i = 1}^{N} p(y_i \given x_i, \theta) p(x_i \given \theta)
					\end{equation}
				% end

				\paragraph{Algorithm}
					\(Y\) is not known, but the current guess \( \theta^{(i - 1)} \) of the parameters \(\theta\) can be used to predict \(Y\). Formally, this is to compute the expected value of the complete log-likelihood given the data \(X\) and the current estimation \(\theta^{(i - 1)}\):
					\begin{equation}
						Q(\theta, \theta^{(i - 1)}) \coloneqq \E_Y\big( \ln p(X, Y \given \theta) \given X, \theta^{(i - 1)} \big) = \int p(y \given X, \theta^{(i - 1)}) \,\ln p(X, y \given \theta) \dd{y}
					\end{equation}
					Repetition: \(X\) and \(\theta^{(i - 1)}\) are fixed while \(Y\) and \(\theta\) are (random) variables.

					The algorithm then contains two steps:
					\begin{description}[leftmargin = 5cm]
						\item[E-Step (Expectation)] Compute \( p(y \given X, \theta^{(i - 1)}) \).
						\item[M-Step (Maximization)] Maximize the expected value of the log-likelihood to get the next estimation \( \theta^{(i)} \)
					\end{description}
					\begin{equation}
						\theta^{(i)} = \arg\max\limits_\theta Q(\theta, \theta^{(i - 1)})
					\end{equation}
				% end

				\paragraph{Formal Properties}
					\begin{itemize}
						\item The expected log-likelihood of the \(i\)-th iteration is at least as good as that of the \(i - 1\)-th iteration:
					\end{itemize}
					\begin{equation}
						Q(\theta^{(i)}, \theta^{(i - 1)}) \geq Q(\theta^{(i - 1)}, \theta^{(i - 1)})
					\end{equation}
					\begin{itemize}
						\item If this expectation is maximized w.r.t. \(\theta^{(i)}\), then the following holds:
					\end{itemize}
					\begin{equation}
						L(\theta^{(i)} \given X) \geq L(\theta^{(i - 1)} \given X)
					\end{equation}
					\begin{itemize}
						\item Thus, the incomplete log-likelihood increases in every iteration or at least stays the same.
						\item The incomplete log-likelihood is optimized (locally).
						\item In practice, the results depend highly on the initialization. A good initialization is crucial or EM might get stuck in local optima.
					\end{itemize}
				% end

				\paragraph{Gaussian Mixtures}
					\begin{itemize}
						\item In the special case of Gaussian mixtures, there exists a closed form solution.
						\item Also estimate the variance and the prior distribution over the mixture components.
						\item Algorithm~\ref{alg:emUniGaussian} shows the EM algorithm for univariate Gaussian mixture models, where \(M\) is the number of Gaussians and \(N\) is the amount of data.
						\item Figure~\ref{fig:emUniGaussian} shows the algorithm in action and shows the progress between the first and last iteration.
					\end{itemize}

					\begin{algorithm}
						Initialize \( \mu_1, \sigma_1, \pi_1, \cdots, \mu_M, \sigma_M, \pi_M \)

						\For{\( i = 1, \cdots, n \)}{
							\( \alpha_{kj} \gets p(j \given x_k) = \frac{\mathcal{N}\,(x_k \given \mu_j, \sigma_j^2) \pi_j}{\sum_{i = 1}^{M} \mathcal{N}\,(x_k \given \mu_i, \sigma_i^2) \pi_i} \)

							\( N_j \gets \sum_{i = 1}^{N} \alpha_{ij} \)

							\( \mu_j^{\textrm{new}} \gets \frac{1}{N_j} \sum_{i = 1}^{N} \alpha_{ij} x_i \)

							\( \sigma_j^{\textrm{new}} \gets \sqrt{ \frac{1}{N_j} \sum_{i = 1}^{N} \alpha_{ij} \big( x_i - \mu_j^{\textrm{new}} \big) } \)

							\( \pi_j^{\textrm{new}} \gets \frac{N_j}{N} \)
						}

						\Return \( \mu_1, \sigma_1, \pi_1, \cdots, \mu_M, \sigma_M, \pi_M \)

						\caption{EM for Univariate Gaussian}
						\label{alg:emUniGaussian}
					\end{algorithm}

					\begin{figure}
						\centering
						\includegraphics{tmp-mixtureModels-em-gaussian.pdf}
						\caption{EM for Univariate Gaussian}
						\label{fig:emUniGaussian}
					\end{figure}
				% end

				\paragraph{Derivation the Gaussian EM Algorithm}
					The "EM algorithm" itself is not really an algorithm, but instead a method to derive an EM algorithm for a probability distribution. This section covers the derivation of the EM algorithm for univariate Gaussian mixtures. The observed data is \( X = \{ x_1, \cdots, x_M \} \) and the latent data is \( Y = \{ y_1, \cdots, y_M \} \) where \(y_i\) denotes the mixture component a data point \(x_i\) belongs to.

					With discrete \(y_i\), the equation for \(Q(\theta, \theta^{(i - 1)})\) simplifies:
					\begin{equation}
						Q(\theta, \theta^{(i - 1)}) = \sum_{j = 1}^{M} p(y_j \given X, \theta^{(i - 1)}) \, \ln p(X, y_j \given \theta) = \sum_{j = 1}^{M} \sum_{n = 1}^{N} \underbrace{p(y_j \given x_n, \theta^{(i - 1)})}_{\coloneqq \alpha_{nj}} \, \ln p(x_n, y_j \given \theta)
					\end{equation}
					Now calculate \( \alpha_{nj} \):
					\begin{align}
						\alpha_{nj} &= p(y_j \given x_n, \theta^{(i - 1)}) \\
							&= \frac{p(x_n, \theta^{(i - 1)} \given y_j) p(x_n, \theta^{(i - 1)})}{p(y_j)} \\
							&\overset{\dagger}{=} \frac{p(x_n \given \theta^{(i - 1)}) p(\theta^{(i - 1)})}{p(y_j)} \\
							&= \frac{p(x_n \given \mu_j, \sigma_j^2) \pi_j}{\sum_{i = 1}^{M} p(x_n \given \mu_i, \sigma_i^2) \pi_i}
					\end{align}
					Step \(\dagger\) is possible because \( \theta^{(i - 1)} \) is a sufficient statistic and therefore fully determines the probability density.

					This yields the formula for the E-Step.

					To get the formula for the M-Step, insert \( \alpha_{nj} \) into \( Q(\theta^\textrm{new}, \theta^{(i - 1)}) \), simplify, take the derivatives w.r.t. \(\theta^\textrm{new}\) and set them to zero. Let \( N_j \coloneqq \sum_{n = 1}^{N} \alpha_{nj} \).
					\begin{align}
						&& Q(\theta^\textrm{new}, \theta^{(i - 1)}) &= \sum_{j = 1}^{N} \sum_{n = 1}^{N} \alpha_{nj} \ln p\big(x_n, y_j \given \theta^\textrm{new}\big) & \\
						\implies && Q_j &= \sum_{n = 1}^{N} \alpha_{nj} \ln p\big(x_n, y_j \given \mu_j^\textrm{new}, \sigma_j^\textrm{new}\big) & \\
						&& &= \sum_{n = 1}^{N} \alpha_{nj} \ln \frac{1}{\sqrt{2\pi\big(\sigma_j^\textrm{new}\big)^2}} - \alpha_{nj} \frac{1}{2\big(\sigma_j^\textrm{new}\big)^2} \big(x_n - \mu_j^\textrm{new}\big)^2 & \\
						\implies && \nabla_{\mu_j^\textrm{new}} Q_j &= -N_j\frac{1}{\big(\sigma_j^\textrm{new}\big)^2}\mu_j + \frac{1}{\big(\sigma_j^\textrm{new}\big)^2} \sum_{n = 1}^{N} \alpha_{nj} x_n & \\
						\implies && N_j\mu_j^\textrm{new} &= \sum_{n = 1}^{N} \alpha_{nj} x_n & \\
						\iff && \mu_j^\textrm{new} &= \frac{1}{N_j} \sum_{n = 1}^{N} \alpha_{nj} x_n & \\
						\implies && \nabla_{\sigma_j^\textrm{new}} Q_j &= \sum_{n = 1}^{N} -\alpha_{nj} \frac{1}{\sigma_j^\textrm{new}} + \alpha_{nj} \frac{1}{\big(\sigma_j^\textrm{new}\big)^3} \big(x_n - \mu_j^\textrm{new}\big)^2 & \\
						&& &= -\frac{1}{\sigma_j^\textrm{new}} N_j + \frac{1}{\big(\sigma_j^\textrm{new}\big)^3} \sum_{n = 1}^{N} \alpha_{nj} \big(x_n - \mu_j^\textrm{new}\big)^2 & \\
						\implies && \big(\sigma_j^\textrm{new}\big)^2 N_j &= \sum_{n = 1}^{N} \alpha_{nj} \big(x_n - \mu_j^\textrm{new}\big)^2 & \\
						\iff && \big(\sigma_j^\textrm{new}\big)^2 &= \frac{1}{N_j} \sum_{n = 1}^{N} \alpha_{nj} \big(x_n - \mu_j^\textrm{new}\big)^2 & \\
						\iff && \sigma_j^\textrm{new} &= \sqrt{\frac{1}{N_j} \sum_{n = 1}^{N} \alpha_{nj} \big(x_n - \mu_j^\textrm{new}\big)^2}
					\end{align}
					This yields the formula for the M-Step (with the prior calculation \( \pi_j = \frac{N_j}{N} \) as of the definition).

					The same technique can be applied to derive EM algorithms for other distributions (e.g. multivariate Gaussians). Notice that it might not yield a closed form solution, especially for distributions that are not part of the exponential family (they are kind of "easy" because the exponential disappears with the logarithm),
				% end
			% end
		% end

		\subsection{Mixture Components}
			\begin{itemize}
				\item The biggest problem with mixture models is: How many mixture components are needed? More lead to a better likelihood, but are not always better because of overfitting.
				\item There exist some heuristics for automatic selection:
					\begin{itemize}
						\item Find a \(K\) that maximizes the \emph{Akaike information criterion} \( \quad\ln p(X \given \theta_\textrm{ML}) - K\quad \) where \(K\) is the number of parameters.
						\item Or find a \(K\) that maximizes the \emph{Bayesian information criterion} \( \quad\ln p(X \given \theta_\textrm{ML}) - \frac{1}{2} K \ln N\quad \) where \(N\) is the number of data points.
					\end{itemize}
				\item Mixture models are much more general than just mixture of Gaussians, the components can even lie in different distribution families.
			\end{itemize}
		% end
	% end
	
	\section{Wrap-Up}
		\begin{itemize}
			\item Difference between parametric and non-parametric models
			\item The likelihood function and how to derive maximum likelihood estimators
			\item Bayesian estimation
			\item Different non-parametric models (histogram, KDE, KNN)
			\item Mixture models
			\item EM-algorithm
		\end{itemize}
	% end
% end

\chapter{Clustering}
	\emph{Clustering} is about to find meaningful groups of data points and find the group assignment. It is a type of unsupervised learning as no labeled data is needed. Clustering can be split into two different basic types:
	\begin{itemize}
		\item Agglomerative Clustering
			\begin{itemize}
				\item Each data point is made a different cluster.
				\item While the clustering is not satisfactory, the two clusters with the smallest inter-cluster distance are merged.
			\end{itemize}
		\item Divisive Clustering
			\begin{itemize}
				\item All data points lie in a single cluster.
				\item While the clustering is not satisfactory, split the cluster that yields the two components with the largest inter-cluster distance.
			\end{itemize}
	\end{itemize}
	Note: All of the following examples use a dataset of \(1000\) data points that was generated by a mixture of the two-dimensional multivariate Gaussians, plotted in figure~\ref{fig:clusteringExample}.

	\begin{figure}
		\centering
		\includegraphics{tmp-clustering-example.pdf}
		\caption{Cluster Gaussians}
		\label{fig:clusteringExample}
	\end{figure}

	\section{Mean Shift Clustering}
		\emph{Mean shift clustering} is a agglomerative clustering method for finding the modes (maxima) in a cloud of data points where the points are most dense using kernel density estimation and local search.
		\begin{itemize}
			\item The search path starts at different points and "climbs up the hills" (mean shift clustering is a hill climbing algorithm).
			\item Paths that converge at the same point get the same label.
		\end{itemize}

		The grand scheme is to start with a kernel density estimate
		\begin{equation}
			\hat{f}(\vec{x}) = \frac{1}{Nh^d} \sum_{i = 1}^{N} k\Bigg( \frac{\lVert \vec{x} - \vec{x}_i \rVert^2}{h^2} \Bigg)
		\end{equation}
		and then derive the mean shift procedure by taking the gradient of the kernel density estimate to calculate the mean shift \( \vec{m}_{h, g}(\vec{x}) \):
		\begin{equation}
			\vec{m}_{h, g}(\vec{x}) = \frac{\sum_{i = 1}^{N} g\Big( \frac{\lVert \vec{x} - \vec{x}_i \rVert^2}{h^2} \Big) \vec{x}_i}{\sum_{i = 1}^{N} g\Big( \frac{\lVert \vec{x} - \vec{x}_i \rVert^2}{h^2} \Big)} - \vec{x}
		\end{equation}
		where \( g(\vec{u}) = -k'(\vec{u}) \) and move into the direction \( \vec{x} \gets \vec{x} + \vec{m}_{h, g}(\vec{x}) \). Repeat this until convergence and repeat this for each data point. All points that converge to the same data point lie in one cluster.

		Algorithm~\ref{alg:meanShift} shows the mean shift algorithm in its basic form with a data set \( X = \{ \vec{x}1, \cdots, \vec{x}N \} \) and learning rate \(\alpha\).

		\begin{algorithm}
			\For{\( k = 1, \cdots, n \)}{
				\For{\( j = 1, \cdots, N \)}{
					\( \vec{m} \gets \frac{\sum_{i = 1}^{N} g\Big( \frac{\lVert \vec{x}_j - \vec{x}_i \rVert^2}{h^2} \Big) \vec{x}_i}{\sum_{i = 1}^{N} g\Big( \frac{\lVert \vec{x}_j - \vec{x}_i \rVert^2}{h^2} \Big)} - \vec{x}_j \)

					\( \vec{x}_j \gets \vec{x}_j + \alpha\vec{m} \)
				}
			}

			\caption{Mean Shift Clustering}
			\label{alg:meanShift}
		\end{algorithm}

		\paragraph{Example} \todo{Clustering: Mean Shift: Impl. wrong}
			Figure~\ref{fig:meanShiftExample} shows the mean shift clustering algorithm on a mixture of three bivariate Gaussian distributions, figure~\ref{fig:meanShiftExampleWay} shows the way each data point goes.

			\begin{figure}
				\centering
				\includegraphics{tmp-clustering-meanShift.pdf}
				\caption{Mean Shift Clustering}
				\label{fig:meanShiftExample}
			\end{figure}
			\begin{figure}
				\centering
				\includegraphics{tmp-clustering-meanShift-way.pdf}
				\caption{Mean Shift Clustering (Way)}
				\label{fig:meanShiftExampleWay}
			\end{figure}
		% end
	% end
	
	\section{Wrap-Up}
		\begin{itemize}
			\item Different algorithms for clustering
		\end{itemize}
	% end
% end

\chapter{Evaluation}
	The \emph{performance} for parameter estimation and classification has to be measured in order to compare them and to detect under-/overfitting.

	\section{Test Error vs. Training Error}
		\begin{itemize}
			\item The training error might be really low where the test error is really high. This is an indicator for overfitting.
			\item If both errors are large, the model seems to underfit.
			\item The model selection has to be done carefully!
		\end{itemize}
	% end

	\section{Bias and Variance}
		The \emph{bias} of an estimator \(\hat{\theta}\) is the expected derivation of the true parameter \(\theta\) (with a data set \(X\)):
		\begin{equation}
			\Bias\big(\hat{\theta}\big) = \E_X\big(\hat{\theta}(X) - \theta\big)
		\end{equation}
		If the expected value of an estimation differs from the true value, the estimator is called \emph{biased}. If not, is is called \emph{unbiased}. The \emph{variance} is the expected squared error between the estimator and the mean estimator:
		\begin{equation}
			\Var\big(\hat{\theta}\big) \coloneqq \E_X\bigg( \! \Big( \hat{\theta}(X) - \E_X\big(\hat{\theta}(X)\big) \Big)^2 \bigg)
		\end{equation}

		\subsection{MVUE and BLUE}
			\begin{itemize}
				\item An estimator with zero bias and minimum variance is called a \emph{minimum variance unbiased estimator} (MVUE).
				\item A MVUE that is linear in its features is called \emph{best linear unbiased estimator} (BLUE).
			\end{itemize}
		% end

		\subsection{Bias-Variance Tradeoff}
			\begin{itemize}
				\item In practice, an unbiased estimator with a small variance is wanted. But mostly, this is not possible.
				\item The bias represents the structural error whereas the variance represents the estimation error (finite data sets will always have variance).
				\item The expected total error is proportional to \( \textrm{Bias}^2 + \textrm{Variance} \). Typically not both can be minimized.
				\item The learning algorithm has to find the right tradeoff between bias and variance (simple enough to prevent overfitting and yet expressive enough to represent the important parts of the data).
				\item To ensure this, the algorithm has to be evaluated on the test data (see section \ref{sec:validation}).
			\end{itemize}
		% end

		\subsection{Example: MLE of a Gaussian} % 7.24, 7.25
			The following to sections will cover the calculation of the bias of the maximum likelihood estimators for a Gaussian distribution for \(\mu\) (called \(\hat{\mu}\)) and \(\sigma^2\) (called \(\hat{\sigma}^2\)).

			\paragraph{Mean (\(\mu\))}
				The estimator is given as
				\begin{equation}
					\hat{\mu}(X) = \frac{1}{N} \sum_{i = 1}^{N} x_i
				\end{equation}
				So the bias can be calculated as:
				\begin{align}
					\Bias\big(\hat{\mu}(X) - \mu\big) &= \E_X\big(\hat{\mu}(X) - \mu\big) \\
						&= \E_X\Bigg( \frac{1}{N} \sum_{i = 1}^{N} x_i \Bigg) - \mu \\
						&= \frac{1}{N} \sum_{i = 1}^{N} \E_X\big(x_i\big) - \mu \\
						&= \frac{1}{N} \Bigg( \sum_{i = 1}^{N} \mu \Bigg) - \mu \\
						&= \mu - \mu \\
						&= 0
				\end{align}
				So the MLE of the mean of a Gaussian is unbiased.
			% end

			\paragraph{Variance (\(\sigma^2\))}
				The estimator is given as
				\begin{equation}
					\hat{\sigma}^2(X) = \frac{1}{N} \sum_{i = 1}^{N} \big(x_i - \hat{\mu}\big)^2
				\end{equation}
				So the bias can be calculated as:
				\begin{align}
					\Bias\big(\hat{\sigma}^2(X) - \sigma^2\big) &= \E_X\big(\hat{\sigma}^2(X) - \sigma^2\big) \\
						&= \E_X\Bigg( \frac{1}{N} \sum_{i = 1}^{N} \big(x_i - \hat{\mu}\big)^2 \Bigg) - \sigma^2 \\
						&= \frac{1}{N} \sum_{i = 1}^{N} \E_X\Big(\big(x_i - \hat{\mu}\big)^2\Big) - \sigma^2 \\
						&= \frac{1}{N} \sum_{i = 1}^{N} \E_X\big( x_i^2 - 2 x_i \hat{\mu} + \hat{\mu}^2 \big) - \sigma^2 \\
						&= \frac{1}{N} \sum_{i = 1}^{N} \E_X\big( x_i^2 - \hat{\mu}^2 \big) - \sigma^2 \\
						&= \frac{1}{N} \sum_{i = 1}^{N} \E_X\big( x_i^2 \big) - \E_X\big( \hat{\mu}^2 \big) - \sigma^2 \\
						&= \E_X\big( x^2 \big) - \E_X\big( \hat{\mu}^2 \big) - \sigma^2 \\
					\intertext{With \( \sigma^2 = \E\big(x^2\big) - \E\big(x\big)^2 \) and \( \hat{\sigma}^2 = \E\big(\hat{\mu}^2\big) - \E\big(\hat{\mu}\big)^2 \) and \( \E\big(x\big) = \E\big(\hat{\mu}\big) \):}
						&= \Big(\sigma^2 + \E\big(x\big)^2\Big) - \Big(\hat{\sigma}^2 + \E\big(\hat{\mu}\big)^2\Big) - \sigma^2 \\
						&= \Big(\sigma^2 + \mu^2\Big) - \Big(\hat{\sigma}^2 + \mu^2\Big) - \sigma^2 \\
						&= -\hat{\sigma}^2 \\
						&= -\Var\Bigg( \frac{1}{N} \sum_{i = 1}^{N} x_i \Bigg) \\
						&= -\frac{1}{N^2} \Var\Bigg( \sum_{i = 1}^{N} x_i \Bigg) \\
						&= -\frac{1}{N^2} \sum_{i = 1}^{N} \Var(x_i) \\
						&= -\frac{1}{N} \Var(x) \\
						&= -\frac{1}{N} \sigma^2
				\end{align}
				So the MLE of the variance of a Gaussian is biased (slightly below the actual variance).
			% end
		% end

		\subsection{Example: Regression} % 7.26, 7.27
			\todo{Evaluation: Example: Regression}
		% end
	% end

	\section{Model Selection and Occam's Razor}
		\label{sec:validation}

		\begin{itemize}
			\item The essence of Occams's Razor is: Always choose the simplest model that matches the data. Simplest means the model with the smallest complexity (e.g. the polynomial with the lowest degree).
			\item Model selection is a complex task.
			\item The whole data set has to be split into multiple data sets to avoid overfitting and to get better estimation for the prediction error:
				\begin{enumerate}
					\item \emph{Training Set}   \tabto{3cm} Fit parameters.
					\item \emph{Validation Set} \tabto{3cm} Choose the model class or single parameters.
					\item \emph{Test Set}       \tabto{3cm} Estimate the prediction error of the trained model.
				\end{enumerate}
		\end{itemize}

		\subsection{Cross Validation}
			\label{sec:crossvalidation}

			\begin{itemize}
				\item During \emph{cross validation}, the whole data set \(\mathcal{D}\) is split into \(K\) data sets \( \mathcal{D}_\kappa \) and \(K - 1\) sets are used training and one data set is used for validation.
				\item This yields the following computations (where \(\mathcal{M}_j\) is a model):
			\end{itemize}
			\begin{align}
				\vec{\theta}_k (\mathcal{M}_j) &= \arg\min_{\vec{\theta} \,\in\, \mathcal{M}_j} \, \sum_{\kappa \,\neq\, k} \, \sum_{(\vec{x}_i, y_i) \,\in\, \mathcal{D}_\kappa} L_{f_{\vec{\theta}}} (\vec{x}_i, y_i) \\
				L_k(\mathcal{M}_j) &=  \sum_{(\vec{x}_i, y_i) \,\in\, \mathcal{D}_\kappa} L_{f_{\vec{\theta}}} (\vec{x}_i, y_i)
			\end{align}
			\begin{itemize}
				\item There exist multiple variations of cross validation:
					\begin{itemize}
						\item \emph{Exhaustive cross validation} \tabto{5cm} Try all partitioning possibilities. \\
							\tabto{5cm} \qquad \( \implies \) Computationally expensive.
						\item \emph{Bootstrap cross validation} \tabto{5cm} Randomly sample non-overlapping training/validation sets.
					\end{itemize}
			\end{itemize}
		% end

		\subsection{\(K\)-Fold Cross Validation}
			\begin{itemize}
				\item Randomly partition the data set into \(K\) data sets, select one for validation and repeat this \(K\) times, each with a different validation set.
				\item Compute the validation loss in each iteration and choose the model with the lowest average validation loss:
			\end{itemize}
			\begin{equation}
				\mathcal{M}^\ast = \arg\min_\mathcal{M} \frac{1}{K} \sum_{k = 1}^{K} L_k(\mathcal{M})
			\end{equation}
			\begin{itemize}
				\item \(L_k(\mathcal{M})\) is computed as defined in \ref{sec:crossvalidation}.
				\item \emph{Leave-one-out cross-validation (LOOCV):} \(K\) is set to \( K = N - 1 \), which yields a validation set size of \(1\).
			\end{itemize}
		% end

		\subsection{Machine Learning Cycle}
			Figure~\ref{fig:mlcycle} shows the general cycle of machine learning that can/must be repeated multiple times in order to get a good model.

			\begin{figure}
				\centering
				\begin{tikzpicture}[every node/.style = { draw, align = center, minimum width = 4cm, minimum height = 1.5cm }]
					\node (a) {Training Data};
					\node [below = 1 of a] (b) {Train the Algorithm};
					\node [below = 1 of b] (c) {Model};
					\node [below = 1 of c] (d) {Machine Learning \\ Algorithm};
					\node [right = 1 of d] (e) {Prediction};
					\node [right = 1 of b] (f) {Evaluate};
					\node [left = 1 of d] (g) {Input Data};

					\draw [->] (a) -- (b);
					\draw [->] (b) -- (c);
					\draw [->] (c) -- (d);
					\draw [->] (d) -- (e);
					\draw [->] (e) -- (f);
					\draw [->] (f) -- (b);
					\draw [->] (g) -- (d);
				\end{tikzpicture}
				\caption{Machine Learning Cycle}
				\label{fig:mlcycle}
			\end{figure}
		% end
	% end
	
	\section{Wrap-Up}
		\begin{itemize}
			\item Bias and variance of an estimator
			\item Bias-Variance tradeoff
			\item MVUE and BLUE
			\item Difference between unbiased and biased estimators
			\item Mimic test data evaluation using cross-validation
		\end{itemize}
	% end
% end

\chapter{Regression}
	\emph{Regression} is about to learn a mapping \( f : I \to O \), \( y = f(x; \theta) \) from input \(I\) to output \(O\) with the parameters \( \theta \in \Theta \). The parameters are what needs to be "learned" and \(f\) represents the model that is trained. In regression, in output space \(O\) is continuous, e.g. \( O = \R \) or \( O = \R^2 \) etc.

	In general, the training data is given as pairs of in- and output values \( \vec{x}_i, y_i \). This chapter will only cover the case \( y_i \in \R \), but in general \(y_i\) can have multiple dimensions. Let \( X \coloneqq \{\, \vec{x}_1, \cdots, \vec{x}_n \,\} \) and \( Y \coloneqq \{\, y_1, \cdots, y_n \,\} \) be the sets of the training input/output values.

	Note: All of the following examples are based in the true function \( f(x) = \sigma(x) \sin(x) \) with the sigmoid function \( \sigma(x) \) and 50 sampled data points with a noise of \( \mathcal{N}\,(0, 1) \). The true function is shown in figure~\ref{fig:regressionTrue}.
	
	\begin{figure}
		\centering
		\includegraphics{tmp-regression-truth.pdf}
		\caption{Regression: True Function}
		\label{fig:regressionTrue}
	\end{figure}

	\section{Linear Regression}
		In \emph{linear regression}, the function \(f\) to train is a linear function (called \emph{regressor})
		\begin{equation}
			y = \vec{x}^T \vec{w} + w_0
		\end{equation}

		\subsection{Least Squares Regression}
			\begin{itemize}
				\item The linear \( y_i = \vec{x}_i^T \vec{w} + w_0 \) gives \(n\) linear equation, one for each training data pair.
				\item With \( \hat{\vec{x}}_i \coloneqq \begin{bmatrix} \vec{x}_i \\ 1 \end{bmatrix} \) and \( \hat{\vec{w}} \coloneqq \begin{bmatrix} \vec{w} \\ w_0 \end{bmatrix} \) the regressor can be written as \( y_i = \hat{\vec{x}}_i \hat{\vec{w}} \).
				\item Using the matrices \( \hat{X} = \big[\, \hat{\vec{x}}_i, \cdots, \hat{\vec{x}}_n \,\big] \) and the vector \( \vec{y} = [\, y_1, \cdots, y_n \,] \) the complete problem can be summarized into one matrix-vector equation:
			\end{itemize}
			\begin{equation}
				\hat{X}^T \hat{\vec{w}} = \vec{y}
			\end{equation}
			This is an overdetermined linear equation system that therefore will most likely not yield a solution. So instead use least squares optimization and solve the (unbounded) optimization problem
			\begin{equation}
				\hat{\vec{w}} = \arg\min\limits_{\vec{w}} \, \big\lVert \hat{X} \vec{w} - \vec{y} \big\rVert^2
			\end{equation}
			which yields the solution
			\begin{equation}
				\hat{\vec{w}} = \Big( \hat{X} \hat{X}^T \Big)^{-1} \hat{X} \vec{y}
			\end{equation}
			using the left pseudo-inverse of \(\hat{X}\).

			\paragraph{Problems}
				\begin{itemize}
					\item LSR depends on the inversion of a \( D \times D \) matrix, where \(D\) is the dimension.
					\item Naive matrix version takes \( \mathcal{O}(D^3) \) and is numerically instable.
					\item As \(D\) grows, other methods like gradient descent have to be taken into account.
					\item LSR indirectly assumes that the targets are Gaussians!
				\end{itemize}
			% end

			\paragraph{Regularized Least Squares Regression}
				To regularize LSR, a \emph{regularization term} \(\lambda\) can be added to the estimator, yielding the following minimization objective:
				\begin{equation}
					\hat{\vec{w}} = \arg\min\limits_{\vec{w}} \, \frac{1}{2} \big\lVert \hat{X}^T \vec{w} - \vec{y} \big\rVert^2 + \frac{\lambda}{2} \lVert \vec{w} \rVert^2
				\end{equation}
				Solving this yields the following solution:
				\begin{equation}
					\hat{\vec{w}} = \Big( \hat{X} \hat{X}^T + \lambda I \Big)^{-1} \hat{X} \vec{y}
				\end{equation}

				\warning{The regularization term assumes that both the noise and the targets are Gaussian distributed!}
			% end
		% end
	% end

	\section{Generalized Linear Regression}
		\emph{Generalized linear regression} and can learn arbitrary polynomials that are nonlinear w.r.t. to input variables \(\vec{x}\). The regressor has the following general form:
		\begin{equation}
			y(\vec{x}) = \vec{w}^T \phi(\vec{x}) = \sum_{i = 0}^{M} w_i \phi_i(\vec{x})
		\end{equation}
		with the \emph{basis functions} \( \phi_i(\cdot) \). Also, \( \phi_0(\vec{x}) = 1 \) is assumed for every \(\vec{x}\).

		\begin{itemize}
			\item With basis functions like \( \phi(x) = \begin{bmatrix} 1 & x & x^2 & x^3 \end{bmatrix} \), the regressor is nonlinear w.r.t. \(\vec{x}\).
			\item But the model is still linear w.r.t. to the parameters \(\vec{w}\), so the learning methods for linear regression can still be applied for polynomial regression (with small adjustments).
			\item Note that higher polynomials can easily lead to massive overfitting!
		\end{itemize}
	
		Assuming a Gaussian distribution and using \( \vec{y} = \begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix} \) and \( \Phi = \begin{bmatrix} \phi\big(\vec{x}_1\big) & \cdots & \phi\big(\vec{x}_n\big) \end{bmatrix} \), the least squares solution for generalized linear regression is
		\begin{equation}
			\hat{\vec{w}} = \big( \Phi \Phi^T \big)^{-1} \Phi \vec{y}
		\end{equation}
		
		\paragraph{Example}
			This example used a nonlinear polynomial transformation \( \phi_d(\cdot) \) that contains all polynomials up to the \(d\)-th degree, i.e. for \( d = 2 \):
			\begin{equation}
				\phi(x) =
					\begin{bmatrix}
						1 \\
						x \\
						x^2
					\end{bmatrix}
			\end{equation}
			Figure~\ref{fig:regressionLsr} shows the results from three different polynomial degrees:
			\begin{itemize}
				\item \(d = 4\) which underfits,
				\item \(d = 7\) which fits just right and
				\item \(d = 20\) which massively overfits.
			\end{itemize}
		
			\begin{figure}
				\centering
				\includegraphics{tmp-regression-lsr.pdf}
				\caption{Regression: Least Squares, Underfitting}
				\label{fig:regressionLsr}
			\end{figure}
		% end
	% end

	\section{Maximum Likelihood Approach}
		\subsection{Probabilistic Regression}
			For probabilistic regression, two assumptions have to be made:
			\begin{enumerate}
				\item The target function values are generated by adding noise \(\epsilon\) to the function estimate:
			\end{enumerate}
			\begin{equation}
				y = f(\vec{x}, \vec{w}) + \epsilon
			\end{equation}
			\begin{enumerate} \setcounter{enumi}{1}
				\item The noise is a random variable that is Gaussian distributed (with the precision \( \beta \) and variance \( \beta^{-1} \)):
			\end{enumerate}
			\begin{equation}
				\epsilon \sim \mathcal{N}\,\big(0, \beta^{-1}\big) \quad\implies\quad p\big(y \given \vec{x}, \vec{w}, \beta\big) = \mathcal{N}\,\Big(y \given f\big(\vec{x}, \vec{w}\big), \beta^{-1}\Big)
			\end{equation}
			So \(y\) is now a random variable that is Gaussian distributed with the underlying probability distribution \( p\big(y \given \vec{x}, \vec{w}, \beta\big) \)!
		% end

		\subsection{Maximum Likelihood Regression}
			Probabilistic regression gives the possibility to use well-known procedures like MLE for regression, called \emph{maximum likelihood regression}.

			\paragraph{Conditional Likelihood}
				With the input data points \( X = \big[\, \vec{x}_1, \cdots, \vec{x}_n \,\big] \in \R^{D \times n} \) (dimensions \(D\)) and the output data points \( \vec{Y} = [\, y_1, \cdots, y_n \,] \), the \emph{conditional likelihood} can be formulated as (with the general linear regressor \( \vec{w}^T \phi\big(\vec{x}\big) \)):
				\begin{equation}
					p\big(\vec{y} \given X, \vec{w}, \beta\big) \,\overset{\textrm{i.i.d.}}{=}\, \prod_{i = 1}^{n} \mathcal{N}\,\Big(y_i \given f\big(\vec{x}_i, \vec{w}\big), \beta^{-1}\Big) \,\overset{\textrm{linear model}}{=}\, \prod_{i = 1}^{n} \mathcal{N}\,\Big(y_i \given \vec{w}^T \phi\big(\vec{x}_i\big), \beta^{-1}\Big)
				\end{equation}
				The maximum likelihood is approach is to maximize the conditional w.r.t. \(\vec{w}\) and \(\beta\).
			% end

			\paragraph{Maximization}
				Using \( \vec{y} = \begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix} \), \( \vec{w} = \begin{bmatrix} w_1 \\ \vdots \\ w_n \end{bmatrix} \) and \( \Phi = \begin{bmatrix} \phi\big(\vec{x}_1\big) & \cdots & \phi\big(\vec{x}_n\big) \end{bmatrix} \), \(\vec{w}\) and \(\beta\) can be estimated with the standard ML-estimation (take the derivative of the log-likelihood, set it to zero and solve for \(\vec{w}\) or \(\beta\), respectively). This yields the following estimators:
				\begin{align*}
					\vec{w}_\textrm{ML} &= \big(\Phi \Phi^T\big)^{-1} \Phi \vec{y} \\
					\beta_\textrm{ML}^{-1} &= \frac{1}{n} \sum_{i = 1}^{n} \Big(y_i - \vec{w}_\textrm{ML}^T \phi\big(\vec{x}_i\big)\Big)^2
				\end{align*}
			% end

			\paragraph{Properties}
				\begin{itemize}
					\item Maximum likelihood yields the same solution as squared errors, so LSR indirectly assumes that the targets are Gaussian distributed (not distribution free).
					\item But MLR can also estimate \(\beta\), identifying how certain the estimation is about the result (bigger \(\beta\) \(\to\) more certain as the variance \(\beta^{-1}\) gets less).
				\end{itemize}
			% end

			\paragraph{Example}
				This example used a nonlinear polynomial transformation \( \phi_d(\cdot) \) that contains all polynomials up to the \(d\)-th degree, i.e. for \( d = 2 \):
				\begin{equation}
					\phi(x) =
						\begin{bmatrix}
							1 \\
							x \\
							x^2
						\end{bmatrix}
				\end{equation}
				The following figures show the results for maximum likelihood regression, together with the standard deviation estimated with \( \beta_\textrm{ML}^{-1} \):
				\begin{itemize}
					\item Figure~\ref{fig:regressionMlUnderfit} used \( d = 4 \) which slightly underfits,
					\item Figure~\ref{fig:regressionMlRight} used \( d = 7 \) which seems to fit very good and finally
					\item Figure~\ref{fig:regressionMlOverfit} used \( d = 20 \) which overfits.
				\end{itemize}
				Of course the estimation for the curve itself is the same as with least squares, as the equation for computing it is identical.
				
				\begin{figure}
					\centering
					\includegraphics{tmp-regression-mlr-4.pdf}
					\caption{Regression: Maximum Likelihood, Underfitting}
					\label{fig:regressionMlUnderfit}
				\end{figure}
				\begin{figure}
					\centering
					\includegraphics{tmp-regression-mlr-7.pdf}
					\caption{Regression: Maximum Likelihood, Just Right}
					\label{fig:regressionMlRight}
				\end{figure}
				\begin{figure}
					\centering
					\includegraphics{tmp-regression-mlr-20.pdf}
					\caption{Regression: Maximum Likelihood, Overfitting}
					\label{fig:regressionMlOverfit}
				\end{figure}
			% end
		% end

		\subsection{Loss Functions}
			\begin{itemize}
				\item MLR yields a probability distribution \( p\big(y \given \vec{x}, \vec{w}, \beta\big) \) for the function value \(y\).
				\item To actually estimate the function value \(y_t\) for a new data points \(\vec{x}_t\), a \emph{loss function}
			\end{itemize}
			\begin{equation}
				L : \R \times R \to \R^+ : \Big(y_t, f\big(\vec{x}_t\big)\Big) \mapsto L\Big(y_t, f\big(\vec{x}_t\big)\Big)
			\end{equation}
			\begin{itemize}
				\item[] is needed.
				\item Then, the expected loss has to be minimized (w.r.t. \(f(\vec{x})\)):
			\end{itemize}
			\begin{equation}
				\E_{\vec{x}, y \sim p(\vec{x}, y)}(L) = \iint L\Big(y, f\big(\vec{x}\big)\Big) p\big(\vec{x}, y\big) \dd{\vec{x}} \dd{y}
			\end{equation}

			The simplest case is the squared loss function \( L\big(y, f(\vec{x})\big) = \big(y - f(\vec{x})\big)^2 \) which yields the solution
			\begin{equation}
				f(\vec{x}) = \E_{y \sim p(y \given \vec{x})} (y) = \E(y \given \vec{x})
			\end{equation}
			So, under the squared error, the \emph{optimal regression function} is just the mean of the posterior distribution \( p(y \given \vec{x}) \) (also called \emph{mean prediction}).

			For the generalized linear regression function, this is just the value of the function:
			\begin{equation*}
				f(\vec{x}) = \E(y \given x) = \vec{w}^T \phi(\vec{x})
			\end{equation*}
		% end
	% end

	\section{Bayesian Linear Regression}
		\begin{itemize}
			\item In \emph{Bayesian linear regression}, a prior is placed on the parameters \(\vec{w}\) to tame the instabilities and to reduce overfitting.
			\item As in all Bayesian interpretations, it is based on Bayes rule.
			\item Also, Bayesian linear regression no more produces a single value for \(\vec{w}\), but rather a probability distribution over the parameters.
			\item Idea: Put a Gaussian prior on \(\vec{w}\) with a spherical covariance matrix with precision \( \alpha \) (variance \(\alpha^{-1}\))
		\end{itemize}
		\begin{equation}
			\vec{w} \sim p(\vec{w} \given \alpha) = \mathcal{N}\,\big(\vec{w} \given \vec{0}, \alpha^{-1} I\big)
		\end{equation}
		\begin{itemize}
			\item[] the mean can be set to a different value, but zero makes the following computations easier.
			\item Using this prior, the posterior distribution becomes:
		\end{itemize}
		\begin{equation}
			p(\vec{w} \given X, y, \alpha, \beta) \propto p(\vec{y} \given X, \vec{w}, \beta) \, p(\vec{w} \given \alpha) = p(\vec{y} \given X, \vec{w}, \beta) \,\,\mathcal{N}\,\big(\vec{w} \given \vec{0}, \alpha^{-1} I\big)
		\end{equation}

		\subsection{Maximum A-Posteriori (MAP)}
			Using \emph{maximum a-posteriori estimation} (take the derivative of the log-posteriori, set it to zero and solve for the parameter of interest), \(\vec{w}\) can be estimated as
			\begin{equation}
				\vec{w}_\textrm{MAP} = \Bigg(\! \Phi\Phi^T + \frac{\alpha}{\beta} I \!\Bigg)^{-1} \Phi \vec{y}
			\end{equation}
			The prior has the effect that it regularizes the pseudo-inverse via the parameter \( \frac{\alpha}{\beta} \). This is also called \emph{ridge regression}.

			Sometimes, only the ridge parameter \(\lambda = \frac{\alpha}{\beta}\) is given, as known of the regularized least squares regression.
		% end

		\subsection{Full Bayesian Regression}
			\begin{itemize}
				\item The actual value of \(\vec{w}\) is not really a point of interest, as the goal is to predict a function value rather than a parameter value.
				\item The idea of full Bayesian regression is to remove \(\vec{w}\) by marginalizing over it:
			\end{itemize}
			\begin{equation}
				p\big(y_t \given \vec{x}_t, X, \vec{y}\big) = \int p\big(y_t, \vec{w} \given \vec{x}_t, X, \vec{y}\big) \dd{\vec{w}} = \int \underbrace{p\big(y_t \given \vec{w}, \vec{x}_t\big)}_\textrm{regression model} \, \underbrace{p\big(\vec{w} \given X, \vec{y}\big)}_\textrm{posterior} \dd{\vec{w}}
			\end{equation}
			\begin{itemize}
				\item[] where \( y_t \) is the predicted value, \( \vec{x}_t \) is the test input, \(X\) are the training data points and \(\vec{y}\) are the training function values.
				\item The marginalized probability distribution \( p\big(y_t \given \vec{x}_t, X, \vec{y}\big) \) is called the \emph{predictive distribution}.
				\item For Gaussian distributions, this is solvable in a closed form, leading to \emph{Gaussian processes}.
			\end{itemize}

			\paragraph{Gaussians}
				For Gaussians, the predictive distribution is given as
				\begin{equation}
					p\big(y_t \given \vec{x}_t, X, \vec{y}\big) = \mathcal{N}\,\Big(\vec{y}_t \given \mu\big(\vec{x}_t\big), \sigma^2\big(\vec{x}_t\big)\Big)
				\end{equation}
				with the parameters
				\begin{align}
					\mu\big(\vec{x}_t\big) &= \phi^T\big(\vec{x}_t\big) \, \Bigg( \frac{\alpha}{\beta} I + \Phi\Phi^T \!\Bigg)^{-1} \Phi^T \vec{y} \\
					\sigma^2\big(\vec{x}_t\big) &= \frac{1}{\beta} \phi^T\big(\vec{x}_t\big) \, \big( \alpha I + \beta\Phi\Phi^T \big)^{-1} \phi\big(\vec{x}_t\big)
				\end{align}
				So the mean and variance are state dependent!

				This leads to Gaussian processes (see section \ref{sec:gaussianProcesses}) getting more certain about the estimate as more data points are taken into account.
			% end

			\paragraph{Example}
				This example used a nonlinear polynomial transformation \( \phi_d(\cdot) \) that contains all polynomials up to the \(d\)-th degree, i.e. for \( d = 2 \):
				\begin{equation}
					\phi(x) =
							\begin{bmatrix}
							1 \\
							x \\
							x^2
						\end{bmatrix}
				\end{equation}
				The following figures show the results for full Bayesian regression with the noise parameter \( \alpha^{-1} = 1 \) and the prior \( \beta 0.01 \), together with the standard deviation.
				\begin{itemize}
					\item Figure~\ref{fig:regressionFb2} used \( n = 2 \) samples does not fit the function very well and has a very high variance,
					\item Figure~\ref{fig:regressionFb8} used \( n = 8 \) samples, fitting the function better with still a high variance and
					\item Figure~\ref{fig:regressionFb20} used \( n = 20 \) (all) samples fitting the function really well with high variance on both sides.
				\end{itemize}
				This is expected as regression gets more accurate the more data points are used. Note that in this example, the polynomial degree stayed fixed!
				
				\begin{figure}
					\centering
					\includegraphics{tmp-regression-fb-2.pdf}
					\caption{Regression: Full Bayesian Regression (2 Samples)}
					\label{fig:regressionFb2}
				\end{figure}
				\begin{figure}
					\centering
					\includegraphics{tmp-regression-fb-8.pdf}
					\caption{Regression: Full Bayesian Regression (8 Samples)}
					\label{fig:regressionFb8}
				\end{figure}
				\begin{figure}
					\centering
					\includegraphics{tmp-regression-fb-20.pdf}
					\caption{Regression: Full Bayesian Regression (All Samples)}
					\label{fig:regressionFb20}
				\end{figure}
			% end
		% end
	% end

	\section{Kernel Regression}
		A \emph{kernel} is an inner product of feature vectors (or feature transformations)
		\begin{equation}
			K(\vec{x}, \vec{y}) = \phi(\vec{x})^T \phi(\vec{y})
		\end{equation}
		which is symmetric (\( K(\vec{x}, \vec{y}) = K(\vec{y}, \vec{x}) \)).
		
		Example kernels:
		\begin{itemize}
			\item Stationary kernels:  \tabto{3cm} \( K(\vec{x}, \vec{y}) = \hat{K}(\vec{x} - \vec{y}) \)
			\item Linear kernel:       \tabto{3cm} \( K(\vec{x}, \vec{y}) = \vec{x}^T \vec{y} \)
			\item Homogeneous kernels: \tabto{3cm} \( K(\vec{x}, \vec{y}) = \hat{K}\big( \lvert \vec{x} - \vec{y} \rvert \big) \)
		\end{itemize}
	
		Working with kernels instead of handcrafted features has a lot of advantages:
		\begin{itemize}
			\item Can work entirely in the features space with the help of kernels.
			\item Regression can even consider infinite feature spaces (e.g. with the Gaussian RBF kernel).
			\item Many algorithms can be derived from the dual representation.
			\item Many old problems of RBFs (how many kernels, which metric, etc.) can be solved in a principled way.
		\end{itemize}
		But: Kernel regression requires the inversion of a \( N \times N \) matrix, where \(N\) is the number of samples. This can be very costly!
		
		\paragraph{Example}
			Figure~\ref{fig:regressionKernelA} shows kernel regression with an RBF kernel with the bandwidth \( \sigma^2 = 0.01 \), performing not so good. Figure~\ref{fig:regressionKernelB} also uses an RBF kernel but with the bandwidth \( \sigma^2 = 1 \), performing much better. Both are using a ridge parameter \( \lambda 0.01 \).
			
			\begin{figure}
				\centering
				\includegraphics{tmp-regression-kernel-RBF-1.pdf}
				\caption{Regression: Kernel Regression (RBF, \( \sigma^2 = 0.01 \))}
				\label{fig:regressionKernelA}
			\end{figure}
			\begin{figure}
				\centering
				\includegraphics{tmp-regression-kernel-RBF-100.pdf}
				\caption{Regression: Kernel Regression (RBF, \( \sigma^2 = 1 \))}
				\label{fig:regressionKernelB}
			\end{figure}
		% end

		\subsection{Dual Representation of Regression}
			The primal formulation for (regularized) regression is
			\begin{equation}
				J(\vec{w}) = \frac{1}{2} \sum_{i = 1}^{N} \big( \vec{w}^T \phi(\vec{x}_i) - y_i \big)^2 + \frac{\lambda}{2} \vec{w}^T \vec{w}
			\end{equation}
			taking the gradient w.r.t. \( \vec{w} \), setting it to zero and solving for \(\vec{w}\):
			\begin{align}
				&& \frac{\partial J(\vec{w})}{\partial \vec{w}} &= \sum_{i = 1}^{N} \big(\vec{w}^T \phi(\vec{x}_i) - y_i\big) \phi(\vec{x}_i) + \lambda \vec{w} \overset{!}{=} 0 & \\
				\implies && \vec{w} &= -\frac{1}{\lambda} \sum_{i = 1}^{N} \big( \vec{w}^T \phi(\vec{x}_i) - y_i \big) \phi(\vec{x}_i) = \sum_{i = 1}^{N} a_i \phi(\vec{x}_i) = \Phi^T \vec{a}
			\end{align}
			with \( \Phi = \begin{bmatrix} \phi(\vec{x}_1)^T & \cdots & \phi(\vec{x}_N)^T \end{bmatrix} \in \R^{N \times D} \). Thus, \(\vec{w}\) is a linear combination of the features \(\phi(\vec{x}_i)\)! The dual representation then focuses on solving for \(\vec{a}\), not \(\vec{w}\).
			
			Inserting this into the cost function yields the dual formulation:
			\begin{align}
				&& J(\vec{w}) &= \frac{1}{2} \sum_{i = 1}^{N} \big( \vec{w}^T \phi(\vec{x}_i) - y_i \big)^2 + \frac{\lambda}{2} \vec{w}^T \vec{w} & \\
				\implies && \tilde{J}(\vec{w}) &= \frac{1}{2} \sum_{i = 1}^{N} \big( \vec{a}^T \Phi \phi(\vec{x}_i) - y_i \big)^2 + \frac{\lambda}{2} \vec{a}^T \Phi \Phi^T \vec{a} & \\
					&& &= \frac{1}{2} \sum_{i = 1}^{N} \big( \vec{a}^T \Phi \phi(\vec{x}_i) \phi(\vec{x}_i)^T \Phi^T \vec{a} - 2 \vec{a}^T \Phi \phi(\vec{x}_i)y_i + y_i^2 \big) + \frac{\lambda}{2} \vec{a}^T \Phi \Phi^T \vec{a} & \\
					&& &= \frac{1}{2} \sum_{i = 1}^{N} \vec{a}^T \Phi \phi(\vec{x}_i) \phi(\vec{x}_i)^T \Phi^T \vec{a} - \sum_{i = 1}^{N} \vec{a}^T \Phi \phi(\vec{x}_i)y_i + \frac{1}{2} \sum_{i = 1}^{N} y_i^2 + \frac{\lambda}{2} \vec{a}^T \Phi \Phi^T \vec{a} & \\
					&& &= \frac{1}{2} \vec{a}^T \Phi \Phi^T \Phi \Phi^T \vec{a} - \vec{a}^T \Phi \Phi^T \vec{y} + \frac{1}{2} \vec{y}^T \vec{y} + \frac{\lambda}{2} \vec{a}^T \Phi \Phi^T \vec{a} & \\
				\intertext{Let \( \hat{K} \coloneqq \Phi\Phi^T \) be the \emph{Gram matrix} with \( \hat{K} \coloneqq K(\vec{x}_i, \vec{x}_j) \):}
				&& &= \frac{1}{2} \vec{a}^T \hat{K} \hat{K} \vec{a} - \vec{a}^T \hat{K} \vec{y} + \frac{1}{2} \vec{y}^T \vec{y} + \frac{\lambda}{2} \vec{a}^T \hat{K} \vec{a} & \\
			\end{align}
			Now solve the dual problem for \(\vec{a}\) by taking the derivative and set it to zero:
			\begin{align}
				&& \tilde{L}(\vec{a}) &= \frac{1}{2} \vec{a}^T \hat{K} \hat{K} \vec{a} - \vec{a}^T \hat{K} \vec{y} + \frac{1}{2} \vec{y}^T \vec{y} + \frac{\lambda}{2} \vec{a}^T \hat{K} \vec{a} & \\
				\implies && \frac{\partial \tilde{L}(\vec{a})}{\partial \vec{a}} &= \hat{K} \hat{K} \vec{a} - \hat{K} \vec{y} + \frac{\lambda}{2} \hat{K} \vec{a} = \hat{K} (\hat{K}\vec{a} - \vec{y} + \lambda\vec{y}) \overset{!}{=} 0 & \\
				\implies && \vec{a} &= (\hat{K} + \lambda I)^{-1} \vec{y}
			\end{align}
			As of the definition of the Gram matrix \(\hat{K}\), is is positive semi-definite, thus \(\hat{K}^{-1}\) exists.
			
			A prediction can then be computed as
			\begin{equation}
				y(\vec{x}) = \vec{w}^T \phi(\vec{x}) + b = \vec{a}^T \Phi \phi(\vec{x}) = \hat{k}(\vec{x})^T (\hat{K} + \lambda I)^{-1} \vec{y}
			\end{equation}
			with \( \hat{k}(\vec{x}) = \begin{bmatrix} K(\vec{x}, \vec{x}_1) & \cdots & K(\vec{x}, \vec{x}_N) \end{bmatrix}^T \).
			
			So all computations can be expressed in terms of the kernel function!
		% end

		\subsection{Useful Kernels}
			\paragraph{Polynomial Kernels of Degree \(d\)}
				\begin{equation}
					K(\vec{x}, \vec{y}) = (\vec{x}^T \vec{y})^d
				\end{equation}
			% end
			
			\paragraph{Gaussian Kernel}
				Also known as \emph{Radial Basis Function} (RBF).
				\begin{equation}
					K(\vec{x}, \vec{y}) = \exp \Bigg\{ -\frac{\lVert \vec{x} - \vec{y} \rVert^2}{2\sigma^2} \Bigg\}
				\end{equation}
				This kernel has a feature space with infinity number of radial functions!
				
				The parameter \( \sigma^2 \) is the \emph{bandwidth} of the kernel. This is basically the inverse of the "variance" of the kernel and a measure of similarity. If it is small, far away points will be considered similar while if it is big, nearby points will be considered more similar.
			% end
		% end
	% end

	\section{Gaussian Processes Regression}
		\label{sec:gaussianProcesses}
	
		A \emph{Gaussian process} (GP) is a probability distribution over functions \(y(\vec{x})\) such that any finite set of function values evaluates at some input is jointly Gaussian distributed. A Gaussian process is fully specified by the 2nd order statistics (mean and covariance).
		\begin{itemize}
			\item The \emph{prior mean function} is the expected function before observing any data,
			\item The \emph{covariance function} encodes some structural assumptions (e.g. smoothness) (e.g. multivariate Gaussian kernel).
		\end{itemize}
		Thus, a GP is fully defined by
		\begin{align}
			\E(\vec{y}) &= \E(\Phi\vec{w}) = \Phi \E(\vec{w}) = 0 \\
			\E\big( y(\vec{x}_1) y(\vec{x}_j) \big) &= K(\vec{x}, \vec{y})
		\end{align}

		\subsection{Regression}
			Assume the generative model to have some noise \(\epsilon\) that is Gaussian distributed with \( \epsilon \sim \mathcal{N}\,(0, \beta^{-1}) \):
			\begin{equation}
				t_i = y(\vec{x}_i) + \epsilon
			\end{equation}
			This makes \(y\) a random variable that is also Gaussian distributed with
			\begin{equation}
				p(t_i \given y_i) = \mathcal{N}\,(t_i \given y_i, \beta^{-1})
			\end{equation}
			The kernel function that determines \(K\) is typically chosen to express the property that, for similar points \( \vec{x}_i \) and \( \vec{x}_j \), the corresponding values \( y(\vec{x}_i) \) and \( y(\vec{x}_j) \) will be more strongly correlated that for dissimilar points. The definition of similarity highly depends on the application.
		% end

		\subsection{Function Value Prediction}
			\begin{itemize}
				\item Prior over functions (GP): \( p(y) \)
				\item Likelihood (measurement/noise model): \( p(t \given y) \)
				\item Posterior over function via Bias theorem:
			\end{itemize}
			\begin{equation}
				p(y \given t) = \frac{p(t \given y) \, p(y)}{p(t)}
			\end{equation}
			
			Given a training set \( \vec{t}_n = \begin{bmatrix} t_1 & \cdots & t_n \end{bmatrix}^T \) with corresponding \( \vec{x}_1, \cdots, \vec{x}_n \), the goal is to predict a new \( t_{n + 1} \) for \( x_{n + 1} \).
			
			Approach: Evaluate the predictive distribution
			\begin{equation}
				p(t_{n + 1} \given x_{n + 1}, t_{1:n}, x_{1:n})
			\end{equation}
			Remember that GP assumes that \( p(t_1, \cdots, t_n, t_{n + 1}) \) is jointly Gaussian distributed, so the predictive distribution is also Gaussian distributed.
			
			Assume that \(\vec{x}\) is Gaussian distributed and it can be partitioned into two disjoint subsets \( \vec{x}_a \) and \( \vec{x}_b \), the distribution can be rewritten in term of the mean and the covariance matrix of \(\vec{x}_a\) and \(\vec{x}_b\):
			\begin{align}
				p(\vec{x}) &= \mathcal{N}\,(\vec{x} \given \vec{\mu}, \Sigma) \\
					\vec{x} &= \begin{bmatrix} \vec{x}_a \\ \vec{x}_b \end{bmatrix} \quad \vec{\mu} = \begin{bmatrix} \vec{\mu}_a \\ \vec{\mu}_b \end{bmatrix} \quad \Sigma = \begin{bmatrix} \Sigma_{aa} & \Sigma_{ab} \\ \Sigma_{ba} & \Sigma_{bb} \end{bmatrix}
			\end{align}
			The conditional distribution is also Gaussian:
			\begin{align}
				p(\vec{x}_a \given \vec{x}_b) &= \mathcal{N}\,(\vec{x}_a \given \vec{\mu}_{a \given b}, \Sigma_{a \given b}) \\
					\vec{\mu}_{a \given b} &= \vec{\mu}_a + \Sigma_{ab} \Sigma_{bb}^{-1} (\vec{x}_b - \vec{\mu}_b) \\
					\Sigma_{a \given b} &= \Sigma_{aa} - \Sigma_{ab} \Sigma_{bb}^{-1} \Sigma_{ba}
			\end{align}
			
			Thus, the predictive distribution can be expressed as:
			\begin{align}
				p(\vec{t}_{n + 1}) &= \mathcal{N}\,(\vec{t}_{n + 1} \given 0, C_{n + 1}) \\
				C_{n + 1} &=
					\begin{bmatrix}
						C_n & \vec{k} \\
						\vec{K} & c
					\end{bmatrix} \\
				\vec{k} &=
					\begin{bmatrix}
						K(\vec{x}_1, \vec{x}_{n + 1}) \\
						\vdots \\
						K(\vec{x}_n, \vec{x}_{n + 1})
					\end{bmatrix} \\
				c &= K(\vec{x}_{n + 1}, \vec{x}_{n + 1}) + \beta^{-1}
			\end{align}
			
			Yielding the following prediction equations:
			\begin{align}
				m(\vec{x}_{n + 1}) &= \vec{K}^T C_N^{-1} \vec{t} \\
				\sigma^2(\vec{x}_{n + 1}) &= c - \vec{K}^T C_N^{-1} \vec{K}
			\end{align}
			This gives an estimation for the function value as well as gauges the uncertainty about that estimate!
			
			\paragraph{Example}
				\todo{Reg: GP: Implementation}
			% end
		% end

		\subsection{Conclusion}
		    \begin{itemize}
		    	\item The computational complexity for building the model is \( \mathcal{O}(N^3) \) and for predicting one function value it is \(\mathcal{O}(N^2)\) (for the variance).
		    	\item The key advantage of GPR is that it is non-parametric and probabilistic.
		    	\item Naive implementations can deal with 10\,000 to 20\,000 data points, while advanced methods (e.g. sparse GPs) can deal with far more than 50\,000 data points.
		    	\item Hyperparameter (parameters for the kernel/covariance function) optimization is really important, e.g. for the squared-exponential kernel:
		    \end{itemize}
		    \begin{equation}
			    K(\vec{x}, \vec{y}) = \sigma_f^2 \exp \Bigg\{ -\frac{(\vec{x} - \vec{x})^2}{2l^2} \Bigg\} + \sigma_n^2 \delta_{ij}
		    \end{equation}
			\begin{itemize}
				\item[] where \( \sigma_f^2 \) is the signal variance, \(l\) is the length-scale and \(\sigma_n^2\) is the noise variance.
				\item Gaussian processes are Bayesian approaches to regression with (possible infinity) feature spaces.
				\item The resulting prediction equations are straightforward obtained in a closed form because of Gaussian properties.
				\item The hyperparameter optimization is more complex and expensive.
				\item While it is very computationally expensive, it is one of the most used approaches to statistical learning for regression.
			\end{itemize}
		% end
	% end
	
	\section{Wrap-Up}
		\begin{itemize}
			\item Formulation of a linear regression problem
			\item Different methods to perform linear regression (least squares, maximum likelihood, Bayesian)
			\item Derivation of the equations for different methods
			\item Influence of a prior distribution over the parameters to overfitting
			\item Kernels, construction and benifits
			\item Derivation of the dual formulation and pros/cons
			\item Gaussian processes and made assumptions
			\item GP closed form
			\item Regression with GPs yields mean and variance
			\item Kernels to not scale with data
		\end{itemize}
	% end
% end

\chapter{Classification}
	In \emph{classification}, the goal is to find a mapping \( f : I \to O \) that maps the input space \(I\) onto a discrete (and mostly finite) output space \(O\), called \emph{classes}.

	As seen in Bayesian decision theory, this breaks down into finding the a-posteriori probability (posterior) of the class \(C_k\) given an observation (feature) \(x\)
	\begin{equation}
		p(C_k \given x) = \frac{p(x \given C_k) \, p(C_k)}{p(x)} = \frac{p(x \given C_k) \, p(C_k)}{\sum_j p(x \given C_j) \, p(C_j)}
	\end{equation}
	and then device for class \(k\) iff \( p(C_k \given x) > p(C_l \given x) \) for all \( l \neq k \). A classifier that obeys this rule is called a \emph{Bayes optimal classifier}. See chapter~\ref{c:bayesianDecisionTheory} for more details.

	Note: All of the following examples use a dataset of \(250\) data points per class that were generated by a mixture of two-dimensional multivariate Gaussians, plotted in figure~\ref{fig:classificationExampleSeparable} (for linearly separable data).

	\begin{figure}
		\centering
		\includegraphics{tmp-classification-separable.pdf}
		\caption{Classification: Example Data (Linear Separable)}
		\label{fig:classificationExampleSeparable}
	\end{figure}

	\section{Generative vs. Discriminative}
		There are essentially two different views to solve the classification problem:
		\begin{description}
			\item[Generative] Model the class-condition distributions \( p(x \given C_k) \) and use Bayes rule and some prior to compute the class posterior.
			\item[Discriminative] Model the class posterior \( p(C_k \given x) \) directly, e.g. by separating the data points using a function. These types of models only care about getting the classification right and not whether the class-conditional fits well.
		\end{description}
	% end

	\section{Discriminant Functions}
		\begin{itemize}
			\item \emph{Discriminant functions} model the decision boundary and directly without modeling the densities while still minimizing the error probability.
			\item In comparison with generative models, discriminative models have the advantage that they are not so sensitive to outliers which the class-conditional-based have to consider when estimating the class-condition distribution, even if they do not matter at the end.
				\begin{itemize}
					\item This reduces the complexity of the overall model once the model has learned where to place the decision boundary.
					\item This shall not mean that such classifiers are inherently superior to probabilistic ones (e.g. they cannot take priors into account)!
				\end{itemize}
			\item For two classes, decide for class \(C_1\) iff \( y_1(x) > y_2(x) \). This is equivalent to defining a function \( y(x) = y_1(x) - y_2(x) \) and decide for class \(C_1\) iff \( y(x) > 0 \).
			\item Some discriminant functions are directly given from a Bayes classifier:
		\end{itemize}
		\begin{align}
			y_k(x) &= p(C_k \given x) \\
			y_k(x) &= p(x \given C_k) \, p(C_k) \\
			y_k(x) &= \ln\big(p(x \given C_k)\big) + \ln\big(p(C_k)\big)
		\end{align}
		\begin{itemize}
			\item[] The logarithm in the last step is applicable because \(\ln(x)\) is a strictly rising function and, for distributions of the exponential family, drastically reduces the computational overhead and thus reduces numeric instabilities.
		\end{itemize}

		\subsection{Multiple Classes}
			\label{sec:classificationMultiple}

			\begin{itemize}
				\item Normal multi-class classifiers based on binary (two-class) decisions may lead to ambiguities ("regions of uncertainty").
				\item A better solution is to have multiple discriminant functions \( y_1, \cdots, y_k \) and choose \( C_k \) iff \( y_k(x) > y_l(x) \) for all \( l \neq k \).
				\item Using this decision rule and linear discriminant functions, the decision regions are connected and convex which removes the "regions of uncertainty" and the ambiguities.
			\end{itemize}
		% end

		\subsection{Linear Discriminant Functions}
			\begin{itemize}
				\item In \emph{linear discriminant functions}, the decision boundaries are hyperplanes defined by a linear function
			\end{itemize}
			\begin{equation}
				y(\vec{x}) = \vec{w}^T \vec{x} + w_0
			\end{equation}
			\begin{itemize}
				\item[] where \(\vec{w}\) is the normal vector and \(w_0\) is the offset.
			\end{itemize}

			\subsubsection{Linear Separability}
				\begin{itemize}
					\item Not all data points may be separable by a linear function (e.g. if the data points overlap).
					\item Figure~\ref{fig:linearSeparability} shows two cases where the first is linearly separable and the second is not.
				\end{itemize}

				\begin{figure}
					\centering
					\begin{tikzpicture}
						\begin{axis}[
									name = plot1,
									scatter/classes = { a={ mark = o, draw = red }, b={ mark = o, draw = blue } },
									title = Linearly Separable
								]
							\addplot [scatter, scatter src = explicit symbolic, mark = none] table {
								x	y
								2	4.5
								4	0
							};
							\addplot [scatter, only marks, scatter src = explicit symbolic] table[meta = label] {
								x	y	label
								1	1	a
								1.5	0.5	a
								1.5	1.5	a
								2.2	2.5	a
								2	2	a
								2.5	1.6	a
								2.1	0.2	a
								2.2	1	a
								3.2	3	b
								3.5	2.5	b
								3.5	3.5	b
								4.2	4.5	b
								4	4	b
								4.5	3.6	b
								4.1	2.2	b
								4.2	3	b
							};
						\end{axis}
						\begin{axis}[
									at = (plot1.right of south east),
									anchor = left of south west,
									xshift = 0.5cm,
									scatter/classes = { a={ mark = o, draw = red }, b={ mark = o, draw = blue } },
									title = Not Linearly Separable
								]
							\addplot [scatter, only marks, scatter src = explicit symbolic] table[meta = label] {
								x	y	label
								1	1	a
								1.5	0.5	a
								1.5	1.5	a
								2.2	2.5	a
								2	2	a
								2.5	1.6	a
								2.1	0.2	a
								2.2	1	a
								1.2	3	b
								1.5	2.5	b
								1.5	3.5	b
								2.2	4.5	b
								2	4	b
								2.5	3.6	b
								2.1	2.2	b
								2.2	3	b
							};
						\end{axis}
					\end{tikzpicture}
					\caption{Linear Separability}
					\label{fig:linearSeparability}
				\end{figure}
			% end
		% end
	% end

	\section{Fisher Discriminant Analysis}
		\subsection{Least Squares Classification}
			\begin{itemize}
				\item In \emph{least squares classification}, the discriminant function shall output the values \( y(\vec{x}) = +1 \) or \( y(\vec{x}) = -1 \), indicating that \(\vec{x}\) belongs to class \(C_1\) or class \(C_2\), respectively.
				\item With training data inputs \( X = \{\, \vec{x}_1 \in \R^d, \cdots, \vec{x}_n \,\} \) and training outputs \( Y = \{\, y_1 \in \{\, +1, -1 \,\}, \cdots, y_n \,\} \), this yields an overdetermined equation system
			\end{itemize}
			\begin{equation}
				y_i = \vec{w}^T \vec{x}_i + w_0
			\end{equation}
			\begin{itemize}
				\item[] for all data pairs \( (\vec{x}_i, y_i) \).
				\item The formula can be rewritten in matrix-vector notation yielding just one formula to express everything:
			\end{itemize}
			\begin{align}
				\hat{\vec{x}}_i &\coloneqq \begin{bmatrix} \vec{x}_i & 1 \end{bmatrix}^T \\
				\hat{\vec{w}} &\coloneqq \begin{bmatrix} \vec{w} & w_0 \end{bmatrix}^T \\
				\hat{X} &\coloneqq \begin{bmatrix} \hat{\vec{x}}_1 & \cdots & \hat{\vec{x}}_n \end{bmatrix} \in \R^{d \times n} \\
				\vec{y} &\coloneqq \begin{bmatrix} y_1 & \cdots & y_n \end{bmatrix}^T \\
				\implies\quad \vec{y} &= \hat{X}^T\hat{\vec{w}}
			\end{align}
			\begin{itemize}
				\item This is an overdetermined equation system and thus not solvable in general, so instead of solving it the squared error of the equation system is minimized:
			\end{itemize}
			\begin{align} % TODO: Class: LSC Derivation
				\hat{\vec{w}}^\ast &= \arg\min\limits_{\hat{\vec{w}}} \, \lVert \hat{X}^T \hat{\vec{w}} - \vec{y} \rVert^2 \\
								   &= \arg\min\limits_{\hat{\vec{w}}} \big( \hat{X}^T \hat{\vec{w}} - \vec{y} \big)^T \big( \hat{X}^T \hat{\vec{w}} - \vec{y} \big) \\
								   &= \arg\min\limits_{\hat{\vec{w}}} \hat{\vec{w}}^T \hat{X} \hat{X}^T \hat{\vec{w}} - 2\vec{y}^T\hat{X}^T\hat{\vec{w}} + \vec{y}^T\vec{y}
			\end{align}
			Take the derivative w.r.t. \( \hat{\vec{w}} \) and set it to zero:
			\begin{align}
				&& 0 &\overset{!}{=} \frac{\partial}{\partial \hat{\vec{w}}} \big( \hat{\vec{w}}^T \hat{X} \hat{X}^T \hat{\vec{w}} - 2\vec{y}^T\hat{X}^T\hat{\vec{w}} + \vec{y}^T\vec{y} \big) & \\
				\iff && 0 &= \hat{X} \hat{X}^T \hat{\vec{w}} + \hat{X}^T \hat{X} \hat{\vec{w}} - 2\vec{y}^T\hat{X}^T & \\
			\end{align}
			\begin{itemize}
				\item This yields the following best-effort solution for \( \hat{\vec{w}}^\ast \) by utilizing the left pseudo-inverse of \(\hat{X}\):
			\end{itemize}
			\begin{equation}
				\hat{\vec{w}}^\ast = \big( \hat{X} \hat{X}^T \big)^{-1} \hat{X} \vec{y}
			\end{equation}

			\paragraph{Problems}
				\begin{itemize}
					\item The least-squares solution for discriminative classification is very sensitive to outliers and breaks down even if there are only a few. This is due to the squared error which treats outliers more important than small errors.
					\item Calculating the matrix inverse is computationally expensive (but can be rewritten as solving a linear equation system).
				\end{itemize}
			% end
			
			\paragraph{Example}
				Figure~\ref{fig:classificationLS} shows the result from applying least squares regression to the sample data. As the data is linearly separable, there is no misclassification.
				
				\begin{figure}
					\centering
					\includegraphics{tmp-classification-ls.pdf}
					\caption{Classification: Least Squares}
					\label{fig:classificationLS}
				\end{figure}
			% end
		% end

		\subsection{Fishers' Linear Discriminant}
			\begin{itemize}
				\item Idea: Find a linear projection of the data and classify the projected values on this line.
				\item Same as for linear discriminant functions, check against a threshold:
			\end{itemize}
			\begin{equation}
				\vec{w}^T \vec{x} + w_0 \geq 0
			\end{equation}
			
			\paragraph{First Attempt: Maximize the Distance}
				Take the two means
				\begin{equation}
					\vec{m}_1 = \frac{1}{\abs{C_1}} \sum_{i \in C_1} \vec{x}_i \qquad \vec{m}_2 = \frac{1}{\abs{C_2}} \sum_{i \in C_2} \vec{x}_i
				\end{equation}
				with the projections \( m_1 = \vec{w}^T \vec{m}_1 \) and \( m_2 = \vec{w}^T \vec{m}_2 \) and maximize the distance \( (m_1 - m_2)^2 \). Here rises the problem that the distance grows unbounded with \(\vec{w}\), so fix the norm of \( \vec{w} \) to \( \norm{\vec{w}} = 1 \). This yields the following optimization problem:
				\begin{align}
					\arg\max\limits_{\vec{w}} \, J(\vec{w}) &= (\vec{w}^T \vec{m}_1 - \vec{w}^T \vec{m}_2)^2 \\
					\textrm{s.t.} \quad
					\norm{\vec{w}}^2 &= 1
				\end{align}
				By performing Lagrangian optimization, the solution is
				\begin{equation}
					\vec{w} = \frac{\vec{m}_1 - \vec{m}_2}{\norm{\vec{m}_1 - \vec{m}_2}}
				\end{equation}
				This parameter causes a large class overlap, so do more: Maximize the mean distance while minimizing the variance of each class.
			% end
			
			\paragraph{Final Attempt: Maximize the Distance, Minimize the Variance}
				Let \( s_1^2 \) and \( s_2^2 \) be in \emph{within-class variances}:
				\begin{equation}
					s_1^2 = \sum_{i \,\in\, C_1} (\vec{w}^T \vec{x}_i - m_1)^2 \qquad s_2^2 = \sum_{i \,\in\, C_2} (\vec{w}^T \vec{x} - m_2)^2
				\end{equation}
				with \( m_1 = \vec{w}^T \vec{m}_1 \) and \( m_2 = \vec{w}^T \vec{m}_2 \). The \emph{Fisher criterion} now formulates the optimization problem as:
				\begin{align}
					\arg\max\limits_{\vec{w}} \, J(\vec{w}) &= \frac{(m_1 - m_2)^2}{s_1^2 + s_2^2}
				\end{align}
				The nominator and the denominator can be rewritten to make the criterion easier to optimize:
				\begin{align}
					(m_1 - m_2)^2 &= (\vec{w}^T \vec{m}_1 - \vec{w}^T \vec{m}_2)^2 \\
						&= \big(\vec{w}^T (\vec{m}_1 - \vec{m}_2)\big)^2 \\
						&= \vec{w}^T \underbrace{(\vec{m}_1 - \vec{m}_2) (\vec{m}_1 - \vec{m}_2)^T}_{\underbrace{S_B \coloneqq}_{\textrm{between-class covariance}}} \vec{w} \\
					s_1^2 + s_2^2 &= \sum_{i \,\in\, C_1} (\vec{w}^T \vec{x}_i - m_1)^2 + \sum_{i \,\in\, C_2} (\vec{w}^T \vec{x} - m_2)^2 \\
					s_1^2 + s_2^2 &= \sum_{i \,\in\, C_1} \big(\vec{w}^T (\vec{x}_i - \vec{m}_1)\big)^2 + \sum_{i \,\in\, C_2} \big(\vec{w}^T (\vec{x} - \vec{m}_2)\big)^2 \\
					s_1^2 + s_2^2 &= \sum_{i \,\in\, C_1} \vec{w}^T (\vec{x}_i - \vec{m}_1) (\vec{x}_i - \vec{m}_1)^T \vec{w} + \sum_{i \,\in\, C_2} \vec{w}^T (\vec{x}_i - \vec{m}_2) (\vec{x}_i - \vec{m}_2)^T \vec{w} \\
					s_1^2 + s_2^2 &= \vec{w}^T \underbrace{\Bigg[ \sum_{i \,\in\, C_1} (\vec{x}_i - \vec{m}_1) (\vec{x}_i - \vec{m}_1)^T + \sum_{i \,\in\, C_2} (\vec{x}_i - \vec{m}_2) (\vec{x}_i - \vec{m}_2)^T \Bigg]}_{\underbrace{S_W \coloneqq}_{\textrm{within-class covariance}}} \vec{w}
				\end{align}
				This way, the cost function can be rewritten:
				\begin{equation}
					J(\vec{w}) = \frac{(m_1 - m_2)^2}{s_1^2 + s_2^2} = \frac{\vec{w}^T S_B \vec{w}}{\vec{w}^T S_W \vec{w}}
				\end{equation}
				Differentiating the cost function w.r.t. \(\vec{w}\) and setting it to zero yields
				\begin{equation}
					(\vec{w}^T S_B \vec{w}) S_W \vec{w} = (\vec{w}^T S_W \vec{w}) S_B \vec{w}
				\end{equation}
				As the factory \( \vec{w}^T S_B \vec{w} \) and \( \vec{w}^T S_W \vec{w} \) are scalars, the vector \( S_W \vec{w} \) and \( S_B \vec{w} \) are colinear:
				\begin{eqnarray}
					S_W \vec{w} \,\Vert\, S_B \vec{w}
				\end{eqnarray}
				Thus, with \( S_B \vec{w} = (\vec{m}_1 - \vec{m}_2) (\vec{m}_1 - \vec{m}_2)^T \vec{w} \) it also holds that \( S_B \vec{w} \,\Vert\, (\vec{m}_1 - \vec{m}_2) \) leading to \emph{Fisher's linear discriminant}:
				\begin{equation}
					S_W \vec{w} \,\Vert\, S_B \vec{w} \,\Vert\, (\vec{m}_1 - \vec{m}_2) \quad\implies\quad \vec{w} \propto S_W^{-1} (\vec{m}_1 - \vec{m}_2)
				\end{equation}
				
				\begin{itemize}
					\item Caution: Fisher's linear discriminant only yields a projection, the threshold \(w_0\) is still missing and has to be found, e.g. by using a Bayes classifier with Gaussian class-conditionals.
					\item Fisher's linear discriminant is Bayes optimal iff the class-conditional distributions (likelihoods) are equal with diagonal covariance.
					\item It is essentially equivalent to linear discriminant analysis (it is equivalent to a certain case of the least squares classifier).
					\item Problem: It is still very sensitive to noise.
				\end{itemize}
			% ednd
		% end
	% end

	\section{Perceptron Algorithm} % 9.40, 9.41, 9.42, 9.43, 9.48
		The \emph{perceptron algorithm} tries to find a separating hyperplane, given that the data is linearly separable. It depends on the following discriminator (called \emph{perceptron discriminant function}):
		\begin{equation}
			y(\vec{x}) = \sign\big(\vec{w}^T \vec{x} + b\big)
		\end{equation}
		with the sign function
		\begin{equation}
			\sign : \R \to \{\, -1, 0, +1 \,\} : o \mapsto
				\begin{cases}
					-1 & \textrm{iff } o < 0 \\
					 0 & \textrm{iff } o = 0 \\
					+1 & \textrm{iff } o > 0
				\end{cases}
		\end{equation}
		Algorithm\ref{alg:perceptron} shows the perceptron algorithm for a dataset \( \mathcal{D} = \big\{\, (\vec{x}, y) \forwhich \vec{x} \in \R^d, y \in \{\, -1, +1 \,\} \,\big\} \) for \(n\) iterations. The initialization vectors can be chosen differently.

		\begin{algorithm}
			\( \vec{w}^{(1)} \gets \vec{1} \)

			\( b^{(1)} \gets 0 \)

			\For{\( k = 1, \cdots, n \)}{
				\( \vec{w}^{(k + 1)} \gets \vec{w}^{(k)} \)

				\( b^{(k + 1)} \gets b^{(k)} \)

				\For{\( \forall (\vec{x}_i, y_i) \in \mathcal{D} \)}{
					\If{\( \sign\big(\vec{w}^T \vec{x} + b\big) \neq y \)}{
						\If{\( y = -1 \)}{
							\( \vec{w}^{(k + 1)} \gets \vec{w}^{(k + 1)} - \vec{x} \)

							\( b^{(k + 1)} \gets b^{(k + 1)} - 1 \)
						}

						\If{\( y = +1 \)}{
							\( \vec{w}^{(k + 1)} \gets \vec{w}^{(k + 1)} + \vec{x} \)

							\( b^{(k + 1)} \gets b^{(k + 1)} + 1 \)
						}
					}
				}
			}

			\caption{Perceptron Algorithm}
			\label{alg:perceptron}
		\end{algorithm}

		\paragraph{Example}
			Figure~\ref{fig:perceptronExample} shows the result of the perceptron algorithm after convergence (took \(7\) iterations).

			\begin{figure}
				\centering
				\includegraphics{tmp-classification-perceptron.pdf}
				\caption{Classification: Perceptron (\(7\) Iterations)}
				\label{fig:perceptronExample}
			\end{figure}
		% end

		\subsection{Intuition} % 9.44, 9.45, 9.46, 9.47
			\todo{Class: Perceptron: Intuition}
		% end

		\subsection{Linear Separability}
			\begin{itemize}
				\item The perceptron algorithm is not able to handle non linearly separable data, e.g. the XOR function.
				\item This halted research for decades.
				\item Simple solution: Transform the input space nonlinearly to make it linearly separable.
				\item Insight: Create features and learn from them, not from the raw data! This automatically done by neural networks.
			\end{itemize}
		% end
	% end

	\section{Probabilistic Discriminative Models}
		The class posterior can be expressed using Bayes rule and the sigmoid function (for two classes):
		\begin{equation}
			p(C_1 \given \vec{x}) = \frac{p(\vec{x} \given C_1) \, p(C_1)}{p(\vec{x})} = \frac{p(\vec{x} \given C_1) \, p(C_1)}{p(\vec{x} \given C_1) \, p(C_1) + p(\vec{x} \given C_2) \, p(C_2)} = \frac{1}{1 + \frac{p(\vec{x} \given C_1) \, p(C_1)}{p(\vec{x} \given C_2) \, p(C_2)}} = \sigma(a)
		\end{equation}
		for \( a = \ln \frac{p(\vec{x} \given C_1) \, p(C_1)}{p(\vec{x} \given C_2) \, p(C_2)} \).

		\subsection{Logistic Regression} % TODO: Class: LR: Classifier?
			\begin{itemize}
				\item In \emph{logistic regression}, it is assumed that \( a \) is given by a discriminant function \( a = \vec{w}^T \vec{x} + w_0 \), so the challenge is to find \( \vec{w} \) and \( w_0 \) to model the class posterior the best.
				\item This is an appropriate assumption if:
					\begin{itemize}
						\item The class conditionals are Gaussians with equal covariances.
						\item But also for a other distributions.
						\item There must be some independence of the form of the class-conditionals.
					\end{itemize}
				\item Logistic regression works by maximizing the likelihood \( p(Y \given X, \vec{w}, w_0) \) (where \( y_i \) is \(0\) iff \( \vec{x}_i \in C_1 \) and is \( 1 \) iff \( \vec{x}_i \in C_2 \)), assuming the data is drawn i.i.d.:
			\end{itemize}
			\begin{align} % TODO: Class: LR: If time, derive!
				p(Y \given X, \vec{w}, w_0) &= \prod_{i = 1}^{N} p(y_i \given \vec{x}_i, \vec{w}, w_0) \\
					&= \prod_{i = 1}^{N} p(C_1 \given \vec{x}_i, \vec{w}, w_0)^{1 - y_i} \, p(C_2 \given \vec{x}_i, \vec{w}, w_0)^{y_i} \\
					&= \prod_{i = 1}^{N} \sigma(\vec{w}^T \vec{x}_i + w_0)^{1 - y_i} \, (1 - \sigma(\vec{w}^T \vec{x}_i + w_0))^{y_i}
			\end{align}
			\begin{itemize}
				\item The key idea is to now apply the logarithm and do gradient descent, for a derivation see Bishop 4.3.
				\item More robust classifiers can be retrieved by incorporating priors and taking a Bayesian approach.
			\end{itemize}
		% end
	% end
	
	\section{Wrap-Up}
		\begin{itemize}
			\item Bayes optimal classifier
			\item Discriminant functions
			\item Formalization (intuitively and mathematically) of classification as linearly separable
			\item Computation of the least squares solution for classification and its failure
			\item Fisher's linear discriminant and difference to least squares
			\item Perceptron and its failure for XOR and how to overcome it
			\item Difference between generative and discriminative models
			\item Logistic regression
		\end{itemize}
	% end
% end

\chapter{Linear Dimensionality Reduction}
	In this chapter, linear models are implied for simplicity, if not stated otherwise.
	
	\begin{itemize}
		\item \emph{Dimensionality reduction} is part of the unsupervised learning methods which reduces the dimension of the data.
		\item One possible application is the visualization of the data.
			\item Motivation from least squares regression: LSR requires the inversion of a \( d \times d \) matrix, where \(d\) is the dimension. If it is possible to find a new \( d^\textrm{new} \ll d \) which represents the data well enough, the computation cost can be reduced while not loosing precision.
			\item The key problem is to find representations (especially transformations) of the data into a lower-dimensional subspace, that capture the "essence" of the data.
			\item More formally: For every original data point \( \vec{x}^n \in \R^M \), find a low-dimensional representation \( \vec{a}^n \in \R^D \) with \( D \ll M \). This is a mapping \( f : \R^M \to \R^D : \vec{x}^n \mapsto \vec{a}^n \).
			\item For simplicity, restrict this mapping function to be linear with a matrix \( B \in \R^{D \times M} \):
	\end{itemize}
	\begin{equation}
		\vec{a}^n = B \vec{x}^n
	\end{equation}

	\section{Introduction}
		\paragraph{Linear Combinations}
			\begin{itemize}
				\item A vector can always be written as a linear combination
			\end{itemize}
			\begin{equation}
				\vec{x} = \sum_{i = 1}^{M} a_i \vec{u}_i
			\end{equation}
			\begin{itemize}
				\item[] where \( \vec{u}_i^T \vec{u}_j = \delta_{ij} \). Thus, the \( \vec{u}_i \) build an orthonormal basis of the feature space.
				\item By rewriting the linear combination, \( a_i \) can be expressed as a projection \( a_i = \vec{u}_i^T \vec{x} \):
			\end{itemize}
			\begin{align}
				&& \vec{x} &= \sum_{i = 1}^{M} a_i \vec{u}_i = a_j \vec{u}_j + \sum_{\substack{i \,=\,  1 \\ i \,\neq\, j}}^{M} a_i \vec{u}_i & \\
				\iff && a_j \vec{u}_j &= \vec{x} - \sum_{\substack{i \,=\, 1 \\ i \,\neq\, j}}^{M} a_i \vec{u}_i & \\
				\iff && a_j &= \vec{u}_j^T \vec{x} - \sum_{\substack{i \,=\, 1 \\ i \,\neq\, j}}^{M} a_i \underbrace{\vec{u}_j^T \vec{u}_i}_{ =\, \delta_{ji} \underset{j \,\neq\, i}{=}\, 0 } & \\
				\iff && a_j &= \vec{u}_j^T \vec{x} &
			\end{align}
			\begin{itemize}
				\item The linear combination can be decomposed as
			\end{itemize}
			\begin{equation}
				\vec{x}^n = \sum_{i = 1}^{D} a_i \vec{u}_i + \underbrace{\sum_{j = D + 1}^{M} b_j \vec{u}_j}_\textrm{Error} \approx \tilde{\vec{x}}
			\end{equation}
			\begin{itemize}
				\item[] with the reconstructed data \(\tilde{\vec{x}}\), yielding the following optimization problem (minimizing the mean squared error over the training data):
			\end{itemize}
			\begin{equation}
				\vec{u}_1, \cdots, \vec{u}_D = \arg\min\limits_{\vec{u}_1, \cdots, \vec{u}_D} \, E(\vec{u}_1, \cdots, \vec{u}_D) = \arg\min\limits_{\vec{u}_1, \cdots, \vec{u}_D} \, \sum_{n = 1}^{N} \, \lVert \vec{x}^n - \tilde{\vec{x}}^n \rVert^2
			\end{equation}
		% end

		\paragraph{Minimizing the Error}
			\begin{itemize}
				\item The error can be rewritten (assuming a single basis vector to find the first principal direction):
			\end{itemize}
			\begin{align}
				E(\vec{u}) &= \sum_{n = 1}^{N} \, \lVert \vec{x}^n - \tilde{\vec{x}}^n \rVert^2 \\
					&= \sum_{n = 1}^{N} \, \lVert \vec{x}^n - (\vec{u}^T \vec{x}^n) \vec{u} \rVert^2 \\
					&= \sum_{n = 1}^{N} \, \lVert \vec{x}^n \rVert^2 - 2 (\vec{u}^T \vec{x}^n)^2 + (\vec{u}^T \vec{x}^n)^2 \, \vec{u}^T \vec{u} \\
					&= \sum_{n = 1}^{N} \, \lVert \vec{x}^n \rVert^2 - (\vec{u}^T \vec{x}^n)^2 \\
					&= \sum_{n = 1}^{N} \, \lVert \vec{x}^n \rVert^2 - a_n^2
			\end{align}
			\begin{itemize}
				\item So minimizing the error is equivalent to maximizing the variance of the projection (assuming a zero mean on the data, this can be achieved by subtracting the mean from every data point \( \vec{x}^n - \bar{\vec{x}} \)).
				\item Thus, the goal changes to finding the axis with the largest variance.
				\item The resulting axis are orthogonal and decorrelate the data (in the coordinate frame of the new axis, the data is uncorrelated). This only works for Gaussians!
			\end{itemize}
		% end
	% end

	\section{Principal Component Analysis}
		\begin{itemize}
			\item The goal of \emph{principal component analysis} is to find the so-called \emph{principal directions} and the variance of the data along each principal direction.
			\item In the following, \( \lambda_i \) is the \emph{marginal variance} along the principal direction \(\vec{u}_i\).
			\item The first principal direction \(\vec{u}_1\) is the direction along the variance of the data is maximal (\(C\) is the covariance matrix):
		\end{itemize}
		\begin{equation}
			\vec{u}_1 = \arg\max\limits_{\vec{u}} \vec{u}^T C \vec{u}
		\end{equation}
		\begin{itemize}
			\item The second principal direction maximizes the variance of the data in the orthogonal complement of the first principal direction.
		\end{itemize}

		\subsection{Derivation}
			Let \( X = \begin{bmatrix} \vec{x}^2 & \cdots & \vec{x}^n \end{bmatrix} \in \R^{M \times N} \) be a matrix of \(N\) vectors in a \(M\)-dimensional input space. Let \( \vec{u} \in \R^M \) be a unit vector in the input space. The projection of the vector \( \vec{x}^j \) onto the vector \(\vec{u}\) can be computed as:
			\begin{equation}
				a_j = \vec{u}^T \vec{x}^j = \sum_{i = 1}^{M} X_{ij} u_i
			\end{equation}
			The goal is to find a direction \(\vec{u}\) that maximizes the variance of the projections of all input vectors.

			\paragraph{Variance of the Projection} % TODO: LDR: PCA: More derivation.
				The variance of the projection can be computed as (with \( \mu_i = \frac{1}{N} \sum_{j = 1}^{N} X_{ij} \)):
				\begin{align}
					\bar{a} = \frac{1}{N} \sum_{j = 1}^{N} a_j = \frac{1}{N} \sum_{j = 1}^{N} \sum_{i = 1}^{M} X_{ij} u_i = \sum_{i = 1}^{M} u_i \mu_i
				\end{align}
				\begin{align}
					\sigma^2 &= \frac{1}{N} \sum_{j = 1}^{N} (a_j - \bar{a})^2 = \frac{1}{N} \sum_{j = 1}^{N} \Bigg( \sum_{i = 1}^{M} u_i X_{ij} - \sum_{i = 1}^{M} u_i \mu_i \Bigg)^2 = \vec{u}^T C \vec{u}
				\end{align}
			% end

			\paragraph{Maximizing the Variance}
				The variance has to be maximized with the constraint \( \lVert \vec{u} \rVert = 1 \):
				\begin{align}
					\max_{\vec{u}} \, J(\vec{u}) &= \vec{u}^T C \vec{u} \\
					\textrm{s.t.} \qquad
					\lVert \vec{u} \rVert & = 1
				\end{align}
				The Lagrangian formulation
				\begin{equation}
					L(\vec{u}, \lambda) = \vec{u}^T C \vec{u} - \lambda \sum_{k = 1}^{M} \big(u_k^2 - 1\big) = \sum_{i = 1}^{M} \sum_{j = 1}^{M} u_i C_{ij} u_j - \lambda \sum_{k = 1}^{M} \big(u_k^2 - 1\big)
				\end{equation}
				yields the solution
				\begin{equation}
					C\vec{u} = \lambda\vec{u}
				\end{equation}
				
				This is the Eigenvalue-Eigenvector equation! So solving for the eigenvalues and eigenvectors gives the following results:
				\begin{itemize}
					\item The larges eigenvalue gives the maximal variance and
					\item the corresponding eigenvector gives the direction with the maximal variance.
				\end{itemize}
			% end
		% end
		
		\subsection{Conclusion}
			As the covariance matrix \(C\) is real, symmetric and positive-definite, the eigenvalues and -vectors can be grouped in an Eigendecomposition:
			\begin{equation}
				C = U \Lambda \, U^T =
				\begin{bmatrix} \vec{u}_1 & \cdots & \vec{u}_M \end{bmatrix}
				\begin{bmatrix}
					\lambda_1 &        &  \\
					          & \ddots &  \\
					          &        & \lambda_M
				\end{bmatrix}
				\begin{bmatrix} \vec{u}_1^T \\ \vdots \\ \vec{u}_M^T \end{bmatrix}
			\end{equation}
			
			\begin{itemize}
				\item If \( \lambda_k \approx 0 \) for \( k > D \) for some \( D \ll M \), the subset of the first \(D\) eigenvectors (with the greatest value) can be used as a basis to approximate the data vectors.
				\item The eigenvalues \(\lambda_k\) then represent the \emph{explained variance} in that direction.
				\item This representation has the minimal mean squared error of all linear representations of dimension \(D\).
				\item The following steps have to be done to represent the data in a lower space (and reconstruct it):
					\begin{enumerate}
						\item Center the data around the mean (compute it and subtract it from all data points). May standardize the variance (compute the variance and divide the normalized data by it).
						\item Compute the covariance matrix, decompose it into eigenvalues and -vectors and find the first \(D\) eigenvalues with the highest corresponding eigenvalues. As the covariance matrix must be positive semi-definite, all eigenvalues are \( \lambda_i \geq 0 \).
						\item Transform the data into a lower dimensional space (with \( B = \begin{bmatrix} \vec{u}_1 & \cdots & \vec{u}_D \end{bmatrix} \)):
					\end{enumerate}
			\end{itemize}
			\begin{equation}
				\vec{a}^n = B^T \big( \vec{x}^n - \bar{\vec{x}} \big)
			\end{equation}
			\begin{itemize}
				\item[] If the data was standardized, the last term has to be divided by the variance.
					\begin{enumerate}
						\setcounter{enumi}{3}
						\item To reconstruct the data, reverse-apply the formula:
					\end{enumerate}
			\end{itemize}
			\begin{equation}
				\tilde{\vec{x}}^n = \bar{\vec{x}} + B \vec{a}^n
			\end{equation}
			\begin{itemize}
				\item[] If the data was standardized, multiply it by the variance.
			\end{itemize}
		
			\paragraph{Example}
				Figure~\ref{fig:pcaIris} shows principal component analysis applied to the iris dataset with the data of flowers. The explained variance vs. the number of components is shown in figure~\ref{fig:pcaIrisVariance}
				
				\begin{figure}
					\centering
					\includegraphics{tmp-pca-iris.pdf}
					\caption{Principal Component Analysis: Iris Dataset}
					\label{fig:pcaIris}
				\end{figure}
				\begin{figure}
					\centering
					\includegraphics{tmp-pca-variance.pdf}
					\caption{Principal Component Analysis: Iris Dataset (Explained Variance)}
					\label{fig:pcaIrisVariance}
				\end{figure}
			% end
		% end
	% end

	\section{Choosing the target Dimension}
		\begin{itemize}
			\item A larger \(D\) leads to a better approximation (with \( D = M \), \(100\%\) accuracy as the dimension is not changed).
			\item There exist two good possibilities to choose the target dimension:
				\begin{enumerate}
					\item Choose \(D\) based on the application performance (choose the smallest \(D\) that makes the application work well enough).
					\item Choose \(D\) so that the basis captures some fraction of the variance, so choose \( D \) so that, for a given \( \eta \):
				\end{enumerate}
		\end{itemize}
		\begin{equation}
			\sum_{i = 1}^{D} \lambda_i \geq \eta \sum_{i = 1}^{M} \lambda_i
		\end{equation}
	% end

	\section{Applications} % 10.37, 10.29, 10.30, 10.31, 10.32, 10.33, 10.34, 10.35, 10.36
		\todo{LDR: Applications}
	% end
	
	\section{Wrap-Up}
		\begin{itemize}
			\item Dimensionality reduction and why its needed
			\item Intuition behind PCA
			\item Maximization of the variance of the projection
			\item Relation of PCA to eigenvector and -values
		\end{itemize}
	% end
% end

\chapter{Statistical Learning Theory}
	\begin{itemize}
		\item In classical statistical learning, the parameters \(\vec{w}\) are estimated for a \emph{fixed model} (the \emph{learning machine}).
		\item Learning occurs only by optimizing the model parameters. It is assumed that the correct model is known in advance (except in Bayesian learning).
		\item Selecting the "correct" features is hard and an overly complex model leads to overfitting.
		\item The real point of interest is the generalization ability and the corresponding risk.
		\item In \emph{statistical learning theory}, these assumptions are not made and the goal is to find an optimal model from a specified set of models. Optimally means the ability to generalize, i.e. to have the lowest error probability on all data, not just the test data. It is concerned with the question on how to control the generalization abilities of a learning machine.
		\item It aims at a formal description of the generalization ability and the goal is to develop a rigorous theory as opposed to commonly used heuristics.
		\item This is a good goal, but the theory itself does not say much about real problems\dots
	\end{itemize}

	\section{Supervised Learning}
		\begin{itemize}
			\item The environment is stationary, the data points have an unknown but fixed probability density
		\end{itemize}
		\begin{equation}
			\vec{x}_i \sim p_X
		\end{equation}
		\begin{itemize}
			\item The supervisor returns the intended classification label for every data point \(\vec{x}\), possibly with some noise \( \epsilon \)
		\end{itemize}
		\begin{equation}
			y = g(\vec{x}, \epsilon)
		\end{equation}
		\begin{itemize}
			\item The learning machine is represented through a class of functions with parameters \(\vec{w}\) that return an output \(y\) for every input \(\vec{x}\)
		\end{itemize}
		\begin{equation}
			y = f(\vec{x}, \vec{w})
		\end{equation}
		
		\begin{itemize}
			\item From the view of the learning machine, choose a particular function \( y = f(\vec{x}, \vec{w}) \) given a set of training examples \( \{\, \vec{x}_i, y_i \,\}_{i = 1}^N \). Goal: Approximate the desired output \(y\) optimally.
			\item This optimality can be expressed by a loss function, e.g. quadratic loss
		\end{itemize}
		\begin{equation}
			L\big(L, f(\vec{x}, \vec{w})\big) = \big(y - f(\vec{x}, \vec{w})\big)^2
		\end{equation}
	% end

	\section{Assessment of Optimality: Risk}
		The risk for a particular data point can be expressed with a loss function
		\begin{equation}
			L\big(y, f(\vec{x}, \vec{w})\big)
		\end{equation}
		This yields the \emph{empirical risk} as the average over all available samples
		\begin{equation}
			R_{{\emp}}(\vec{w}) = \frac{1}{N} \sum_{i = 1}^{N} L\big(y_i, f(\vec{x}, \vec{w})\big)
		\end{equation}
		where \(N\) is the number of samples.
		
		In reality, the \emph{true risk}
		\begin{equation}
			R(\vec{w}) = \int L\big(y, f(\vec{x})\big) \, p(\vec{x}, y) \dd{\vec{x}} \dd{y} = \E_{ \vec{x}, y \sim p(\vec{x}, ) }\Big(\! L\big(y, f(\vec{x}, \vec{w})\big) \Big)
		\end{equation}
		is far more interesting, where \( p(\vec{x}, y) \) is the joint probability density of \(\vec{x}\) and \(y\). The risk is the expected error over all data sets and is the expectation of the generalization error.
		
		\textbf{Problem:} The probability density \( p(\vec{x}, y) \) is fixed, but unknown. So the true risk cannot be computed directly.

		\subsection{Empirical vs. True Risk}
			\begin{itemize}
				\item \textbf{True Risk}
					\begin{itemize}
						\item Advantage: The actual measure for the generalization ability.
						\item Disadvantage: Depends on \( p(\vec{x}, y) \) which is unknown \( \implies \) the true risk cannot be computed directly.
					\end{itemize}
				\item \textbf{Empirical Risk}
					\begin{itemize}
						\item Disadvantage: No "real" measure for the generalization ability.
						\item Advantage: Does not depend on \( p(\vec{x}, y) \) and can be computed directly.
						\item Learning algorithms usually minimize the empirical risk.
					\end{itemize}
				\item The interest point are the dependencies between the two risks.
				\item The empirical risk is an approximation for the true risk that works well if the distribution is very concentrated. It gets very good if there are infinitely many samples.
			\end{itemize}
		% end

		\subsection{Convergence Properties}
			At first assume that the empirical risk converges to the true risk with more samples (\(\inf\) is the infimum):
			\begin{equation}
				\lim\limits_{N \to \infty} \inf\limits_{\vec{w}} \, R_{{\emp}}(\vec{w}) = \inf\limits_{\vec{w}} \, R(\vec{w})
			\end{equation}
			Also assume that the convergence has to be uniform:
			\begin{equation}
				\lim\limits_{N \to \infty} P\,\Bigg(\! \sup\limits_{\vec{w}} \, \big\lvert\, R(\vec{w}) - R_{{\emp}}(\vec{w}) \,\big\rvert > \epsilon \Bigg) = 0
			\end{equation}
			Intuition: "The learning machine gets better the more data it has."
			
			If the convergence is uniform \( P\,\Bigg(\! \sup\limits_{\vec{w}} \, \big\lvert\, R(\vec{w}) - R_{{\emp}}(\vec{w}) \,\big\rvert > \epsilon \Bigg) < p^\ast \) for some \( p^\ast > 0 \), then with probability \( 1 - p^\ast \) it holds that
			\begin{align}
				\big\lvert\, R(\vec{w}_{{\emp}}) - R_{{\emp}}(\vec{w}_{{\emp}}) \,\big\rvert < & \,\epsilon \\
				\big\lvert\, R(\vec{w}_0) - R_{{\emp}}(\vec{w}_0) \,\big\rvert <       & \,\epsilon
			\end{align}
			Hence it holds that
			\begin{equation}
				P\Big( \big\lvert\, R(\vec{w}_0) - R_{{\emp}}(\vec{w}_{{\emp}}) \,\big\rvert > 2\epsilon \Big) < p^\ast \\
			\end{equation}
			
			Under the necessary and sufficient condition that the convergence is uniform, minimizing the empirical risk guarantees the minimization of the true risk in the limit of \( N \to \infty \).
			
			\begin{itemize}
				\item \textbf{Advantages}
					\begin{itemize}
						\item Existence of a formal criterion to what can be expected in terms of generalization.
						\item The necessary and sufficient condition is the uniform convergence.
					\end{itemize}
				\item \textbf{Disadvantages}
					\begin{itemize}
						\item In reality, the training data is very limited.
						\item "Taking the limit" with \( N \to \infty \) is impossible.
					\end{itemize}
			\end{itemize}
		% end
	% end

	\section{Risk Bound}
		Idea: Determine an upper \emph{risk bound} on the true risk based on the empirical risk
		\begin{equation}
			R(\vec{w}) \leq R_{{\emp}}(\vec{w}) + \epsilon(N, p^\ast, h)
		\end{equation}
		where \(N\) is the number of training samples, \(p^\ast\) is the probability that the bound is met and \(h\) is the \emph{learning power} of the learning machine, formally called \emph{VC-dimension}.

		\subsection{VC-Dimension}
			\begin{itemize}
				\item VC stands for VapnikChervonenkis, the developers of the VC-theory.
				\item Informal definition of the VC-dimension:
					\begin{itemize}
						\item The VC-dimension of a family of functions is the maximum number of samples that can be correctly classified by a function from that family (independent of the label configuration).
						\item The VC-dimension is a measure of the capacity ("learning power") of a classifier.
						\item The VC-dimension is the number of data points that can be shattered by a function.
					\end{itemize}
				\item Example: The VC-dimension of linear classifiers (hyperplanes) in \(\R^n\) is \( (n + 1) \).
				\item Often (but not always!) the VC-dimension is directly related to the number of parameters.
			\end{itemize}
		% end

		\subsection{Example}
			For the loss function, the true risk and the empirical risk
			\begin{align}
				L\big(y, f(\vec{x}, \vec{w})\big) &= \frac{1}{2} \big\lvert y - f(\vec{x}, \vec{w}) \big\rvert \\
				R(\vec{w}) &= \int \frac{1}{2} \big\lvert y - f(\vec{x}, \vec{w}) \big\rvert \, p(\vec{x}, y) \dd{\vec{x}} \dd{y} \\
				R_{\emp}(\vec{w}) &= \frac{1}{2N} \sum_{i = 1}^{N} \big\lvert y_i - f(\vec{x}_i, \vec{w}) \big\rvert
			\end{align}
			with probability \( p^\ast \) it holds that
			\begin{equation}
				R(\vec{w}) \leq R_{\emp}(\vec{w}) + \sqrt{\frac{h\big(\ln(2N / h) + 1\big) - \ln\big((1 - p^\ast) / 4\big)}{N}}
			\end{equation}
			\begin{itemize}
				\item The upper bound is independent of \( p(\vec{x}, y) \)!
				\item As the true risk is not computable, but the VC-dimension is known, a bound of the type
			\end{itemize}
			\begin{equation}
				R(\vec{w}) \leq R_{\emp}(\vec{w}) + \epsilon(N, p^\ast, h)
			\end{equation}
			\begin{itemize}
				\item[] with a confidence interval \(\epsilon\) can always be computed.
				\item However, in practice, this bound is very loose and the true risk may be much lower.
			\end{itemize}
		% end
	% end

	\section{Structural Risk Minimization}
		\begin{itemize}
			\item Given a family of \(n\) models \( f_i(\vec{x}_i, \vec{w}_i) \) with \( h_1 \leq h_2 \leq \cdots \leq h_n \).
			\item Minimize the empirical risk for every model and choose the model that minimizes the risk bound (the right side of the risk bound equation).
			\item In general, this is not the same model that minimizes the empirical risk.
			\item This formally lowers the upper bound on the true risk.
			\item The result is only sensible if the upper bound in the true risk is a tight bound (which is typically not).
		\end{itemize}
	% end
	
	\section{Wrap-Up}
		\begin{itemize}
			\item Statistical learning theory
			\item Empirical vs. true risk
			\item Incompleteness of the empirical risk
			\item VC-Dimension
			\item Relation between the VC-dimension and the model complexity
		\end{itemize}
	% end
% end

\chapter{Neural Networks}
	\begin{itemize}
		\item Selecting the "right" features for a problem is really hard, but the representation of the data matters a lot.
		\item Neural networks learn complex data representations by combining simpler ones (features of features).
		\item The big shifts that lead to neural networks are:
			\begin{itemize}
				\item Too little data \(\to\) too much data.
				\item Linear and convex \(\to\) nonlinear and nonconvex.
				\item Intuitive features \(\to\) harder features, key focus on learning.
				\item "Right number of parameters" \(\to\) "always too many".
				\item Optimization becomes easier by being deep.
			\end{itemize}
		\item Neural networks have a long history\dots
			\begin{itemize}
				\item \textbf{Pre-computational (1888-):} Neuron in biology fully isolated by Ramon y Cajal
				\item \textbf{Fields Starts (1943-):} McCullogh\&Pitts Neuron and Networks
				\item \textbf{1st Hype (1957-):} Rosenblatt's Perceptron
				\item \textbf{1st Winter (1969-):} Papert/Minsky book perceptron with XOR example (not linear separable)
				\item \textbf{2nd Hype (1986-1994):} Rummelthart/Hinton/Williams rediscover backpropagation
				\item \textbf{2nd Winter (1994-):} Optimization is really hard, Kernels are better!
				\item \textbf{2007:} Rebooted by NIPS workshops
				\item \textbf{3rd Hype (2013-now):} Amazing results in computer vision (ImageNet), Natural Language Processing, (Deep) Reinforcement Learning, \dots
			\end{itemize}
		\item Neural networks can be adapted to regression or classification!
			\begin{itemize}
				\item A linear output node gives a linear regression function.
				\item Using a sigmoid output node gives something similar to logistic regression.
				\item In either case, by taking the sign of the output, classification can be obtained.
				\item Typically not maximum likelihood is used for learning but a different learning criterion.
			\end{itemize}
		\item The actual power of neural networks comes from the extensions for multi-class classification and multi-layer perceptrons.
	\end{itemize}

	\section{Abstraction of a Neuron}
		\begin{itemize}
			\item A single neuron can be represented as
		\end{itemize}
		\begin{equation}
			y = f\big( \sum_{i = 1}^{n} \vec{W}_i x_i + b \big) = f(\vec{W}^T \vec{x} + b) = f(\hat{\vec{W}}^T \hat{\vec{x}})
		\end{equation}
		\begin{itemize}
			\item[] with the input \( \hat{\vec{x}} = \begin{bmatrix} \vec{x}^T & 1 \end{bmatrix}^T \), parameters and weights \( \hat{\vec{W}} = \begin{bmatrix} \vec{W}^T & b \end{bmatrix}^T \) and an activation function \(f\).
			\item Neurons are pooled together in \emph{layers} of \(m\) input and \(n\) outputs, where each layer has
				\begin{itemize}
					\item Weight matrix \( W \in \R^{n \times m} \)
					\item Bias vector \( \vec{b} \in \R^{n \times 1} \)
					\item Input vector \( \vec{x} \in \R^{m \times 1} \)
					\item Pre-activation vector \( \vec{z} = W\vec{x} + \vec{b} \)
					\item Output vector \( \vec{y} = \vec{f}(\vec{z}) \) with \( f : \R^{n \times 1} \to \R^{n \times 1} \)
				\end{itemize}
		\end{itemize}
	% end

	\section{Single-Layer Neural Networks}
		\subsection{Logistic Regression}
			In logistic regression, the class posterior is modeled as
			\begin{equation}
				p(C_1 \given \vec{x}) = \sigma(\vec{W}^T \vec{x} + b)
			\end{equation}
			and a solution for \(\vec{W}\) and \(b\) is found by maximizing the likelihood \( p(Y \given X, \vec{W}, b) \).
			
			This is equivalent to a neural network as shown in figure~\ref{fig:nnSingleLayer} with a sigmoid activation function.
			
			\begin{figure}
				\centering
				\begin{tikzpicture}
					\node[input neuron, pin = left:{\(x_0\)}] (I-1) at (0, -1) {};
					\node[input neuron, pin = left:{\raisebox{5.5pt}{\( \vdots \)}}] (I-V) at (0, -2) {};
					\node[input neuron, pin = left:{\(x_N\)}] (I-N) at (0, -3) {};
					
					\node[output neuron, pin = {[pin edge={->}]right:\(y\) }, right = of I-V] (O-1) {};
					
					\draw [->] (I-1) -- (O-1);
					\draw [->] (I-V) -- (O-1);
					\draw [->] (I-N) -- (O-1);
					
					\node [annot, above = 0 of I-1] {Input Layer};
					\node [annot, above = of O-1] {Output Layer};
				\end{tikzpicture}
				\caption{Neural Network: Single-Layer}
				\label{fig:nnSingleLayer}
			\end{figure}
		% end
		
		\subsection{Multi-Class Network}
			\begin{itemize}
				\item A single layer network can also have multiple output neurons, yielding multidimensional linear regression.
				\item Nonlinear extension is straightforward by applying the sigmoid for a logistic output.
			\end{itemize}
		% end

		\subsection{Least-Squares Loss Function}
			\begin{itemize}
				\item In supervised learning, \(N\) data points \( X = \begin{bmatrix} \vec{x}^1 & \cdots & \vec{x}1N \end{bmatrix} \) are given.
				\item For each data point there are \(c\) possible target values \( k \in 1, \cdots, c \): \( T_k = \begin{bmatrix} t_k^1 & \cdots & t_k^N \end{bmatrix} \).
				\item The model can compute \( y_k(\vec{x}^n, W) \), yielding the least-squares error/loss function:
			\end{itemize}
			\begin{equation}
				E(W) \frac{1}{2} \sum_{n = 1}^{N} \sum_{k = 1}^{c} \big( y_k(\vec{x}^n, W) - t_k^n \big)^2 = \frac{1}{2} \sum_{n = 1}^{N} \sum_{k = 1}^{c} \Bigg( f\Bigg( \sum_{i = 1}^{d} W_{ki} \phi_i(\vec{x}^n) \Bigg) - t_k^n \Bigg)^2
			\end{equation}
			with arbitrary feature transformations \( \phi_i(\cdot) \).
		% end

		\subsection{Learning with Gradient Descent}
			Assuming the output with a linear activation \( y_k(\vec{x}^n) = \sum_{i = 1}^{d} W_{ki} \phi_i(\vec{x}^n) \), the error function ans its derivative w.r.t. the weights compute as:
			\begin{gather}
				E(W) = \sum_{n = 1}^{N} \frac{1}{2} \sum_{k = 1}^{c} \Bigg( \sum_{i = 1}^{d} W_{ki} \phi_i(\vec{x}^n) - t_k^n \Bigg)^2 = \sum_{n = 1}^{N} E^n(W) \\
				\frac{\partial E^n(W)}{\partial W_{lj}} = \Bigg( \sum_{i = 1}^{d} W_{li} \phi_i(\vec{x}^n) - t_l^n \Bigg) \phi_j(\vec{x}^n) = \big( y_l(\vec{x}^n) - t_l^n \big) \phi_j(\vec{x}^n)
			\end{gather}
			Then the weights can be updated using gradient descent:
			\begin{align}
				W_{lj} &\gets W_{lj} - \eta \frac{\partial E(W)}{\partial W_{lj}} \Bigg\vert_W \\
				\frac{\partial E(W)}{\partial W_{lj}} &= \sum_{n = 1}^{N} \frac{\partial E^n(W)}{\partial W_{lj}}
			\end{align}
			This is computationally expensive if all data points are used for the gradient estimation.
			
			In a network with a nonlinear activation \( y_k(\vec{x}^n) = f(a_k) = f\Big( \sum_{i = 1}^{d} W_{ki} \phi_i(\vec{x}^n) \Big) \) the error derivative gets:
			\begin{equation}
				\frac{\partial E^n(W)}{\partial W_{li}} = f'(a_l) \, \big( y_l(\vec{x}^n) - t_l^n \big) \phi_j(\vec{x}^n)
			\end{equation}
			In a logistic neural network:
			\begin{equation}
				f(a) = \sigma(a) \qquad \sigma'(a) = \sigma(a) \, \big( 1 - \sigma(a) \big)
			\end{equation}
		% end
	% end

	\section{Multi-Layer Neural Networks}
		\emph{Multi-layer neural networks} have input and output layers like the single-layer networks, but contain so-called \emph{hidden layers} which lie in between the input and output layers. An example network is shown in figure~\ref{fig:nnMultiLayer}, which also has multiple output nodes. Neural networks that have more than one hidden layer are called \emph{deep neural networks}.
		
		In a network with just one hidden layer the output computes as
		\begin{equation}
			y_k(\vec{x}) = f^{(2)} \BIG( \sum_{i = 0}^{h} W_{ki}^{(2)} \underbrace{f^{(1)} \Bigg( \sum_{j = 0}^{d} W_{ij}^{(1)} x_j \Bigg)}_{z_i} \BIG)
		\end{equation}
		The function \( f^{(k)} \) is the activation function for the \(k\)-th layer (the output layer counts as a layer). The hidden layer may have an arbitrary number of nodes \(h\).
		
		A multi-layer network (also called \emph{multi-layer perceptron}) is calculated as
		\begin{equation*}
			y_k(\vec{x}) = f^{(N)} \BIG( \sum_{i_{N - 1} = 0}^{h_{N - 1}} W_{k i_{N - 1}}^{(N)} f^{(N - 1)} \BIG( \sum_{i_{N - 2} = 0}^{h_{N - 2}} W_{i_{N - 1} i_{N - 2}}^{(N - 1)} f^{(N - 2)} \BIG( \cdots f^{(2)} \BIG( \sum_{i_1 = 0}^{h} W_{i_2 i_1}^{(2)} f^{(1)} \BIG( \sum_{i_0 = 0}^{d} W_{i_1 i_0}^{(1)} x_{i_0} \BIG) \BIG) \BIG) \BIG) \BIG)
		\end{equation*}
		A multi-layer network can be seen as a machine that builds features on top of features.
		
		\begin{figure}
			\centering
			\begin{tikzpicture}
				\node [input neuron, pin = left:{\(x_0\)}] (I-1) at (0, -1) {};
				\node [input neuron, pin = left:{\raisebox{5.5pt}{\( \vdots \)}}] (I-V) at (0, -2) {};
				\node [input neuron, pin = left:{\(x_N\)}] (I-N) at (0, -3) {};
				
				\node [neuron, right = of I-1] (H-1) {};
				\node [neuron, right = of I-V] (H-V) {};
				\node [neuron, right = of I-N] (H-N) {};
				
				\node [output neuron, pin = {[pin edge={->}]right:\(y_1\) }, right = of H-1] (O-1) {};
				\node [output neuron, pin = {[pin edge={->}]right:{\raisebox{5.5pt}{\( \vdots \)}} }, right = of H-V] (O-V) {};
				\node [output neuron, pin = {[pin edge={->}]right:\(y_N\) }, right = of H-N] (O-N) {};
				
				\draw [->] (I-1) -- (H-1);
				\draw [->] (I-1) -- (H-V);
				\draw [->] (I-1) -- (H-N);
				\draw [->] (I-V) -- (H-1);
				\draw [->] (I-V) -- (H-V);
				\draw [->] (I-V) -- (H-N);
				\draw [->] (I-N) -- (H-1);
				\draw [->] (I-N) -- (H-V);
				\draw [->] (I-N) -- (H-N);
				
				\draw [->] (H-1) -- (O-1);
				\draw [->] (H-1) -- (O-V);
				\draw [->] (H-1) -- (O-N);
				\draw [->] (H-V) -- (O-1);
				\draw [->] (H-V) -- (O-V);
				\draw [->] (H-V) -- (O-N);
				\draw [->] (H-N) -- (O-1);
				\draw [->] (H-N) -- (O-V);
				\draw [->] (H-N) -- (O-N);
				
				\node [annot, above = 0 of I-1] {Input Layer};
				\node [annot, above = 0 of O-1] {Output Layer};
			\end{tikzpicture}
			\caption{Neural Network: Multi-Layer}
			\label{fig:nnMultiLayer}
		\end{figure}

		\subsection{One hidden Layer?}
			The universal function approximation theorem says that one hidden layer can represent every function arbitrarily accurate (Cybenko/Hornik). But this needs and exponential number of units (neurons)! Instead, multiple layers allow a similar effect with much less units.
		% end

		\subsection{Model Type and Model Class}
			\paragraph{Model Type}
				The model type is the choice of the nonlinear parametric model. It is determined by:
				\begin{itemize}
					\item Choice of topology: How are the neural layers connected and how many neurons per layer?
					\item Choice of neural elements: How is the neuron modeled?
				\end{itemize}
				Widely talking, everything in ML is a neural network, maybe with just one layer and one activation function.
				
				\begin{itemize}
					\item Feedforward neural networks are acyclic directed graphs.
						\begin{itemize}
							\item Multi-layer perceptrons are fully connected, while
							\item Convolutional networks are smartly pruned with weight-sharing.
						\end{itemize}
					\item Recurrent neural networks are cyclic directed graphs with internal states.
				\end{itemize}
			% end
			
			\paragraph{Model Class}
				The model class is the number of hidden neurons and the number of layers.
			% end
		% end
	% end

	\section{Output Neurons, Activation and Loss Functions}
		\subsection{Output Neurons}
			The type of the problem determines the type of the output neurons, all having probabilistic interpretations:
			\begin{itemize}
				\item Linear for regression:
			\end{itemize}
			\begin{equation}
				\vec{f}(\vec{z}) = \vec{z} \qquad p(\vec{y} \given \vec{x}) = \mathcal{N}\,(\vec{y} \given \vec{z}, \sigma^2 I)
			\end{equation}
			\begin{itemize}
				\item Sigmoid for (two-class) classification:
			\end{itemize}
			\begin{equation}
				f(z) = \sigma(z) \equiv \frac{1}{1 + e^{-z}} \qquad p(y \given z) = \sigma(z)^y \, \big( 1 - \sigma(z) \big)^{1 - y}
			\end{equation}
			\begin{itemize}
				\item Categorical Distribution/Softmax for multi-class classification:
			\end{itemize}
			\begin{equation}
				f_i(\vec{z}) = \frac{e^{z_i}}{\sum_{j = 1}^{n} e^{z_j}} \equiv p(y = i \given \vec{z})
			\end{equation}
		% end

		\subsection{Loss Functions}
			Just like the type of the output neuron is linked to the problem, is the loss function linked to the problem:
			\begin{itemize}
				\item Regression
					\begin{itemize}
						\item Linear output \(\implies\) Squared loss
					\end{itemize}
				\item Classification
					\begin{itemize}
						\item Linear output \(\implies\) Hinge loss
						\item Sigmoid \(\implies\) Nonlinear log-likelihood
					\end{itemize}
				\item Multi-Class Classification
					\begin{itemize}
						\item Softmax \(\implies\) Nonlinear log-likelihood
					\end{itemize}
			\end{itemize}
			All these are derivable from maximum likelihood.
		% end

		\subsection{Activation Functions}
			\begin{itemize}
				\item Hidden neurons may be chosen freely, because it is unknown what they actually do (but the derivative controls how much of a rule a neuron plays in learning).
				\item All the technical choices remain voodoo and depend on intuition.
				\item There are best practices and heuristics which one to choose.
			\end{itemize}

			\subsubsection{Sigmoid}
				Figure~\ref{fig:sigmoid} shows the sigmoid.
				\begin{equation}
					f(z) = \sigma(z) \qquad f'(z) = \sigma(z) \, \big(1 - \sigma(z)\big)
				\end{equation}
				Problem: The derivative is zero almost everywhere, causing a zero gradient during backpropagation and may stop learning.
				
				\begin{figure}
					\centering
					\begin{tikzpicture}
						\begin{axis}[
									domain = -8:8,
									samples = 100,
									xlabel = \(z\),
									ylabel = \(a\),
									ymin = 0,
									ymax = 1,
									xmin = -8,
									xmax = 8,
									axis x line = center,
									axis y line = center,
									ytick = {0, 0.5, 1}
								]
							\addplot [color = TUDa-1b, line width = 2pt, smooth] { 1 / (1 + exp(-x)) };
						\end{axis}
					\end{tikzpicture}
					\caption{Sigmoid \( \sigma(z) \)}
					\label{fig:sigmoid}
				\end{figure}
			% end

			\subsubsection{Hyperbolic Tangent}
				Figure~\ref{fig:tanh} shows the hyperbolic tangent.
				\begin{equation}
					f(z) = \tanh(z) \qquad f'(z) = 1 - \tanh^2(z)
				\end{equation}
				
				\begin{figure}
					\centering
					\begin{tikzpicture}
						\begin{axis}[
									domain = -8:8,
									samples = 100,
									xlabel = \(z\),
									ylabel = \(a\),
									ymin = -1,
									ymax = 1,
									xmin = -3,
									xmax = 3,
									axis x line = center,
									axis y line = center,
									ytick = {-1, 0, 1}
								]
							\addplot [color = TUDa-1b, line width = 2pt, smooth] { tanh(x) };
						\end{axis}
					\end{tikzpicture}
					\caption{Sigmoid \( \tanh(z) \)}
					\label{fig:tanh}
				\end{figure}
			% end

			\subsubsection{Rectified Linear Unit (ReLU)}
				Figure~\ref{fig:relu} shows the rectified linear unit.
				\begin{equation}
					f(z) = \max(0, z) \qquad f'(z) =
						\begin{cases}
							1 & \textrm{iff } z > 0 \\
							0 & \textrm{iff } z < 0
						\end{cases}
				\end{equation}
				Problem: A bad initialization of the parameters can lead to a zero gradient. In practice, initialize the bias to a positive value.
				
				\begin{figure}
					\centering
					\begin{tikzpicture}
						\begin{axis}[
									domain = -8:8,
									samples = 100,
									xlabel = \(z\),
									ylabel = \(a\),
									ymin = 0,
									xmin = -3,
									xmax = 3,
									axis x line = center,
									axis y line = center
								]
							\addplot [color = TUDa-1b, line width = 2pt, no marks] { max(0, x) };
						\end{axis}
					\end{tikzpicture}
					\caption{Rectified Linear Unit ReLU \( \max(0, z) \)}
					\label{fig:relu}
				\end{figure}
			% end
		% end
	% end

	\section{Forward- and Backpropagation}
		\begin{itemize}
			\item \emph{Forward propagation} computes the activations for each layer, the outputs for each layer and the resulting loss function.
			\item \emph{Backward propagation} computes the contribution of each parameter to the loss (the gradient) and updates the parameters using gradient descent.
		\end{itemize}

		\subsection{Backpropagation}
			\begin{itemize}
				\item Backpropagation, also known as \emph{backprop}, calculates the gradient with the chain rule.
				\item Problems in multi-layer networks:
					\begin{itemize}
						\item Non-convex, many local optima
						\item Might get stuck in a poor local optima
						\item The design of a working backpropagation algorithm is quite complex, causing the second winter of ML between 2000 and 2014.
					\end{itemize}
				\item But these methods work very well!
			\end{itemize}

			\paragraph{Example}
				In a simple neural network with no hidden layer and just one neuron per layer, the derivative for the bias \( \frac{\partial L}{\partial b} \) computes as
				\begin{equation}
					\frac{\partial L}{\partial b} = \frac{\partial L}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial a_1} \frac{\partial a_1}{\partial z_0} \frac{\partial z_0}{\partial a_0} \frac{\partial a_0}{\partial b}
				\end{equation}
				The computations for the weight matrix \( \frac{\partial L}{\partial W} \) are similar
			% end

			\subsubsection{Skip Connections}
				\begin{itemize}
					\item For parameters that are closer to the input, the gradient needs to flow from the loss to those parameters.
					\item In very deep neural networks, the application of the chain rule may lead to a zero gradient, causing no learning to happen.
					\item One solution is to use \emph{skip connections} to "jump" over layers.
				\end{itemize}
			% end
		% end

		\subsection{Formulas}
			Notice that the loss function is not always squared error, so the derivative of the loss function might change. Let \(\vec{y}^d\) be the desired output.
			
			\paragraph{Forwardpropagation}
				\begin{align}
					L(\vec{y}^d, \vec{y}) &= \frac{1}{2} (\vec{y}^d - \vec{y})^T (\vec{y}^d - \vec{y}) \\
					\vec{y} &= \hat{W}_n \begin{bmatrix} \vec{a}_n^T & 1 \end{bmatrix} \\
					\vec{a}_n &= \vec{f}_{n - 1} (\vec{z}_{n - 1}) \\
					\vec{z}_{n - 1} &= \hat{W}_{n - 1} \begin{bmatrix} \vec{a}_{n - 1}^T & 1 \end{bmatrix} \\
					\vec{a}_{n - 1} &= \vec{f}_{n - 2} (\vec{z}_{n - 2}) \\
					\vec{z}_{n - 2} &= \hat{W}_{n - 2} \begin{bmatrix} \vec{a}_{n - 2}^T & 1 \end{bmatrix} \\
					&\hspace{0.2cm}\vdots \\
					\vec{a}_2 &= \vec{f}_1(\vec{z}_1) \\
					\vec{z}_1 &= \hat{W}_1 \begin{bmatrix} \vec{a}_1^T & 1 \end{bmatrix} \\
					\vec{a}_1 &= \vec{x}
				\end{align}
			% end
			
			\paragraph{Backpropagation} % TODO: NN: Backprop: Wrong?
				\begin{align}
					\dd{L} &= -(\vec{y}^d - \vec{y})^T \dd{y} \\
					\dd{\vec{y}} &= W_n \dd{\vec{a}_n} \\
					\dd{\vec{a}_n} &= \vec{f}_{n - 1}' (\vec{z}_{n - 1}) \dd{\vec{z}_{n - 1}} \\
					\dd{\vec{z}_{n - 1}} &= W_{n - 1} \dd{\vec{a}_{n - 1}} \\
					\dd{\vec{a}_{n - 1}} &= \vec{f}_{n - 2}' (\vec{z}_{n - 2}) \dd{\vec{z}_{n - 2}} \\
					\dd{\vec{z}_{n - 2}} &= W_{n - 2} \dd{\vec{a}_{n - 2}} \\
					&\hspace{0.2cm}\vdots \\
					\dd{\vec{a}_2} &= \vec{f}_1' (\vec{z}_1) \dd{\vec{z}_1} \\
					\dd{\vec{z}_1} &= W_1 \dd{\vec{a}_1} \\
					\dd{\vec{a}_1} &= \textrm{d}\vec{x}
				\end{align}
				
				So \( \textrm{d}L \) can be computes as
				\begin{equation}
					\dd{L} = -(\vec{y}^d - \vec{y})^T \Bigg( \prod_{k = n}^{K + 1} W_k \vec{f}_{k - 1}' (\vec{z}_{k - 1}) \Bigg) \dd{\vec{z}_K}
				\end{equation}
				for all layers \( K \in \{\, 1, 2, \cdots, n \,\} \).
			% end

			\subsubsection{Vectorized} % 11.50, 11.51
				\todo{NN: Vectorized}
			% end
		% end

		\subsection{Approximating the Gradient}
			Instead of calculating backpropagation, the gradient can also be estimated using the finite differences for changes in each parameter \( W_j \):
			\begin{equation}
				\frac{\partial L}{\partial w_j} \approx \frac{L(\vec{w} + \varepsilon \vec{u}_j) - L(\vec{w})}{\varepsilon}
			\end{equation}
			where \(\varepsilon\) is a small perturbation and \(\vec{u}_j\) is a unit vector in the \(j\) direction.
			
			But, for a network with \(M\) parameters, forward propagation has to be done \(M\) times! This is very costly for large networks.
			
			Backpropagation can compute the derivatives by forwardpropagate and backpropagate each one time, no matter how many parameters.
		% end
	% end

	\section{Gradient Descent}
		The basic update rule for gradient descent is
		\begin{equation}
			\hat{W}^{k + 1} = \hat{W}^k - \alpha \nabla_W L
		\end{equation}
		with the learning rate \(\alpha\) and the gradient \( \nabla_W L \) from backpropagation.
		
		The key question are:
		\begin{itemize}
			\item How to update \(W\)?
			\item How to choose \(\alpha\)?
			\item How to initialize \(W\)?
		\end{itemize}

		\subsection{When to update \(W\)?}
			\begin{itemize}
				\item \textbf{Full} Gradient Descent
					\begin{itemize}
						\item Use the whole training set at once.
						\item This is expensive for large data sets.
					\end{itemize}
			\end{itemize}
			\begin{equation}
				\nabla_W J = \frac{1}{n} \sum_{i = 1}^{n} \nabla_W L(\vec{x}_i, \vec{y}_i, W)
			\end{equation}
			\begin{itemize}
				\item \textbf{Stochastic} Gradient Descent
					\begin{itemize}
						\item Use one data point of the training set.
						\item Needs an adaptive learning rate \( \eta_t \) with \( \sum_{t = 1}^{\infty} \eta_t = \infty \) and \( \sum_{t = 1}^{\infty} \eta_t^2 < \infty \).
						\item The gradient estimation has a high variance.
					\end{itemize}
			\end{itemize}
			\begin{equation}
				\nabla_W J \approx \nabla_W L(\vec{x}_i, \vec{y}_i, W)
			\end{equation}
			\begin{itemize}
				\item \textbf{Mini-Batch} Gradient Descent
					\begin{itemize}
						\item Use a subset of the training set.
					\end{itemize}
			\end{itemize}
			\begin{equation}
				\nabla_W J \approx \frac{1}{k} \sum_{i = 1}^{k} \nabla_W L(\vec{x}_i, \vec{y}_i, W)
			\end{equation}
			\begin{itemize}
				\item The collected data can introduce a strong bias in successive data samples, so the data must be shuffled before applying stochastic or mini-batch gradient descent. This way, the bias can be reduced (but not removed).
				\item Nowadays, the usage of the term \emph{stochastic gradient descent} refers to mini-batch gradient descent.
			\end{itemize}
		% end

		\subsection{Adaptive Learning Rate}
			\label{sec:adaptiveLearningRate}
			
			\begin{itemize}
				\item A very high learning can increase the loss a lot, while a too low learning rate causes the algorithm to run long until convergence.
				\item Finding the right learning rate is pretty hard.
				\item Adaptive learning rates that change over time can help as the learning rate should be higher in flat regions, but small in valleys (to not "jump out").
			\end{itemize}

			\paragraph{Momentum}
				\subparagraph{Insight}
				\begin{itemize}
					\item Running Average
				\end{itemize}
				\begin{equation}
					\bar{m}_0 = 0, \quad \bar{m}_{k + 1} = \gamma_k\bar{m}_k + (1 - \gamma_k) m_k
				\end{equation}
				\begin{itemize}
					\item Geometric Average (constant \(\gamma\))
				\end{itemize}
				\begin{equation}
					\bar{m}_{k + 1} = (1 - \gamma) \sum_{i = 1}^{k} \gamma^{k - i} m_i
				\end{equation}
				\begin{itemize}
					\item Arithmetic Average (\( \gamma_k = \frac{k - 1}{k} \))
				\end{itemize}
				\begin{equation}
					\bar{m}_{k + 1} = \frac{1}{k} \sum_{i = 1}^{k} m_i
				\end{equation}
				
				\subparagraph{Practically}
				Applied to momentum terms with \( M_0 = 0 \):
				\begin{align}
					M_{k + 1} &= \gamma_k M_k + (1 - \gamma_k) \nabla_W J(W_k) \\
					W_{k + 1} &= W_k - \alpha_k M_{k + 1}
				\end{align}
			% end

			\paragraph{Adadelta}
				\subparagraph{Insight}
				Take large steps in plateaus as they do not have much risk and take smaller steps in steep areas.
				
				\subparagraph{Practically}
				Normalize by the running average of the gradient norm with a small \(\varepsilon\) to prevent from dividing by zero, \( V_0 = 0 \) and the Hadamard product \( \odot \):
				\begin{align}
					G_k &= \nabla_W J(W_k) \\
					V_{k + 1} &= \gamma V_k + (1 - \gamma) G_k \odot G_k \\
					W_{k + 1, ij} &= W_{k + 1, ij} - \frac{\alpha_k}{\sqrt{V_{k, ij} + \varepsilon}} G_{k, ij}
				\end{align}
				
				\subparagraph{Note}
				There exist two versions: One with the \(\varepsilon\) in and out of the square root, but both in the fraction.
			% end

			\paragraph{Adam}
				\subparagraph{Insight}
				Combine momentum term with Adagrad.
				
				\subparagraph{Practically}
				Just combine both equations:
				\begin{align}
					G_k &= \nabla_W J(W_k) \\
					V_{k + 1} &= \gamma_1 V_k + (1 - \gamma_1) G_k \odot G_k \\
					M_{k + 1} &= \gamma_2 M_k + (1 - \gamma_2) G_k \\
					W_{k + 1, ij} &= W_{k + 1, ij} - \frac{\alpha_k}{\sqrt{\eta_{\gamma_1^k} V_{k, ij} + \varepsilon}} \eta_{\gamma_1^k} M_{k + 1, ij}
				\end{align}
				The initialization \( V_0 = M_0 = 0 \) leads to an underestimation fixed by \( \eta_{\gamma_i^k} = \frac{1}{1 - \gamma_i^k} \). With \( \gamma_1 = 0.9 \), \( \gamma_2 = 0.999 \) and \( \varepsilon = 10^{-8} \), Adam is not too sensitive to parameter changes.
				
				\subparagraph{Note}
				Adam violates the convergence guarantees\dots
			% end
		% end

		\subsection{Small Neural Networks}
			For small neural networks, there exist better methods to get the direction of descent. But these are all too expensive for big networks.

			\paragraph{Hessian Approaches}
				\begin{itemize}
					\item Get second-order descent with \( \delta\vec{w} = H^{-1} \nabla J \) and the Hessian \( H = \nabla^2 J \).
					\item Estimate the Hessian with BFGS method.
					\item Use line search instead of a fixed learning rate.
				\end{itemize}
			% end

			\paragraph{Conjugate Gradient}
				\begin{itemize}
					\item Momentum term with variable learning rate, e.g.
				\end{itemize}
				\begin{equation}
					\delta\vec{w}_t = \nabla J(\vec{w}_t) + \frac{\lVert \nabla J(\vec{w}_t) \rVert ^2}{\lVert \nabla J(\vec{w}_{t - 1}) \rVert^2} \delta\vec{w}_{t}
				\end{equation}
				\begin{itemize}
					\item[] with Powell restarts.
					\item Problem: Does not work well with stochastic gradient descent.
				\end{itemize}
			% end

			\paragraph{Levenberg-Marquart}
				Linearize the network
				\begin{equation}
					f(\vec{x}_i, \vec{w}) = f(\vec{x}_i, b) + \nabla_{\vec{w}} f(\vec{x}_i, \vec{b}) \big\vert_{\vec{w} = b}^T \delta\vec{w} = \vec{f}_{i0} + \vec{J}_i \delta\vec{w}
				\end{equation}
				and solve the least squares regression problem
				\begin{equation}
					J \approx \frac{1}{2} \lVert \vec{y} - (\vec{f}_0 + \vec{J} \delta\vec{w}) \rVert^2 + \frac{1}{2} \delta\vec{w}^T W \delta\vec{w}
				\end{equation}
				yielding \( \delta\vec{w} = (J^T J + W)^{-1} \vec{J}_i^T (\vec{y} - \vec{f}_0) \)
				
				\begin{itemize}
					\item This is basically the Gauss-Newton-Method.
					\item Levenberg \( W = \lambda I \) keeps the matrix invertible
					\item Marquardt \( W = \lambda \, \textrm{diag}(J^T J) \)
					\item Adadelta approximates Levenbergs method parameterwise.
				\end{itemize}
			% end
		% end

		\subsection{Initialization}
			\paragraph{Random Initialization}
				\begin{itemize}
					\item Can lead to problems in gradient descent.
					\item For instance, large absolute values cause problems with sigmoid and negative values cause problems with ReLU.
				\end{itemize}
			% end

			\paragraph{Gaussian Initialization}
				\begin{itemize}
					\item Draw the parameters from a Gaussian: \( W_{kij} \sim \mathcal{N}\,(0, m^{-1}) \), \( b_k \sim \mathcal{N}\,(0, 1) \)
					\item This basically normalizes the parameters.
				\end{itemize}
			% end

			\paragraph{Xavier/Normalized Initialization}
				Initialize the weights from a uniform distribution (where \(n_i\) is the number of neurons in the \(i\)-th layer and \(W_j\) are the parameters of the layer connecting the hidden layer \(j\) and the next hidden layer \(j + 1\)):
				\begin{equation}
					W_j \sim U\Bigg[ -\frac{\sqrt{6}}{\sqrt{n_j + n_{j + 1}}}, \quad \frac{\sqrt{6}}{\sqrt{n_j + n_{j + 1}}} \Bigg]
				\end{equation}
				Note: Xavier assumes that the activation functions are symmetric and linear around zero, so this works for \(\tanh\) or sigmoid, but not for ReLU!
			% end
		% end
	% end

	\section{Overfitting}
		\begin{itemize}
			\item Neural networks have hundreds, thousands or millions of parameters.
			\item But in most cases, no datasets with such many samples are available.
			\item So neural networks are prone to overfit.
			\item Overfitting can be fought with an algorithmic realization of a prior on the parameters:
				\begin{itemize}
					\item \textbf{Regularization}
					\item \textbf{Early stopping} \\ Stop the training when the validation error starts rising again.
					\item \textbf{Input noise augmentation} \\ Adding noise \(\vec{\epsilon}_i\) to the inputs reduces the chance of overfitting \( \tilde{\vec{x}}_i = \vec{x}_i + \vec{\epsilon}_i \).
					\item \textbf{Dropout} \\ Focus efficiently on the relevant neurons and prune others by zeroing out the weights intermittently and letting a subset of neurons predict:
				\end{itemize}
		\end{itemize}
		\begin{equation}
			a_i = f_i(z) d_i \textrm{ with } d_i \in \{\, 0, 1 \,\} \textrm{ and } p(d_i = 1) = p_\textrm{dropout} = 0.5
		\end{equation}
		\begin{itemize}
			\item[]
				\begin{itemize}
					\item \textbf{Weight decay} \\ A ridge loss \( J(\vec{w}) = L(\vec{w}) + \lambda \vec{w}^T \vec{w} \) yields weight decay:
				\end{itemize}
		\end{itemize}
		\begin{equation}
			\vec{w}_{k + 1} = \vec{w}_k - \alpha_k \big( \nabla_{\vec{w}} L(\vec{w}_k) + \lambda \vec{w}_k \big) = (1 - \alpha_k) \vec{w}_k + \alpha_k \nabla_{\vec{w}} L(\vec{w}_k)
		\end{equation}

		\subsection{Batch Normalization}
			\begin{itemize}
				\item Covariate shift
					\begin{itemize}
						\item Changes in the input distribution make learning hard.
						\item This is especially problematic with mini-batches.
						\item Hidden values change as their preceding layers change.
					\end{itemize}
				\item This can be fought by \emph{batch normalization}:
			\end{itemize}
			\begin{equation}
				\tilde{x}_i = \frac{x_i - \mu_i}{\sqrt{\sigma_i^2 + \epsilon}}
			\end{equation}
			\begin{itemize}
				\item[]
					\begin{itemize}
						\item This is like dropout with better performance.
						\item Similar to normalization in ridge regression.
						\item More complex: removal of batch normalization.
					\end{itemize}
			\end{itemize}
		% end
	% end

	\section{Theoretical Results}
		\begin{itemize}
			\item The features are learned rather than hand-crafted by the machine learner.
			\item More layers capture more invariances.
			\item More data is needed to train deeper networks.
			\item More computation power (e.g. on GPUs).
			\item Better regularization methods like dropout.
			\item New nonlinearities: max pooling, ReLU
			\item However, the understanding of what deep networks really so remains shallow.
			\item Theory Fields
				\begin{itemize}
					\item Approximation, depth width and invariance theory
					\item Generalization and regularization theory
				\end{itemize}
		\end{itemize}
	% end

	\section{Other Network Architectures}
		% TODO: NN: Hubel and Wisel Receptive Fields; slide 11.92

		\subsection{Convolutional Neural Network (CNN)}
			\begin{itemize}
				\item \emph{Convolutional neural networks} (CNNs) are particularly suited for feature extraction in spatially correlated data like images.
				\item Feature maps are computed by applying convolutional kernels to the input or feature maps.
				\item Pooling reduces the dimensionality. For instance, \( \textrm{max\_pooling}(k) \) takes the pixels with the largest values among \(k\) neighboring pixels.
				\item Instead of computing the pre-activation of a layer with a matrix, use a convolution operation:
			\end{itemize}
			\begin{equation}
				s(t) = (x \ast w)(t) = \int x(a) \, w \, (t - a) \dd{a}
			\end{equation}
			\begin{itemize}
				\item[] where \(x\) is the input signal and \(w\) is often called the kernel.
				\item This acts as a filter on the input.
			\end{itemize}
		
			\paragraph{Fully Connected vs. Convolutional}
				\begin{itemize}
					\item \textbf{Fully Connected}
						\begin{itemize}
							\item With high dimensional input data the number of parameters explodes (gray image with 1000x1000 pixels, hidden layer with 1000 neurons has 1 billion parameters, just for the first layer).
							\item Does not extract local features which are usually present in images.
						\end{itemize}
					\item \textbf{Convolutional}
						\begin{itemize}
							\item The learned parameters are the kernel weights which are much smaller than the input and shared over the whole input.
							\item Computes local features since the output of a kernel involves a computation over adjacent pixels.
						\end{itemize}
				\end{itemize}
			% end
		% end

		\subsection{Recurrent Neural Network (RNN)}
			\begin{itemize}
				\item \emph{Recurrent neural networks} (RNNs) are networks with memory where the output is fed into the input again.
				\item This can be used for time dependent/series data:
					\begin{itemize}
						\item Natural language processing
						\item Speech recognition
						\item Dynamical systems
						\item Stock market
						\item Brain-computer interface
						\item etc.
					\end{itemize}
			\end{itemize}
		% end

		\subsection{Long Short-Term Memory Network (LSTM)}
			\begin{itemize}
				\item Gradient computation in RNNs is done with \emph{backpropagation through time} (BPTT). A parameter is updates by adding all contributions to the loss over time.
				\item This leads to vanishing and exploding gradients.
				\item \emph{Long short-term memory networks} (LSTMs) fight the gradient problems with a different architecture to let the gradient flow better in BPTT and are thus capable of more efficient learning than traditional RNNs.
			\end{itemize}
		% end
	% end

	\section{Applications} % N/A
		\todo{NN: Applications}

		\subsection{Computer Vision} % 11.99, 11.100
			\todo{NN: Applications: CV}
		% end

		\subsection{Autonomous Systems} % 11.101, 11.102, 11.103, 11.104
			\todo{NN: Applications: AS}
		% end
	% end
	
	
	\section{Radial Basis Function Networks} % 13.7, 13.8, 13.9?, 13.10, 13.11, 13.12?, 13.13, 13.14, 13.15
		\todo{NN: RADIAL BASIS FUNCTION NETWORKS}
		
		A multi-layer perceptron uses univariate projections to span the space of data.
		\begin{itemize}
			\item Pros
				\begin{itemize}
					\item Universal function approximation
					\item Large range generalization (extrapolation)
					\item Good for high dimensional data
				\end{itemize}
			\item Cons
				\begin{itemize}
					\item Hard to train
					\item Danger of interference
				\end{itemize}
		\end{itemize}
		\emph{Radial basis function networks} (RBFNs) use a different approach:
		\begin{itemize}
			\item Only one hidden layer
			\item Use spatially localized kernels for learning (note: there are basis functions that are not spatially localized).
			\item They use radial basis functions as activation functions, i.e. functions \( \phi\big(\lVert \vec{x} - \vec{c} \rVert\big) \) that only depend on the norm of some data \(\vec{x}\) around some center \(\vec{c}\), e.g. the Gaussian kernel (note that \(k\) is the iteration of gradient descent):
		\end{itemize}
		\begin{equation}
			\phi(\vec{x}, \vec{x}_k) = \exp\Bigg\{ -\frac{(\vec{x} - \vec{c}_k)^T D (\vec{x} - \vec{x}_k)}{2} \Bigg\}
		\end{equation}
		\begin{itemize}
			\item[] with some positive definite \(D\).
			\item The "output layer" then is just a linear regression \( y = \sum_{i = 1}^{k} w_i \phi(\vec{x}, \vec{x}_k) = \vec{w}^R \Phi\phi(\vec{x}, \vec{x}_k) \)
			\item They often need regularization (e.g. ridge regression). The non-ridge case with squares loss yields the solution
		\end{itemize}
		\begin{equation}
			\vec{w} = (\Phi^T \Phi)^{-1} \Phi^T \vec{t}
		\end{equation}
		\begin{itemize}
			\item[] with \( \vec{t} = \begin{bmatrix} t_1 & \cdots & t_n \end{bmatrix}^T \) and \( \Phi = \begin{bmatrix}
				\phi_{11} & \cdots & \phi_{1m} \\
				\vdots & \ddots & \cdots \\
				\phi_{n1} & \cdots & \phi_{nm}
				\end{bmatrix} \).
			\item The "input layer" can be optimized using gradient descent w.r.t. the distance metric and the center of the RBFs.
			\item Gradient descent can make \(D\) non-positive definite \(\implies\) use Cholesky decomposition.
			\item An iterative procedure is needed for optimization, i.e. alternately update of \(\vec{w}\) and \(\vec{x}_k\) and \(D_k\).
			\item Summarized, RBFs are powerful and efficient for learning, but the number of RBFs and the hyperparameter optimization is important and difficult!
			\item Theoretical remark: Poggio and Girosi (1990) showed that RBF networks arise naturally from minimizing the penalized cost function
		\end{itemize}
		\begin{equation}
			J = \frac{1}{2} \sum_n \big(t_n - y(x_n)\big)^2 + \frac{1}{2} \gamma \int \big\lvert G(\vec{x}) \big\rvert^2 \dd{\vec{x}}
		\end{equation}
		\begin{itemize}
			\item[] with, e.g. \( G(\vec{x}) = \frac{\partial^2 y}{\partial \vec{x}^2} \), a smoothless prior.
		\end{itemize}
	% end
	
	\section{Wrap-Up}
		\begin{itemize}
			\item Neural networks and the relation to the brain
			\item Building of stacks of features
			\item Why one network layer is enough but impractical
			\item Forward and backwardpropagation
			\item Different ways of gradient descent
				\begin{itemize}
					\item Full, stochastic, mini-batch
					\item Speedup via learning rate adaption
					\item Initialization of parameters
				\end{itemize}
			\item Overfitting causes and defenses
			\item CNNs for spatially correlated data
			\item LSTMs for time series data
			\item RBF networks
		\end{itemize}
	% end
% end

\chapter{Support Vector Machines}
	All machine learning is generally about lowering the structural risk bound in the true risk:
	\begin{equation}
		R(\vec{w}) \leq R_{\emp}(\vec{w}) + \epsilon(N, p^\ast, h)
	\end{equation}
	where \( N \) is the number of samples, \(p^\ast\) is the probability that the bound is met and \(h\) is the VC-dimension.
	
	\begin{itemize}
		\item Classical machine learning algorithms keep \( \epsilon(N, p^\ast, h) \) constant and try to minimize \( R_{\emp}(\vec{w}) \). The confidence interval is fixed by keeping some model parameters fixed, e.g. the number of neurons on a neural network.
		\item \emph{Support vector machines} keep \(R_{\emp}(\vec{x})\) constant and minimize \( \epsilon(N, p^\ast, h) \). With separable data, \( R_{\emp}(\vec{x}) = 0 \). The confidence interval is controlled by changing the VC-dimension ("capacity control").
	\end{itemize}

	\section{Linear SVMs}
		\begin{itemize}
			\item Use linear classifiers.
			\item Approximate implementation of the structural risk minimization principle.
			\item If the data is linearly separable, the empirical risk of the SVM will be zero and the risk bound will be approximately minimized.
			\item SVMs have built-in "guaranteed" generalization abilities.
		\end{itemize}
	
		Assuming linearly separable data and given \(N\) sample training points \( \{\, \vec{x}_i, y_i \,\}_{i = 1}^N \) with \( \vec{x}_i \in \R^d \) and \( y_i \in \{\, -1, +1 \,\} \). The there exist a hyperplane \( y(\vec{x}) = \vec{w}^T \vec{x} + b \) that separates the data. Intuitively, the "correct" hyperplane is the hyperplane with the maximum distance to the data of each class (called the \emph{margin}). So the goal is to maximize this margin to minimize the VC-dimension.
		
		Formally, this makes sense given the key result from Vapnik: If the data points lie in a sphere of radius \(R\), \( \lVert \vec{x}_i \rVert < R \) and the margin of the linear classifier in \(d\) dimensions is \(\gamma\), then
		\begin{equation}
			h \leq \min\Bigg\{\, d, \quad \bigg\lceil \frac{2R^2}{\gamma^2} \bigg\rceil \,\Bigg\}
		\end{equation}
		So maximizing the margin lowers the upper bound on the VC-dimension!
		
		\paragraph{Example}
			Figure~\ref{fig:svm} shows a linear support vector machine working on a linear separable dataset with marked support vectors and the margin.
			
			\begin{figure}
				\centering
				\includegraphics{tmp-svm-linear.pdf}
				\caption{Linear Support Vector Machine}
				\label{fig:svm}
			\end{figure}
		% end

		\subsection{Optimization Formulation}
			Find a hyperplane that separates the data linearly
			\begin{equation}
				y_i (\vec{w}^T \vec{x}_i + b) \geq 1 \quad\forall i
			\end{equation}
			and enforce \( y_i (\vec{w}^T \vec{x}_i + b) = 1 \) for at least one data point. The data points lie directly on the margin and are called \emph{support vectors}.
			
			The distance to the hyperplane is
			\begin{equation}
				\frac{y(\vec{x}_i)}{\lVert \vec{w} \rVert} = \frac{\vec{w}^T \vec{x}_i + b}{\lVert \vec{w} \rVert}
			\end{equation}
			so the margin is \( \frac{1}{\lVert \vec{w} \rVert} \).
			
			As maximizing the margin \( \frac{1}{\lVert \vec{w} \rVert} \) is equivalent to minimize \( \lVert \vec{w} \rVert^2 \), the problem can be formulated as a quadratic minimization problem with linear constraints:
			\begin{align}
				\arg\min_{ \vec{w}, b } \, J(\vec{w}, b) &= \frac{1}{2} \lVert \vec{w} \rVert^2 \\
				\textrm{s.t.} \qquad
				y_i (\vec{w}^T \vec{x}_i + b) - 1 &\geq 0 \quad\forall i
			\end{align}
			This yields the Lagrangian formulation:
			\begin{equation}
				L(\vec{w}, b, \vec{\alpha}) = \frac{1}{2} \lVert \vec{w} \rVert^2 - \sum_{i = 1}^{N} \alpha_i \big( y_i (\vec{w}^T \vec{x}_i + b) - 1 \big)
			\end{equation}
			Taking the derivative w.r.t. \(\vec{w}\) and \(b\) yields:
			\begin{align}
				\frac{\partial L(\vec{w}, b, \vec{\alpha})}{\partial \vec{w}} \overset{!}{=} 0 &\implies \vec{w} = \sum_{i = 1}^{N} \alpha_i y_i \vec{x}_i \label{eq:primal1} \\
				\frac{\partial L(\vec{w}, b, \vec{\alpha})}{\partial b} \overset{!}{=} 0 &\implies \sum_{i = 1}^{N} \alpha_i y_i = 0 \label{eq:primal2} \\
			\end{align}
			So the separating hyperplane is a linear combination of the input data. But the \(\alpha_i\) are still unknown.

			\subsubsection{Dual Formulation} % 12.16, 12.17, 12.18, 12.19, 12.20
				First rewrite the Lagrangian and then insert the equations \ref{eq:primal1} and \ref{eq:primal2} to get the dual:
				\begin{align}
					&& L(\vec{w}, b, \vec{\alpha}) &= \frac{1}{2} \lVert \vec{w} \rVert^2 - \sum_{i = 1}^{N} \alpha_i \big( y_i (\vec{w}^T \vec{x}_i + b) - 1 \big) & \\
					&& &= \frac{1}{2} \lVert \vec{w} \rVert^2 - \sum_{i = 1}^{N} \alpha_i y_i \vec{w}^T \vec{x}_i - b \sum_{i = 1}^{N} \alpha_i y_i + \sum_{i = 1}^{N} \alpha_i & \\
					\intertext{Using \( \sum_{i = 1}^{N} \alpha_i y_i = 0 \):}
					\implies && \hat{L}(\vec{w}, \vec{\alpha}) &= \frac{1}{2} \lVert \vec{w} \rVert^2 - \sum_{i = 1}^{N} \alpha_i y_i \vec{w}^T \vec{x}_i + \sum_{i = 1}^{N} \alpha_i & \\
					\intertext{Using \( \vec{w} = \sum_{i = 1}^{N} \alpha_i y_i \vec{x}_i \):}
					&& &= \frac{1}{2} \lVert \vec{w} \rVert^2 - \sum_{i = 1}^{N} \sum_{j = 1}^{N} \alpha_i \alpha_j y_i y_j \big(\vec{x}_j^T \vec{x}_i\big) + \sum_{i = 1}^{N} \alpha_i & \\
					&& &= \frac{1}{2} \vec{w}^T \vec{w} - \sum_{i = 1}^{N} \sum_{j = 1}^{N} \alpha_i \alpha_j y_i y_j \big(\vec{x}_j^T \vec{x}_i\big) + \sum_{i = 1}^{N} \alpha_i & \\
					\implies && \tilde{L}(\vec{\alpha}) &= \frac{1}{2} \sum_{i = 1}^{N} \sum_{j = 1}^{N} \alpha_i \alpha_j y_i y_j \vec{x}_i^T \vec{x}_j - \sum_{i = 1}^{N} \sum_{j = 1}^{N} \alpha_i \alpha_j y_i y_j \big(\vec{x}_j^T \vec{x}_i\big) + \sum_{i = 1}^{N} \alpha_i & \\
					&& &= \sum_{i = 1}^{N} \alpha_i - \frac{1}{2} \sum_{i = 1}^{N} \sum_{j = 1}^{N} \alpha_i \alpha_j y_i y_j \big(\vec{x}_j^T \vec{x}_i\big) & \label{eq:wolfe}
				\end{align}
				The last equation \ref{eq:wolfe} is called the \emph{Wolfe dual formulation}.
				
				The original problem can now be solved by maximizing the dual function \(\tilde{L}\):
				\begin{align}
					\arg\min_{ \vec{\alpha} } \, \tilde{L}(\alpha) &= \sum_{i = 1}^{N} \alpha_i - \frac{1}{2} \sum_{i = 1}^{N} \sum_{j = 1}^{N} \alpha_i \alpha_j y_i y_j \big(\vec{x}_j^T \vec{x}_i\big) \\
					\textrm{s.t.} \qquad
					\alpha_i &\geq 0 \quad\forall i \\
					\sum_{i = 1}^{N} \alpha_i y_i &= 0
				\end{align}
				As almost all \(\alpha_i \approx 0\), the separating hyperplane is given by \(N_S\) support vectors
				\begin{equation}
					\vec{w} = \sum_{i = 1}^{N_S} \alpha_i y_i \vec{x}_i
				\end{equation}
				The offset \(b\) can also be computed, but the derivation is skipped here:
				\begin{equation}
					b = \frac{1}{N_S} \sum_{i = 1}^{N_s} \Bigg( y_i - \sum_{j = 1}^{N_S} \alpha_j y_j \big( \vec{x}^T \vec{x} \big) \Bigg)
				\end{equation}
				
				\begin{itemize}
					\item Both the original (primal) SVM formulation and the dual rare quadratic optimization problems with linear constraints, called \emph{quadratic programming problems}. These are convex and have a unique optima that can easily be compute, e.g. with libraries like \texttt{cvxopt}.
					\item The dual form is especially useful for nonlinear SVMs!
				\end{itemize}
			% end
		% end

		\subsection{Sparsity}
			\begin{itemize}
				\item Almost all \(\alpha_i\) are roughly zero, so there are only a few support vectors.
				\item As the hyperplane was written as
			\end{itemize}
			\begin{equation}
				\vec{w} = \sum_{i = 1}^{N} \alpha y_i \vec{x}_i \label{eq:svmW}
			\end{equation}
			\begin{itemize}
				\item[] it can be described by only a few data points.
				\item So by once calculating the support vectors, the SVM is described by only a few vectors that do not take up much memory/storage!
			\end{itemize}
		% end
	% end

	\section{Nonlinear SVMs}
		Nonlinear SVMs are more powerful by using a feature transformation
		\begin{equation}
			\vec{x} \in \R^d \qquad \phi : \R^d \rightarrow H
		\end{equation}
		into a (possible higher-dimensional) feature space \(H\). The hyperplane is then written in this feature space, yielding a linear classifier in \(H\)
		\begin{equation}
			\vec{w}^T \phi(\vec{x}) + b = 0
		\end{equation}
		But in \(R^d\), the classifier is nonlinear and thus can separate nonlinear data. This is the same trick as in least-squares regression: Make the data linear separable rather than building a complex nonlinear classifier.

		\subsection{Optimization Formulation}
			The nonlinear dual form (with nonlinear transformations) can be obtained as
			\begin{align}
				\arg\min_{ \vec{\alpha} } \, \tilde{L}(\alpha) &= \sum_{i = 1}^{N} \alpha_i - \frac{1}{2} \sum_{i = 1}^{N} \sum_{j = 1}^{N} \alpha_i \alpha_j y_i y_j \big(\phi(\vec{x}_j)^T \phi(\vec{x}_i)\big) \\
				\textrm{s.t.} \qquad
				\alpha_i &\geq 0 \quad\forall i \\
				\sum_{i = 1}^{N} \alpha_i y_i &= 0
			\end{align}
			where the actual feature transformation \( \phi(\vec{x}_i) \) only appears in scalar products with another \( \phi(\vec{x}_j) \).
			
			Also the discriminant function \( y(\vec{x}) = \vec{w}^T \phi(\vec{x}) + b \) can be written by just scalar products of the feature transformations (using \( \vec{w} = \sum_{i = 1}^{N_S} \alpha_i y_i \phi(\vec{x}_i) \)):
			\begin{equation}
				y(\vec{x}) = \vec{w}^T \phi(\vec{x}) + b = \sum_{i = 1}^{N_S} \alpha_i y_i \big(\phi(\vec{x}_i)^T \phi(\vec{x})\big) + b
			\end{equation}
			
			This leads to the kernel trick.
		% end

		\subsection{Kernel Trick}
			\label{sec:svmKernelTrick}
		
			As both the dual and the discriminant function can be written in terms of the scalar product of the features, only this has to be calculated. The \emph{kernel trick} replaces every occurrence of the scalar product with a kernel function
			\begin{equation}
				K(\vec{x}_i, \vec{x}_j) = \phi(\vec{x}_i)^T \phi(\vec{x}_j)
			\end{equation}
			So if a function can be found that is equivalent to this scalar product, the mapping into a higher-dimensional features space can be avoided. This even means the feature space can be infinity-dimensional!

			\subsubsection{Polynomial Kernel}
				A polynomial kernel \( K(\vec{x}, \vec{y}) = (\vec{x}^T \vec{y})^d \) of degree \(d\) is the kernel for a feature transformation into the space of all ordered monomials of degree \(d\). The transformed space H then has the dimensionality
				\begin{equation}
					\dim(H) = { d + N - 1 \choose d }
				\end{equation}
				where \(N\) is the dimension of the untransformed input space. The classifier then has the VC-dimension \( \dim(H) + 1 \).
				
				\paragraph{Example: \(d = 2\)}
					For \(d = 2\), the kernel becomes
					\begin{equation}
						K(\vec{x}, \vec{y}) = (\vec{x}^T \vec{y})^2 = x_1^2 y_1^2 + 2 x_1 x_2 y_1 y_2 + y_1^2 y_2^2
					\end{equation}
					which is equivalent to the scalar product
					\begin{equation}
						\phi(\vec{x})^T \phi(\vec{y}) =
							\begin{bmatrix}
								x_1^2 \\
								\sqrt{2} x_1 x_2 \\
								x_2^2
							\end{bmatrix}^T
							\begin{bmatrix}
								y_1^2 \\
								\sqrt{2} y_1 y_2 \\
								y_2^2
							\end{bmatrix}
						= x_1^2 y_1^2 + 2 x_1 x_2 y_1 y_2 + x_2^2 y_2^2
					\end{equation}
				% end
			% end

			\subsubsection{Radial Basis Function Kernel (RBF)}
				\begin{equation}
					K(\vec{x}, \vec{y}) = \exp\Bigg\{ -\frac{\lVert \vec{x} - \vec{y} \rVert^2}{2\sigma^2} \Bigg\}
				\end{equation}
				\begin{itemize}
					\item Measures the similarity between \(\vec{x}\) and \(\vec{y}\).
					\item Construct an infinite-dimensional feature space \(H\) so that the hyperplane also has infinity VC-dimension.
					\item If the radius \(\sigma\) of the kernel is chosen too low, every data point has its "own" kernel, leading to massive overfitting. So the radius has to be bound to limit the VC-dimension.
				\end{itemize}
			% end
			
			\subsubsection{Mercer's Condition}
				To check whether a kernel really is a kernel, Mercer's condition can be used: A function \( K(\vec{x}, \vec{y}) \) is a valid kernel, if for every \( g(\vec{x}) \) with a converging integral \( \int g(\vec{x})^2 \dd{\vec{x}} < \infty \) it holds that
				\begin{equation}
					\iint K(\vec{x}, \vec{y}) \, g(\vec{x}) \, g(\vec{y}) \dd{\vec{x}} \dd{\vec{y}} \geq 0
				\end{equation}
				
				These kernels are known to satisfy Mercer's condition:
				\begin{itemize}
					\item Inhomogeneous polynomial kernel:
				\end{itemize}
				\begin{equation}
					K(\vec{x}, \vec{y}) = (\vec{x}^T \vec{y} + c)^d
				\end{equation}
				\begin{itemize}
					\item Gaussian RBF kernel:
				\end{itemize}
				\begin{equation}
					K(\vec{x}, \vec{y}) = \exp \{ -\frac{\lVert \vec{x} - \vec{y} \rVert^2}{2\sigma^2} \}
				\end{equation}
				\begin{itemize}
					\item Hyperbolic tangent kernel:
				\end{itemize}
				\begin{equation}
					K(\vec{x}, \vec{y}) = \tanh(a\vec{x}^T \vec{y} + b)
				\end{equation}
			% end
			
			\subsubsection{Constructing Kernels}
				\begin{itemize}
					\item Intuition: A kernel measures the similarity of two data points in the transformed space.
					\item The construction of a kernel most always be done by finding a feature transformation and encoding it in a kernel.
					\item The notion of similarity can also be encoded in the kernel function directly.
					\item Also, kernels can be combined with a few simple rules. Let \( K_1(\vec{x}, \vec{y}) \) and \( K_2(\vec{x}, \vec{y}) \) are valid kernels, then all of the following are also valid kernels:
				\end{itemize}
				\begin{gather}
					cK_1(\vec{x}, \vec{y}) \\
					K_1(\vec{x}, \vec{y}) + K_2(\vec{x}, \vec{y}) \\
					K_1(\vec{x}, \vec{y}) \, K_2(\vec{x}, \vec{y}) \\
					f(\vec{x}) \, K_2(\vec{x}, \vec{y}) \, f(\vec{y}) \\
					\cdots
				\end{gather}
			% end
		% end
	% end

	\section{Non-Separable Data}
		If the data is not separable, there are multiple solutions:
		\begin{enumerate}
			\item Simple solution: Transform the data points into a higher dimensional feature space so that they become linearly separable. But this leads to a high VC-dimension and is prone to overfit.
			\item Better solution: Let some data point allow to violate the margin.
		\end{enumerate}

		\subsection{Slack Variables}
			Introduce \emph{slack variables} \( \xi_i \geq 0 \) that, instead of requiring perfect linearly separable with
			\begin{equation}
				y_i (\vec{w}^T \vec{x}_i + b) \geq 1
			\end{equation}
			allow small violations from perfect separation:
			\begin{equation}
				y_i (\vec{w}^T \vec{x}_i + b) \geq 1 - \xi_i
			\end{equation}
			This allows the data points to be off by \( \xi_i \) from the margin.
			
			Even if the data is separable, it may be good to introduce slack variable for an occasional penalty.

			\subsubsection{Optimization Formulation}
				This changes the optimization problem to:
				\begin{align}
					\arg\min_{ \vec{w}, b } \, J(\vec{w}, b) &= \frac{1}{2} \lVert \vec{w} \rVert^2 + C\sum_{i = 1}^{N} \xi_i \\
					\textrm{s.t.} \qquad
					y_i(\vec{w}^T \vec{x}_i + b) - 1 + \xi_i &\geq 0 \quad\forall i \\
					\xi_i &\geq 0 \quad\forall i
				\end{align}
				with the weight \(C\) specifying the tradeoff that is made. A larger \(C\) allows more violations.
				
				This introduces a \emph{box constraint} \( 0 \leq \alpha_i \leq C \) to the dual formulation:
				\begin{align}
					\arg\min_{ \vec{\alpha} } \, \tilde{L}(\alpha) &= \sum_{i = 1}^{N} \alpha_i - \frac{1}{2} \sum_{i = 1}^{N} \sum_{j = 1}^{N} \alpha_i \alpha_j y_i y_j \big(\phi(\vec{x}_j)^T \phi(\vec{x}_i)\big) \\
					\textrm{s.t.} \qquad
					0 &\leq \alpha_i \leq C \quad\forall i \\
					\sum_{i = 1}^{N} \alpha_i y_i &= 0
				\end{align}
				This is also a quadratic programming problem and thus can be solved efficiently.
			% end
		% end
		
		\subsection{Lack of Sparseness}
			\begin{itemize}
				\item If there is a large class overlap, SVMs may need many support vectors.
				\item This reduces the sparsity.
				\item Alternative: Relevant vector machines (RVMs)
					\begin{itemize}
						\item Probabilistic alternative to SVMs.
						\item Gives much sparser results.
						\item But no notion of margin maximization.
					\end{itemize}
			\end{itemize}
		% end
	% end

	\section{Applications}
		\subsection{Text Classification}
			\begin{itemize}
				\item Problem: Classify document into a number of categories.
				\item The text is represented using word statistics, i.e. histograms of the frequency.
					\begin{itemize}
						\item Count how often every word occurs and ignore their order ("bag of words").
						\item Very high-dimensional feature space (roughly 10\,000 dimensions).
						\item Very few features that are not relevant (thus it is not feasible to apply dimensionality reduction).
					\end{itemize}
				\item SVMs are doing really well on this problem.
			\end{itemize}
		% end

		\subsection{Handwritten Digit Classification} % 12.49, 12.50, 12.51, 12.52
			\todo{SVM: Applications: HDC}
		% end

		\subsection{Support Vector Regression}
			SVMs may also be adapted to regression tasks, but this does not work very well.
		% end
	% end
	
	\section{Wrap-Up}
		\begin{itemize}
			\item Main idea of SVMs
			\item Reason for maximizing the margin
			\item Translation of the SVM problem into an quadratic programming problem
			\item Interpretation of the support vectors
			\item Using SVMs for non-linearly separable data
			\item Kernel trick
			\item Construction of kernels
			\item Formulation of SVMs with slack variables
		\end{itemize}
	% end
% end





\appendix

\chapter{Self-Test Questions}
	The text below also contains answers for the self-test questions! To prevent you from reading the answers accidentally, the questions start on a new page after the demo questions.

	\section{Demo}
		\paragraph{In what case can you toggle the visibility of the answers?}
		\answer{If my PDF viewer supports it.}

		\paragraph{In what case can you definitely not toggle the visibilities of the answers?}
		\answer{If I have printed the document.}
	% end
	
	\newpage

	\section{Organization}
		\paragraph{What are some of Machine Learning applications?}
		\answer{For example natural language processing and autonomous driving.}

		\paragraph{When can we benefit from using Machine Learning methods?}
		\answer{Machine learning can be helpful if the problem is too hard to program by hand (e.g. image recognition and natural language processing).}

		\paragraph{What are the different types of learning?}
		\answer{
			\begin{itemize}
				\item Supervised: Given labeled data (input/output pairs).
				\item Unsupervised Given unlabeled data (only input).
				\item Semi-Supervised:  Given some labeled and some unlabeled data.
				\item Reinforcement Learning: No data given.
			\end{itemize}
		}

		\paragraph{What is the difference between classification and regression? Can you give some examples of both tasks (and identify the domain and codomain)?}
		\answer{
			\begin{itemize}
				\item Classification sorts data into discrete classes. A sample use case is the recognition of hand-written digits. The domain are images and the codomain are the number from zero to one.
				\item Regression maps data onto a continuous output space and is able to extrapolate missing data. A sample use case is the analysis and prediction of weather. The domain are date or date-times and the codomain may be the temperature.
			\end{itemize}
		}

		\paragraph{What are the challenges when solving a Machine Learning problem?}
		\answer{
			\begin{itemize}
				\item Generalization: The learned function should generalize and work for new data and not only for the training data, called
				\item Overfitting: The algorithm just "memorized" the learning data and cannot handle other (new) data.
				\item Features: Choosing the right features is hard but important.
				\item Curse of dimensionality: Too high-dimensional features cause problems.
			\end{itemize}
		}

		\paragraph{What is generalization? What is overfitting?}
		\answer{Overfitting is quite the opposite of generalization. If an algorithm overfits, it just memorizes the training data and is not capable of handling new data. If it is, the function ha generalized.}
	% end

	\section{Linear Algebra Refresher}
		\paragraph{Remember vectors and what you can do with them.}
		\answer{Yeah I do remember.}

		\paragraph{Remember matrices and what you can do with them.}
		\answer{Yeah I do remember.}

		\paragraph{What is a projection? How do you use it?}
		\answer{N/A} % TODO: Q&A: Answer.

		\paragraph{How to compute the inverse of a matrix?}
		\answer{One method is the Gaussian algorithm: You write the identity matrix to the right and the given matrix to the left and then transform both matrices in parallel until the identity matrix is on the left. Then the inverse is on the right.}

		\paragraph{What are Eigenvalues and Eigenvectors?}
		\answer{The eigenvectors of a matrix are those (non-trivial) vectors that do not get rotated but only scaled when multiplied by the matrix. The corresponding eigenvalue of an eigenvector is the factor it gets scaled by.}

		\paragraph{What is a change of basis? What is a linear transformation? Are they the same?}
		\answer{N/A} % TODO: Q&A: Answer.
	% end

	\section{Statistics Refresher}
		\paragraph{What is a random variable?}
		\answer{A random variable is a variable that can have multiple values that, if randomly sampled, follow a specific probability distribution.}
		
		\paragraph{What is a distribution?}
		\answer{A probability distribution defines the probability that the value of a random variable falls into a specific region or has a specific value. It maps all possible values of the domain onto a probability between \(0\) and \(1\).}

		\paragraph{What is a Binomial distribution?}
		\answer{The binomial distribution \(\textrm{Bin}(k \given N, \mu)\) is the probability that in \(N\) trials with the singular probability \(\mu\) exactly \(k\) trials have been a success.}

		\paragraph{How does a Poisson distribution relate to Binomial distributions?}
		\answer{The Poisson distribution is the Binomial distribution with \( N \to \infty \).}

		\paragraph{What is a Gaussian distribution?}
		\answer{The Gaussian distribution is the most common probability distribution and has some neat properties, e.g. that the sum of \( N \to \infty \) random i.i.d. variables is Gaussian distributed. Its density function is given as \[ \mathcal{N}\,(x \given \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\Bigg\{ -\frac{(x - \mu)^2}{2\sigma^2} \Bigg\} \]}

		\paragraph{What is an expectation?}
		\answer{The expectation value \(\E(X)\) of a random variable \(X\) is the value that the random variable has in the mean. For continuous distributions with probability density \( p(x) \), the expectation value is given as \[ \E(X) = \int_{-\infty}^{\infty} x p(x) \dd{x} \]}

		\paragraph{What is a joint distribution?}
		\answer{A joint distribution \( p(x_1, \cdots, x_n) \) of \(n\) random variables is the probability that the tuple of both numbers fall into any particular range of set of values. For independent variables the distribution is \( p(x, y) = p(x) \, p(y) \)}

		\paragraph{What is a conditional distribution?}
		\answer{The conditional distribution \( p(x \given y) \) is the probability of \(x\) given that \(y\) is true. It is given as \[ f(x \given y) = \frac{f(x, y)}{f(y)} \]}

		\paragraph{What is a distribution with a lot of information?}
		\answer{N/A} % TODO: Q&A: Answer. (High Entropy?)

		\paragraph{How to measure the difference between distributions?}
		\answer{The difference (or similarity) between distribution can be measured using the Kullback-Leibler divergence (KL-divergence): \[ \textrm{KL}(p \,\Vert\, q) = -\int p(x) \, \ln \frac{q(x)}{p(x)} \dd{x} \]}
	% end

	\section{Optimization Refresher}
		\paragraph{Why is optimization important for machine learning?}
		\answer{Every machine learning problem is an optimization problem or can be reduced to be one.}

		\paragraph{What do well-formulated learning problems look like?}
		\answer{
			Well-formulated problems have a cost function \( J(\theta) \) that has to be minimized or maximized, given some equality constraints \( f(\theta) = 0 \) and some inequality constraints \( g(\theta) \geq 0 \). It is commonly notated like this:
			\begin{align*}
				\arg\min\limits_\theta \, J(\theta) & \\
				\textrm{s.t.} \quad
				f(\theta) &= 0 \\
				g(\theta) &\geq 0
			\end{align*}
			Every minimization problem can be a maximization problem and vice versa by multiplying the cost function with \(-1\).
		}

		\paragraph{What is a convex set and what is a convex function?}
		\answer{A convex set is a set where every point that lies on a line between any two points is also part of the set. Similarly, for a convex function, every line that can be drawn between any two points of the function does not cross the function. Formal: A set \( C \subseteq \R^n \) is convex iff \[ \forall \vec{x}, \vec{y} \in C : \forall \alpha \in [0, 1] : \alpha\vec{x} + (1 - \alpha)\vec{y} \in C \] and a function \( f : \R^n \to \R \) is convex iff \[ \forall \vec{x}, \vec{y} \in \textrm{Domain}(f) : \forall \alpha \in [0, 1] : (\alpha\vec{x} + (1 - \alpha)\vec{y}) \leq \alpha f(\vec{x}) + (1 - \alpha) f(\vec{y}) \]}

		\paragraph{How do I find the maximum of a vector-valued function?}
		\answer{Take the gradient w.r.t. to the variable, yielding a vectorial gradient. Then set each component to zero and solve for the variable (this may be complicated due to overdetermined equation systems and similar).}

		\paragraph{How to deal with constrained optimization problems?}
		\answer{Formulate the Lagrangian \( L(\theta) = J(\theta) + \lambda f(\theta) + \mu g(\theta) \), take the derivatives w.r.t. \(\theta\) and the Lagrangian multipliers \(\lambda\) and \(\mu\), set them to zero and solve for \(\theta\).}

		\paragraph{How to solve such problems numerically?}
		\answer{One method for solving them is gradient descent, also called steepest descent. It works by calculating the gradient and then reducing the value of \(\theta\) by the gradient iteratively. For maximization, the gradient has to be added.}
	% end

	\section{Bayesian Decision Theory}
		\paragraph{How can we decide on classifying a query based on simple and general loss functions?}
		\answer{N/A} % TODO: Q&A: Answer. (Bayes Rule?)

		\paragraph{What does "Bayes Optimal" mean?}
		\answer{A "Bayes optimal" classifier obeys the rule that it chooses class \(C_1\) over \(C_2\) iff \( \frac{p(x \given C_1)}{p(x \given C_2)} > \frac{p(C_2)}{p(C_1)} \).}

		\paragraph{How to deal with two or more classes?}
		\answer{Choose class \( C_i \) iff \( \frac{p(x \given C_i)}{p(x \given C_j)} > \frac{p(C_j)}{p(C_i)} \quad \forall i \neq j \).}

		\paragraph{How to deal with high dimensional feature vectors?}
		\answer{The decision rules still apply, but the posterior probability densities \( p(\vec{x} \given C_k) \) have to handle multiple features (be multivariate).}

		\paragraph{How to incorporate prior knowledge on the class distribution?}
		\answer{This is done through the prior \( p(C_k) \) for class \(C_k\) which can be determined, e.g. by simple counting (if and only if the sample data points are representative).}

		\paragraph{What are the equations for misclassification rate and risk?}
		\answer{The risk can be encoded as \( \lambda(\alpha_i \given C_j) \), which is the loss of classifying \( x \) as class \(C_i\) if \( C_j \) is the actual class. The risk is then encoded as \( R(\alpha_i \given x) = \E_{C_k \sim p(C_k \given x)} \big(\lambda(\alpha_i \given C_k)\big) = \sum_j \lambda(\alpha_i \given C_j) \, p(C_j \given x) \) which is the expected risk for classifying \(x\) as class \(C_i\). Then decide for the class with the lowest risk.}
	% end

	\section{Probability Density Estimation}
		\paragraph{Where do we get the probability of data from?}
		\answer{The probability densities can be estimated if sample data is available. This problem is called "Probability Density Estimation".}

		\paragraph{What are parametric methods and how to obtain their parameters?}
		\answer{Parametric models depend on distributions like Gaussians that have specific parameters (e.g. the mean \(\mu\) and the variance \(\sigma^2\)). The values of these parameters can then by obtained by estimation, e.g. vi maximum likelihood or maximum a-posteriori, where the last one is a Bayesian approach.}

		\paragraph{How many parameters have non-parametric methods?}
		\answer{Non-parametric models have any number of parameters as the raw data is used as "parameters".}

		\paragraph{What are mixture models?}
		\answer{Mixture models are built out of multiple single probability densities. They are all added together with a prior \(\pi_i\), which is the probability that a data point is samples from the \(i\)-th distribution (also called "weight"). The general formula is \( p(x) = \sum_i \pi_i p_i(x) \), where \(\pi_i\) is the prior and \(p_i(x)\) is the \(i\)-th probability density.}

		\paragraph{Should gradient methods be used for training mixture models?}
		\answer{No, because the derivatives of these models contain cyclic dependencies on the other parameters which makes gradient methods mostly useless and slow.}

		\paragraph{How does the EM algorithm work?}
		\answer{
			The whole idea behind EM is to maximize the complete log-likelihood \( Q(\theta, \theta^{i - 1}) = \int p(y \given X, \theta^{(i - 1)}) \, \ln p(X, y \given \theta) \dd{\theta} \) in two steps:
			\begin{itemize}
				\item E-Step: Compute the probability density \( p(y \given X, \theta^{(i - 1)}) \) using the previously estimated (or initialized) parameters \( \theta^{(i - 1)} \).
				\item M-Step: Maximize the complete log-likelihood w.r.t. \( \theta \) with maximum likelihood by using the values that have been computed in the E-Step.
			\end{itemize}
		}

		\paragraph{What is the biggest problem of mixture models?}
		\answer{The number of components and type of components that were used to draw the samples are typically unknown and it is (currently) impossible to determine them by an algorithm. There are some heuristics and trial and error can be used, but no more.}
	% end

	\section{Clustering and Evaluation}
		\paragraph{How can we find meaningful clusters in the data?}
		\answer{Cluster can be found, e.g. with mean shift clustering that starts at every data point and builds up nets of data points that are nearby (by climbing up the gradient). Another method is, for example, \(k\)-means.}

		\paragraph{How does density estimation with mixture models relate to clustering?} % TODO: Q&A: Check.
		\answer{Mixture models can generate clustered data, this can the estimation of them yield an estimation which pile of data was generated by which component. Thus, mixture model estimation is kind of a more powerful clustering method as it also yields the densities.}

		\paragraph{What is the bias-variance trade-off?}
		\answer{An estimator can typically have a low bias or a low variance, but not both.}

		\paragraph{What is a BLUE estimator?}
		\answer{An BLUE estimator ("best linear unbiased estimator") is an MVUE estimation that is linear in its features. An MVUE estimation has zero bias and a minimum of variance (called "minimum variance unbiased estimator").}

		\paragraph{Are maximum likelihood estimators always unbiased?}
		\answer{No, they are not. E.g. the MLE for the variance of a Gaussian is biased with \( \Bias(\hat{\sigma}^2) = -\frac{1}{N} \sigma^2 \) where \(N\) is the number of samples and \(\sigma^2\) is the real variance.}

		\paragraph{What is leave on out cross-validation? What do we need it for?}
		\answer{In LOOCV, the whole data set is used for training with the exception of one data point that is used for testing. It is needed if not so many data is available to detect overfitting and to validate the model in general.}
	% end

	\section{Regression}
		\paragraph{What is regression (in general) and linear regression (in particular)?}
		\answer{Regression maps an input space to a continuous output space. Linear regression depends on function \( y(\vec{x}) = \vec{w}^T \phi(\vec{x}) \) that are linear in the parameters.}

		\paragraph{What is the cost function of regression and how can I interpret it?}
		\answer{The cost function of regression defines the penalty for a misclassified sample (e.g. least squares). The goal is to minimize this loss function and to have a way to get an actual value from the calculated probability density. This loss function is then minimized w.r.t. to the regression function \( f(\vec{x}) \) to get the actual function value. For least squares, this is just mean of he probability density, thus equal to \( f(\vec{x}) \).}

		\paragraph{What is overfitting?}
		\answer{If the regressor overfits, it perfectly goes through the data points but does not really follow the actual function. This is due to the regressor just "memorizing" where the data points lie without generalizing.}

		\paragraph{How can I derive a Maximum-Likelihood Estimator for Regression?}
		\answer{For MLE for regression, the given samples must be generated with some noise \( \epsilon \) following some probability distribution, e.g. Gaussian \( \epsilon \sim \mathcal{N}\,(0, \beta^{-1}) \). The function value then also is a random variable \( y \sim \mathcal{N}\,\big(f(\vec{x}), \beta^{-1}\big) \). An estimator for \( f(\vec{x}) \) and the precision \(\beta\) can then be derived by using the typical ML approach (take the derivative of the log-likelihood, set it to zero).}

		\paragraph{Why are Bayesian methods important?}
		\answer{Bayesian methods allow to tame overfitting by putting a prior on the parameters, thus generating a probability distribution over the parameters. This gives much better and more accurate results.}

		\paragraph{What is MAP and how is it different to full Bayesian regression?}
		\answer{MAP is like the ML approach to regression, but instead of maximizing the likelihood, the posterior \( p(\vec{w} \given X, \vec{y}, \alpha, \beta) \propto p(\vec{y} \given X, \vec{w}, \beta) \, p(\vec{w}, \alpha) \) is maximized. This allows to put a prior on the parameters and thus regularizing the overfitting.}
	% end

	\section{Classification}
		\paragraph{How do we get from Bayesian optimal decisions to discriminant functions?} % TODO: Q&A: Check.
		\answer{Discriminant functions model the class-conditional posterior that is used in Bayes decision rule directly, e.g. \( y_1(x) = p(C_1 \given x) \) and \( y_2(x) = p(C_2 \given x) \) with a combined discriminant function \( y(x) = y_1(x) - y_2(x) \). Then decide for class \(C_1\) iff \( y(x) > 0 \) and for class \( C_2 \) iff \( y(x) < 0 \), this is equivalent the Bayes optimal decision rule.}

		\paragraph{How to derive a discriminant function from a probability distribution?} % TODO: Q&A: Check.
		\answer{Given class-conditional posteriors \( p(C_1 \given x) \) and \( p(C_2 \given x) \), a discriminant function can be derived as \( y(x) = p(C_1 \given x) - p(C_2 \given x) \) and similar if only the likelihood and the prior are given (the normalization term can be abandoned as its the same for both distributions, like in Bayes decision like).}

		\paragraph{How to deal with more than two classes?}
		\answer{Build each discriminant function \( y_i(x) \) to formulate how strong the classifiers believes in that class. Then decide for class \( C_i \) iff \( y_i(x) > y_j(x) \) for all \( i \ne qj \).}

		\paragraph{What does "linearly separable" mean?}
		\answer{Intuitively, a line (or hyperplane) can be drawn through all data points while perfectly separating them (one class on the one side and the other on the other).}

		\paragraph{What is Fisher discriminant analysis? How does it relate to regression?} % TODO: Q&A: Check.
		\answer{Fisher's discriminant analysis finds a projection through the data points where the data points then can be separated by a decision boundary (which is not given by Fisher's discriminant analysis). This projection is similar to regression as it also finds a "line" through the data.}

		\paragraph{Is Fisher's linear discriminant Bayes optimal?}
		\answer{Yes, if the classes have equal and diagonal class-conditional likelihood covariance matrices.}

		\paragraph{What are perceptrons? How can we train them?} % TODO: Q&A: Check.
		\answer{Perceptrons are simple neural networks, typically with no hidden layer and just one output neuron. The basic perceptron (no hidden layer, one output neuron with the sign-"activation function") is trained using the perceptron algorithm which is a version of gradient descent with the gradients "inserted".}

		\paragraph{What is logistic regression? How can be derive the parameter update rule?}
		\answer{Logistic regression formulates the class-conditional posterior as \( p(C_1 \given x) = \sigma(a) \) where it assumes that \(a\) is given by some linear discriminant function \( a = \vec{w}^T \vec{x} + w_0 \). The parameter update rule can then be derived by applying maximum likelihood estimation and then to gradient descent.}
	% end

	\section{Linear Dimensionality Reduction and Statistical Learning Theory}
		\paragraph{What does dimensionality reduction mean?}
		\answer{The goal is to find a dimension \( D \ll N \) that is lower than the original dimension \(N\), e.g. to reduce the computation cost in kernel regression (where a \( D \times D \) matrix has to be inverted) or to visualize the data.}

		\paragraph{What is PCA? What are the three things that it does?} % TODO: Q&A: PCA, three things?
		\answer{Principal component analysis (PCA) finds the principal components of the data. That is, it finds the directions in which the variance is the highest and finds how high the variance is in this directions. It also finds the so-called "explained variance", the amount of variance a component direction explains.}

		\paragraph{What are the roles of Eigenvectors and Eigenvalues in PCA?}
		\answer{The eigenvectors are the principal components and the corresponding eigenvalues encode how much variance is explained in that direction.}

		\paragraph{Can you describe applications of PCA?}
		\answer{PCA can be used to decompose images of faces into lower dimensions, change some parameters and then project it back into the original feature space. This can be used to morph images, e.g. to make them more masculine or feminine.}

		\paragraph{What does risk in statistical learning theory mean?}
		\answer{"Risk" is the expectation of misclassifying a sample, which indirectly encodes the generalization abilities of an estimation. If the risk is high, it seems to overfit.}

		\paragraph{How is the true risk different from the empirical risk?}
		\answer{The true risk depends on the underlying probability density via an integral over all data points and cannot be calculated directly, but is the real point of interest. The empirical risk applies the loss function onto some sample data points, giving an estimator for the true risk.}

		\paragraph{What is the learning power of a function approximator?}
		\answer{The learning power expresses how much "capacity" an approximator has. The more learning power an approximator has, the more accurate can the approximations be, but this can also lead to overfitting.}

		\paragraph{What is expressed by a VC-Dimension?}
		\answer{The VC-dimension specifies how much data points can be scattered by a function (or family of functions). For hyperplanes (linear functions), the VC-dimension is always \( \dim(H) + 1 \), where \(H\) is the feature space.}

		\paragraph{Is the VC-Dimension always correlated with the number of parameters?}
		\answer{
			No it is not, a counter example is the function
			\begin{align*}
				f(x, \vec{w}) &= g\big(\sin(w_1 x + w_0)\big) \\
				g(x) &=
					\begin{cases}
						+1 & \textrm{iff } x > 0 \\
						-1 & \textrm{iff } x \leq 0
					\end{cases}
			\end{align*}
			which has only two parameters but an infinite VC-dimension.
		}
	% end

	\section{Neural Networks}
		\paragraph{How does logistic regression relate to neural networks?}
		\answer{A NN with no hidden layer and just one output neuron equals logistic regression if the output layer has sigmoid as the activation function.}

		\paragraph{How do neural networks relate to the brain?}
		\answer{The brain is made up of neurons that are connected with each other, grouped in so-called "sheets". Every neuron takes inputs from the previous sheet and "fires" if the "value" is above some threshold.}

		\paragraph{What kind of functions can single layer neural networks learn?}
		\answer{Assuming this means "a single hidden layer", it can learn an arbitrary function, but the number of parameters are growing exponentially!}

		\paragraph{Why do two layers help? How many layers do you need to represent arbitrary functions?}
		\answer{Two layers help to reduce the number of parameters needed to approximate a function, which eases the learning. Theoretically, one hidden layer is enough to represent arbitrary functions.}

		\paragraph{Why were neural networks abandoned in the 1970s, and later in the 1990s? Why did neural networks re-awaken in the 2010s?}
		\answer{They were abandoned in the 1970s because of a book noticing that things like the perceptron do not work for simple nonlinear separable data like the XOR. In the 1990s they were abandoned because kernels were much better for optimization. In the 2010s, the big shift from too less data to too many data made them come back as now there is enough data to train such a network.}

		\paragraph{What output layer and loss function to use given the task (regression, classification)?}
		\answer{
			\begin{itemize}
				\item Regression
					\begin{itemize}
						\item Activation function: Linear
						\item Loss function: Squared loss
					\end{itemize}
				\item Classification
					\begin{itemize}
						\item Activation function: Sigmoid for two-class and softmax for multi-class
						\item Loss function: Nonlinear log-likelihood or cross-entropy
					\end{itemize}
			\end{itemize}
		}

		\paragraph{Why use a ReLU activation instead of a sigmoid?}
		\answer{With sigmoid, nearly all regions of the gradient are zero, causing the learning to stop once it reaches that point. In ReLU, only the negative site has a zero gradient, causing the learning to progress better. But the success of ReLU highly depends on the initial weights. A negative initialization can cause "ReLU-networks" to not start to learn.}

		\paragraph{Derive the equations for forward and backwarpropagation for a simple network.}
		\answer{
			Given a simple network with one hidden layer, an input and an output layer, all with just one neuron, the output computes as:
			\begin{equation}
				y = f_2(w_2 f_1(w_1 x_1 + b_1) + b_2)
			\end{equation}
			or stepwise:
			\begin{align}
				y &= f_2(a_2) \\
				a_2 &= w_2 z_1 + b_2 \\
				z_1 &= f_1(a_1) \\
				a_1 &= w_1 x + b_1
			\end{align}
			
			Take the derivative of the loss w.r.t. \(w_1\) to get the first gradient:
			\begin{align}
				\frac{\partial L}{\partial w_1} &= L'(y) \frac{\partial y}{\partial w_1} \\
				\frac{\partial y}{\partial w_1} &= f_2'(a_2) \frac{\partial a_2}{\partial w_1} \\
				\frac{\partial a_2}{\partial w_1} &= w_2 \frac{\partial z_1}{\partial w_1} \\
				\frac{\partial z_1}{\partial w_1} &= f_1'(a_1) \frac{\partial a_1}{\partial w_1} \\
				\frac{\partial a_1}{\partial w_1} &= x
			\end{align}
			or more beautiful:
			\begin{equation}
				\frac{\partial y}{\partial w_1} = \frac{\partial L}{\partial y} \frac{\partial y}{\partial a_2} \frac{\partial a_2}{\partial z_1} \frac{\partial z_1}{\partial a_1} \frac{\partial a_1}{\partial w_1}
			\end{equation}
			and w.r.t. \(w_2\) to get the second:
			\begin{align}
				\frac{\partial L}{\partial w_2} &= L'(y) \frac{\partial y}{\partial w_2} \\
				\frac{\partial y}{\partial w_2} &= f_2'(a_2) \frac{\partial a_2}{\partial w_2} \\
				\frac{\partial a_2}{\partial w_2} &= z_1
			\end{align}
			or more beautiful:
			\begin{equation}
				\frac{\partial L}{\partial w_2} = \frac{\partial L}{\partial y} \frac{\partial y}{\partial a_2} \frac{\partial a_2}{\partial w_2}
			\end{equation}
		}

		\paragraph{What is mini-batch gradient descent? Why use it instead of SGD or full gradient descent?}
		\answer{Mini-batch gradient descent uses a subset of the samples for training (another one each iteration), thus reduces the computation cost which is the reason why to use it instead of full gradient descent. Stochastic gradient descent has a much higher variance thus is slow and leads to the parameters "jumping" around.}

		\paragraph{Why neural networks can overfit and what are the options to prevent it?}
		\answer{Typically, a neural network has much more parameters than training data is available which makes it easy for the net to just memorize the data. Some of the options to prevent overfitting are regularization, early stopping (step when the validation error rises again), input noise augmentation (apply some noise to the input), dropout (randomly prune some neurons and train all others).}
	% end

	\section{Support Vector Machines}
		\paragraph{How did learning theory motivate support vector machines?} % TODO: Check.
		\answer{The typical machine learning algorithms try to minimize the empirical risk to lower an upper bound on the true risk. SVMs minimize the confidence interval \( \epsilon(N, p^\ast, h) \) to lower the boundary by minimizing the VC-dimension.}

		\paragraph{What does maximum margin separation mean?}
		\answer{The distance of the decision boundary between the classes to the nearest data points to that decision boundary is called margin. This margin is maximized to generalize as much as possible. Intuition: A decision boundary that is close to one of the classes, but far away from the others seems to not fit as well as a decision boundary that lies directly in the center of the classes, thus maximizing the margin to both classes.}

		\paragraph{Why did the SVM-craze drown the Neural-Networks-craze?} % TODO: Check.
		\answer{Neural networks seem to be too hard to train/optimize, while kernel methods (like in kernel SVMs) are much easier to compute (especially because of quadratic programming). This caused the 2nd from 1994 in neural networks.}

		\paragraph{What is a Kernel?}
		\answer{A kernel \( K(x, y) \) is a function that is equivalent to the scalar product of feature transformations \( \Phi(\cdot) \), so \( K(x, y) = \phi(x)^T \phi(y) \) holds. If such a feature transformation only appears in scalar products, the kernel trick can be used to replace the products with a much easier computable kernel that can even represent feature transformations into an infinite feature space!}

		\paragraph{How can I build Kernels from Kernels?}
		\answer{
			Kernels \( K_1(x, y) \) and \( K_2(x, y) \) can be combined, so all of the following are also valid kernels:
			\begin{gather*}
				c K_1(x, y) \\
				K_1(x, y) + K_2(x, y) \\
				K_1(x, y) \, K_2(x, y) \\
				f(x) \, K_1(x, y) \, f(y)
			\end{gather*}
		}

		\paragraph{What functions does the Radial Basis Function Kernel contain?}
		\answer{N/A} % TODO: Q&A: Answer.

		\paragraph{How does support vector regression work?}
		\answer{N/A} % TODO: Q&A: Answer. (Turn around the cost function?)
	% end

	\section{Kernel Regression and Gaussian Processes}
		\paragraph{Why kernel methods for regression?}
		\answer{Using kernel regression, the regression can work entirely in the feature space and can even consider infinite dimensional feature spaces. Also many other algorithms can be derived from the dual formulation of regression.}

		\paragraph{How do you get from radial basis functions to kernels?}
		\answer{N/A} % TODO: Q&A: Answer.

		\paragraph{What is the role of the two pseudo-inverses in kernel regression?}
		\answer{N/A} % TODO: Q&A: Answer.

		\paragraph{Why are kernel regression methods very computationally expensive?}
		\answer{Because they have to invert an \( N \times N \) matrix, where \(N\) is the number of sample data points.}

		\paragraph{Why is kernel regression the dual to linear regression?}
		\answer{By formulating the dual of linear regression, it can be found that the feature transformations \(\phi(\cdot)\) only appear in scalar product, thus allowing the kernel trick which is then called kernel regression.}

		\paragraph{What is the major advantage of GPs over Kernel Ridge Regression?}
		\answer{Gaussian processes can also gauge the uncertainty of the estimate.}

		\paragraph{Why are GPs a Bayesian approach?}
		\answer{Gaussian processes are Bayesian methods because they involve the construction of a prior distribution.}

		\paragraph{What principle allowed deriving GPs from a Bayesian regression point of view?}
		\answer{N/A} % TODO: Q&A: Answer.

		%\paragraph{How to get the hyperparameters in a Bayesian setup?}
		%\answer{N/A} % TODO: Q&A: Answer.
	% end
% end

\chapter{Code}
	All of the following source files can be extracted as plain text by clicking on the paperclip. If the paperclip is not visible, try another PDF viewer.

	The following paperclip extracts all files as a ZIP archive: \attachfile[icon = Paperclip]{tmp-code.zip}

	\section{Utility}
		\subsection{\texttt{genData.py}}
		\code{genData.py}
	% end

	\section{Optimization}
		\subsection{\texttt{optimization.py}}
		\code{optimization.py}
	% end

	\section{Probability Density Estimation}
		\subsection{\texttt{nonParametricModels.py}}
		\code{nonParametricModels.py}

		\subsection{\texttt{mixtureModels.py}}
		\code{mixtureModels.py}
	% end

	\section{Clustering}
		\subsection{\texttt{clustering.py}}
		\code{clustering.py}
	% end

	\section{Regression}
		\subsection{\texttt{regression.py}}
		\code{regression.py}
	% end

	\section{Classification}
		\subsection{\texttt{classification.py}}
		\code{classification.py}
	% end
	
	\section{Linear Dimensionality Reduction}
		\subsection{\texttt{pca.py}}
		\code{pca.py}
	% end
	
	\section{Support Vector Machines}
		\subsection{\texttt{svm.py}}
		\code{svm.py}
	% end
% end

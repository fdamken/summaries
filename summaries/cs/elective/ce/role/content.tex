\lstset{language = Python}



\chapter{Statistics, Linear Algebra and Calculus Refresher} % 5b.4
	\todo{Content}

	\section{Basics} % 5b.5, 5b.6, 5b.7, 5b.8
		\todo{Content}

		\subsection{Entropy} % 5b.9
			\todo{Content}
		% end

		\subsection{Gaussian Distribution and Properties} % 5b.17, 5b.18, 5b.19
			\todo{Content}
		% end
	% end

	\section{Kullback-Leibler Divergence} % 10.36, 10.37, 10.38
		\label{sec:klDivergence}

		\todo{Content}

		\subsection{Fisher Information Matrix} % 10.39, 10.40
			\label{subsec:fisherInformationMatrix}

			\todo{Content}
		% end
	% end

	\section{Monte-Carlo Integration and Gradient Estimation} % 5b.10
		\todo{Content}

		\subsection{Gradient Estimation} % 5b.11, 5b.12, 5b.13
			\todo{Content}
		% end
	% end

	\section{Linear Algebra and Calculus} % 5b.15
		\todo{Content}

		\subsection{Moore-Penrose Pseudo-Inverse} % 5b.16
			\todo{Content}
		% end
	% end
% end

\chapter{Robotics}
	This chapter covers the basics of robotics, covering modeling position, velocity, acceleration and forces as well as representing trajectories. Also the main concepts of linear and model-based control are covered.

	The definition of what a robot are quite diverse. The robotics institute of America defines a robot as follows: "A robot is a reprogrammable multi-functional manipulator designed to move material, parts, tools, or specialized devices through variable programmed motions for the performance of a variety of tasks." Another, rather inverse, definition is from G. Randelov: "A computer is just an amputee robot."

	\section{Modeling Robots}
		Modeling of a robot can be split into two separate categories: kinematics and dynamics. In \emph{kinematics}, only the geometric properties of the robot are modeled, e.g. the link length. In \emph{dynamics}, the forces acting on the links and joints are analyzed. There are two types of joints that can be used to model every joint out there: revolute and prismatic joints, whilst revolute joints are the most common ones. The displacement of a revolute joint is an angle (typically in radians), the displacement of a prismatic joint is a distance (typically in meters).

		The displacements of all joints in a robot is denoted with a vector \(\vec{q}\), the task-space (e.g. the Cartesian coordinate system of the world) is denoted by \(\vec{x}\) and the state (i.e. the variables of the robot and the environment) is denoted by \(\vec{s}\). The set of places \(\vec{x}\) in the task-space that the robot can reach is called the \emph{workspace}. That is, everything outside the workspace is not reachable.

		Actions that can be taken by the controlled are denoted by \(\vec{u}\) or \(\vec{a}\) and are most often in some way related to torques in the joints. A (control) policy \(\pi\) then maps the state \(\vec{s}\) onto some action \(\vec{u}\) to take, either in a deterministic or stochastic way:
		\begin{itemize}
			\item \eqmakebox[modelingRobotsPolicy][l]{Deterministic:} \( \vec{u} = \pi(\vec{s}) \)
			\item \eqmakebox[modelingRobotsPolicy][l]{Stochastic:}    \( \vec{u} \sim \pi(\vec{u} \given \vec{s}) \)
		\end{itemize}
		The complete system of a robot including the generation of the trajectory to perform as well as the controller is shown in \autoref{fig:robotsCompleteBlockDiagram}.

		\begin{figure}
			\centering
			\begin{tikzpicture}[block/.style = { draw, rectangle, minimum height = 1cm, minimum width = 2.5cm }]
				\node [block] (t) {\textbf{Trajectory}};
				\node [block, right = 2 of t] (c) {\textbf{Control}};
				\node [block, right = 2 of c] (d) {\textbf{Dynamics}};
				\node [block, right = 2 of d] (k) {\textbf{Kinematics}};

				\coordinate [right = 0 of t] (tr1);
				\coordinate [left = 0 of c] (cl1);
				\coordinate [right = 0 of c] (cr1);
				\coordinate [left = 0 of d] (dl1);
				\coordinate [right = 0 of d] (dr1);
				\coordinate [left = 0 of k] (kl1);

				\coordinate [below = 0 of c] (cb);
				\coordinate [left = 0.5 of cb] (cbl);
				\coordinate [below = 1.75 of cbl] (cblb);
				\coordinate [right = 0.5 of cb] (cbr);
				\coordinate [below = 0.75+0.125 of cbr] (cbrb);
				\coordinate [below = 0 of d] (db);
				\coordinate [below = 0.75+0.125 of db] (dbb);
				\coordinate [below = 0 of k] (kb);
				\coordinate [below = 1.75 of kb] (kbb);

				\draw [->] (tr1) -- node[above]{\( \vec{x}_d \), \( \dot{\vec{x}}_d \), \( \ddot{\vec{x}}_t \)} (cl1);
				\draw [->] (cr1) -- node[above]{\( \vec{u} \)} (dl1);
				\draw [->] (dr1) -- node[above]{\( \vec{q} \), \( \dot{\vec{q}} \), \( \ddot{\vec{q}} \)} (kl1);
				\draw [->] (db) -- (dbb) -- node[above]{\( \vec{q} \), \( \dot{\vec{q}} \), \( \ddot{\vec{q}} \)} (cbrb) -- (cbr);
				\draw [->] (kb) -- (kbb) -- node[above]{\( \vec{x} \), \( \dot{\vec{x}} \), \( \ddot{\vec{x}} \)} (cblb) -- (cbl);
			\end{tikzpicture}
			\caption{Block diagram of a complete system with the desired values \( \vec{x}_d \), \( \dot{\vec{x}}_d \), \( \ddot{\vec{x}}_t \), the joint angles/velocities/accelerations \( \vec{q} \), \( \dot{\vec{q}} \), \( \ddot{\vec{q}} \), the end-effector positions/velocities/accelerations \( \vec{x} \), \( \dot{\vec{x}} \), \( \ddot{\vec{x}} \), and the motor commands/torques \( \vec{u} \).}
			\label{fig:robotsCompleteBlockDiagram}
		\end{figure}

		\subsection{Kinematics}
			Forward kinematics tackle the problem to get the position of the end-effector given the current joint positions or, more general, the position of any point in reference to a set coordinate system. Forward kinematics therefore describe a mapping from joint- to task-space:
			\begin{equation*}
				\vec{x} = \vec{f}(\vec{q})
			\end{equation*}
			For simple robots, e.g. a two-dimensional robot with only few revolute joints or a robot with only prismatic joints, the forward kinematics can be found straightforwardly with geometric knowledge. For more complex robots, however, more sophisticated methods are needed to not waste a lot of time finding the forward kinematics model.

			\subsubsection{Rotations and Euler Angles}
				To model revolute joints, a decent modeling of rotations is needed. In two-dimensional settings, a rotary transformation around an arbitrary angle \(\alpha\) is described by the following matrix:
				\begin{equation*}
					\mat{R}(\alpha) =
					\begin{bmatrix}
						\cos\alpha & -\sin\alpha \\
						\sin\alpha & \cos\alpha
					\end{bmatrix}
				\end{equation*}
				In three-dimensional settings, one transformation is needed for a rotation around one of the Cartesian axis. Letting \(x\), \(y\) and \(z\) be the first, second and third Cartesian axis, respectively, the corresponding rotation matrices around an angle \(\alpha\) are given as:
				\begin{align*}
					\mat{R}_x(\alpha) =
					\begin{bmatrix}
						1 & 0          & 0           \\
						0 & \cos\alpha & -\sin\alpha \\
						0 & \sin\alpha & \cos\alpha
					\end{bmatrix}
					 &  &
					\mat{R}_y(\alpha) =
					\begin{bmatrix}
						\cos\alpha  & 0 & \sin\alpha \\
						0           & 1 & 0          \\
						-\sin\alpha & 0 & \cos\alpha
					\end{bmatrix}
					 &  &
					\mat{R}_z(\alpha) =
					\begin{bmatrix}
						\cos\alpha & -\sin\alpha & 0 \\
						\sin\alpha & \cos\alpha  & 0 \\
						0          & 0           & 1
					\end{bmatrix}
				\end{align*}

				One of many methods of representing arbitrary rotations are \emph{Euler angles} which are parameterized by roll \(\psi\), pitch \(\theta\) and yaw \(\phi\). They transform from a coordinate system \(1\) into a coordinate system \(0\) as follows:
				\begin{equation*}
					\mat{R}_1^0 = \mat{R}_z(\phi) \mat{R}_y(\theta) \mat{R}_x(\psi)
				\end{equation*}
				Problems with Euler angles are that they are not unique, i.e. different Euler angles may describe the same rotation, and it is hard to quantify differences between Euler angles. An alternative for describing rotations are \emph{unit quaternions} and angle-axis formulations, where both a rotation axis as well as a rotation angle are given. Both of them solve the singularities with Euler angles and it is easier to computer differences of orientations.
			% end

			\subsubsection{Homogeneous Transformations}
				\emph{Homogeneous transformations} are a ways to represent combined translation and rotation transformations, e.g.
				\begin{equation*}
					\vec{p}^1 = \mat{R}_2^1 \vec{p}^2 + \vec{\delta}^1
					\quad\implies\quad
					\vec{p}^0 = \mat{R}_1^0 \vec{p}^1 + \vec{\delta}^0 = \mat{R}_1^0 \big( \mat{R}_2^1 \vec{p}^2 + \vec{\delta}^1 \big) + \vec{\delta}^0
				\end{equation*}
				into a single matrix-vector multiplication, making the computation less clumsy:
				\begin{equation*}
					\vec{p}^0 = \mat{R}_1^0 \vec{p}^1 + \vec{\delta}^0
					\quad\implies\quad
					\underbrace{\begin{bmatrix}
							\vec{p}^1 \\
							1
						\end{bmatrix}}_{\tilde{\vec{p}}^1}
					=
					\underbrace{\begin{bmatrix}
							\mat{R}_1^0 & \vec{\delta}^1 \\
							\vec{0}     & 1
						\end{bmatrix}}_{\mat{H}_1^0}
					\underbrace{\begin{bmatrix}
							\vec{p}^0 \\
							1
						\end{bmatrix}}_{\tilde{\vec{p}}^0}
				\end{equation*}
				Hence, multiple translation-rotation transformations can be stacked as follows:
				\begin{equation*}
					\tilde{\vec{p}}^0 = \mat{H}_1^0 \mat{H}_2^1 \cdots \mat{H}_n^{n - 1} \tilde{\vec{p}}^n
				\end{equation*}
			% end

			\subsubsection{Denavit-Hartenberg Convention}
				A common convention for describing the coordinate systems of a robot is the \emph{Denavit-Hartenberg convention} which describes the kinematics of a complete robot with only four parameters per joint. These four parameters are:
				\begin{itemize}
					\item \(\theta_i\), the angle between \(x_{i - 1}\) and \(x_i\), measured around \(z_{i - 1}\). Variable if \(i\) is a revolute joint.
					\item \(d_i\), the distance of the origin of \(S_{i - 1}\) along \(z_{i - 1}\) to the intersection with \(x_i\). Variable if \(i\) is a prismatic joint.
					\item \(a_i\), the distance between the intersection of \(z_{i - 1}\) and \(x_i\) along \(x_i\) towards the origin of \(S_i\).
					\item \(\alpha_i\), the angle between \(z_{i - 1}\) and \(z_i\), measured around \(x_i\).
				\end{itemize}
				Along with that, the following conditions have to hold for the coordinate systems \(S_i\):
				\begin{itemize}
					\item The origins of the coordinate systems lie on the movement axis.
					\item The \(z_{i - 1}\)-axis lies along the movement axis of the \(i\)-th joint.
					\item The \(x_i\)-axis is perpendicular to the \(z_{i - 1}\)-axis and point away from it.
					\item The \(x_i\)-axis and the \(z_{i - 1}\)-axis have an intersection.
				\end{itemize}
				Placing the coordinate system can be formulated as an algorithm, check out the foundations of robotics summary\footnote{\url{https://projects.frisp.org/documents/29} (German)} for more details.
			% end
		% end

		\subsection{Differential Forward Kinematics (Velocities and Accelerations)}
			Often it is necessary to get the velocities \(\dot{\vec{x}}\) and accelerations \(\ddot{\vec{x}}\) of the end-effector. These can be computed straightforwardly using the chain rule
			\begin{align}
				\dot{\vec{x}}  & = \dv{t} \vec{f}(\vec{q}) = \dv{\vec{f}(\vec{q})}{\vec{q}} \dv{\vec{q}}{t} = \underbrace{\mat{J}(\vec{q})}_{\mathclap{\text{Jacobian}}} \dot{\vec{q}}  \label{eq:diffForwardKin} \\
				\ddot{\vec{x}} & = \dot{\mat{J}}(\vec{q}) \dot{\vec{q}} + \mat{J}(\vec{q}) \ddot{\vec{q}}  \nonumber
			\end{align}
			where the end-effector velocities and accelerations are computed from the joint velocities and accelerations.

			\subsubsection{Singularities}
				In some cases, the Jacobian \( \mat{J}(\vec{q}) \) might get rank-deficient, i.e. \( \det \mat{J}(\vec{q}) = 0 \). In this case, infinite velocities in the joints are required to reach a desired end-effector velocities as the Jacobian is not invertible anymore. These positions are called \emph{singularities} of the robot and correspond to a loss in the degrees of freedom, e.g. when the robot stretches out its arm.
			% end

			\subsubsection{Computing the Jacobians}
				There are two main ways of computing the Jacobian:
				\begin{description}
					\item[Analytical] As before, the Jacobian is derived analytically and has the problem of singularities.
					\item[Geometric]  These are derived from geometric insights, maybe circumventing the "representational singularities".
				\end{description}
				Both of these ways are just different representations of the same concept.
			% end
		% end

		\subsection{Inverse Kinematics}
			\label{subsec:inverseKinematics}

			The obvious next problem is how to get the required joint positions to reach a given end-effector pose\footnote{A pose refers to both the position and the orientation at the same time.}. Hence, the \emph{inverse kinematics} are a mapping from the task- to the joint-space:
			\begin{equation*}
				\vec{q} = \vec{f}^{-1}(\vec{x})
			\end{equation*}
			For really simple robots, this can again be solved from a geometric perspective. However, there is most likely more than one solution for a given end-effector position! With certain redundancies in the robot, it is even possible to get infinite solutions.

			An additional problem is that the inverse kinematics equations are often not solvable analytically, hence numerical methods have to be used. But those do not guarantee to find all solutions which might be necessary to move into an "optimal" joint configuration. However, for a lot of industrial robots, invertible solutions are possible due to intelligent joint and link design.
		% end

		\subsection{Dynamics}
			For the dynamics of a robot, a forward model
			\begin{equation*}
				\ddot{\vec{q}} = \vec{f}(\vec{q}, \dot{\vec{q}}, \vec{u})
			\end{equation*}
			is wanted that gives the joint accelerations given the joint positions and velocities and torques/forces (in case of revolute/prismatic joints, respectively). Usually the dynamics are represented in the general form
			\begin{equation}
				\vec{u} = \mat{M}(\vec{q}) \ddot{\vec{q}} + \vec{c}(\vec{q}, \dot{\vec{q}}) + \vec{g}(\vec{q})  \label{eq:dynamicsInverseGeneral}
			\end{equation}
			with the motor commands \(\vec{u}\), the joint positions/velocities/accelerations \(\vec{q}\)/\(\dot{\vec{q}}\)/\(\ddot{\vec{q}}\), the mass matrix \(\mat{M}(\vec{q})\), the Coriolis and centripetal forces \(\vec{c}(\vec{q}, \dot{\vec{q}})\), and the gravity \(\vec{g}(\vec{q})\). This general form can be easily inverted to get the joint accelerations, as the mass matrix is always positive definite and hence invertible:
			\begin{equation}
				\ddot{\vec{q}} = \mat{M}^{-1}(\vec{q}) \big( \vec{u} - \vec{c}(\vec{q}, \dot{\vec{q}}) - \vec{g}(\vec{q}) \big)  \label{eq:dynamicsGeneral}
			\end{equation}

			\subsubsection{Computing the Forces}
				\label{subsubsec:robotsDynamics}

				To compute the rigid body forces, there are two central methods:
				\begin{enumerate}
					\item Newton-Euler Method
						\begin{itemize}
							\item Force-dissection-based approach.
							\item Can be formalized nicely, check out the foundations of robotics summary\footnote{\url{https://projects.frisp.org/documents/29} (German)} for more details.
						\end{itemize}
					\item Lagrangian Method
						\begin{itemize}
							\item Energy-based approach.
							\item Based on Lagrangian mechanics, check out the theoretical physics: classical mechanics summary\footnote{\url{https://projects.frisp.org/documents/31} (German)} for more details.
						\end{itemize}
				\end{enumerate}
				Other forces than rigid body forces, e.g. friction, are a lot harder to model as there is not general recipe for modeling them (friction is not so well understood).

				\paragraph{Newton-Euler Method}
					The Newton-Euler method is based on \emph{force dissection} and exploiting of the required force equilibrium. It yields the \emph{recursive Newton-Euler algorithm} that can be used to straightforwardly compute the torques/forces in the joints.
				% end

				\paragraph{Lagrangian Methods}
					The \emph{Lagrangian method} is energy-based and works by defining the \emph{Lagrangian}
					\begin{equation*}
						L = T - V
					\end{equation*}
					where \(T\) is the kinetic energy and \(V\) is the potential energy. For a one-dimensional point mass with mass \(m\) in a gravity field with the gravity constant \(g\), the energies are given as \( T = \frac{1}{2} m\dot{y}^2 \) and \( V = mgy \), where \(y\) is the one degree of freedom. The Lagrangian is hence given as:
					\begin{equation*}
						L = \frac{1}{2} m \dot{y}^2 - mgy
					\end{equation*}
					Let \(f\) be the force of e.g. a motor that is somehow attached to the mass, the equations of motion are given as the \emph{Lagrangian equations of motion}:
					\begin{equation*}
						\dv{t} \pdv{L}{\dot{y}} - \pdv{L}{y} = f
						\quad\implies\quad
						m \ddot{y} + mg = f
					\end{equation*}
					For multi-dimensional systems, this equation can be generalized straightforwardly as
					\begin{equation*}
						\dv{t} \pdv{L}{\dot{x}_i} - \pdv{L}{x_i} = f_i
					\end{equation*}
					where \(x_i\) is one degree of freedom as \(f_i\) is the respective force.

					For robots, finding the potential energy is straightforward as it is just the sum of all potential energies in the links, i.e.
					\begin{equation*}
						V = \sum_{i = 1}^{n} V_i = \sum_{i = 1}^{n} m_i \vec{g}^T \vec{r}_{c_i},
					\end{equation*}
					where \(\vec{g}\) is the vector describing the gravitational acceleration and \(\vec{r}_{c_i}\) is the position of the center of mass of the \(i\)-th link. The kinetic energy is a tad more complicated as it involves rotational energy and interactions between the different joints, e.g. Coriolis forces. It is given as
					\begin{equation*}
						T = \frac{1}{2} \dot{\vec{q}}^T \Bigg[ \sum_{i = 1}^{n} m_i \mat{J}_{\vec{v}_i}^T(\vec{q}) \mat{J}_{\vec{v}_i}(\vec{q}) + \mat{J}_{\vec{\omega}_i}^T(\vec{q}) \mat{R}_i(\vec{q}) \mat{I}_i \mat{R}_i^T(\vec{q}) \mat{J}_{\vec{\omega}_i}(\vec{q}) \Bigg] \dot{\vec{q}},
					\end{equation*}
					where
					\begin{description}[leftmargin=2cm]
						\item[\(\vec{q}\)/\(\dot{\vec{q}}\)] are the positions/velocities of the joints,
						\item[\(m_i\)] is the mass of the \(i\)-th link,
						\item[\(\mat{J}_{\vec{v}_i}\)] is the linear Jacobian of the \(i\)-th joint,
						\item[\(\mat{J}_{\vec{\omega}_i}\)] is the rotational Jacobian of the \(i\)-th joint
						\item[\(\mat{I}_i\)] is the inertia tensor of the \(i\)-th link, and
						\item[\(\mat{R}_i(\vec{q})\)] is the orientation of the \(i\)-th link.
					\end{description}
				% end

				\paragraph{Comparison of Newton-Euler and Lagrangian}
					Using the Newton-Euler approach manually, i.e. to find the equations directly, is quite tedious and therefore not suitable for large robots. The Lagrangian approach, on the other hand, is a lot faster to execute by hand and therefore appropriate for finding the equations explicitly.

					In most cases, however, the algorithm is used computationally without formulating the equations explicitly. In this case, the Newton-Euler method is better suited as it has a computational complexity of \( \mathcal{O}(n) \) whilst the Lagrangian approach has a complexity of \( \mathcal{O}(n^3) \).
				% end
			% end

			\subsubsection{General Forms}
				If somehow an inverse dynamics model \( \vec{u} = \vec{f}(\vec{q}, \dot{\vec{q}}, \ddot{\vec{q}}) \) is achieved in the general form \eqref{eq:dynamicsInverseGeneral}, it is easy to compute the forward dynamics model \( \ddot{\vec{q}} = \vec{f}(\vec{q}, \dot{\vec{q}}, \vec{u}) \) by inverting the mass matrix as shown in \eqref{eq:dynamicsGeneral}. By integrating \(\ddot{\vec{q}}\), the velocities and positions can be recovered:
				\begin{align*}
					\dot{\vec{q}}(t) = \int_0^t \! \ddot{\vec{q}}(\tau) \dd{\tau}
					 &  &
					\vec{q}(t) = \int_0^t \! \dot{\vec{q}}(\tau) \dd{\tau}
				\end{align*}
				But this is (in most cases) not possible in closed form, so numerical integration techniques are required. One suitable method is for example the \emph{symplectic Euler method}, also called the \emph{semi-implicit Euler method}. It first integrates for the velocity with an explicit Euler method and subsequently integrates for the position with an implicit Euler method:
				\begin{align*}
					\dot{\vec{q}}_{k + 1} = \dot{\vec{q}}_k + h \ddot{\vec{q}}(t_k)
					 &  &
					\vec{q}_{k + 1} = \vec{q}_k + h \dot{\vec{q}}_{k + 1}
				\end{align*}
				Here, \(k\) is the step, \( h \in (0, 1) \) is the step size with \( t_k \coloneqq kh \), and \( \vec{q}_k \) and \( \dot{\vec{q}}_k \) are the respective approximations of the positions and velocities.
			% end
		% end
	% end

	\section{Representing Trajectories}
		\label{sec:roboticsRepresentingTrajectories}

		A trajectory \( \vec{q}_d(t) \)/\( \dot{\vec{q}}_d(t) \)/\( \ddot{\vec{q}}_d(t) \) specifies the joint positions/velocities/accelerations at any point in time and is used to specify movements for the robot to execute. A common way to specify trajectories is to specify via-points the robot has to reach and to interpolate between them as shown in \autoref{fig:robotsViaPoints}. But as the motor commands can only influence the acceleration direction and the velocities and positions are found with integration, the trajectory and the time-derivative of the trajectory must not jump! This can be achieved with polynomial spline-interpolation which will be discussed further in the next sections.

		\begin{figure}
			\centering
			\begin{tikzpicture}[
					dot/.style = {
						circle,
						fill,
						minimum size = 5pt,
						inner sep = 0pt,
						outer sep = 0pt
					},
					cross/.style = {
						draw,
						cross out,
						thick,
						minimum size = 5pt,
						inner sep = 0pt,
						outer sep = 0pt
					}
				]

				\node [dot, label=left:Start] (start) at (0,  0) {};
				\node [dot, label=right:Goal] (goal)  at (4, -4) {};

				\node [cross] (v1) at (2,    0.25) {};
				\node [cross] (v2) at (2.5, -1) {};
				\node [cross] (v3) at (1.5, -3) {};

				\draw (0.5, -0.5) rectangle node[left,  xshift=-0.5cm]{Obstacle} (1.5, -1.5);
				\draw (3,   -2  ) rectangle node[right, xshift= 0.5cm]{Obstacle} (4,   -3);

				\draw plot [smooth] coordinates {(start) (v1) (v2) (v3) (goal)};
			\end{tikzpicture}
			\caption{Illustration of trajectory planning with via-points (the crosses) to avoid obstacles. The trajectory between the via-points is found with interpolation.}
			\label{fig:robotsViaPoints}
		\end{figure}

		\subsection{Splines}
			As already discussed, polynomial splines can be used to avoid jumps in the positions and velocities. The spline must be at least cubic as linear and quadratic splines do not have enough parameters to encode all the required information (and they have jumps in the velocities).

			\subsubsection{Cubic Splines}
				A cubic spline
				\begin{equation*}
					q(t) = a_0 + a_1 t + a_2 t^2 + a_3 t^3
				\end{equation*}
				has four parameters that can be found using the boundary conditions \( q(t_0) = q_0 \), \( \dot{q}(t_0) = v_0 \), \( q(t_f) = q_f \) and \( \dot{q}(t_f) = v_f \) where \(t_0\) and \(t_f\) are the initial and final time step, respectively. This yields a system of linear equations,
				\begin{equation*}
					\begin{bmatrix}
						1 & t_0 & t_0^2 & t_0^3   \\
						0 & 1   & 2 t_0 & 3 t_0^2 \\
						1 & t_f & t_f^2 & t_f^3   \\
						0 & 1   & 2 t_f & 3 t_f^2
					\end{bmatrix}
					\begin{bmatrix}
						a_0 \\
						a_1 \\
						a_2 \\
						a_3
					\end{bmatrix}
					=
					\begin{bmatrix}
						q_0 \\
						v_0 \\
						q_f \\
						v_f
					\end{bmatrix}
				\end{equation*}
				that can be solved for the parameters \(a_0\), \(a_1\), \(a_2\) and \(a_3\). The via-points are the initial and final position of the spline between them. In case of multiple degrees of freedom, multiple splines have to be used, i.e. the parameters become vectors.

				But cubic splines still exhibit jumps in the acceleration which are dangerous at high speeds and might damage the robot!
			% end

			\subsubsection{Quintic Splines}
				An alternative are quintic splines
				\begin{equation*}
					q(t) = a_0 + a_1 t + a_2 t^2 + a_3 t^3 + a_4 t^4 + a_5 t^5
				\end{equation*}
				which do not have jumps in the acceleration by imposing additional constraints on the accelerations, \( \ddot{q}(t_0) = a_0 \) and \( \ddot{q}(t_f) = a_f \). This again yields a system of linear equations:
				\begin{equation*}
					\begin{bmatrix}
						1 & t_0 & t_0^2 & t_0^3   & t_0^4    & t_0^5    \\
						0 & 1   & 2 t_0 & 3 t_0^2 & 4 t_0^3  & 5 t_0^4  \\
						0 & 0   & 2     & 6 t_0   & 12 t_0^2 & 20 t_0^3 \\
						1 & t_f & t_f^2 & t_f^3   & t_f^4    & t_f^5    \\
						0 & 1   & 2 t_f & 3 t_f^2 & 4 t_f^3  & 5 t_f^4  \\
						0 & 0   & 2     & 6 t_f   & 12 t_f^2 & 20 t_f^3
					\end{bmatrix}
					\begin{bmatrix}
						a_0 \\
						a_1 \\
						a_2 \\
						a_3 \\
						a_4 \\
						a_5
					\end{bmatrix}
					=
					\begin{bmatrix}
						q_0 \\
						v_0 \\
						a_0 \\
						q_f \\
						v_f \\
						a_f
					\end{bmatrix}
				\end{equation*}
				By equalizing the accelerations at the start and end of each spline, the acceleration becomes continuous, i.e. no jumps are present anymore.
			% end
		% end

		\subsection{Alternatives}
			Alternatives to splines are for example
			\begin{itemize}
				\item linear segments with parabolic blends,
				\item trapezoidal minimum time trajectories,
				\item potential fields \( V(\vec{q}) \) with \( \dot{\vec{q}} = \pdv{V(\vec{q})}{\vec{q}} \), and
				\item nonlinear dynamical systems \( \ddot{\vec{q}} = \vec{f}(\vec{q}, \dot{\vec{q}}, \vec{\theta}) \) parameterized by \(\vec{\theta}\).
			\end{itemize}
		% end
	% end

	\section{Control in Joint-Space}
		Given a desired trajectory \( \vec{q}_d(t) \)/\( \dot{\vec{q}}_d(t) \)/\( \ddot{\vec{q}}_d(t) \), it is still necessary to find the corresponding motor inputs \(\vec{u}\) to follow this trajectory. This problem is called \emph{control}.

		The generic idea of \emph{feedback control} is to take an action, measure the current state, compare it with the desired state, and adjust the action over and over again. A simple example for this is controlling the water temperature when taking a shower. An illustration of the control loop is shown in \autoref{fig:controlShower} with a simple model of the actual temperature where the control input is purely additive. Additionally, the measured data is noisy, illustrated by the measurement errors \(\epsilon\). The function \( f(T_d, y_t) \) is called the \emph{control law} and determines the control inputs. A special case for this is linear feedback control, which will be discussed in the next section.

		\begin{figure}
			\centering
			\begin{tikzpicture}[align = center, block/.style = { draw, rectangle, minimum height = 1cm, minimum width = 3cm }]
				\node [block, label = above:Controller] (controller) {\( u_t = f(T_d, y_t) \)};
				\node [block, label = above:Plant, right = 2 of controller] (plant) {\( T_{t + 1} = T_t + u_t \)};
				\node [block, label = right:Sensor, below = 1 of plant] (sensor) {\( y_t = T_t + \epsilon \)};
				\node [left = 2 of controller] (desired) {Desired Value \\ \( T_d = \SI{35}{\celsius} \)};

				\draw [->] (desired) -- (controller);
				\draw [->] (controller) -- (plant);
				\draw [->] (plant) -- (sensor);
				\draw [->] (sensor) -| (controller);
			\end{tikzpicture}
			\caption{Illustration of the feedback control loop in shower with a simple linear model of the temperature and the measurement errors \(\epsilon\).}
			\label{fig:controlShower}
		\end{figure}

		\subsection{Linear Feedback Control}
			For \emph{linear feedback control}, the control law \( f(T_d, y_t) \) is a linear function \( f(T_d, y_t) = K (T_d - y_t) \) that determines the control input based on the difference of the desired and the actual temperatures (the error). The parameter \(K\) is called the \emph{gain} which amplifies the error for the control input. Obviously, too high or too low gains would cause an uncomfortable way to shower, either by overshooting around the desired temperature (too high gain) or by never reaching the desired temperature (too small gain). Also, messing up the sign (by choosing a negative \(K\)) can be catastrophic, as in this case it would drive the temperature towards absolute zero.

			The following sections will generalize the idea of a linear feedback controller to multiple variables, where \(K\) becomes a matrix.

			\subsubsection{P-Controller}
				For a desired joint position \(\vec{q}_d\), given the actual joint position \(\vec{q}_t\), a \emph{P-controller} just includes terms proportional to the positioning error:
				\begin{equation*}
					\vec{u}_t = \mat{K}_P (\vec{q}_d - \vec{q}_t)
				\end{equation*}
				But a simple P-controller causes high oscillations in the position, making it not suitable for real control.
			% end

			\subsubsection{PD-Controller}
				An extended version, the \emph{PD-controller}, additionally includes differential values in terms of the joint velocities. The control law is then given as
				\begin{equation*}
					\vec{u}_t = \mat{K}_P (\vec{q}_d - \vec{q}_t) + \mat{K}_D (\dot{\vec{q}}_d - \dot{\vec{q}}_t)
				\end{equation*}
				with the gains \( \mat{K}_P \) and \( \mat{K}_D \). This control law eliminates the oscillations, but a steady-state error remains due to the gravitational force acting on the robot.

				\paragraph{With Gravity Compensation}
					If a model of the gravitational force is available, it can be used to remove the steady-state error by adding the gravitational acceleration to the control low:
					\begin{equation*}
						\vec{u}_t = \mat{K}_P (\vec{q}_d - \vec{q}_t) + \mat{K}_D (\dot{\vec{q}}_d - \dot{\vec{q}}_t) + \vec{g}(\vec{q})
					\end{equation*}
					But of course this required a reasonable model which is not always available.
				% end
			% end

			\subsubsection{PID-Controller}
				Another commonly used control law is the \emph{PID-controller} which, in addition to the proportional and differential terms, adds an integral part that keeps track of the error from the beginning of time:
				\begin{equation*}
					\vec{u}_t = \mat{K}_P (\vec{q}_d - \vec{q}_t) + \mat{K}_D (\dot{\vec{q}}_d - \dot{\vec{q}}_t) + \mat{K}_I \int_0^t \! (\vec{q}_d - \vec{q}_\tau) \dd{\tau}
				\end{equation*}
				This approach also removes the steady-state error for steady-state control, i.e. when the robot should move to a single point. However, for tracking control, this approach is not suitable as there may always be a positioning error, letting the integral part become large very quick. This results in high motor commands may causing damage to the robot.

				Also, there is the problem of wind-up where if the position is not reached, the controller might get saturated due to limitations of the motor. This causes the control law to be effectively useless as it cannot react to other errors anymore.
			% end
		% end

		\subsection{Model-Based Feedback Control}
			Some problems with PD-control with gravity compensation include that there always needs to be an error in either the position or the velocity to generate a control signal. This implies that high gains are needed in order to be accurate. But high gains cause the robot to be really stiff and dangerous as it moves with higher power!

			If a model of the dynamics is present (e.g. using the recursive Newton-Euler algorithm), it can be used to compute the motor inputs for a desired acceleration. Being able to only set the acceleration is no limitation since dynamical systems are second-order systems anyway, so only the accelerations are directly controllable. This is called \emph{model-based feedback control} and it is described by the following equations:
			\begin{align*}
				\ddot{\vec{q}}_t^{\,\mathrm{ref}} & = \mat{K}_P (\vec{q}_d - \vec{q}_t) + \mat{K}_D (\dot{\vec{q}}_d - \dot{\vec{q}}_t) + \ddot{\vec{q}}_d      \\
				\vec{u}_t                         & = \mat{M}(\vec{q}_t) \ddot{\vec{q}}_t^{\,\mathrm{ref}} + \vec{c}(\vec{q}, \dot{\vec{q}}) + \vec{g}(\vec{q})
			\end{align*}
			A major drawback is of course that this needs an accurate model of the dynamics!
		% end

		\subsection{Feedforward Control}
			In feedforward control, it is assumed that \( \vec{q}_t \approx \vec{q}_d \) and \( \dot{\vec{q}}_t \approx \dot{\vec{q}}_d \). This directly yields the following control law:
			\begin{align*}
				\vec{u}_t^\mathrm{FF} & = \mat{M}(\vec{q}_d) \ddot{\vec{q}}_d + \vec{c}(\vec{q}_d, \dot{\vec{q}}_d) + \vec{g}(\vec{q}_d) \\
				\vec{u}_t^\mathrm{FB} & = \mat{K}_P (\vec{q}_d - \vec{q}) + \mat{K}_D (\dot{\vec{q}}_d - \dot{\vec{q}})                  \\
				\vec{u}_t             & = \vec{u}_t^\mathrm{FF} + \vec{u}_t^\mathrm{FB}
			\end{align*}
			As the inverse model does not play such a high role in feedforward control, it can also be used if the model is only approximate or even bad. Hence, feedforward control is highly relevant in practice as models are often not so good that model-based feedback control can be used. Additionally, it is often possible to pre-compute the feedforward terms \( \vec{u}_t^\mathrm{FF} \) such that less real-time computations are needed.
		% end
	% end

	\section{Control in Task-Space}
		So far it was assumed that the space to plan the movements in was the joint-space. However, sometimes it is more practical to plan in task-space, e.g. when trying to hit a ball with a table tennis racket. Hence, an inverse kinematics model \( \vec{q} = \vec{f}^{-1}(\vec{x}) \) as discussed in \autoref{subsec:inverseKinematics} is needed. But is is often hard if not impossible to find such a model!

		\subsection{Differential Inverse Kinematics}
			As already seen, the inverse kinematics problem is hard to solve. But assuming the robot is non-redundant (i.e. \( n = r \), where \(r\) is the dimensionality of the task-space), the Jacobian of the differential forward kinematics model in \eqref{eq:diffForwardKin} becomes square. Assuming not to be in a singularity, the joint velocities can therefore be obtained by inverting \( \mat{J}(\vec{q}) \):
			\begin{equation*}
				\dot{\vec{q}} = \mat{J}^{-1}(\vec{q}) \dot{\vec{x}}
			\end{equation*}
			By (numerically) integrating these velocities, the joint positions can be recovered (if the initial position is known).

			\subsubsection{Jacobian Transpose}
				If the Jacobian is not square, it is not possible to invert it directly. Hence, the task-space error
				\begin{equation*}
					E = \frac{1}{2} \big(\vec{x}_d - \vec{f}(\vec{q})\big)^T \big(\vec{x}_d - \vec{f}(\vec{q})\big)
				\end{equation*}
				has to be minimized (where \(\vec{f}(\vec{q})\) is the forward kinematics model). This can be done by gradient decent "in the system", i.e. the desired joint velocities are computed by minimizing the error. The desired joint positions can then again be recovered by numerical integration. Computing the gradient
				\begin{equation*}
					\pdv{E}{\vec{q}} = -\dv{\vec{f}(\vec{q})}{\vec{q}} \big(\vec{x} - \vec{f}(\vec{q})\big) \doteq -\mat{J}^T(\vec{q}) \big(\vec{x}_d - \vec{f}(\vec{q})\big)
				\end{equation*}
				therefore yields the following desired velocity, where \(\gamma\) is the step size:
				\begin{equation*}
					\dot{\vec{q}}_d = -\gamma \mat{J}^T(\vec{q}) \big(\vec{x}_d - \vec{f}(\vec{q})\big)
				\end{equation*}
				This is called the \emph{Jacobian transpose method}. As already said, the desired joint position \( \vec{q}_d \) can then be recovered by numerically integrating \( \dot{\vec{q}}_d \). All of this can then be fed into an joint-space controller, e.g. a PD- or model-based controller.

				By adding numerical differentiation, it is also possible to use joint-space controllers that control the acceleration.
			% end

			\subsubsection{Jacobian Pseudo-Inverse}
				Under the assumption that the robot is not too far away from the solution, it is possible to also set desired task space velocities whilst finding the smallest \( \dot{\vec{q}}_d \) that keeps that velocity. This can be formulated as the optimization problem
				\begin{equation}
					\begin{aligned}
						\dot{\vec{q}}_d = \arg\min_{\dot{\vec{q}}} \, & \frac{1}{2} \dot{\vec{q}}^T \dot{\vec{q}} \\
						\mathrm{s.t.} \quad                           &
						\begin{aligned}[t]
							\mat{J}(\vec{q}) \dot{\vec{q}} & = \dot{\vec{x}}_d
						\end{aligned}
					\end{aligned}  \label{eq:jacobianPseudoInverseOptProblem}
				\end{equation}
				This can be solved with Lagrangian optimization with the multipliers \( \vec{\lambda} \). For brevity, let \( \mat{J} \coloneqq \mat{J}(\vec{q}) \). Given the Lagrangian
				\begin{equation*}
					\mathcal{L} = \frac{1}{2} \dot{\vec{q}}^T \dot{\vec{q}} - \vec{\lambda}^T \big( \mat{J} \dot{\vec{q}} - \dot{\vec{x}}_d \big),
				\end{equation*}
				setting the derivative w.r.t. \( \dot{\vec{q}} \) to zero yields
				\begin{equation}
					\pdv{\mathcal{L}}{\dot{\vec{q}}} = \dot{\vec{q}} - \mat{J}^T \vec{\lambda} \overset{!}{=} \vec{0}
					\quad\implies\quad
					\dot{\vec{q}} = \mat{J}^T \vec{\lambda}.  \label{eq:jacobianPseudoInversePartial}
				\end{equation}
				Inserting this into the Lagrangian, taking the derivative w.r.t. \( \vec{\lambda} \) and setting it to zero then yields the solution of the optimization problem:
				\begin{align*}
					         &  & \mathcal{L}                      & = \frac{1}{2} \dot{\vec{q}}^T \dot{\vec{q}} - \vec{\lambda}^T \big( \mat{J} \dot{\vec{q}} - \dot{\vec{x}}_d \big) = \frac{1}{2} \vec{\lambda}^T \mat{J} \mat{J}^T \vec{\lambda} - \vec{\lambda}^T \big( \mat{J} \mat{J}^T \vec{\lambda} - \dot{\vec{x}}_d \big) \\
					\implies &  & \pdv{\mathcal{L}}{\vec{\lambda}} & = \mat{J} \mat{J}^T \vec{\lambda} - 2 \mat{J} \mat{J}^T \vec{\lambda} + \dot{\vec{x}}_d = -\mat{J} \mat{J}^T \vec{\lambda} + \dot{\vec{x}}_d \overset{!}{=} 0                                                                                                   \\
					\implies &  & \vec{\lambda}                    & = \big( \mat{J} \mat{J}^T \big)^{-1} \dot{\vec{x}}_d
				\end{align*}
				Plugging this result back into \eqref{eq:jacobianPseudoInversePartial} yields the desired joint velocity and the solution of the optimization problem:
				\begin{equation*}
					\dot{\vec{q}}_d
					= \underbrace{\mat{J}^T \big( \mat{J} \mat{J}^T \big)^{-1}}_{\mat{J}^\dagger} \dot{\vec{x}}_d
					\doteq \mat{J}^\dagger \dot{\vec{x}}_d
				\end{equation*}
			% end
		% end

		\subsection{Task-Prioritization with Null-Space Movements}
			It is possible to modify the task-space control law to simultaneously execute another action in the \emph{null-space}, a space that does not contradict the constraints of the optimization problem \eqref{eq:jacobianPseudoInverseOptProblem}. This can be, for example, to push the robot into a rest position \( \vec{q}_\mathrm{rest} \) where it does not consume energy. This base task can be formulated with a P-controller
			\begin{equation*}
				\dot{\vec{q}}_0 = \mat{K}_P (\vec{q}_\mathrm{rest} - \vec{q}).
			\end{equation*}
			The optimization problem then is as following:
			\begin{equation*}
				\begin{aligned}
					\dot{\vec{q}}_d = \arg\min_{\dot{\vec{q}}} \, & \frac{1}{2} (\dot{\vec{q}} - \dot{\vec{q}}_0)^T (\dot{\vec{q}} - \dot{\vec{q}}_0) \\
					\mathrm{s.t.} \quad                           &
					\begin{aligned}[t]
						\mat{J}(\vec{q}) \dot{\vec{q}} & = \dot{\vec{x}}_d
					\end{aligned}
				\end{aligned}
			\end{equation*}
			The result of this optimization problem is
			\begin{equation*}
				\dot{\vec{q}}_d = \mat{J}^\dagger \dot{\vec{x}}_d + \big( \mat{I} - \mat{J}^\dagger \mat{J} \big) \dot{\vec{q}}_0
			\end{equation*}
			where again \( \mat{J} \coloneqq \mat{J}(\vec{q}) \). The null-space is characterized by \( \big( \mat{I} - \mat{J}^\dagger \mat{J} \big) \) which includes all movements \( \dot{\vec{q}}_\mathrm{null} \) that do not contradict the constraint \( \mat{J} \dot{\vec{q}} = \dot{\vec{x}}_d \), i.e. \( \dot{\vec{x}}_d = \mat{J} (\dot{\vec{q}} + \dot{\vec{q}}_\mathrm{null}) \) or equivalent \( \mat{J} \dot{\vec{q}}_\mathrm{null} = \vec{0} \) holds.
		% end

		\subsection{More Advanced Solutions}
			It is also possible to use an acceleration formulation which has the solution
			\begin{equation*}
				\ddot{\vec{q}}_d = \mat{J}^\dagger ( \ddot{\vec{x}}_d - \dot{\mat{J}} \dot{\vec{q}} ) + \big( \mat{I} - \mat{J}^\dagger \mat{J} \big) \ddot{\vec{q}}_0.
			\end{equation*}
			There is a whole class of so-called \emph{operational space control laws}, i.e. task-space control laws, that can all be derived from the following most general optimization problem:
			\begin{equation*}
				\begin{aligned}
					\min_{\vec{u}} \,   & \frac{1}{2} (\vec{u} - \vec{u}_0)^T (\vec{u} - \vec{u}_0) \\
					\mathrm{s.t.} \quad &
					\begin{aligned}[t]
						\mat{A}(\vec{q}, \dot{\vec{q}}, t) \ddot{\vec{q}} & = \dot{\vec{b}}(\vec{q}, \dot{\vec{q}}, t)                     \\
						\vec{u}_0                                         & = \vec{g}(\vec{q}, \dot{\vec{q}}, t)                           \\
						\mat{M}(\vec{q}) \ddot{\vec{q}}                   & = \vec{u} + \vec{c}(\vec{q}, \dot{\vec{q}}) + \vec{g}(\vec{q})
					\end{aligned}
				\end{aligned}
			\end{equation*}
		% end

		\subsection{Singularities and Damped Pseudo-Inverse}
			If the Jacobian gets rank-deficient, i.e. singular, it is not longer possible to evaluate the pseudo-inverse \( \mat{J}^\dagger = \mat{J}^T \big( \mat{J} \mat{J}^T \big)^{-1} \) due to the last inversion. It is therefore numerically more stable to use a damped pseudo-inverse \( \mat{J}^{\dagger(\lambda)} = \mat{J}^T \big( \mat{J} \mat{J}^T + \lambda \mat{I} \big)^{-1} \) as a replacement of the regular pseudo-inverse. This avoids singularities in the inversion.
		% end
	% end
% end

\chapter{Machine Learning Foundations}
	This chapter is a basic introduction into the foundations of machine learning, focusing on the tools that are important for robot learning. Machine learning has become the best approach to various problems (e.g. in robotics and computer vision) in terms of speed, engineering time and robustness due to the shear amount of data that is generated on a daily basis. The goal of machine learning is to \emph{describe} data make \emph{predictions}, and make \emph{decisions} based on data. To learn from previously seen data, several assumptions has to be made like all training data is i.i.d., queries are drawn from the same distribution as the training data or that the answer comes from a set of possible answers known in advance. Of course different machine learning models and algorithms impose different assumptions and some impose severe while others impose mild conditions. In general, there are two core problems when building a machine learning model:
	\begin{description}[leftmargin=3.5cm]
		\item[Estimation] Is it possible to obtain an uncertain quantity of interest?
		\item[Generalization] Is it possible to predict results for previously unseen situations?
	\end{description}

	\section{The Six Machine Learning Choices}
		When building a model, there are six choices to be made, the former three of them describing the problem and the latter three describing the solution:
		\begin{enumerate}
			\item \textbf{Problem Class} \\
				What is the nature of the training data and what kind of queries can be made?
			\item \textbf{Problem Assumptions} \\
				What is known about the source of the data or the form of the solution?
			\item \textbf{Evaluation Criteria} \\
				What is the goal? How to evaluate answers to specific queries? \\
				How to measure the overall performance?
			\item \textbf{Model Type} \\
				Will there be an intermediate model? \\
				What aspects of the problem will be modeled? \\
				How will the model be used?
			\item \textbf{Model Class} \\
				What (parametric) class of models will be used? \\
				What criteria are used to pick a particular model from the class?
			\item \textbf{Algorithm} \\
				What process is used to fit the model to the data and to make predictions?
		\end{enumerate}

		\subsection{Problem Class} % 5b.30, 5b.31, 5b.32, 5b.33, 5b.34, 5b.35
			\todo{Content}
		% end

		\subsection{Problem Assumptions}
			Typical assumptions are for example:
			\begin{itemize}
				\item Learning is actually possible (e.g. there exists a causal structure).
				\item There is some smoothness in the solution, e.g. for regression.
				\item The process generating the data is stationary.
				\item Process assumptions, e.g. i.i.d., Markovian, \dots
				\item Observability.
				\item Frequentist assumption: there exists a true model.
			\end{itemize}
		% end

		\subsection{Evaluation} % 5b.37, 5b.38, 5b.39, 5b.40, 5b.41
			\todo{Content}
		% end

		\subsection{Model Type} % 5b.42
			\todo{Content}
		% end

		\subsection{Model Class Selection} % 5b.43, 5b.44, 5b.45
			\todo{Content}
		% end

		\subsection{Algorithm Realization} % 5b.46
			\todo{Content}
		% end

		\subsection{Example} % 5b.47, 5b.48, 5b.49, 5b.50, 5b.51, 5b.52, 5b.53, 5b.54, 5b.55, 5b.56, 5b.57, 5b.58, 5b.59, 5b.60, 5b.61
			\todo{Content}
		% end
	% end

	\section{Evaluation} % 5b.63, 5b.65
		\todo{Content}

		\subsection{Occams Razor} % 5b.66
			\todo{Content}
		% end

		\subsection{Bias and Variance} % 5b.67, 5b.68, 5b.69
			\todo{Content}
		% end

		\subsection{Model Selection} % 5b.70, 5b.71, 5b.72, 5b.73
			\todo{Content}
		% end
	% end

	\section{Frequentist vs. Bayesian Assumptions} % 5b.74, 5b.75
		\todo{Content}

		\subsection{Maximum Likelihood Estimation} % 5b.76, 5b.77, 5b.78, 5b.79
			\todo{Content}
		% end

		\subsection{Bayesian Thinking and Maximum A-Posteriori} % 5b.80, 5b.81, 5b.82
			\todo{Content}

			\subsubsection{Ridge Regression (Tikhonov Regularized Regression)} % 5b.83, 5b.84
				\todo{Content}
			% end

			\subsubsection{Predictions} % 5b.85, 5b.86, 5b.87
				\todo{Content}
			% end
		% end

		\subsection{Bayesian Regression} % 5b.88, 5b.89, 5b.90, 5b.91, 5b.92
			\todo{Content}
		% end
	% end

	\section{Hand-Crafted Feature Construction} % 5b.93, 5b.94
		\todo{Content}

		\subsection{Discrete Inputs} % 5b.95, 5b.96
			\todo{Content}
		% end

		\subsection{Continuous Inputs} % 5b.97
			\todo{Content}

			\subsubsection{One-Hot} % 5b.98
				\todo{Content}
			% end

			\subsubsection{Radial Basis Functions (RBFs)} % 5b.99, 5b.100, 5b.101
				\todo{Content}
			% end
		% end
	% end

	\section{Automatic (Linear) Feature Construction} % 5b.102, 5b.103, 5b.104, 5b.105, 5b.106, 5b.107
		\todo{Content}

		\subsection{Respective Field Weighted Regression (RFWR)} % 5b.108, 5b.109, 5b.110
			\todo{Content}
		% end

		\subsection{Automatic Adaption of RFWR} % 5b.111, 5b.112, 5b.113
			\todo{Content}
		% end
	% end

	\section{Non-Parametric Approaches} % 5b.114, 5b.115, 5b.116, 5b.117
		\todo{Content}

		\subsection{Weighted Linear Regression} % 5b.118, 5b.119, 5b.120, 5b.121, 5b.122
			\todo{Content}
		% end

		\subsection{Locally Weighted Bayesian Linear Regression} % 5b.123
			\todo{Content}
		% end

		\subsection{Kernel Methods} % 5b.124, 5b.125, 5b.128
			\todo{Content}

			\subsubsection{Kernel Ridge Regression} % 5b.126, 5b.127
				\todo{Content}
			% end
		% end

		\subsection{Bayesian Kernel Regression: Gaussian Processes (GPs)} % 5b.129, 5b.130, 5b.131, 5b.136
			\todo{Content}

			\subsubsection{GP-Posterior} % 5b.132, 5b.133, 5b.134, 5b.135
				\todo{Content}
			% end
		% end
	% end

	\section{Neural Networks} % 5c.1, 5c.2, 5c.4, 5c.5, 5c.6, 5c.7, 5c.7, 5c.8, 5c.9
		\todo{Content}

		\subsection{Biology and Neuron Abstraction} % 5c.10, 5c.11, 5c.12, 5c.13
			\todo{Content}
		% end

		\subsection{Components of a Neural Network} % N/A
			\todo{Content}

			\subsubsection{Single- and Multi-Layer Networks} % 5c.14, 5c.15, 5c.16, 5c.17, 5c.18, 5c.19
				\todo{Content}
			% end

			\subsubsection{Topologies} % 5c.20, 5c.21
				\todo{Content}
			% end

			\subsubsection{Activation Functions} % 5c.22, 5c.23, 5c.24, 5c.25
				\todo{Content}
			% end

			\subsubsection{Output Neurons} % 5c.26
				\todo{Content}
			% end

			\subsubsection{Loss Functions} % 5c.27
				\todo{Content}
			% end
		% end

		\subsection{Forward- and Backpropagation} % 5c.28, 5c.29, 5c.30
			\todo{Content}

			\subsubsection{Forwardpropagation} % 5c.31, 5c.32
				\todo{Content}
			% end

			\subsubsection{Backpropagation} % 5c.33, 5c.34, 5c.35, 5c.37
				\todo{Content}

				\paragraph{Skip Connections} % 5c.36
					\todo{Content}
				% end
			% end

			\subsubsection{Finite Differences} % 5c.38, 5c.39
				\todo{Content}
			% end

			\subsubsection{Automatic Differentiation} % 5c.40
				\todo{Content}
			% end
		% end

		\subsection{Efficient Gradient Descent} % 5c.42, 5c.43, 5c.44, 5c.47
			\todo{Content}

			\subsubsection{Stochastic Gradient Descent} % 5c.45, 11.19, 11.20
				\label{subsubsec:sgd}

				\todo{Content}
			% end

			\subsubsection{Mini-Batch Gradient Descent} % 5c.46
				\todo{Content}
			% end
		% end

		\subsection{Choosing the Learning Rate} % 5c.48, 5c.49, 5c.50, 5c.51
			\todo{Content}

			\subsubsection{Plateaus and Valleys} % 5c.52
				\todo{Content}
			% end

			\subsubsection{Adaptive Learning Rates} % N/A
				\todo{Content}

				\paragraph{Momentum} % 5c.53
					\todo{Content}
				% end

				\paragraph{Adadelta} % 5c.54
					\todo{Content}
				% end

				\paragraph{Adam} % 5c.55
					\todo{Content}
				% end
			% end
		% end

		\subsection{Choosing the Descent Direction} % N/A
			\todo{Content}

			\subsubsection{Hessian Approaches} % 5c.56
				\todo{Content}
			% end

			\subsubsection{Conjugate Gradient} % 5c.57
				\todo{Content}
			% end

			\subsubsection{Levenberg-Marquardt} % 5c.58
				\todo{Content}
			% end
		% end

		\subsection{Initialization of the Parameters} % 5c.59, 5c.60
			\todo{Content}
		% end

		\subsection{Overfitting} % 5c.61, 5c.62, 5c.63, 5c.64, 5c.65, 5c.66
			\todo{Content}

			\subsubsection{Weight Decay} % 5c.67
				\todo{Content}
			% end

			\subsubsection{Early Stopping} % 5c.68
				\todo{Content}
			% end

			\subsubsection{Input Noise Augmentation} % 6.69
				\todo{Content}
			% end

			\subsubsection{Dropout} % 5c.70
				\todo{Content}
			% end

			\subsubsection{Batch Normalization} % 5c.71
				\todo{Content}
			% end
		% end

		\subsection{Theoretical Analysis} % 5c.72, 5c.73, 5c.75, 5c.76, 5c.77
			\todo{Content}

			\subsubsection{Universal Function Approximation Theorem} % 5c.74
				\todo{Content}
			% end
		% end

		\subsection{Network Architectures} % 5c.78, 5c.79, 5c.80
			\todo{Content}

			\subsubsection{Convolutional Neural Networks (CNNs)} % 5c.81, 5c.82, 5c.83, 5c.84, 5c.85, 5c.86
				\todo{Content}
			% end

			\subsubsection{Recurrent Neural Networks (RNNs)} % 5c.78
				\todo{Content}
			% end
		% end

		\subsection{Neural Networks in Robotics} % 5c.88, 5c.89
			\todo{Content}

			\subsubsection{Value Functions} % 5c.90, 5c.91
				\todo{Content}
			% end

			\subsubsection{Policies} % 5c.92, 5c.93
				\todo{Content}
			% end
		% end
	% end

	\section{Wrap-Up} % 5b.138, 5c.95
		\todo{Content}
	% end
% end

\chapter{Optimal Control}
	\label{c:optimalControl}

	In lots of scenarios it is too costly to program a desired behavior. This phenomenon is none from e.g. image understanding where the state-of-the-art approach is to use machine learning. So supervised learning is not enough for acquiring certain behaviors, e.g. due to imperfect demonstrations, the correspondence problem and the simple fact that it is impossible to demonstrate everything. The correspondence problem describes the fact that the robot works differently than humans, i.e. the transfer from a human motion to joint movements or a robot is not straightforward.

	Hence, self-improvement is needed! The robot explores the environment by trial-and-error and the environment (or the engineers of the environment) give feedback based on the current states and the executed actions. This is called the \emph{reward}. In optimal control and reinforcement learning settings, it is common to talk about the \emph{reward}, whereas in other literature it is more common to talk about the \emph{cost}, which is just an inverted notion of the same concept, so \( \mathit{Reward} = -\mathit{Cost} \) and maximizing the reward is equivalent to minimizing the cost.

	This chapter is split into two sections: optimal control for discrete and continuous state-action spaces. In both settings the time is assumed to be discrete (controlling a robot is most often done with a certain sampling rate anyway).

	The vast field of optimal control is based on the \emph{principle of optimality} by Richard Bellman (1957): "An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision." This principle can be broken done in two main ideas:
	\begin{enumerate}
		\item Breaking down the optimal solutions in small parts, each part must be optimal.
		\item The initial and previous states do not matter for the optimal policy, i.e. only the current state is relevant for determining the optimal action.
	\end{enumerate}
	This principle leads to the principle of \emph{dynamic programming}. It breaks down the problem into small sub-problems which are then solved optimally. All sub-solutions together than constitute an optimal (global) policy. Famous examples for algorithms implementing the paradigm of dynamic programming are for example Dijkstra's algorithm for the shortest-path-problem.

	\section{Discrete State-Action Space: Dynamic Programming}
		\label{sec:discreteOptimalControl}

		Optimal control for discrete state-action spaces are based on \emph{Markov decision processes} (MDPs) which are defined by
		\begin{itemize}
			\item state space \( \vec{s} \in \mathcal{S} \),
			\item action space \( \vec{a} \in \mathcal{A} \),
			\item transition dynamics \( p_t(\vec{s}_{t + 1} \given \vec{s}_t, \vec{a}_t) \),
			\item reward function \( r_t : \mathcal{S} \times \mathcal{A} \to \R \), and
			\item initial state distribution \( \mu(\vec{s}) \).
		\end{itemize}
		The \emph{Markov property} describes that the transition dynamics are only dependent on the previous state and the taken action, but not on any previous states or actions, i.e.
		\begin{equation*}
			p_t(\vec{s}_{t + 1} \given \vec{s}_t, \vec{a}_t, \vec{s}_{t - 1}, \vec{a}_{t - 1}, \,\cdots\!\,) = p_t(\vec{s}_{t + 1} \given \vec{s}_t, \vec{a}_t)
		\end{equation*}
		holds. MDPs can be visualized nicely as transition diagrams as illustrated in \autoref{fig:smallMdp}.

		A \emph{policy} \(\pi\) in an MDPs describes the action to take given a state. It can either be \emph{deterministic}, i.e. \( \vec{a} = \pi(\vec{s}) \), \( \pi : \mathcal{S} \to \mathcal{A} \) or \emph{stochastic}, i.e. \( \vec{a} \sim \pi(\cdot \given \vec{s}) \), \( \pi : \mathcal{S} \times \mathcal{A} \to \R^+ \). As deterministic policies can also be represented as stochastic policies with an action probability of \num{1}, all of the following assumes a stochastic policy if not stated otherwise. But this stochastic policy might encode deterministic behavior.

		\begin{figure}
			\centering
			\begin{tikzpicture}[state/.style = { draw, circle, minimum width = 1cm, minimum height = 1cm }]
				\node [state] (s0) {\( s_{\,\mathclap{1}} \)};
				\node [state, right = 1.5 of s0] (s1) {\( s_{\,\mathclap{2}} \)};

				\draw [->] (s0) to[out=180+20, in=180-20, looseness=8] node[left]{\( a_1 \)} (s0);
				\draw [->] (s0) to[out=-20, in=180+20] node[below]{\( a_2 \)} (s1);
				\draw [->] (s1) to[out=180-20, in=20] node[above]{\( a_1 \), \( a_2 \)} (s0);
			\end{tikzpicture}
			\caption{Simple Markov decision process with two states and actions.}
			\label{fig:smallMdp}
		\end{figure}

		\subsection{Finite-Horizon Optimal Control}
			For optimal control with a finite horizon \(T\), the cumulative reward over \(T\) time steps is maximized. That is, a policy \(\pi(\vec{a} \given \vec{s})\) is searched that maximizes the expected reward
			\begin{equation}
				J_\pi = \E_{\mu, p, \pi}\Bigg[ r_T(\vec{s}_T) + \sum_{t = 1}^{T - 1} r_t(\vec{s}_t, \vec{a}_t) \Bigg]\!,  \label{eq:expectedLongTermReward}
			\end{equation}
			where \( \E_{\mu, p, \pi}[\cdot] \) is a short form for \( \E_{\vec{s}_1 \sim \mu(\cdot),\, \vec{s}_{t + 1} \sim p(\cdot \given \vec{s}_t, \vec{a}_t),\, \vec{a}_t \sim \pi(\cdot \given \vec{s}_t) } \). The final reward \( r_T(\vec{s}_T) \) does not depend on the action as there is no action to take in the last state. In all of the following, the final reward might also be referred to as \( r_T(\vec{s}_T, \vec{a}_T) \), but it does still only depend on \( \vec{s}_T \) as there is no \( \vec{a}_T \)! It is convenient as it clears up some of the summations.

			\subsubsection{Value and State-Action Value Functions}
				To assess the "quality" of a state or state-action pair, two functions are defined: The value and the state-action value functions, where the latter is often called the Q-function. For some policy \(\pi\), they are defined as follows:
				\begin{align*}
					V_t^\pi(\vec{s})          & = \E_{p, \pi}\Bigg[ \sum_{\tau = t}^{T} r_\tau(\vec{s}_\tau, \vec{a}_\tau) \Bigggiven \vec{s}_t = \vec{s} \Bigg]                        \\
					Q_t^\pi(\vec{s}, \vec{a}) & = \E_{p, \pi}\Bigg[ \sum_{\tau = t}^{T} r_\tau(\vec{s}_\tau, \vec{a}_\tau) \Bigggiven \vec{s}_t = \vec{s},\, \vec{a}_t = \vec{a} \Bigg]
				\end{align*}
				The intuition of these functions is as follows:
				\begin{description}[leftmargin = 3.5cm]
					\item[Value Function] How "good" is it to be in state \( \vec{s} \) under the policy \(\pi\)?
					\item[Q-Function]     How "good" is it to take action \( \vec{a} \) in state \( \vec{s} \) when subsequently following the policy \(\pi\)?
				\end{description}
				Given the optimal policy \( \pi^\ast \), i.e. the policy that maximizes \eqref{eq:expectedLongTermReward}, the respective value and Q-functions are usually called \( V_t^\ast \) and \( Q_t^\ast \). For the optimal policy, the value and Q-function can be calculated from each other straightforwardly:
				\begin{equation*}
					\begin{aligned}
						V_t^\ast(\vec{s})          & = \max_{\vec{a}} \, Q_t^\ast(\vec{s}, \vec{a})                                                                    \\
						Q_t^\ast(\vec{s}, \vec{a}) & = r_t(\vec{s}, \vec{a}) + \E_{\vec{s}' \sim p(\cdot \given \vec{s}, \vec{a})}\big[ V_{t + 1}^\ast(\vec{s}') \big]
					\end{aligned}  \label{eq:valueQFuncRelations}
				\end{equation*}
			% end

			\subsubsection{Value Iteration}
				Applying the principle of dynamic programming to an MDP is straightforward. As in the last time step there is no action, the value function for the last time step is simply
				\begin{equation*}
					V_T^\ast(\vec{s}) = r_T(\vec{s})
				\end{equation*}
				for every state \(\vec{s}\) (this has to be evaluated). Then \emph{value iteration} iterates backwards in time, by maximizing the Q-function that is computed from the value function for \( t = \dotsrange{T - 1}{1} \):
				\begin{equation*}
					V_t^\ast(\vec{s}) = \max_{\vec{a}} \, Q_t(\vec{s}, \vec{a}) = \max_{\vec{a}} \, \Big(\! r_t(\vec{s}, \vec{a}) + \E_{\vec{s}' \sim p(\cdot \given \vec{s}, \vec{a})}\big[ V_{t + 1}^\ast(\vec{s}') \biggiven \vec{s}, \vec{a} \big] \!\Big)
				\end{equation*}
				Following this pattern, the optimal value function for time step \(t\) is obtained after \( T - t + 1 \) iterations. The (deterministic) optimal policy \( \pi_t^\ast : \mathcal{S} \to \mathcal{A} \) is then obtained by maximizing the Q-function
				\begin{equation*}
					\pi_t^\ast(\vec{s}) = \arg\max_{\vec{a}} \, Q_t^\ast(\vec{s}, \vec{a})
				\end{equation*}
				which can either be obtained from the value function or, ideally, has been stored whilst executing the value iteration.

				A pseudo-code version of value iteration is shown in \autoref{alg:valueIterationFiniteHorizon}. Note that the computation of the value function was split into computing the Q-function on \autoref{algline:valueIterationFiniteHorizon-qComputation} and then computing the value function on \autoref{algline:valueIterationFiniteHorizon-vComputation} for to simplify the final computation of the policy on \autoref{algline:valueIterationFiniteHorizon-piComputation}.

				\begin{algorithm}  \DontPrintSemicolon
					\( V_T^\ast(\vec{s}) \gets r_T(\vec{s}) \) for all \( \vec{s} \in \mathcal{S} \) \;
					\For{\( t = \dotsrange{T - 1}{1} \)}{
						\tcp{Compute Q-Function for time step \(t\) for all \( \vec{s} \in \mathcal{S} \) and \( \vec{a} \in \mathcal{A} \):}
						\( Q_t^\ast(\vec{s}, \vec{a}) \gets r_t(\vec{s}, \vec{a}) + \sum_{\vec{s}'} p_t(\vec{s}' \given \vec{s}, \vec{a}) V_{t + 1}^\ast(\vec{s}') \)  \label{algline:valueIterationFiniteHorizon-qComputation} \;

						\tcp{Compute V-Function for time step \(t\) for all \( \vec{s} \in \mathcal{S} \):}
						\( V_t^\ast(\vec{s}) \gets \max_{\vec{a}} \, Q_t^\ast(\vec{s}, \vec{a}) \)  \label{algline:valueIterationFiniteHorizon-vComputation} \;
					}
					\tcp{Return optimal policy for each time step \(t\) for all \( \vec{s} \in \mathcal{S} \):}
					\Return \( \pi_t^\ast(\vec{s}) \gets \arg\max_{\vec{a}} \, Q_t^\ast(\vec{s}, \vec{a}) \)  \label{algline:valueIterationFiniteHorizon-piComputation} \;

					\caption{Value Iteration for Finite-Horizon Problems}
					\label{alg:valueIterationFiniteHorizon}
				\end{algorithm}
			% end

			\subsubsection{Consequences of a Finite Time Horizon}
				Choosing a finite time horizon over an infinite one has some major consequences on the environment and the e.g. the value function. First of all, it matters how many time steps are left. This implies that not only the state transition model, but also the policy and the reward function might be time-dependent as denoted by the index \(t\). This leads to also the value and Q-function being time-dependent! However, having a finite amount of steps makes it possible to find the optimal value function in a finite amount of steps.
			% end
		% end

		\subsection{Infinite-Horizon Optimal Control}
			Using an infinite time horizon, i.e. \( T = \infty \), the time index is not part of the state! Hence, also the optimal policy as well as the value and Q-function are time-independent and also the reward function and the state transition model are time-independent. But this comes at the cost of the sum of rewards, i.e. the optimization objective, being infinite and hence divergent.

			The simplest and most straightforward approach is to introduce a \emph{discount factor} \( \gamma \in [0, 1) \) that trades of the long term vs. the immediate reward. The optimization objective, the discounted sum of rewards, now is
			\begin{equation*}
				J_\pi = \E_{\mu, p, \pi} \Bigg[ \sum_{t = 1}^{\infty} \gamma^t r(\vec{s}_t, \vec{a}_t) \Bigg]  \label{eq:expectedDiscountedLongTermReward}
			\end{equation*}
			analogous to the objective for the finite horizon \eqref{eq:expectedLongTermReward}. The value and state-action value functions are defined accordingly as
			\begin{align*}
				V^\pi(\vec{s})          & = \E_{p, \pi}\Bigg[ \sum_{t = 1}^{\infty} \gamma^t r(\vec{s}_t, \vec{a}_t) \Bigggiven \vec{s}_1 = \vec{s} \Bigg]                           \\
				Q^\pi(\vec{s}, \vec{a}) & = \E_{p, \pi}\Bigg[ \sum_{t = 1}^{\infty} \gamma^t r(\vec{s}_t, \vec{a}_t) \Bigggiven \vec{s}_1 = \vec{s},\, \vec{a}_1 = \vec{a} \Bigg]\!,
			\end{align*}
			with the relations
			\begin{align*}
				V^\pi(\vec{s})          & = \E_\pi\big[ Q^\pi(\vec{s}, \vec{a}) \biggiven \vec{a} \big]                                                 \\
				Q^\pi(\vec{s}, \vec{a}) & = r(\vec{s}, \vec{a}) + \gamma \E_{\vec{s}' \sim p(\cdot \given \vec{s}, \vec{a})}\big[ V^\pi(\vec{s}') \big]
			\end{align*}
			between them. These are analogous to the finite-horizon case relations in \eqref{eq:valueQFuncRelations}, however in the last formula the expectation is multiplied with the discount factor. Also the maximization in the first equation was replaced with an expectation as now the policy might be suboptimal. In the finite case, only the optimal policy was used which equals using the maximum operator.

			\subsubsection{Value Iteration}
				Finding the optimal Q-function is again possible using value iteration with \( T \to \infty \) as shown in \autoref{alg:valueIterationInfiniteHorizon}. The only slight change (except for making the value function time-independent) is again that the expectation for computing the Q-function has to be multiplied with the discount factor in \autoref{algline:valueIterationInfiniteHorizon-qComputation}.

				\begin{algorithm}  \DontPrintSemicolon
					\( V^\ast(\vec{s}) \gets 0 \) for all \( \vec{s} \in \mathcal{S} \) \;
					\Repeat{convergence of \( V^\ast \)}{
						\tcp{Compute Q-Function for all \( \vec{s} \in \mathcal{S} \) and \( \vec{a} \in \mathcal{A} \):}
						\( Q^\ast(\vec{s}, \vec{a}) \gets r(\vec{s}, \vec{a}) + \gamma \sum_{\vec{s}'} p(\vec{s}' \given \vec{s}, \vec{a}) V^\ast(\vec{s}') \)  \label{algline:valueIterationInfiniteHorizon-qComputation} \;

						\tcp{Compute V-Function for all \( \vec{s} \in \mathcal{S} \):}
						\( V^\ast(\vec{s}) \gets \max_{\vec{a}} \, Q^\ast(\vec{s}, \vec{a}) \) \;
					}
					\tcp{Return optimal policy for all \( \vec{s} \in \mathcal{S} \):}
					\Return \( \pi^\ast(\vec{s}) \gets \arg\max_{\vec{a}} \, Q^\ast(\vec{s}, \vec{a}) \) \;

					\caption{Value Iteration for Infinite-Horizon Problems}
					\label{alg:valueIterationInfiniteHorizon}
				\end{algorithm}
			% end

			\subsubsection{Policy Iteration}
				One major drawback of value iteration as introduced before is that a lot of redundant maximization operations have to be performed when calculating the Q-function values. This drawback can be overcome by \emph{policy iteration}, a similar approach for computing the optimal value and Q-function. It is composed of two steps that are after each other until convergence:
				\begin{enumerate}
					\item \eqmakebox[policyIteration][l]{\emph{Policy Evaluation:}} Estimation of the quality of the states and actions for the current policy.
					\item \eqmakebox[policyIteration][l]{\emph{Policy Improvement:}} Improve the policy by taking the actions with the highest quality.
				\end{enumerate}
				The process of policy evaluation is shown in \autoref{alg:policyEvaluation}. The main difference is that the value and Q-functions are not the optimal ones any more but the ones for the given policy \(\pi\). This results in the computation of the value function on \autoref{algline:policyEvaluationInfiniteHorizon-vComputation} to become an expectation (the policy is not optimal anymore and so is the Q-function, so using the maximum action would be wrong). The next step for policy iteration is to improve the policy by taking the actions with the highest quality. This corresponds to choosing the policy as
				\begin{equation*}
					\pi(\vec{a} \given \vec{s}) =
					\begin{cases}
						1 & \text{if } \vec{a} = \arg\max_{\vec{a}'} \, Q^\pi(\vec{s}, \vec{a}') \\
						0 & \text{otherwise}
					\end{cases}
				\end{equation*}
				where \( Q^\pi \) is the Q-function found via policy evaluation. Putting it all together, policy iteration is shown in \autoref{alg:policyIteration}.

				\begin{algorithm}  \DontPrintSemicolon
					\KwIn{Policy \(\pi\)}
					\( V^\pi(\vec{s}) \gets 0 \) for all \( \vec{s} \in \mathcal{S} \) \;
					\Repeat{convergence of \( V^\pi \)}{
						\tcp{Compute Q-Function for all \( \vec{s} \in \mathcal{S} \) and \( \vec{a} \in \mathcal{A} \):}
						\( Q^\pi(\vec{s}, \vec{a}) \gets r(\vec{s}, \vec{a}) + \gamma \sum_{\vec{s}'} p(\vec{s}' \given \vec{s}, \vec{a}) V^\pi(\vec{s}') \) \;

						\tcp{Compute V-Function for all \( \vec{s} \in \mathcal{S} \):}
						\( V^\pi(\vec{s}) \gets \sum_{\vec{a}} \pi(\vec{a} \given \vec{s}) Q^\pi(\vec{s}, \vec{a}) \)  \label{algline:policyEvaluationInfiniteHorizon-vComputation} \;
					}
					\Return \( V^\pi \) and/or \( Q^\pi \) as needed \;

					\caption{Policy Evaluation for Infinite-Horizon Problems}
					\label{alg:policyEvaluation}
				\end{algorithm}

				\begin{algorithm}  \DontPrintSemicolon
					\KwIn{Policy \(\pi\)}
					\( V^\pi(\vec{s}) \gets 0 \) for all \( \vec{s} \in \mathcal{S} \) \;
					\Repeat{convergence of \( \pi \)}{
						\Repeat{convergence of \( V^\pi \)}{
							\tcp{Compute Q-Function for all \( \vec{s} \in \mathcal{S} \) and \( \vec{a} \in \mathcal{A} \):}
							\( Q^\pi(\vec{s}, \vec{a}) \gets r(\vec{s}, \vec{a}) + \gamma \sum_{\vec{s}'} p(\vec{s}' \given \vec{s}, \vec{a}) V^\pi(\vec{s}') \) \;

							\tcp{Compute V-Function for all \( \vec{s} \in \mathcal{S} \):}
							\( V^\pi(\vec{s}) \gets \sum_{\vec{a}} \pi(\vec{a} \given \vec{s}) Q^\pi(\vec{s}, \vec{a}) \) \;
						}
						\tcp{Improve policy for all \( \vec{s} \in \mathcal{S} \) and \( \vec{a} \in \mathcal{A} \):}
						\(
						\pi(\vec{a} \given \vec{s}) =
						\begin{cases}
							1 & \text{if } \vec{a} = \arg\max_{\vec{a}'} \, Q^\pi(\vec{s}, \vec{a}') \\
							0 & \text{otherwise}
						\end{cases}
						\)
					}
					\Return \( \pi \) \;

					\caption{Policy Iteration for Infinite-Horizon Problems}
					\label{alg:policyIteration}
				\end{algorithm}
			% end
		% end
	% end

	\section{Continuous State-Action Space: Linear Quadratic Regulator}
		In this section the application of the principle of dynamic programming will be applied to continuous state-action spaces. For continuous systems, this is often just called \emph{optimal control} and a slightly different notation is used, namely \(\vec{x}\) for the state and \(\vec{u}\) for the action. This section will stick to this convention.

		Sadly, the optimal control problem of maximizing the expected long-term reward is not tractable for almost all systems. In fact, the only system where it is is for linear transition dynamics with additive Gaussian noise and a quadratic reward. The resulting optimal policy is called the \emph{Linear Quadratic Regulator} (LQR). The first part of this section will focus on these tractable systems, following by an approach for approximating nonlinear systems and optimal control with learned models.

		\subsection{Linear Quadratic Regulator (LQR)}
			\label{subsec:lqr}

			% TODO: Linear terms in reward!

			LQR systems is defined by the state and action spaces, \( \vec{x} \in \R^n \) and \( \vec{u} \in \R^m \), the linear (possible time-dependent) transition dynamics
			\begin{equation*}
				p_t(\vec{x}_{t + 1} \given \vec{x}_t, \vec{u}) = \mathcal{N}\big(\vec{x}_{t + 1} \biggiven \mat{A}_t \vec{x}_t + \mat{B}_t \vec{u}_t + \vec{b}_t,\, \mat{\Sigma}_t \big)
			\end{equation*}
			with (additive) Gaussian noise, the quadratic reward function
			\begin{equation*}
				r_t(\vec{x}, \vec{u}) = -(\vec{x} - \vec{x}_d)^T \mat{R}_t (\vec{x} - \vec{x}_d) - \vec{u}_t^T \mat{H}_t \vec{u}_t,
			\end{equation*}
			with symmetric and positive-definite matrices \(\mat{R}_t\) and \(\mat{H}_t\), and the initial state distribution \( \mu(\vec{x}) = \mathcal{N}(\vec{x} \given \vec{\mu}, \mat{\Sigma}) \). The final reward is given as \( r_T(\vec{x}) = r_T(\vec{x}, \vec{0}) \). The objective is to maximize the expected long-term reward with a finite time horizon \(T\):
			\begin{equation*}
				J_\pi = \E_{\mu, p, \pi}\Bigg[ r_T(\vec{x}) + \sum_{t = 1}^{T - 1} r_t(\vec{x}_t, \vec{u}_t) \Bigg]
			\end{equation*}
			The value \(\vec{x}_d\) can be seen as the desired state. Deviations from \(\vec{x}_d\) punish the system with a highly negative reward.

			\subsection{Solving the Optimal Control Problem}
				Applying the principle of optimal control now says to start with the last time step \(T\) by setting \( V_T^\ast(\vec{x}) = r_T(\vec{x}) \) for all states. Subsequently, iterate over \( t = \dotsrange{T - 1}{1} \) and compute
				\begin{equation*}
					V_t^\ast(\vec{x}) = \max_{\vec{u}} \, \Big(\! r_t(\vec{x}_t, \vec{u}_t) + \E_{\vec{x}' \sim p(\cdot \given \vec{x}, \vec{u})}\big[ V_{t + 1}^\ast(\vec{x}') \biggiven \vec{x}, \vec{u} \big] \!\Big).
				\end{equation*}
				For applying this to continuous state-action spaces, the expectation and the maximization have to be solved. And this step is only possible for LQR problems! Applying dynamic programming can be split further into the following steps:
				\begin{enumerate}
					\item Compute the value function for the last time step:
						\begin{equation*}
							V_T^\ast(\vec{x}) = r_T(\vec{x})
						\end{equation*}
					\item To get from \(t + 1\) to \(t\), compute the Q-function for all states and actions:
						\begin{equation*}
							Q_t^\ast(\vec{x}, \vec{u}) = r_t(\vec{x}_t, \vec{u}_t) + \E_{\vec{x}' \sim p(\cdot \given \vec{x}, \vec{u})}\big[ V_{t + 1}^\ast(\vec{x}') \biggiven \vec{x}, \vec{u} \big]
						\end{equation*}
					\item Compute the optimal policy for time step \(t\) for all states:
						\begin{equation*}
							\pi_t^\ast(\vec{x}) = \arg\max_{\vec{u}} \, Q_t^\ast(\vec{x}, \vec{u})
						\end{equation*}
					\item Compute the optimal value function for time step \(t\) for all states:
						\begin{equation*}
							V_t^\ast(\vec{x}) = Q_t^\ast\big(\vec{x}, \pi_t^\ast(\vec{x})\big)
						\end{equation*}
					\item Repeat from 2 until \( t = 1 \) is reached.
				\end{enumerate}
				The next paragraphs will focus on these steps separately.

				\paragraph{1. Compute the Value Function for the Last Time Step}
					This can be computed straightforwardly as
					\begin{equation*}
						V_T^\ast(\vec{x}) = r_T(\vec{x}) = -(\vec{x} - \vec{x}_d)^T \mat{R}_T (\vec{x} - \vec{x}_d) \doteq -(\vec{x} - \vec{x}_d)^T \mat{V}_T (\vec{x} - \vec{x}_d)
					\end{equation*}
					with \( \mat{V}_t \coloneqq \mat{R}_T \).
				% end

				\paragraph{2. Compute the Q-Function}
					To compute the Q-function
					\begin{equation*}
						Q_t^\ast(\vec{x}, \vec{u}) = r_t(\vec{x}_t, \vec{u}_t) + \E_{\vec{x}' \sim p(\cdot \given \vec{x}, \vec{u})}\big[ V_{t + 1}^\ast(\vec{x}') \biggiven \vec{x}, \vec{u} \big],
					\end{equation*}
					firstly the expectation has to be computed. By assuming a quadratic structure for the value function of time step \(t + 1\), i.e. \( V_{t + 1}^\ast(\vec{x}) = -(\vec{x} - \vec{x}_d)^T \mat{V}_{t + 1} (\vec{x} - \vec{x}_d) \) with a symmetric and positive-definite \( \mat{V}_{t + 1} \), the expectation becomes
					\begin{align*}
						\E_{\vec{x}' \sim p(\cdot \given \vec{x}, \vec{u})}\big[ V_{t + 1}^\ast(\vec{x}') \biggiven \vec{x}, \vec{u} \big]
						 & = \int\! V_{t + 1}^\ast(\vec{x}') \, p(\vec{x}' \given \vec{x}, \vec{u}) \dd{\vec{x}'}                                                                                                                             \\
						 & = - \int\! (\vec{x} - \vec{x}_d)^T \mat{V}_{t + 1} (\vec{x} - \vec{x}_d) \, \mathcal{N}\big(\vec{x}' \biggiven \mat{A}_t \vec{x} + \mat{B}_t \vec{u}_t + \vec{b}_t,\, \mat{\Sigma}_t \big) \dd{\vec{x}'}           \\
						 & = -\big( \mat{A}_t \vec{x} + \mat{B}_t \vec{u}_t + \vec{b}_t - \vec{x}_d \big)^T \mat{V}_{t + 1} \big( \mat{A}_t \vec{x} + \mat{B}_t \vec{u}_t + \vec{b}_t - \vec{x}_d \big) - \tr(\mat{V}_{t + 1} \mat{\Sigma}_t)
					\end{align*}
					where in the last step equation (380) from the \matrixcookbook was used. Plugging in the reward function yields the Q-function:
					\begin{equation*}
						\begin{aligned}
							Q_t^\ast(\vec{x}, \vec{u}) =
							 & \, -(\vec{x} - \vec{x}_d)^T \mat{R}_t (\vec{x} - \vec{x}_d) - \vec{u}^T \mat{H}_t \vec{u}                                                                                     \\
							 & \, -\big( \mat{A}_t \vec{x} + \mat{B}_t \vec{u}_t + \vec{b}_t - \vec{x}_d \big)^T \mat{V}_{t + 1} \big( \mat{A}_t \vec{x} + \mat{B}_t \vec{u}_t + \vec{b}_t - \vec{x}_d \big) \\
							 & \, -\tr(\mat{V}_{t + 1} \mat{\Sigma}_t)
						\end{aligned}  \label{eq:qFunctionLQR}
					\end{equation*}
				% end

				\paragraph{3. Compute the Optimal Policy}
					Computing the optimal policy is done by maximizing the Q-function w.r.t. the action \(\vec{u}\). This means taking the derivative of \eqref{eq:qFunctionLQR} and setting it to zero:
					\begin{align*}
						         &                                                                                                                                    & \pdv{\vec{u}} Q_t^\ast(\vec{x}, \vec{u})
						         & = -2 \mat{H}_t \vec{u} - 2 \mat{B}_t^T \mat{V}_{t + 1} \big( \mat{A}_t \vec{x} + \mat{B}_t \vec{u}_t + \vec{b}_t - \vec{x}_d \big) &                                                                                                                                                                                                                                                               \\
						         &                                                                                                                                    &                                          & = -2 \mat{H}_t \vec{u} - 2 \mat{B}_t^T \mat{V}_{t + 1} \mat{A}_t \vec{x} - 2 \mat{B}_t^T \mat{V}_{t + 1} \mat{B}_t \vec{u}_t - 2 \mat{B}_t^T \mat{V}_{t + 1} \vec{b}_t + 2 \mat{B}_t^T \mat{V}_{t + 1} \vec{x}_d & \\
						         &                                                                                                                                    &                                          & = -2 \big( \mat{H}_t + \mat{B}_t^T \mat{V}_{t + 1} \mat{B}_t \big) \vec{u} - 2 \mat{B}_t^T \mat{V}_{t + 1} \big( \mat{A}_t \vec{x} + \vec{b}_t - \vec{x}_d \big) \overset{!}{=} \vec{0}                          & \\
						\implies &                                                                                                                                    & \pi_t^\ast(\vec{x}) = \vec{u}^\ast       & = -\big( \mat{H}_t + \mat{B}_t^T \mat{V}_{t + 1} \mat{B}_t \big)^{-1} \mat{B}_t^T \mat{V}_{t + 1} \big( \mat{A}_t \vec{x} + \vec{b}_t - \vec{x}_d \big)                                                          &
					\end{align*}
					% TODO: What about v_t?
				% end

				\paragraph{4. Compute the Value Function}
					To compute the value function, plug the obtained optimal policy into the Q-function:
					% TODO: What about the trace?
				% end

				% TODO: Continue and finish derivation of LQR! Also summarize at the end.
			% end
		% end

		\subsection{Optimal Control with Learned Models}
			The problem with local linearization is that it only really works in simulation and not on the real robot. This is caused by the fact that the models for robots are often really bad. So learning models instead of hand-crafting them would be a good alternative\footnote{Model learning will be covered in more detail in \autoref{c:modelLearning}.}!

			Major challenges in model-based policy learning are that the methods mainly work for balancing tasks where the state space is highly restricted. For more complex problems, using learned models becomes a lot harder. The main cause for this is that the model is likely to be inaccurate the further the robot moves away from known locations. Such inaccuracies can be exploited by the optimizer, causing the policy to not work on the real system. Jumping to an area off the state space by exploiting the model can again make the policy inherently unstable.

			\subsubsection{Questions in Model Learning}
				When learning or designing models, a lot of questions have to be answered, for example what the actions \(\vec{u}\) are. They might be motor torques, but this requires a good forward dynamics model for control. Other options are e.g. the joint accelerations, which requires a good inverse dynamics model, or the accelerations in task space. The latter is ideal for control, but requires a good task-space control law.

				Another big question is how to design the reward. It can encode basic information like whether the task was successful or not (using a binary reward) or more task knowledge like how far the end-effector is away from the goal. Typically a mixture is needed with usually hand-crafted weighting between them.
			% end

			\subsubsection{Human Motor Cost Functions}
				Research in cognitive science shows that also human movements can be explained very well using cost (or reward) functions! Reaching movements for example include minimum jerk, minimum torque change, minimum end-point variance. Even complicated things like locomotion can be explained with minimum metabolic energy consumption.
			% end

			\subsubsection{State-Of-The-Art Approaches}
				Probabilistic Inference for Learning Control (PILCO):
				\begin{itemize}
					\item Learn Gaussian process forward models.
					\item Use uncertainty predicted by the GP model to predict the long-term reward.
					\item Policy optimization with analytical gradient of the expected reward.
				\end{itemize}
				Guided Policy Search by Trajectory Optimization:
				\begin{itemize}
					\item Learn time-dependent linear forward models.
					\item LQR-like algorithm for trajectory optimization with the additional constraint that the new trajectory should stay close to the data. This increases the stability!
					\item Use the optimized trajectories to train a generalizing neural network policy.
				\end{itemize}
			% end
		% end
	% end
% end

\chapter{Approximate Optimal Control}
	\label{c:approximateOptimalControl}

	In contrast to the discrete and linear systems covered in \autoref{c:optimalControl}, optimal control problems are usually not solvable in closed form for nonlinear continuous systems. Hence, approximations are needed to control these systems.

	\section{Discrete State-Action Space: Approximate Dynamic Programming}
		This section covers \emph{approximate dynamic programming}, so an extension of \autoref{sec:discreteOptimalControl} using function approximators for the V- or Q-function. These are needed as dynamic programming suffers from both the curse of dimensionality in the number of states and dimensionality of the state representation and in the "curse of modeling" as there often is no model. The idea is to use an approximation \( \hat{V}_{\vec{\theta}}(\vec{s}) \) to approximate the true \( V(\vec{s}) \) where \( \vec{\theta} \in \Theta \) are the parameters to be optimized. This way, the size of the state space \( \lvert \mathcal{S} \rvert \) is reduced to the size of the parameter space, \( \lvert \Theta \rvert \), so less parameters have to be estimated. But this advantage comes at the drawback of questionable expressiveness of \( \hat{V}_{\vec{\theta}}(\vec{s}) \), i.e. there might not be a \( \vec{\theta} \in \Theta \) to approximate an arbitrary \( V(\vec{s}) \). As this is essentially a regression problem, lots of models from machine learning can be used with the usual drawbacks like linear models being easier to learn, but neural networks are more expressive but prone to overfit.

		\subsection{Approximate Value Iteration}
			In order to get the target values for fitting the regression model (by minimizing the MSE), value iteration with value function approximation invokes classical value iteration for a subset of the states. Subsequently the value function is fitted and the procedure starts over. A procedural description is shown on \autoref{alg:approxVIteration}.

			\begin{algorithm}  \DontPrintSemicolon
				Initialize \( \vec{\theta} \) somehow. \;
				\Repeat{convergence of \(\vec{\theta}\)}{
				Get a subset \( \mathcal{S}' \subseteq \mathcal{S} \) with \( \lvert \mathcal{S}' \rvert \ll \lvert \mathcal{S} \rvert \). \;
				Calculate \( V(\vec{s}') \) for all \( \vec{s}' \in \mathcal{S} \) using value/policy iteration. \;
				Minimize MSE:\,\, \(\displaystyle \vec{\theta}^\mathrm{new} \gets \arg\min_{\vec{\theta}'}\, \frac{1}{\lvert \mathcal{S}' \rvert} \sum_{\vec{s}' \,\in\, \mathcal{S}'} \big( \hat{V}_{\vec{\theta}'}(\vec{s}') - V(\vec{s}') \big)^2 \) \;
				}

				\caption{Approximate Value Iteration}
				\label{alg:approxVIteration}
			\end{algorithm}

			\subsubsection{Convergence Analysis}
				While the algorithm seems simple, a big question is whether it actually converges. To analyze convergence, the \emph{Bellman operator} \(T\) has to be defined. Applied to a value function \(V\), the Bellman operator has the form
				\begin{equation*}
					T V(\vec{s}) = \max_{\vec{a} \,\in\, \mathcal{A}}\, \Big(\! r(\vec{s}, \vec{a}) + \gamma \!\sum_{\vec{s}' \,\in\, \mathcal{S}} p(\vec{s}' \given \vec{s}, \vec{a}) V\big(\vec{s}'\big) \!\Big),
				\end{equation*}
				i.e. an application of the Bellman equation. If the Bellman operator is a contraction w.r.t. the infinite norm
				\begin{equation*}
					\lVert V \rVert_\infty \coloneqq \max_{\vec{s} \,\in\, \mathcal{S}}\, \big\lvert V(\vec{s}) \big\rvert
				\end{equation*}
				with contraction number \( \gamma^k \),
				\begin{equation*}
					\big\lVert T V_k - T V^\ast \big\rVert_\infty \leq \gamma^k \big\lVert V_k - V^\ast \big\lVert_\infty,
				\end{equation*}
				it ensures convergence to a fixed point:
				\begin{equation*}
					\lim\limits_{k \,\to\, \infty} V_k = V^\ast
				\end{equation*}
				So convergence is guaranteed if the function approximator ensures that the Bellman operator is a contraction! Extensive research focused on the fixed-point view of certain algorithms. e.g. TD, Q-Learning, SARSA, \dots Recent work from Dai et al. (2018) proposed the SBEED algorithm that uses nonlinear function approximations with convergence guarantees.
			% end

			\subsubsection{Approximation Error and Performance Loss}
				Another big question when dealing with function approximations is how the approximation error
				\begin{equation*}
					\big\lVert \hat{V} - V^\ast \big\lVert_\infty
				\end{equation*}
				relates to the performance loss
				\begin{equation*}
					\big\lVert V^\pi - V^\ast \big\lVert_\infty
				\end{equation*}
				with the greedy policy
				\begin{equation*}
					\pi(\vec{s}) = \arg\max_{\vec{a} \,\in\, \mathcal{A}}\, \Big(\! r(\vec{s}, \vec{a}) + \gamma \!\sum_{\vec{s}' \,\in\, \mathcal{S}}^ p(\vec{s}' \given \vec{s}, \vec{a}) \hat{V}\big(\vec{s}'\big) \!\Big).
				\end{equation*}
				For approximate value iteration, an upper bound on the performance loss is
				\begin{equation*}
					\big\lVert V^\pi - V^\ast \big\lVert_\infty \leq \frac{2 \gamma}{1 - \gamma} \big\lVert \hat{V} - V^\ast \big\lVert_\infty,
				\end{equation*}
				so if for any \(\varepsilon\), \( \big\lVert V^\pi - V^\ast \big\lVert_\infty \leq \varepsilon \) can be achieved, the policy is optimal.
			% end
		% end

		\subsection{Approximate Policy Iteration}
			For policy iteration, only policy evaluation has to be altered from the vanilla policy iteration algorithm. But given a policy \(\pi\), it is only possible to estimate \(V^\pi\) from samples or estimate the Bellman operator \( T^\pi \hat{V}_{\vec{\theta}} \), which are both not in the function approximation space \(\Theta\), so the fixed point theorem is not applicable! Hence, projection back to the space \(\Theta\) is needed. Two methods are possible for this:
			\begin{itemize}
				\item \emph{Direct Policy Evaluation:} Directly minimize the projected cost function (e.g. MSE).
				\item \emph{Indirect Policy Evaluation:} Solve the projected form of the value function \( \Pi V = \Pi T_\Pi V \), where \(\Pi\) is the projection.
			\end{itemize}

			The approximation error again draws an upper bound on the performance loss:
			\begin{equation*}
				\limsup\limits_{k \,\to\, \infty} \big\lVert V^{\pi_k} - V^\ast \big\lVert_\infty \leq \frac{2 \gamma}{(1 - \gamma)^2} \limsup\limits_{k \,\to\, \infty} \big\lVert V^{\pi_k} - V_k \big\lVert_\infty
			\end{equation*}
		% end
	% end

	\section{Continuous State-Action Space: Differential Dynamic Programming}
		\label{sec:ddp}

		For continuous optimal control problems, the LQR case (i.e. linear dynamics and quadratic reward functions) is the only solvable system. However, in robotics the dynamics as well as the cost functions are often nonlinear (e.g. by defining the reward function in task-space). So a big question is whether it is possible to perform LQR-like control for nonlinear systems with approximations?

		The simplest idea for approximating nonlinear systems is to locally linearize them. This means Taylor-expanding the dynamics \( \vec{x}_{t + 1} = \vec{f}_t(\vec{x}_t, \vec{a}_t) \) around a point \( (\tilde{\vec{x}}_t, \tilde{\vec{u}}_t) \) as
		\begin{equation*}
			\vec{f}_t(\vec{x}, \vec{u})
			\approx \underbrace{\vec{f}_t(\tilde{\vec{x}}_t, \tilde{\vec{u}}_t)}_{\tilde{\vec{f}}_t \,\coloneqq}
			+ \underbrace{\pdv{\vec{f}_t}{\vec{x}} \bigg\vert_{\vec{x} = \tilde{\vec{x}}_t,\, \vec{u} = \tilde{\vec{u}}_t}}_{\mat{B}_t \,\coloneqq} \vec{\Delta x}_t
			+ \underbrace{\pdv{\vec{f}_t}{\vec{u}} \bigg\vert_{\vec{x} = \tilde{\vec{x}}_t,\, \vec{u} = \tilde{\vec{u}}_t}}_{\mat{B}_t \,\coloneqq} \vec{\Delta u}_t
			\doteq \tilde{\vec{f}}_t + \mat{A}_t \vec{\Delta x}_t + \mat{B}_t \vec{\Delta u}_t
		\end{equation*}
		with \( \vec{\Delta x}_t \coloneqq \vec{x}_t - \tilde{\vec{x}}_t \) and \( \vec{\Delta u}_t \coloneqq \vec{u}_t - \tilde{\vec{u}}_t \). Similarly the reward can be linearized using a second-order Taylor expansion
		\begin{align*}
			r_t(\vec{x}_t, \vec{u}_t) \approx\,
			         & \eqmakebox[optimalControlApproxNonLin][l]{\(\underbrace{r_t(\tilde{\vec{x}}_t, \tilde{\vec{u}}_t)}_{\mathclap{r_t \,\coloneqq}}\)} + \pdv{r_t}{\vec{x}}\bigg\vert_{\vec{x} = \tilde{\vec{x}}_t,\, \vec{u} = \tilde{\vec{u}}_t} \vec{\Delta x}_t + \vec{\Delta x}_t^T \pdv[2]{r_t}{\vec{x}}\bigg\vert_{\vec{x} = \tilde{\vec{x}}_t,\, \vec{u} = \tilde{\vec{u}}_t} \vec{\Delta x}_t \\
			         & \eqmakebox[optimalControlApproxNonLin][l]{} + \pdv{r_t}{\vec{u}}\bigg\vert_{\vec{x} = \tilde{\vec{x}}_t,\, \vec{u} = \tilde{\vec{u}}_t} \vec{\Delta u}_t + \vec{\Delta u}_t^T \pdv[2]{r_t}{\vec{u}}\bigg\vert_{\vec{x} = \tilde{\vec{x}}_t,\, \vec{u} = \tilde{\vec{u}}_t} \vec{\Delta u}_t                                                                                        \\
			         & \eqmakebox[optimalControlApproxNonLin][l]{} + \vec{\Delta x}_t^T \pdv{r_t}{\vec{x}}{\vec{u}}\bigg\vert_{\vec{x} = \tilde{\vec{x}}_t,\, \vec{u} = \tilde{\vec{u}}_t} \vec{\Delta u}_t + \vec{\Delta u}_t^T \pdv{r_t}{\vec{u}}{\vec{x}}\bigg\vert_{\vec{x} = \tilde{\vec{x}}_t,\, \vec{u} = \tilde{\vec{u}}_t} \vec{\Delta x}_t                                                      \\
			\doteq\, & r_t +
			\begin{bmatrix}
				\vec{\Delta x}_t^T & \vec{\Delta u}_t^T
			\end{bmatrix}
			\begin{bmatrix}
				r_t^{\vec{x}} \\
				r_t^{\vec{u}}
			\end{bmatrix}
			+
			\begin{bmatrix}
				\vec{\Delta x}_t^T & \vec{\Delta u}_t^T
			\end{bmatrix}
			\underbrace{\begin{bmatrix}
					            r_t^{\vec{x} \vec{x}}    & r_t^{\vec{x} \vec{u}} \\
					            r_t^{\vec{x} \vec{u}, T} & r_t^{\vec{u} \vec{u}}
				            \end{bmatrix}}_{\mat{R} \,\coloneqq}
			\begin{bmatrix}
				\vec{\Delta x}_t \\
				\vec{\Delta u}_t
			\end{bmatrix}
		\end{align*}
		with the gradients \( r_t^{\vec{x}} \) and \( r_t^{\vec{u}} \) and Hessians \( r_t^{\vec{x} \vec{x}} \), \( r_t^{\vec{u} \vec{u}} \), and \( r_t^{\vec{x} \vec{u}} \) w.r.t. the respective parameters evaluated at \( (\tilde{\vec{x}}_t, \tilde{\vec{u}}_t) \).

		This is again the linear optimal control problem\footnote{Note that in this case the matrix \(\mat{R}\) has to be negative definite as there is no minus in front of the quadratic term!} which is solvable (the LQR problem), see \autoref{subsec:lqr}. But it suffers from one big problem: the LQR problem can be constructed locally around the optimal trajectory, but the local LQR parameters are needed to compute the optimal trajectory! Hence, the optimal solver is iterative, updating the approximation until it converges to a (locally) optimal solution. This algorithm is known as \emph{differential dynamic programming} (DDP).

		\subsection{Differential Dynamic Programming}
			In \emph{differential dynamic programming} (DDP) it is, in contrast to LQR, often useful to work with the Q-function instead of the value function. For an arbitrary value function \( V_t(\vec{s}) \) and an arbitrary reward function \( r_t(\vec{s}, \vec{a}) \), the Q-function is given as
			\begin{equation*}
				Q_t(\vec{s}, \vec{a}) = r_t(\vec{s}, \vec{a}) + V_{t + 1}\big(\vec{f}_t(\vec{s}, \vec{a})\big).
			\end{equation*}
			The first- and second-order derivatives are therefore:
			\begin{equation}
				\begin{aligned}
					Q_t^{\vec{x}} \coloneqq \pdv{Q_t}{\vec{x}}                  & = \pdv{r_t}{\vec{x}} + \pdv{\vec{f}_t}{\vec{x}} \pdv{V_{t + 1}}{\vec{s}}                                                                                                                                       \\
					Q_t^{\vec{u}} \coloneqq \pdv{Q_t}{\vec{u}}                  & = \pdv{r_t}{\vec{u}} + \pdv{\vec{f}_t}{\vec{u}} \pdv{V_{t + 1}}{\vec{s}}                                                                                                                                       \\
					Q_t^{\vec{x} \vec{x}} \coloneqq \pdv[2]{Q_t}{\vec{x}}       & = \pdv[2]{r_t}{\vec{x}} + \bigg(\! \pdv{\vec{f}_t}{\vec{x}} \!\bigg)^{\!T} \pdv[2]{V_{t + 1}}{\vec{x}} \pdv{\vec{f}_t}{\vec{x}} + \pdv{V_{t + 1}}{\vec{s}} \cdot \pdv[2]{\vec{f}_t}{\vec{x}}                   \\
					Q_t^{\vec{x} \vec{u}} \coloneqq \pdv[2]{Q_t}{\vec{u}}       & = \pdv[2]{r_t}{\vec{u}} + \bigg(\! \pdv{\vec{f}_t}{\vec{u}} \!\bigg)^{\!T} \pdv[2]{V_{t + 1}}{\vec{u}} \pdv{\vec{f}_t}{\vec{u}} + \pdv{V_{t + 1}}{\vec{s}} \cdot \pdv[2]{\vec{f}_t}{\vec{u}}                   \\
					Q_t^{\vec{x} \vec{u}} \coloneqq \pdv{Q_t}{\vec{x}}{\vec{u}} & = \pdv{r_t}{\vec{x}}{\vec{u}} + \bigg(\! \pdv{\vec{f}_t}{\vec{x}} \!\bigg)^{\!T} \pdv{V_{t + 1}}{\vec{x}}{\vec{u}} \pdv{\vec{f}_t}{\vec{u}} + \pdv{V_{t + 1}}{\vec{x}} \cdot \pdv{\vec{f}_t}{\vec{x}}{\vec{u}}
				\end{aligned}  \label{eq:ddpDerivatives}
			\end{equation}
			These derivatives can now be used to solve the optimization problem
			\begin{equation*}
				\vec{\delta u}^\ast(\vec{\delta x}) = \arg\max_{\vec{\delta u}}\, \tilde{Q}_t(\vec{\delta x}, \vec{\delta u}),
			\end{equation*}
			which describes the optimal action perturbation \( \vec{\delta u}^\ast \) at a point \( (\vec{x}, \vec{u}) \) with the Q-function for a perturbation defined as
			\begin{align}
				\tilde{Q}_t(\vec{\delta x}, \vec{\delta u})
				\coloneqq & \, Q(\vec{x} + \vec{\delta x}, \vec{u} + \vec{\delta u}) - Q(\vec{x}, \vec{u})  \nonumber                                                                                                                                          \\
				=         & \, r_t(\vec{x} + \vec{\delta x}, \vec{u} + \vec{\delta u}) - r_t(\vec{x}, \vec{u}) + V_{t + 1}\big( \vec{f}(\vec{x} + \vec{\delta x}, \vec{u} + \vec{\delta u}) \big) -  V_{t + 1}\big( \vec{f}(\vec{x}, \vec{u}) \big)  \nonumber \\
				\intertext{and the second-order Taylor approximation}
				\approx   & \, \frac{1}{2}
				\begin{bmatrix}
					1 & \vec{\delta x}^T & \vec{\delta u}^T
				\end{bmatrix}
				\begin{bmatrix}
					0             & Q_t^{\vec{x}, T}      & Q_t^{\vec{u}, T}      \\
					Q_t^{\vec{x}} & Q_t^{\vec{x} \vec{x}} & Q_t^{\vec{x} \vec{u}} \\
					Q_t^{\vec{u}} & Q_t^{\vec{u} \vec{x}} & Q_t^{\vec{u} \vec{u}}
				\end{bmatrix}
				\begin{bmatrix}
					1              \\
					\vec{\delta x} \\
					\vec{\delta u}
				\end{bmatrix}  \label{eq:qTildeApprox}
			\end{align}
			This optimization problem can be solved in closed form by taking the derivative of the quadratic expansion \eqref{eq:qTildeApprox} and setting it to zero:
			\begin{align}
				                  &           & \pdv{\tilde{Q}_t}{\vec{\delta u}}
				                  & \approx
				\begin{bmatrix}
					0 & 0 & 1
				\end{bmatrix}
				\begin{bmatrix}
					0             & Q_t^{\vec{x}, T}      & Q_t^{\vec{u}, T}      \\
					Q_t^{\vec{x}} & Q_t^{\vec{x} \vec{x}} & Q_t^{\vec{x} \vec{u}} \\
					Q_t^{\vec{u}} & Q_t^{\vec{u} \vec{x}} & Q_t^{\vec{u} \vec{u}}
				\end{bmatrix}
				\begin{bmatrix}
					1              \\
					\vec{\delta x} \\
					\vec{\delta u}
				\end{bmatrix}
				=
				\begin{bmatrix}
					Q_t^{\vec{u}} & Q_t^{\vec{u} \vec{x}} & Q_t^{\vec{u} \vec{u}}
				\end{bmatrix}
				\begin{bmatrix}
					1              \\
					\vec{\delta x} \\
					\vec{\delta u}
				\end{bmatrix} & \nonumber                                                                                                                                                                                                                                    \\
				                  &           &                                      & = Q_t^{\vec{u}} + Q_t^{\vec{u} \vec{x}} \vec{\delta x} + Q_t^{\vec{u} \vec{u}} \vec{\delta u} \vec{\delta u} \overset{!}{=} \vec{0}  \nonumber                                        \\
				\implies          &           & Q_t^{\vec{u} \vec{u}} \vec{\delta u} & = -Q_t^{\vec{u}} - Q_t^{\vec{u} \vec{x}} \vec{\delta x}                                                                                            & \nonumber                        \\
				\iff              &           & \vec{\delta u}^\ast                  & = -\big(Q_t^{\vec{u} \vec{u}}\big)^{-1} \big(Q_t^{\vec{u}} + Q_t^{\vec{u} \vec{x}} \vec{\delta x}\big) \doteq \vec{k}_t + \mat{K}_t \vec{\delta x} & \label{eq:ddpFeedbackControlLaw}
			\end{align}
			Here, \( \vec{k}_t \coloneqq -\big(Q_t^{\vec{u} \vec{u}}\big)^{-1} Q_t^{\vec{u}} \) and \( \mat{K}_t \coloneqq -\big(Q_t^{\vec{u} \vec{u}}\big)^{-1} Q_t^{\vec{u} \vec{x}} \vec{\delta x} \). Plugging this result back into \eqref{eq:qTildeApprox} yields a quadratic model for the value function:
			\begin{align*}
				V_t(\vec{x}) = -\frac{1}{2} \vec{k}_t^T Q_t^{\vec{u} \vec{u}} \vec{k}
				 &  &
				V_t^{\vec{x}}(\vec{x}) = Q_t^{\vec{x}} - \mat{K}_t^T Q_t^{\vec{u} \vec{u}} \vec{k}
				 &  &
				V_t^{\vec{x} \vec{x}}(\vec{x}) = Q_t^{\vec{x} \vec{x}} - \mat{K}^T Q_t^{\vec{u} \vec{u}} \mat{K}
			\end{align*}
			This can be iterated backwards in time to compute a local approximation of the value function.

			The DDP algorithm then consists of three steps:
			\begin{enumerate}
				\item Execute \( \vec{u}_{1:T - 1} \) forward in time, linearize the dynamics \(\vec{f}\) and quadratize the reward \(r\).
				\item Update the local Q- and V-functions backwards in time, via dynamic programming. Compute the time-varying control law.
				\item Update the new action sequence using line search.
			\end{enumerate}

			\subsubsection{Implementation Details}
				\paragraph{Computing the Quadratic Expansion}
					There are multiple different methods of computing the quadratic expansion \eqref{eq:qTildeApprox}:
					\begin{itemize}
						\item Analytical solution (exact, computationally efficient but personally expensive).
						\item Automatic differentiation (exact, computationally expensive but personally cheap).
						\item Finite difference approximation (inexact, computationally okay-ish and personally cheap).
					\end{itemize}
				% end

				\paragraph{Line Search}
					As the update is only valid locally, a line search is used to optimize the step size of the update. Each DDP iteration \(i\) gives a local update \( \vec{\delta u}^{(i)} \), so the updated control law is
					\begin{equation*}
						\hat{\vec{u}}^{(i)} = \vec{u}^{(i)} + \alpha \vec{k}^{(i)} + \mat{K}^{(i)} \big( \hat{\vec{x}}^{(i)} - \vec{x}^{(i)} \big) \\
					\end{equation*}
					where usually \(\alpha\) is evaluated over a range of values to maximize the reward improvement
					\begin{equation*}
						\grad{J(\alpha)} = \alpha \sum_{i = 1}^{N - 1} \vec{k}^{(i), T} Q_t^{\vec{u}, (i)} + \frac{\alpha^2}{2} \sum_{i = 1}^{N} \vec{k}^{(i), T} Q_t^{\vec{u} \vec{u}, (i)} \vec{k}^{(i)}.
					\end{equation*}
				% end
			% end

			\subsubsection{Issues with DDP}
				Differential dynamic programming has two major issues:
				\begin{enumerate}
					\item The local approximation is only valid along the optimized trajectory. So if the state diverges (e.g. due to modeling errors or disturbances), future controllers are invalid. \\
						This can be solved by updating the controllers online using model-predictive control.
					\item The Hessian terms are expensive to compute, prohibiting online computation. \\
						This can be solved by using Hessian approximations (e.g. BFGS).
				\end{enumerate}
			% end
		% end

		\subsection{Iterative LQR}
			When using Gauss-Newton optimization, the Hessians are approximated from the Jacobians (i.e. the Gradients). By leaving out the second-order derivatives of the state dynamics in the derivatives of the Q-function \eqref{eq:ddpDerivatives}, computation time is saved and the DDP model is effectively transformed into a Gauss-Newton equivalent. This is called \emph{iterative LQR} (iLQR). In DDP it is often necessary to regularize \( Q_t^{\vec{u} \vec{u}} \): \( \hat{Q}_t^{\vec{u} \vec{u}} \coloneqq Q_t^{\vec{u} \vec{u}} + \mu \mat{I} \). But this regularization affects the controller as \( Q_t^{\vec{u} \vec{u}} \) defines the feedback control law \eqref{eq:ddpFeedbackControlLaw}. As \( \mu \to \infty \), \( \mat{K}_t \to \mat{O} \). In iLQR the regularization is directly applied to the Hessian of the value function in \eqref{eq:ddpDerivatives} with the motivation of reducing the trajectory update rather than the controller specifically.
		% end

		\subsection{Stochastic DDP}
			In stochastic DDP, a state-action dependent disturbance \( F(\vec{x}, \vec{u}) \) is considered. This yields equations similar to DDP with additional differential terms due to the disturbances. For constant noise, however, the solution is equivalent to DDP and for state-action dependent noise, the optimal solution seeks lower disturbance regions.
		% end

		\subsection{Guided Policy Search}
			Guided Policy Search (GPS) mitigates the problem of DDP only being valid along a trajectory by using a DDP-like algorithm for generating multiple controllers which are then used to train a neural network controller. GPS will be discussed in more detail in \autoref{subsec:gps}.
		% end
	% end

	\section{Wrap-Up}
		This chapter covered two methods for solving nonlinear optimal control problems: differential and approximate dynamic programming. The former applies LQR to nonlinear problems by iteratively linearizing the dynamics around the current trajectory to get time-varying linear controllers. As the algorithm is a bit brittle and only valid along a single trajectory, it is often used online in a model-predictive control fashion. In practice, approximations are used to compute the Hessians which is expensive to do otherwise (e.g. iLQR). These local controllers can then also be used to fit a global controller (guided policy search). The latter uses function approximators to find a global value function while using vanilla value/policy iteration to find local value functions.
	% end
% end

\chapter{State Estimation} % 6.1, 6.2, 6.5, 6.6
	\label{c:stateEstimation}

	\todo{Content}

	\section{Kalman Filter as an Optimal Filter} % 6.8
		\todo{Content}

		\subsection{Observers} % 6.9, 6.10
			\todo{Content}
		% end

		\subsection{Optimal Observers} % 6.11, 6.12, 6.13, 6.14, 6.15, 6.16, 6.17, 6.18
			\todo{Content}
		% end

		\subsection{Geometric Perspective} % 6.19
			\todo{Content}
		% end
	% end

	\section{Kalman Filter as Bayesian Inference} % 6.20, 6.21, 6.22, 6.23, 6.24
		\todo{Content}
	% end

	\section{Partially Observed Optimal Control} % 6.27, 6.28, 6.29, 6.30
		\todo{Content}
	% end

	\section{Extended, Unscented and Particle Filter} % 6.32, 6.33
		\todo{Content}

		\subsection{Extended Kalman Filter (EKF)} % 6.34
			\todo{Content}
		% end

		\subsection{Cubature Kalman Filter (CKF)} % 6.35, 6.36, 6.37, 6.38
			\todo{Content}
		% end

		\subsection{Unscented Kalman Filter (UKF)} % 6.39
			\todo{Content}
		% end

		\subsection{Particle Filter / Sequential Monte Carlo (PF/SMC)} % 6.40, 6.41, 6.56
			\todo{Content}

			\subsubsection{Importance Sampling} % 6.42, 6.43
				\todo{Content}
			% end

			\subsubsection{Sequential Importance Sampling} % 6.48, 6.49, 6.50, 6.51, 6.52
				\todo{Content}
			% end

			\subsubsection{Sequential Importance Resampling} % 6.53, 6.54, 6.55
				\todo{Content}
			% end
		% end

		\subsection{Examples} % N/A
			\todo{Content}

			\subsubsection{Approximate Message Passing} % 6.44, 6.45, 6.46, 6.47
				\todo{Content}
			% end

			\subsubsection{Pendulum} % 6.57, 6.58, 6.59, 6.60
				\todo{Content}
			% end
		% end
	% end

	\section{Wrap-Up} % 6.62, 6.63
		\todo{Content}
	% end
% end

\chapter{Model Learning}
	\label{c:modelLearning}

	This chapter focuses on why learning models is needed in robotics and how this can be done. Also a specific algorithm for linear Gaussian dynamical systems is studied in \autoref{subsec:lgds}. Models have a wide range of applications in robotics, e.g. to simulate control and motion planning algorithms, detect faults, assess stability and safety, use them for state estimations (see \autoref{c:stateEstimation}) and to design controllers using control theory and trajectory optimization. One of the first decisions to make is which model to learn as there are a lot of models that are frequently used in robotics, for example:
	\begin{itemize}
		\item \eqmakebox[modelsInRobotics][l]{continuous-time dynamics:}       \( \vec{F}(\vec{q}, \dot{\vec{q}}, \ddot{\vec{q}}, \vec{u}) = \vec{0} \)
		\item \eqmakebox[modelsInRobotics][l]{discrete-time forward dynamics:} \( \vec{x}_{t + 1} = \vec{f}(\vec{x}_t, \vec{u}_t) \)
		\item \eqmakebox[modelsInRobotics][l]{discrete-time inverse dynamics:} \( \vec{u}_t = \vec{f}^{-1}(\vec{x}_t, \vec{x}_{t + 1}) \)
		\item \eqmakebox[modelsInRobotics][l]{kinematics:}                     \( \vec{x} = \vec{f}(\vec{q}) \)
		\item \eqmakebox[modelsInRobotics][l]{sensors:}                        \( \vec{y} = \vec{f}(\vec{x}) \)
	\end{itemize}
	But getting a good model is hard! There are three major approaches for this:
	\begin{enumerate}
		\item Use the data-sheet of the robot and derive the dynamics using physics.
		\item Disassemble the system and measure every component.
		\item Learn the model from data.
	\end{enumerate}
	The first approach might be the fastest, but often the data given from the manufacturer is not correct or has a wide confidence region, e.g. in the friction. Disassembling the systems and measuring everything might yield the best model, but is also time-intensive. This also has to be redone quite often as the systems changes (e.g. due to wear of the motor). This chapter focuses on learning the model from data, which might be a good tradeoff between the former two approaches.

	\section{Modeling Assumptions: White, Black and Gray}
		When creating models, assumptions have to be made. The spectrum of assumptions ranges from white-box models (lots of usage of prior knowledge) to black-box models (no usage of prior knowledge) with gray-box models in between. The spectrum of different modeling strategies is quite large with lots of intermediate stages.

		\subsection{White-Box Strategy}
			For white-box models, lots of prior literature and domain knowledge is used, e.g. in the recursive Newton-Euler algorithm. The physical parameters (e.g. the center of mass) is retrieved from the data-sheet or by disassembling and measuring the system (with all the caveats discussed before). These parameters can then be fine-tuned using data from the real system. Some up- and downsides of this strategy are shown in \autoref{tab:whiteBox}.

			\begin{table}
				\centering
				\begin{tabular}{l|l}
					\textbf{Positives}     & \textbf{Negatives}                                   \\ \hline
					Leverages prior work   & Lots of manual labor                                 \\
					Interpretable          & Unmodeled phenomena (e.g. friction) may cause issues \\
					Should generalize well &
				\end{tabular}
				\caption{Positives and negatives of the white-box modeling strategy.}
				\label{tab:whiteBox}
			\end{table}
		% end

		\subsection{Black-Box Strategy}
			The black-box strategy is the opposite end of the spectrum compared to white-box models. No prior work or domain knowledge is used, instead lots of training data is collected and then a model (e.g. a neural network, Gaussian process) is fit to the data. Some up- and downsides of this strategy are shown in \autoref{tab:blackBox}.

			\begin{table}
				\centering
				\begin{tabular}{l|l}
					\textbf{Positives}          & \textbf{Negatives}                                \\ \hline
					Only required training data & Amount of data needed is unknown                  \\
					Quick to implement          & Generalization is questionable                    \\
					                            & May be computationally expensive to train and use
				\end{tabular}
				\caption{Positives and negatives of the black-box modeling strategy.}
				\label{tab:blackBox}
			\end{table}
		% end

		\subsection{Gray-Box Strategy}
			Gray-box models combine aspects of the white- and black-box models, for example:
			\begin{itemize}
				\item Fit a white-box model and handle the remaining error with a black-box model (e.g. a Gaussian process with a physics-based mean).
				\item Fit a black-box model with special constraints (e.g. ensure energy conservation in a closed system).
				\item Construct a model with white- and black-box components that can be trained "end-to-end" (e.g. a differentiable physics model combined with a neural network friction model, jointly trained with gradient decent).
			\end{itemize}
		% end
	% end

	\section{Collecting the Data for Training the Model}
		One big problem in model learning is to get the data from the real robot. The main question is what input signal is the best for learning models, i.e. what controls should the robot be given to perform interesting movements?

		For linear systems, so-called \emph{responses} are identified, which are the output given a certain signal as an input. These signals used for identifying the responses usually are the simplest component of a broader class of signals. For analog signals, usually delta functions are a good signal. For digital and periodic signals, step functions and sinusoidal harmonics are used, respectively.

		\subsection{Impulse Response}
			The delta function (an \emph{impulse}) is widely used in physics and signal processing. It is defined to have the following properties:
			\begin{align*}
				\delta(t - \tau) =
				\begin{cases}
					1 & \text{if } t = \tau \\
					0 & \text{otherwise}
				\end{cases}
				 &  &
				\int_{-\infty}^{\infty} \! \delta(t - \tau) \dd{t} = 1
				 &  &
				\int_{-\infty}^{\infty} \! f(t) \delta(t - \tau) \dd{t} = f(\tau)
			\end{align*}
			Impulse functions can be used to approximate an arbitrary function as a sequence of impulses as
			\begin{equation*}
				f(t) \approx \sum_{n = -\infty}^{\infty} f(n\Delta_t) \delta(t - n\Delta_t)
			\end{equation*}
			where \(\Delta_t\) is a time step. As \(\Delta_t \to 0\), the function is approximated perfectly, i.e. the approximation becomes an equality. This is the main idea of a \emph{convolution}, which is an important part of signal processing and system identification theory.
		% end

		\subsection{Step Response}
			The step function (also called \emph{Heaviside step function}) is closely related to the impulse, however ot contains a jump and is defined as:
			\begin{align*}
				\mathit{step}(t - \tau) =
				\begin{cases}
					1 & \text{if } t \geq \tau \\
					0 & \text{otherwise}
				\end{cases}
				 &  &
				\dv{t} \mathit{step}(t - \tau) = \delta(t - \tau)
			\end{align*}
			Similarly to the impulse, the step function can be used to represent digital (quantized) signals.
		% end

		\subsection{Characterization of Dynamical Systems}
			Dynamical systems are usually characterized using the terms \emph{damping} and \emph{resonant frequencies}, which both origin in linear system theory:
			\begin{description}[leftmargin=5cm]
				\item[Damping] Describes how the system dissipates energy.
					\begin{itemize}
						\item Heavily damped: Energy is lost quickly, the dynamics are "slow".
						\item Lightly damped: Energy is lost fast, the dynamics are "quick" and oscillations are common.
						\item "Critically" damped: Energy is lost "ideally", that is, as fast as possible without oscillations.
					\end{itemize}
				\item[Resonant Frequencies] These are the frequencies that maximally excite a system. This is important when designing structures like bridges, where hitting the resonant frequencies may cause catastrophic accidents.
			\end{description}
			This intuition holds locally for nonlinear system by linear approximation (Taylor-expansion), but the properties vary with the state (and thus the time)!
		% end

		\subsection{Frequency Analysis}
			Another viewpoint to analyze whether a signal is good for system identification is their frequency spectrum. The broader the bandwidth is, the more informative the response becomes. These frequencies can be obtained with a Fourier transform. For an impulse signal, all frequencies are covered and for sinusoidal harmonics only specific frequencies are covered. Motivate from this, other signal types are studied, for example chirps and white Gaussian noise. Chirps describe an oscillation of which the frequency gets higher and higher, like \( \sin(x^2) \). White Gaussian noise also covers a wide range of frequencies, however both of these signals are impractical in reality: Physical systems require smooth inputs for safety, limiting the frequency. A practical solution to this problem is to low-pass the original signal, but this obviously causes a loss in the amount of frequencies that are covered.
		% end

		\subsection{Ornstein-Uhlenbeck Process and Active Learning}
			The frequency analysis motivates the \emph{Ornstein-Uhlenbeck process} for identification, which is smoothed noise:
			\begin{equation*}
				\vec{u}_{t + 1} = \alpha \vec{u}_t + \vec{\eta},\quad \vec{\eta} \sim p(\cdot)
			\end{equation*}
			For higher \(\alpha\), this causes more movements in the actions.

			Another approach in machine learning is to use the model itself to identifying the signals. This task of choosing the data points which inform the model the most is called \emph{active learning} and it is highly coupled with \emph{information theory}. There are several objectives that can be used for selecting the data point, for example:
			\begin{itemize}
				\item Uncertainty: Variance/Entropy: \( \Var[\vec{x}_t] \), \( H[\vec{x}_t] \)
				\item Expected Variance Reduction: \( \Var\big[ \mathcal{M} \biggiven \mathcal{D} \cup (\vec{x}_{1:T}, \vec{u}_{1:T - 1}) \big] \) \\ "Variance reduction in model \(\mathcal{M}\) by adding the trajectory to the dataset \(\mathcal{D}\)."
				\item Information Gain: \( \frac{1}{2} \log \frac{\Var[\vec{x}_t]}{\Var_\mathrm{noise}} \) \\ Equivalent to \( H[\vec{x}] - H_\mathrm{min} \) for Gaussian beliefs.
			\end{itemize}
			Active learning require a model that is capable of expressing the model uncertainty, hence it requires a Bayesian model, e.g. a Gaussian process. Optimizing the trajectories with active learning is still an open research topic!

			In practice, out-of-phase sinuses on every joint create very diverse end-effector trajectories that are useful for learning.
		% end
	% end

	\section{Learning Models}
		Now given good training data, what is the best objective \(L\) for learning a good model? There are multiple options, e.g.
		\begin{itemize}
			\item \eqmakebox[learningModels][l]{One-step regression, i.e.}           \( L = \big\lVert \vec{x}_{t + 1} - \vec{f}(\vec{x}_t) \big\rVert_2^2 \)
			\item \eqmakebox[learningModels][l]{Multi-step regression, i.e.}         \( L = \big\lVert \vec{x}_{t + k} - \vec{f}^k(\vec{x}_t) \big\rVert_2^2 \)
			\item \eqmakebox[learningModels][l]{Inference over the trajectory, i.e.} \( L = \log p(\vec{x}_{1:T}) \)
		\end{itemize}
		where \( \vec{f}^k(\vec{x}_k) \) refers to applying \( \vec{f}(\vec{x}_k) \) \(k\) times to the state \( \vec{x}_t \), or rather the respective result of the previous application. In general an objective that considers a longer prediction horizon (a higher \(k\)) should yield models that have a better long-horizon prediction, but are more expensive to train. It becomes even harder when the problem is only partially observed. One such interesting (and solvable!) case are Linear Gaussian Dynamical Systems (LGDS) which are covered in the next section.

		\subsection{Linear Gaussian Dynamical Systems (LGDS)}
			\label{subsec:lgds}

			Linear Gaussian dynamical systems are given as linear state transitions in an unobserved (latent) space \(\vec{x}_t\) from which measurements \(\vec{y}_t\) are taken (also linearly). Both of these parts underlie additive Gaussian noise, forming the following probabilistic model
			\begin{align*}
				p(\vec{x}_{t + 1} \given \vec{x}_t) & = \mathcal{N}\big( \vec{x}_{t + 1} \biggiven \mat{A} \vec{x}_t,\, \mat{\Sigma}_\eta \big) \\
				p(\vec{y}_t \given \vec{x}_t)       & = \mathcal{N}\big( \vec{y}_t \biggiven \mat{C} \vec{x}_t,\, \mat{\Sigma}_\zeta \big)
			\end{align*}
			with the initial state distribution \( \vec{x}_1 \sim \mathcal{N}(\cdot \given \vec{\mu}_1, \mat{\Sigma}_1) \). The model parameters are defined as
			\begin{equation*}
				\vec{\theta} \coloneqq \big\{\, \mat{A},\, \mat{C},\, \mat{\Sigma}_\eta,\, \mat{\Sigma}_\zeta,\, \vec{\mu}_1,\, \mat{\Sigma}_1 \,\big\}
			\end{equation*}
			which are to be estimated from a sequence of measurements \( \vec{y}_{1:T} \). This is done using an EM algorithm that in the E-step uses the model to infer the hidden states \(\vec{x}_t\) using a Kalman filter and in the M-step maximizes the expected log-likelihood to improve the model.

			As the system behaves Markovian (the next state only depends on the current state and not the past), the likelihood can be factorized as
			\begin{equation*}
				p(\vec{x}_{1:T}, \vec{y}_{1:T}) = p(\vec{x}_1) \prod_{t = 1}^{T - 1} p(\vec{x}_{t + 1} \given \vec{x}_t) \prod_{t = 1}^{T} p(\vec{y}_t \given \vec{x}_t).
			\end{equation*}
			Hence, the log-likelihood is
			\begin{equation*}
				\log p(\vec{x}_{1:T}, \vec{y}_{1:T}) = \log p(\vec{x}_1) + \sum_{t = 1}^{T - 1} \log p(\vec{x}_{t + 1} \given \vec{x}_t) + \sum_{t = 1}^{T} \log p(\vec{y}_t \given \vec{x}_t).
			\end{equation*}
			As the latent states \(\vec{x}_t\) are unknown, it is not possible to maximize the log-likelihood directly, but rather the \emph{expected} log-likelihood over \(\vec{x}_t\), conditioned on the observations, has to be maximized. This can be done straightforwardly by exploiting the invariance of the trace under cycling permutations. This (rather ugly) process\footnote{See \url{https://www.ias.informatik.tu-darmstadt.de/uploads/Team/JoeWatson/Damken_Bsc_2020.pdf}, pages 75 to 82 for the full derivation.} yields the expected log-likelihood
			\begin{align*}
				\E_{\vec{x}_{1:T}}\big[\! \log p(\vec{x}_{1:T}, \vec{y}_{1:T}) \biggiven \vec{y}_{1:T} \big] =\,
				 & \underbrace{-\frac{T (k + p)}{2} \log(2\pi) - \frac{1}{2} \log \lvert \mat{\Sigma}_1 \rvert - \frac{T - 1}{2} \log \lvert \mat{\Sigma}_\eta \rvert - \frac{T}{2} \log \lvert \mat{\Sigma}_\zeta \rvert}_\text{Entropies}                                                            \\
				 & \underbrace{-\frac{1}{2} \tr\Big( \mat{P}_1^{-1} \Big(\! \mat{\Sigma}_1 - \hat{\vec{x}}_1 \vec{\mu}_1^T - \vec{\mu}_1 \hat{\vec{x}}_1^T + \vec{\mu}_1 \vec{\mu}_1^T \Big) \!\Big)}_\text{Initial State Error}                                                                       \\
				 & \underbrace{-\frac{1}{2} \tr\Big( \mat{\Sigma}_\eta^{-1} \Big(\! \textstyle\sum_{t = 2}^{T}\displaystyle \mat{P}_t - \mat{P}_{t, t - 1} \mat{A}^T - \mat{A} \mat{P}_{t, t - 1} + \mat{A} \mat{P}_{t - 1} \mat{A}^T \Big) \!\Big)}_\text{Dynamics Error}                             \\
				 & \underbrace{-\frac{1}{2} \tr\Big( \mat{\Sigma}_\zeta^{-1} \Big(\! \textstyle\sum_{t = 1}^{T}\displaystyle \vec{y}_t \vec{y}_t^T - \vec{y}_t \hat{\vec{x}}_t^T \mat{C}^T - \mat{C} \hat{\vec{x}}_t \vec{y}_t^T + \mat{C} \mat{P}_t \mat{C}^T \Big) \!\Big)}_\text{Observation Error}
			\end{align*}
			with the expectations
			\begin{align*}
				\mat{P}_t \coloneqq \E_{\vec{x}_{1:T}}\big[ \vec{x}_t \vec{x}_t^T \biggiven \vec{y}_{1:T} \big]
				 &  &
				\mat{P}_{t, t - 1} \coloneqq \E_{\vec{x}_{1:T}}\big[ \vec{x}_t \vec{x}_{t - 1}^T \biggiven \vec{y}_{1:T} \big]
				 &  &
				\hat{\vec{x}}_t \coloneqq \E_{\vec{x}_{1:T}}[ \vec{x}_t \given \vec{y}_{1:T} ]
			\end{align*}
			that encode the self and cross correlation and the expected states, respectively. By maximizing the expected log-likelihood, closed-form update rules for the parameters can be derived given an expected latent trajectory. This yields the following update rules:
			\begin{align*}
				\mat{A}^\mathrm{new}            & = \Bigg(\! \sum_{t = 2}^{T} \mat{P}_{t, t - 1} \!\Bigg) \Bigg(\! \sum_{t = 2}^{T} \mat{P}_{t - 1} \!\Bigg)^{-1}       \\
				\mat{C}^\mathrm{new}            & = \Bigg(\! \sum_{t = 1}^{T} \vec{y}_t \hat{\vec{x}}_t^T \!\Bigg) \Bigg(\! \sum_{t = 1}^{T} \mat{P}_t \!\Bigg)         \\
				\mat{\Sigma}_\eta^\mathrm{new}  & = \frac{1}{T - 1} \Bigg( \sum_{t = 2}^{T} \mat{P}_t - \mat{A}^\mathrm{new} \sum_{t = 2}^{T} \mat{P}_{t, t - 1} \Bigg) \\
				\mat{\Sigma}_\zeta^\mathrm{new} & = \frac{1}{T} \sum_{t = 1}^{T} \vec{y}_t \vec{y}_t^T - \mat{C}^\mathrm{new} \hat{\vec{x}}_t \vec{y}_t^T               \\
				\vec{\mu}_1^\mathrm{new}        & = \hat{\vec{x}}_1                                                                                                     \\
				\mat{\Sigma}_1^\mathrm{new}     & = \mat{P}_1 - \hat{\vec{x}}_1 \hat{\vec{x}}_1^T
			\end{align*}

			Plugging this together with the Kalman smoother, this yields an expectation-maximization (EM) algorithm with the E-step being the Kalman smoother and the M-step being the above update rules. For the LGDS, this guarantees monotonic improvement in the expected log-likelihood! For nonlinear settings, it is possible to approximate the expectations (e.g. using cubature rules or particle methods) and update the parameters using gradient descent. This is called \emph{approximate} EM.
		% end
	% end

	\section{Case Studies}
		This section looks into some specific examples of model learning that go beyond LGDS.

		\subsection{Combining Rigid Body Dynamics and Gaussian Processes}
			Inverse rigid body dynamics can be expressed as a linear equation \( \vec{\tau} = \mat{\Phi}(\vec{q}, \dot{\vec{q}}, \ddot{\vec{q}}) \vec{\beta} \) where the matrix \( \mat{\Phi} \) contains nonlinear functions of the joint angles/velocities/accelerations. The weights \(\vec{\beta}\) contain physical parameters like masses. However, this model misses nonlinear effects like friction! One method to handle this is to combine the rigid body model with a Gaussian process to fit the residual error (with \( \vec{x} = \{ \vec{q}, \dot{\vec{q}}, \ddot{\vec{q}} \} \) for brevity):
			\begin{equation*}
				\vec{\tau}(\vec{x}) \sim \mat{\Phi}(\vec{x}) \vec{\beta} + \mathcal{GP}\big( \vec{0},\, k(\vec{x}, \vec{x}') \big)
			\end{equation*}
			While standard kernels are not well-suited for this problem, it is possible to define a linear kernel using the feature functions \(\vec{\phi}_k\) of \(\mat{\Phi}\):
			\begin{equation*}
				k_\mathrm{rbd}^k(\vec{x}_p, \vec{x}_q) = \vec{\phi}_k^T(\vec{x}_p) \mat{W}_k \vec{\phi}_k(\vec{x}_q) + \sigma_k^2 \delta_{pq}
			\end{equation*}
			This kernel was found to have superior performance in handling the residual error.
		% end

		\subsection{Deep Lagrangian Networks}
			Deep Lagrangian networks (DeLaNs) use neural networks to model the Lagrangian equations of motions, resulting in the inductive bias of energy conservation. This should yield to more physically plausible model predictions. See Lutter et al., "Deep Lagrangian Networks: Using Physics as Model Prior for Deep Learning" (2019) for more information on this.
		% end

		\subsection{The Differentiable Recursive Newton-Euler Algorithm}
			Implementing the recursive Newton-Euler algorithm that was introduced in \autoref{subsubsec:robotsDynamics} in an automatic differentiation library, physical models can be trained with gradient decent! However, care has to be taken on the parameters, e.g. to enforce positive masses (this can be done, for example, by training the square-root instead of the parameter itself). As it is trained with gradient decent, black-box models can be easily integrated. Some example (where \( f_\mathrm{NN} \) is a feedforward neural network) are:
			\begin{description}[leftmargin=3.5cm]
				\item[Friction]       \( \tau = \tau_d - \sign(\dot{q}) \big\lVert f_\mathrm{NN}(q, \dot{q}; \psi_F) \big\rVert_1 \)
				\item[Residual Error] \( \tau = \tau_d - f_\mathrm{NN}(q, \dot{q}; \psi_R) \)
				\item[Full NN]        \( \tau = f_\mathrm{NN}(\tau-d, q, \dot{q}; \psi_M) \)
			\end{description}

			Nonholonomic systems can also be modeled by using maximal rather than intrinsic coordinates:
			\begin{description}
				\item[Holonomic] Only constrained in position.
				\item[Non-Holonomic] Also constrained on differential terms, e.g. velocities.
				\item[Intrinsic Coordinates] Internal reference frame.
				\item[Maximal Coordinates] External reference frame.
			\end{description}
			One example for a non-holonomic constraint is the string in the ball-in-cup environment. Black-box models like neural networks or recurrent neural networks fail on this, and the learned policies are not transferable to the real system. The differential recursive Newton-Euler algorithm can plan with the string constraint, and the resulting policies work on the real robot!
		% end
	% end
% end

\chapter{Policy Representations}
	To train a policy \( \pi(\vec{a} \given \vec{s}) \), this policy has to be represented somehow. In control theory, this policy is commonly known as the \emph{controller} and has the form of a PID controller, for example. In robot learning, however, these policies have to be represented in a "learnable" way. This chapter covers parametric policies with off-the-shelf methods like neural networks and movement primitives which can encode complex behavior in a modular fashion. This chapter focuses on parametric policies, i.e. policies that is represented by a conditional probability distribution \( \pi(\vec{a} \given \vec{s}; \vec{\theta}) \) with the parameters \(\vec{\theta}\). Desirable properties for policy representations are:
	\begin{description}[leftmargin=3.5cm]
		\item[Compactness]   The number of parameters should be low.
		\item[Learnability]  Easy to learn from demonstrations and by reinforcement learning.
		\item[Stochasticity] Ability to encode exploration and variability.
		\item[Optimality]    Should be able to encode the optimal behavior.
		\item[Scalability]   Usable for a high number of degrees of freedom.
		\item[Modularity]    Adaptability for a new environment and co-activation and blending of movements.
		\item[Usability]     Should be usable for stroke-based (point-to-point) and rhythmic movement.
	\end{description}
	A stochastic policy is useful as it can be used for exploration in reinforcement learning and can capture variability in the movements. Different exploration models are for example:
	\begin{itemize}
		\item \eqmakebox[policyRepr][l]{No exploration:}       \eqmakebox[policyReprPol][l]{\( \vec{a} = \pi(\vec{s}) = \vec{f}_{\vec{w}}(\vec{s}) \),}\quad \( \vec{\theta} = \vec{w} \)
		\item \eqmakebox[policyRepr][l]{Uncorrelated:}         \eqmakebox[policyReprPol][l]{\( \vec{a} \sim \pi(\cdot \given \vec{s}; \vec{\theta}) = \mathcal{N}\big(\cdot \given \vec{f}_{\vec{w}}(\vec{s}), \sigma^2 \mat{I}\big) \),}\quad \( \vec{\theta} = \{ \vec{w}, \sigma^2 \} \)
		\item \eqmakebox[policyRepr][l]{Correlated:}           \eqmakebox[policyReprPol][l]{\( \vec{a} \sim \pi(\cdot \given \vec{s}; \vec{\theta}) = \mathcal{N}\big(\cdot \given \vec{f}_{\vec{w}}(\vec{s}), \mat{\Sigma}\big) \),}\quad \( \vec{\theta} = \{ \vec{w}, \mat{\Sigma} \} \)
		\item Complex distributions like Gaussian mixture models, normalizing flows, \dots
	\end{itemize}

	\section{Off-The-Shelf Policies}
		Some examples for off-the-shelf policies are neural networks, radial basis functions (RBF) networks, Gaussian processes and locally weighted regression models. While they are easy to use, off-the-self policies have some major drawbacks: they usually generalize bad if the controller has not been trained in the requested area of the state space, they are not robust (so also small changes in the system may lead to catastrophic failures), and they are often really sample-inefficient.

		\subsection{Linear Basis Functions}
			Linear controllers \( \vec{a} = \vec{f}_{\vec{w}}(\vec{s}) = \vec{\phi}^T(\vec{s}) \vec{w} \), in the simplest case a PD-controller with \( \vec{\phi}^T(\vec{s}) = \begin{bmatrix} 1 & \vec{s}^T \end{bmatrix} \), may yield really good policies using a very compact representation. Also they are easy to learn with linear regression. Whoever, a good feature representation has to be used! This representation was successfully applied to balancing a pole on a cart, but even learning the swing-up can be hard.
		% end

		\subsection{Radial Basis Functions (RBFs)}
			A more expressive policy representation are weighted radial basis functions (RBFs) \( \vec{f}_{\vec{w}}(\vec{s}) = \vec{\phi}^T(\vec{s}) \vec{\beta} \) with the features
			\begin{equation*}
				\phi_i(\vec{s}) = \exp{ -\frac{1}{2} \sum_{j = 1}^{D} \frac{(s_j - \mu_{ij})^2}{h_ij} }.
			\end{equation*}
			The parameters in are \( \vec{w} = \{ \vec{\beta}, \vec{\mu}_{1:K}, \vec{h}_{1:K} \} \), where \(K\) is the number of RBFs and \(\vec{\mu}_{1:K}\) and \(\vec{h}_{1:K}\) are the locations and bandwidths of the RBFs, respectively. But as the features are really localized, it might be possible that no basis function is active at some point. This can be fixed by using normalized RBFs,
			\begin{equation*}
				\vec{f}_{\vec{w}}^\mathit{norm}(\vec{s}) = \frac{\sum_{i = 1}^{K} \phi_i(\vec{s} \beta_i)}{\sum_{j = 1}^{K} \phi_i(\vec{s})},
			\end{equation*}
			which do not suffer from this problem. However, this can lead to numerically issues as the denominator gets really small if every basis function is far from being active. Also, this problem is really hard to optimize as the optimization for \( \vec{h}_{1:K} \) is non-convex!
		% end

		\subsection{Trajectory Following Controller}
			Another method for policy representation is to use the previously discussed methods for representing a trajectory (see \autoref{sec:roboticsRepresentingTrajectories}) and use a feedback controller
			\begin{equation*}
				\pi(\vec{q}, \dot{\vec{q}}, t; \vec{w}) = \mat{K}_P\big( \vec{q}_d(\vec{q}, \dot{\vec{q}}, t; \vec{w}) - \vec{w} \big) + \mat{K}_D\big( \dot{\vec{q}}_d(\vec{q}, \dot{\vec{q}}, t; \vec{w}) - \dot{\vec{q}} \big)
			\end{equation*}
			to follow the desired trajectory \( \big( \vec{q}_d(\vec{q}, \dot{\vec{q}}, t; \vec{w}), \dot{\vec{q}}_d(\vec{q}, \dot{\vec{q}}, t; \vec{w}) \big) \). Usually the matrices \( \mat{K}_P \) and \( \mat{K}_D \) are hand-tuned, but they may also be learned.
		% end
	% end

	\section{Movement Primitives}
		\emph{Movement primitives} are a compact representation of movements that are often represented as trajectory generators \( \vec{\tau} = \vec{f}(\vec{w}) \) with the parameters \( \vec{w} \) of the primitive. They can represent both stroke-based and rhythmic movements and can be time-dependent, state-dependent or both. The basic idea is to use dynamical systems to represent trajectories as integrating it results in a trajectory. Dynamical systems (defined by a second-order differential equation) can encode various different movements and the whole class of stability analysis from ODEs can be used to generate e.g. stroke-based movements by exploiting a single-point attractor. These movement primitives are known as \emph{dynamic} movement primitives (DMPs).

		\subsection{Dynamic Movement Primitives (DMPs)}
			Dynamic movement primitives can encode lots of desirable properties like stability, perturbation robustness and stroke-based and rhythmic behaviors. They are also really easy to learn and scale well with a high number of degrees of freedom. By intelligent parametrization, they can also encode temporal scaling, i.e. it is possible to execute a movement slower.

			Starting from the model of a spring-damper system, \( \ddot{y} = \alpha \big( \beta (g - y) - \dot{y} \big) \), which always converges to the goal attractor \(g\) as time goes by\footnote{More formally, \( \lim_{t \to \infty} y(t) = g \).}, a \emph{forcing function} \( f_{\vec{w}}(t) \) can be added to obtain a moving attractor
			\begin{equation}
				\ddot{y} = \alpha \big( \beta (g - y) - \dot{y} \big) + f_{\vec{w}}(t) = \alpha \Bigg(\! \beta \bigg(\! g + \frac{f_{\vec{w}}(t)}{\alpha \beta} - y \!\bigg) - \dot{y} \!\Bigg)  \label{eq:nonTemporalScaledDMP}
			\end{equation}
			that converges to \( g + f_{\vec{w}}(t) / (\alpha \beta) \). The forcing function \( f_{\vec{w}} \) is a learnable function with parameters \(\vec{w}\) to adapt to all kinds of movements.

			Using DMPs for multiple degrees of freedom is also possible by using an individual DMP per DoF, while sharing the phase variable \(z\). For periodic movements, periodic phase variables can be used as these control the trajectory.

			\subsubsection{Temporal Scaling}
				To achieve temporal scaling, a phase variable \( z(t) \) is added to the system to replace the time \(t\) in \eqref{eq:nonTemporalScaledDMP}:
				\begin{align*}
					\ddot{y} & = \tau^2 \alpha \big( \beta (g - y) - \dot{y} / \tau \big) + \tau^2 f_{\vec{w}}(z) \\
					\dot{z}  & = -\tau \alpha_z z
				\end{align*}
				The variable \(\tau\) is the \emph{temporal scaling variable} and \(z\) is called the \emph{phase}. Higher values of \(\tau\) correspond to higher movement speeds and a value of \( \tau = 1 \) corresponds to the "original" movement speed.
			% end

			\subsubsection{Representation of the Forcing Function}
				One approach for representing the forcing function is to use the function
				\begin{equation*}
					f_{\vec{w}} = \vec{\psi}^T(z) \vec{w}
				\end{equation*}
				with \(K\) normalized RBF basis functions\footnote{Notice the multiplication with the phase variable \(z\) in the RBF! This is used to make the DMP stable and is just more convenient to put it there instead of in the forcing function itself.}
				\begin{align}
					\psi_i(z) = \frac{\phi_i(z) z}{\sum_{j = 1}^{K} \phi_j(z)}
					 &  &
					\phi_i(z) = \exp{ -\frac{1}{2} \frac{(z - c_i)^2}{h_i} }  \label{eq:normRbf}
				\end{align}
				with the locations \(c_{1:K}\) and the bandwidths \(h_{1:K}\). These dynamics movement primitives are stable by design as for \( t \to \infty \) the forcing function vanishes\footnote{This is due to \( z \to 0 \) for \( t \to \infty \) as \( \dot{z} = -\tau \alpha_z z \) and \( \tau, \alpha_z > 0 \).} and the  becomes a PD controller.
			% end

			\subsubsection{Imitation Learning}
				Dynamic movement primitives with radial basis functions can be easily used for imitation learning using linear (ridge) regression. Given a trajectory \( (\vec{q}_{1:T}, \dot{\vec{q}}_{1:T}, \ddot{\vec{q}}_{1:T}) \), a goal attractor \(g\), parameters \(\alpha\), \(\beta\), \(\alpha_z\) and a temporal scaling variable \(\tau\), first compute the target values of the forcing function for each \(t\):
				\begin{equation*}
					f_t = \frac{\ddot{\vec{q}}_t}{\tau^2} - \alpha \big( \beta (g - q_t) - \dot{q}_t / \tau^2 \big),
				\end{equation*}
				Subsequently, compute the weights \(\vec{w}\) of \( f_{\vec{w}} \) with
				\begin{equation*}
					\vec{w} = \big( \mat{\Psi}^T \mat{\Psi} + \sigma^2 \mat{I} \big)^{-1} \mat{\Psi} \vec{f}
				\end{equation*}
				where \( \mat{\Psi} = \begin{bmatrix} \vec{\psi}_1 & \vec{\psi}_2 & \cdots & \vec{\psi}_T \end{bmatrix} \) is the feature matrix and \(\vec{f}\) is the vector of target values. The parameter \(\sigma^2\) is used for regularization and can also be chosen \( \sigma^2 = 0 \) for non-ridge regression.
			% end
		% end

		\subsection{Probabilistic Movement Primitives (ProMPs)}
			Another approach on movement primitives are \emph{probabilistic movement primitives} (ProMPs), a stochastic representation of trajectories \( \vec{\tau} \sim p(\cdot \given \vec{w}) \). This is represented by learning a (parameterized) distribution \( p(\vec{w} \given \vec{\theta}) \) over the parameters \(\vec{w}\), each representing a single trajectory
			\begin{equation*}
				\vec{\tau} = \vec{f}(\vec{w}) + \vec{\epsilon}
			\end{equation*}
			with noise \(\epsilon\). The weights can then be integrated out to obtain \( p(\vec{\tau \given \vec{\theta}}) \). Stochastic movement primitives are useful as it allows to represent uncertainty which gives information on the importance of time points. Also probabilistic operations can be applied.

			One possible representation of a trajectory is to use multiple normalized RBF basis, one for each degree of freedom. Let
			\begin{equation*}
				\psi_i^{(d)}(z) = \frac{\phi_i^{(d)}(z) z}{\sum_{j = 1}^{K} \phi_j^{(d)}(z)}
				\qquad\text{with}\qquad
				\phi_i^{(d)}(z) = \exp{ -\frac{1}{2} \frac{(z - c_i)^2}{h_i} }
			\end{equation*}
			be the \(i\)-th RBF feature (\( i = \dotsrange{1}{K} \)) of the \(d\)-th degree of freedom (\( d = \dotsrange{1}{D} \)). Then let
			\begin{equation*}
				\mat{\Psi}_t^T \coloneqq
				\begin{bmatrix}
					\psi_1^{(1)}(z_t) & \psi_2^{(1)}(z_t) & \cdots & \psi_K^{(1)}(z_t) \\
					\psi_1^{(2)}(z_t) & \psi_2^{(2)}(z_t) & \cdots & \psi_K^{(2)}(z_t) \\
					\vdots            & \vdots            & \ddots & \vdots            \\
					\psi_1^{(D)}(z_t) & \psi_2^{(D)}(z_t) & \cdots & \psi_K^{(D)}(z_t)
				\end{bmatrix}
			\end{equation*}
			be the phase-dependent basis for the ProMP. One trajectory is given as
			\begin{equation*}
				\vec{y}_t \sim \mathcal{N}\big( \cdot \biggiven \mat{\Psi}_t^T \vec{w},\, \mat{\Sigma}_y \big),
			\end{equation*}
			with additive Gaussian noise, where \( \vec{y}_t \) is the value of the trajectory at time step \(t\). This can e.g. be the positions and velocities of the joints. Then the probabilistic model for the whole trajectory therefore is given as
			\begin{equation*}
				p(\vec{\tau} \given \vec{w}) = \prod_t \mathcal{N}\big( \vec{y}_t \biggiven \mat{\Psi}_t^T \vec{w},\, \mat{\Sigma}_y \big).
			\end{equation*}
			Given the distribution \( p(\vec{w} \given \vec{\theta}) \) of the weights, the trajectory distribution \( p(\vec{\tau} \given \vec{\theta}) \) can be obtained by marginalization:
			\begin{equation*}
				p(\vec{\tau} \given \vec{\theta}) = \int\! p(\vec{\tau} \given \vec{w}) p(\vec{w} \given \vec{\theta}) \dd{\vec{w}}
			\end{equation*}
			By using a Gaussian \( p(\vec{w} \given \vec{\theta}) = \mathcal{N}(\vec{w} \given \vec{\mu}_w, \mat{\Sigma}_w) \) with \( \vec{\theta} = \{ \vec{\mu}_w, \mat{\Sigma}_w \} \) for the weight distribution, the trajectory distribution can be computed in closed form for a time step \(t\):
			\begin{align*}
				p(\vec{y}_t \given \vec{\theta})
				 & = \int\! p(\vec{y}_t \given \vec{w}) p(\vec{w} \given \vec{\theta}) \dd{\vec{w}}                                                                                     \\
				 & = \int\! \mathcal{N}\big( \vec{y}_t \biggiven \mat{\Psi}_t^T \vec{w},\, \mat{\Sigma}_y \big) \, \mathcal{N}(\vec{w} \given \vec{\mu}_w, \mat{\Sigma}_w) \dd{\vec{w}} \\
				 & = \mathcal{N}\big( \vec{y}_t \biggiven \mat{\Psi}_t^T \vec{\mu}_w,\, \mat{\Psi}_t^T \mat{\Sigma}_w \mat{\Psi}_t + \mat{\Sigma}_y \big)
			\end{align*}

			\subsubsection{Conditioning}
				It is possible to condition the distribution over the parameters on a position \( \vec{y}_{t^\ast}^\ast \) at time step \(t^\ast\) with covariance \( \mat{\Sigma}_y^\ast \) by using Bayes rule:
				\begin{equation*}
					p(\vec{w} \given \vec{y}_{t^\ast}^\ast, \mat{\Sigma}_y^\ast) \propto \mathcal{N}\big( \vec{y}_{t^\ast}^\ast \biggiven \mat{\Psi}_{t^\ast}^T \vec{w},\, \mat{\Sigma}_y^\ast \big) p(\vec{w})
				\end{equation*}
				This allows ProMPs to generalize to other end-points of the movement or reaching intermediate points. Using Gaussian weight distributions, Bayes rule can be computed in closed form:
				\begin{align*}
					\vec{\mu}_w^\mathrm{new}    & = \vec{\mu}_w + \mat{\Sigma} \mat{\Psi}_{t^\ast} \big( \mat{\Psi}_{t^\ast}^T \mat{\Sigma}_w \mat{\Sigma}_{t^\ast} + \mat{\Sigma}_y^\ast \big)^{-1} \big( \vec{y}_{t^\ast}^\ast - \mat{\Psi}_{t^\ast}^T \vec{\mu}_w \big) \\
					\mat{\Sigma}_w^\mathrm{new} & = \mat{\Sigma}_w - \mat{\Sigma}_w \mat{\Psi}_{t^\ast} \big( \mat{\Psi}_{t^\ast}^T \mat{\Sigma}_w \mat{\Psi}_{t^\ast} + \mat{\Sigma}_y^\ast \big)^{-1} \mat{\Psi}_{t^\ast}^T \mat{\Sigma}_w
				\end{align*}
			% end

			\subsubsection{Combination}
				It is possible to combine multiple ProMPs into one movement to solve a combination of tasks. For the distributions \( \{ p_i(\vec{y}_t) \}_{i = 1}^{N} \), the combined ProMP \( p_\mathrm{co}(\vec{y}_t) \) is given as
				\begin{equation*}
					p^\mathrm{co}(\vec{y}_t) \propto \prod_{i = 1}^{N} p_i(\vec{y}_t)^{\alpha_i(t)}
				\end{equation*}
				with the activation factors \( \alpha_i(t) \). For Gaussian distributions \( p_i(\vec{y}_t) = \mathcal{N}\big( \vec{y}_t \biggiven \vec{\mu}_t^{[i]}, \mat{\Sigma}_y^{[i]} \big) \), the combined distribution has the following mean and covariance:
				\begin{align*}
					\vec{\mu}_t^\mathrm{co}    & = \big( \mat{\Sigma}_t^\mathrm{co} \big)^{-1} \Bigg( \sum_{i = 1}^{N} \Big( \mat{\Sigma}_t^{[i]} / \alpha_t^{[i]} \Big)^{-1} \vec{\mu}_t^{[i]} \Bigg) \\
					\mat{\Sigma}_t^\mathrm{co} & = \Bigg( \sum_{i = 1}^{N} \Big( \mat{\Sigma}_t^{[i]} / \alpha_t^{[i]} \Big)^{-1} \Bigg)^{-1}
				\end{align*}
				Using the time-varying activation factors \( \alpha_i(t) \), it is also possible to achieve \emph{blending}, i.e. shifting smoothly from one primitive (movement) to another.
			% end
		% end

		\subsection{Time-Independent Stable Movement Primitives}
			Both the DMPs and ProMPs consider the time as part of the state, however, knowing the exact time is hard and required external algorithms for phase estimation. It is possible to eliminate the dependency on the time in the state by learning \emph{nonlinear stable dynamical systems}
			\begin{equation*}
				\ddot{\vec{q}} = \vec{f}(\vec{q}, \dot{\vec{q}}; \vec{w}).
			\end{equation*}
			For linear dynamical systems, it is easy to construct them stable by choosing the state dynamics matrix to be negative definite. For nonlinear systems this is harder. But by combining a linear system with an invertible transformation \( \vec{y} = \vec{f}(\vec{z}) \), it is possible to create stable nonlinear systems from linear systems. This is due to the fact that the potential energy will be the same in both systems, i.e. \( V(\vec{z}) = V(\vec{y}) \). Examples for such invertible transformations are scalar multiplication or translations in the state-space (this will change the equilibrium!).

			\subsubsection{Imitation Flows}
				A wide range of such invertible transformations can be found by using invertible flows. Invertible flows are a set of neural networks that can be computed forward and backward, i.e. they are bijective. These can represent very complex nonlinear, yet stable, dynamics and are capable of representing both stroke-based and rhythmic movements.
			% end
		% end

		\subsection{Libraries of Primitives}
			Usually, one primitive is not enough. Ideally, the robot has a library of primitives which are all good at what they do and the robot "just" has to decide which movement to make by choosing the corresponding primitive.
		% end
	% end
% end

\chapter{Model-Based Reinforcement Learning}
	\label{c:modelBasedRL}

	This chapter introduces model-based reinforcement learning methods. In reinforcement learning, the goal is to maximize the expected long-term reward like in optimal control (see \autoref{c:optimalControl} and \ref{c:approximateOptimalControl}), however, no knowledge about the environment is given in comparison to solving a given MDP. This has lead to two strategies: model-based RL and model-free RL. In the former a dynamics model of the environment is learned which is then used for solving the optimal control problem. In the latter to (explicit) model is learned but rather the policy is learned directly from data. But the distinction of model-based and model-free RL is not very clean: model-based RL could still use model-free methods to optimize the policy, e.g. by using the learned model to improve accuracy (e.g. DYNA algorithm), use the dynamics to generate data for MFRL or to perturb the parameters to generate robust MFRL policies (sim-to-real). Analogously, MFRL methods often also use models: the V- and Q-function implicitly encode the dynamics of the system, but through the cumulative reward rather than the state.

	A typical distinction between MBRL and MFRL is as follows: "MBRL is more sample-efficient than MFRL, but MFRL has better asymptotic performance, due to modeling errors introducing performance bias. Modeling errors can be catastrophic for MBRL so the algorithms are brittle." This gives rise to three questions which will be addresses in the following sections:
	\begin{itemize}
		\item \emph{When} is MBRL more sample-efficient than MFRL?
		\item \emph{Why} does MBRL have a performance bias?
		\item \emph{When} are modeling errors catastrophic?
	\end{itemize}

	\section{Sample Efficiency}
		The key distinction of MBRL and MFRL is the availability of an inductive bias to generalize outside of the data (OOD). While in a discrete MDP the transition matrix usually has to structure, an LQR system has -- and this structure can be exploited by MBRL. Therefore, MBRL only benefits in terms of sample efficiency if there is domain knowledge that can be exploited.

		\paragraph{Discrete MDP}
			For a discrete MDP with state space \(\mathcal{S}\) and action space \(\mathcal{A}\) and a time horizon \(T\), the \emph{minimum}\footnote{The actual amount of samples required is more evolved and an active are of research.} amount of samples required to solve the MDP are:
			\begin{itemize}
				\item \eqmakebox[mbrlSamplEffDiscrete][l]{for model-free RL:}
					\( \mathcal{O}\big( \lvert \mathcal{S} \rvert^2 \cdot \lvert \mathcal{A} \rvert \big) \)
				\item \eqmakebox[mbrlSamplEffDiscrete][l]{for model-based RL:}
					\( \mathcal{O}\big( \lvert \mathcal{S} \rvert^2 \cdot \lvert \mathcal{A} \rvert \big) \)
			\end{itemize}
			So both MFRL and MBRL have the same sample efficiency as they both have to traverse the whole state-action space!
		% end

		\paragraph{Continuous Linear System (LQR)}
			For a continuous linear LQR system with state dimensionality \(n_s\) and action dimensionality \(n_a\) and time horizon \(T\), the \emph{minimum} amount of samples required to solve the MDP are:
			\begin{itemize}
				\item \eqmakebox[mbrlSamplEffContinuous][l]{for model-free RL:}
					\eqmakebox[mbrlSamplEffContinuousR][l]{\( \mathcal{O}\big( (n_s + 1) \cdot n_a \cdot T \big) \)} \quad[optimal trajectory]
				\item \eqmakebox[mbrlSamplEffContinuous][l]{for model-based RL:}
					\eqmakebox[mbrlSamplEffContinuousR][l]{\( \mathcal{O}\big( n_s \cdot (n_s + n_a + 1) \big) \)}   \quad[any trajectory]
			\end{itemize}
			As usually \( T \gg n_s \), MBRL is considerable more sample-efficient than MFRL for the LQR case as the amount of samples needed is independent from the time horizon!
		% end
	% end

	\section{Models in Reinforcement Learning}
		To use a model in RL, there has to be useful domain knowledge. This is available in a variety of settings, for example in well understood physics-based problems (e.g. it lots of robotics applications), for "smooth" dynamical systems where Gaussian processes and neural networks can extrapolate locally, or games with simple transition rules, e.g. using Monte Carlo tree search for Go. Some shallow domain knowledge can also be used to encode symmetries in feature transformations, e.g. using sine features for angles to encode the "full rotation" symmetry. But there are also various settings where no or little domain knowledge is available: e.g. most discrete MDPs that have arbitrary structure and complex, stochastic, partially observed environments (Atari games from pixels, table tennis with low-frequency tracking, \dots).

		Problems in MBRL include that a performance bias might get introduced by modeling errors or even the optimizer itself. Modeling errors are especially bad as it is never possible to remove them completely. The famous quote "All models are wrong, but some are useful." (George E. P. Box) is no less true in MBRL: There might be models that are useful for reinforcement learning, but they always have the problem of being wrong. If the optimizer gets to exploit these errors, the resulting policy will not work on a real system! This directly leads to the second source of performance biases: problematic optimizers. Issues in the optimizer are mainly produced by two problems: local optima and numerical stability. The following sections will address these problems separately.

		\subsection{Local Optima and Sample-Based Methods}
			In \autoref{sec:ddp}, differential dynamic programming (DDP) was discussed. This is a method of dynamic programming that used linearizations to find an optimal policy. But these linearizations are susceptible to local optima! A common method for escaping local optima are sample-based methods of which some are discussed in the following sections. But they have the major problem of not scaling well to high-dimensional search (state-action) spaces which makes them infeasible for most problems. Some of these scaling issues can be overcome by combining the method with replanning which will be covered in \autoref{sec:mbrlReplanning}.

			\subsubsection{The Cross-Entropy Method (CEM)}
				The \emph{cross-entropy method} is a relatively simple reinforcement learning method that works similar to evolutionary algorithms: first sample parameters \( {\vec{\theta}_k}_{k \,=\, \subdotsrange{1}{K}} \) from a parametric distribution \( p_{\vec{\omega}}(\vec{\theta}) \) and evaluate them somehow (e.g. by generating a corresponding trajectory and calculating the total reward) using an objective \( J(\vec{\theta}) \). Then select the best \( K_\mathrm{top} \) samples and fit the distribution parameters \(\vec{\omega}\) accordingly (e.g. using a maximum likelihood estimator). This converges to the optimal parameters \(\vec{\omega}^\ast\) parameters for the parameter distribution. For (optimal) control, the parameters are a sequence of actions. A procedural description for a Gaussian distribution \( p_{\vec{\omega}}(\vec{\theta}) = \mathcal{N}(\vec{\theta} \given \vec{\mu}, \mat{\Sigma}) \) with \( \vec{\omega} = \{ \vec{\mu}, \mat{\Sigma} \} \) is shown in \autoref{alg:cemGaussian}.

				The CEM can also be extended for using a more complex policy, e.g. a neural network. Then the parameters are updated using gradient descent instead of directly fitting the new data. To avoid overfitting, only a single GD step is done per iteration.

				\begin{algorithm}  \DontPrintSemicolon
					\( \vec{\mu} \gets \vec{0} \),\, \( \mat{\Sigma} \gets \mat{I} \)
					\quad\tcp{Exemplary initialization.}
					\Repeat{convergence of \(\vec{\mu}\) and \(\mat{\Sigma}\)}{
					\eqmakebox[cemGaussian][r]{\( \vec{\theta}_{k \,=\, \subdotsrange{1}{K}} \sim \)} \eqmakebox[cemGaussianR][l]{\( \mathcal{N}(\vec{\mu}, \mat{\Sigma}) \)}
					\quad\tcp{Sample parameters.}
					\eqmakebox[cemGaussian][r]{\( J_k \gets \)} \eqmakebox[cemGaussianR][l]{\( J(\vec{\theta}_k) \)}
					\quad\tcp{Evaluate parameters for \( k = \dotsrange{1}{K} \)}
					\eqmakebox[cemGaussian][r]{\( \vec{\theta}_{(k) \,=\, \subdotsrange{(1)}{(K)}} \gets \)} \eqmakebox[cemGaussianR][l]{\( \sort\limits_J\, \vec{\theta}_{k \,=\, \subdotsrange{1}{K}} \)}
					\quad\tcp{Sort parameters w.r.t. the performance.}
					\BlankLine
					\tcp{Fit new distribution parameters:}
					\eqmakebox[cemGaussian][r]{\( \vec{\mu}^\mathrm{new} \gets \)} \( K_\mathrm{top}^{-1} \sum_{k = 1}^{K_\mathrm{top}} \vec{\theta}_{(k)} \) \;
					\eqmakebox[cemGaussian][r]{\( \mat{\Sigma}^\mathrm{new} \gets \)} \( K_\mathrm{top}^{-1} \sum_{k = 1}^{K_\mathrm{top}} (\vec{\theta}_k - \vec{\mu}^\mathrm{new}) (\vec{\theta}_k - \vec{\mu}^\mathrm{new})^T \) \;
					}

					\caption{Cross-Entropy Method for Gaussian Parameter Distribution}
					\label{alg:cemGaussian}
				\end{algorithm}
			% end

			\subsubsection{(Model-Predictive) Path Integral Control (MPPI)}
				\emph{Path integral control} is a control-as-inference method initially derived for continuous time. For discrete time it is really similar to CEM: first sample a control sequence from a prior distribution, then compute a rollout of the dynamics and compute the "likelihood". Afterwards the parameters can be updated in a weighted fashion.
			% end
		% end

		\subsection{Numerical Sensitivity}
			Previously, the optimal control problem
			\begin{equation*}
				\begin{aligned}
					\max_{\vec{a}_{1:T - 1}} \, & \sum_{t = 1}^{T} r(\vec{s}_t, \vec{a}_t) \\
					\mathrm{s.t.} \quad         &
					\begin{aligned}
						\vec{s}_{t + 1} & = \vec{f}(\vec{s}_t, \vec{a}_t)
					\end{aligned}
				\end{aligned}
			\end{equation*}
			with \( r(\vec{s}_T, \vec{a}_T) \coloneqq r(\vec{s}_T) \) was assessed from a dynamic programming perspective (DDP). Other techniques are for example \emph{backpropagation-through-time} or \emph{sequential dynamic programming} (check out the optimization of static and dynamical systems summary\footnote{\url{https://projects.frisp.org/documents/32}} for more details on SQP methods).

			In backpropagation-through-time, the trajectory is seen as a computation graph
			\begin{equation*}
				\vec{s}_2 = \vec{f}(\vec{s}_1, \vec{a}_1)
				\quad\implies\quad
				\vec{s}_3 = \vec{f}(\vec{s}_2, \vec{a}_2) = \vec{f}\big(\vec{f}(\vec{s}_1, \vec{a}_1), \vec{a}_2\big)
				\quad\implies\quad
				\cdots
			\end{equation*}
			with the objective \( J = \sum_{t = 0}^{T} r(\vec{s}_t, \vec{a}_t) \). To optimize the objective, it is possible to take the derivative w.r.t. the actions through time. For example for \(\vec{a}_1\):
			\begin{equation*}
				\pdv{J}{\vec{a}_1}
				= \pdv{r}{\vec{a}_1} + \bigg(\! \pdv{r}{\vec{s}_2} + \pdv{r}{\vec{s}_3} \pdv{\vec{s}_3}{\vec{s}_2} + \cdots + \pdv{r}{\vec{s}_T} \pdv{\vec{s}_T}{\vec{s}_{T - 1}} \cdots \pdv{\vec{s}_3}{\vec{s}_2} \pdv{\vec{s}_2}{\vec{s}_1} \!\bigg) \pdv{\vec{s}_2}{\vec{a}_1}
			\end{equation*}
			But this gradient is very poorly conditioned as very small changed in \(\vec{a}_1\) have large effects in \(\vec{s}_t\) if \(t\) is large!
		% end
	% end

	\section{Optimism, Pessimism and Uncertainty in Reinforcement Learning}
		\label{sec:mbrlBayesian}

		In reinforcement learning, \emph{optimism} is similar to \emph{greediness}, describing for aggressive the algorithm pursues the optimal solution compared to exploring the environment more. For model-based RL, too much optimism leads to exploitation of modeling errors that benefit control, for example energy generation if a friction parameter is negative and constraint violation (e.g. solving a maze by walking through walls). Such exploitation may have catastrophic consequences, e.g. robots that accelerate into walls. This issue of algorithms being too optimistic has to be resolved for practical methods. Three main approaches are currently used to tackle this:
		\begin{itemize}
			\item Inductive biases to impose additional structure, e.g. energy conservation.
			\item Regularization to dissuade the optimizer from exploiting modeling errors.
			\item Replanning over short horizons to constantly adapt to the state at additional computational costs.
		\end{itemize}
		Another method for dealing with too optimistic models is to "automatically" regularize the policy by incorporating uncertainty into the model. There are two types of uncertainty:
		\begin{description}[leftmargin=3cm]
			\item[Aleatoric] Inherent statistical uncertainty, e.g. additive noise like in the LGDS.
			\item[Epistemic] Model uncertainty of a Bayesian model arising from a lack of data to estimate the parameters, e.g. when putting a distribution on the state dynamics matrix of a LGDS.
		\end{description}

		\subsection{Optimism Under Uncertainty}
			Adding uncertainty to the model has the effect of softening the reward for uncertain states. For example in a Gaussian setting with the reward \( r(x) = -x^2 \) where \( x \sim \mathcal{N}(\mu, \sigma^2) \), the expected reward include the variance: \( \E\big[r(x)\big] = -(\mu^2 +\sigma^2) \). Therefore the higher the (epistemic) uncertainty, the lower the reward. This is the idea behind Bayesian dynamics models: the epistemic uncertainty should anticipate prediction errors. For Bayesian linear regression models, including Gaussian processes, the decomposition of the uncertainty into aleatoric and epistemic uncertainty is obvious. Take for example Gaussian Bayesian linear regression with the features \( \vec{\phi}(\vec{x}) \):
			\begin{align*}
				y(\vec{x})     & = \vec{\phi}^T(\vec{x}) \vec{w} + \vec{\eta},\quad
				\vec{w} \sim \mathcal{N}(\vec{\mu}_w, \mat{\Sigma}_w),\quad
				\vec{\eta} \sim \mathcal{N}(\vec{0}, \sigma_\eta^2 \mat{I})                                                                                                           \\
				\mu_y(\vec{x}) & = \vec{\phi}^T(\vec{x}) \vec{\mu}_w                                                                                                                  \\
				\sigma_y^2     & = \underbrace{\sigma_\eta^2}_\text{aleatoric} + \underbrace{\sigma_\eta^2 \vec{\phi}^T(\vec{x}) \mat{\Sigma}_w \vec{\phi}(\vec{x})}_\text{epistemic}
			\end{align*}
			Some other approaches are:
			\begin{itemize}
				\item \emph{Bayesian Neural Networks:} Require approximate inference which limits theirs performance. They are an active area of research!
				\item \emph{Neural Linear Models:} Bayesian linear regression with neural features. These are trained to maximize the marginal likelihood, so they overfit the data and uncertainty quantification is poor outside of data.
				\item \emph{Variational Inference:} The predictive distribution is a sample estimate generated by sampling the network weights from a variational weight distribution. Mean field variational inference (MFVI) methods scale better than Markov chain Monte-Carlo (MCMC) methods, but the inference is less accurate.
				\item \emph{Ensembles:} Several neural networks, each trained with different mini-batches. The predictive distribution is computed over all the models. While these are popular, how "Bayesian" this method really is is questionable.
			\end{itemize}
		% end
	% end

	\section{Replanning}
		\label{sec:mbrlReplanning}

		For replanning, the optimization problem for a single action \( \vec{a}_t \) takes the next \(T\) time steps into account\footnote{The notation of \(\arg\max\) is abused a bit here as only the first action is used as the result of the optimization problem. But the other steps can be used as a "warm start" for the upcoming optimization problems.}:
		\begin{equation*}
			\begin{aligned}
				\vec{a}_t^\ast = \arg\max_{\vec{a}_{t:t + T}} \, & \sum_{\tau = t}^{t + T} r(\vec{s}_\tau, \vec{a}_\tau) \\
				\mathrm{s.t.} \quad                              &
				\begin{aligned}
					\vec{s}_{t + 1} & = \vec{f}(\vec{s}_t, \vec{a}_t)
				\end{aligned}
			\end{aligned}
		\end{equation*}
		This is known as \emph{Model-Predictive Control} (MPC) or \emph{Receding Horizon Control} (RHC).

		A huge benefit of replanning is that the controllers can react to disturbances and model errors. But doing the replanning is really costly and limits real-time execution. Also often a good terminal reward function is needed to prevent overly greedy planning.
	% end

	\section{Case Studies}
		\subsection{Probabilistic Learning for Control (PILCO)}
			The PILCO algorithm uses a Gaussian process for the dynamics model with decent sample efficiency and good uncertainty quantification. The policy is represented using e.g. an RBF network and a backpropagation-through-time algorithm is used for computing the gradients. The key idea is that the epistemic uncertainty of the dynamics model regularizes the expected reward. But as backpropagation-through-time is used, the algorithm is a little brittle for large planning horizons, so only small \(T\) may be used.
		% end

		\subsection{Guided Policy Search (GPS)} % 9.48, 9.49, 9.50, 9.51, 9.52
			\label{subsec:gps}

			\todo{Content}
		% end

		\subsection{Probabilistic Ensemble Trajectory Sampling (PETS)}
			PETS uses neural network ensembles with CEM MPC where instead of propagating the distributions with moment matching, particles and randomly selected models from the ensemble are propagated.
		% end
	% end

	\section{Wrap-Up}
		This chapter covered some topics of MBRL. The core of MBRL is to combine model learning with methods from optimal control in various ways. It is often useful if the RL task has some kind of structure that can be incorporated into the model specification, e.g. energy conservation. In practice, regularization is often needed to prevent the optimizer from exploiting beneficial modeling errors like energy generation. Popular methods include inductive biases in these models and uncertainty-based regularization using Bayesian models and replanning.


		For a comparison with the other discussed reinforcement learning methods (value function methods, see \autoref{c:valueFunctionMethods} and policy search, see \autoref{c:policySearch}), the following list gives an overview over the main benefits and drawbacks of \emph{model-based reinforcement learning}:
		\begin{description}
			\item[Model Complexity] Very High
				\begin{itemize}
					\item A forward model \( \vec{f} : \R^{n_s + n_a} \to \R^{n_s} \) has to be learned.
					\item Dynamic programming has to be applicable (e.g. LQR).
					\item Small errors in the model can have a big impact on the policy.
				\end{itemize}
			\item[Scalability] Poor (with some positive exceptions)
				\begin{itemize}
					\item Learning high-dimensional (or discontinuous) models is very hard.
				\end{itemize}
			\item[Data Efficiency] Excellent
				\begin{itemize}
					\item Every transition is used to learn the model.
					\item Model can be reused for different tasks.
				\end{itemize}
			\item[Other Limitations] Distance between two policies is hard to control; huge computation times.
		\end{description}
	% end
% end

\chapter{Value Function Methods}
	\label{c:valueFunctionMethods}

	This chapter covers \emph{value function methods}, a variant of model-free reinforcement learning. Often learning a good model is too hard, and the controller may only achieve good results by exploiting the model. Additionally, most optimal control methods are based on linearizations that only work moderately for nonlinear tasks. \emph{Model-free reinforcement learning} does not impose any assumptions on the structure on the model but rather learns the policy directly. In most model-free RL methods, this means learning the value function to solve the optimal control problem. The value function is learned from samples\footnote{The samples can (and should) be drawn from multiple episodes or sequences, hence the index is called \(i\) rather than \(t\). It does not necessarily refer to the time step!} \( \mathcal{D} = \big\{ \vec{s}_i, \vec{a}_i, r_i, \vec{s}_i' \}_{i = 1}^{N} \) with the state \(\vec{s}_i\), action \(\vec{a}_i\), immediate reward \(r_i\), and the next state \(\vec{s}_i'\).

	\section{Temporal Difference Tabular Learning}
		For discrete states and actions, the Q-function can simply be tabulated. This is called \emph{tabular learning}. Given a transition \( (\vec{s}_i, \vec{a}_i, r_i, \vec{s}_i') \), the idea of \emph{temporal difference learning} is to use the current approximation \( V(\vec{s}_i) \) and the one-step prediction of the current value,
		\begin{equation*}
			\hat{V}(\vec{s}_i) \coloneqq \E_{\vec{a} \sim \pi(\cdot \subgiven \vec{s}_i)} \Big[ r(\vec{s}_i, \vec{a}) + \E_{\vec{s}' \sim p(\cdot \subgiven \vec{s}_i, \vec{a})}\big[ V(\vec{s}') \big] \Big] \approx r_i + \gamma V(\vec{s}_i'),
		\end{equation*}
		to update the current value function. This is done with the temporal difference (TD) error
		\begin{equation}
			\delta_i^V \coloneqq \hat{V}(\vec{s}_i) - V(\vec{s}_i) \doteq r_i + \gamma V(\vec{s}_i') - V(\vec{s}_i)  \label{eq:tdVError}
		\end{equation}
		by updating the value function with a "step size" \( \alpha \in [0, 1] \):
		\begin{equation}
			V^\mathrm{new}(\vec{s}_i) = V(\vec{s}_i) + \alpha \delta_i^V \doteq (1 - \alpha) V(\vec{s}_i) + \alpha \hat{V}(\vec{s}_i)  \label{eq:tdVFunction}
		\end{equation}
		The value \(\alpha\) describes the amount of trust in the prediction. For \(\alpha = 1\), the value function directly jumps to the prediction, and for \(\alpha = 0\) the value function always stays the same. This part of the algorithm can be seen as the \emph{policy evaluation} with temporal differences. To form a complete RL algorithm, also policy improvement is needed. As the algorithm has to also reach areas unseen before, exploration is needed in the policy. Two basic approaches for this problem are \(\epsilon\)-greedy policies
		\begin{equation*}
			\pi(\vec{a} \given \vec{s}) =
			\begin{cases}
				\arg\max_{\vec{a}'} \, Q^\pi(\vec{s}, \vec{a}') & \text{with probability } 1 - \epsilon \\
				\text{any action } \vec{a}                      & \text{with probability } \epsilon
			\end{cases}
		\end{equation*}
		and soft-max policies, enabling also learning the exploration (with a parameter \(\beta\)):
		\begin{equation*}
			\pi(\vec{a} \given \vec{s}) = \frac{\exp{ \beta Q(\vec{s}, \vec{a}) }}{\sum_{\vec{a}'} \exp{ \beta Q(\vec{s}, \vec{a}') }}
		\end{equation*}
		Both of these policies need a Q-function which can also be learned in a TD-fashion, similar to \eqref{eq:tdVFunction}. Let \( (\vec{s}_i, \vec{a}_i, r_i, \vec{s}_i') \) again be a transition of the system. Then the TD-error for the Q-function is defined as
		\begin{equation}
			\delta_i^Q \coloneqq \hat{Q}(\vec{s}_i, \vec{a}_?) - Q(\vec{s}_i) \doteq r_i + \gamma Q(\vec{s}_i', \vec{a}_?) - Q(\vec{s}_i)  \label{eq:tdQError}
		\end{equation}
		with the one-step prediction \( \hat{Q}(\vec{s}_i) \coloneqq r_i + \gamma Q(\vec{s}_i', \vec{a}_?) \), analogous to \eqref{eq:tdVError}. The update of the Q-function is then
		\begin{equation*}
			Q^\mathrm{new}(\vec{s}_i, \vec{a}_i) = Q(\vec{s}_i, \vec{a}_i) + \alpha \delta_i^Q \doteq (1 - \alpha) Q(\vec{s}_i, \vec{a}_i) + \alpha \hat{Q}(\vec{s}_i, \vec{a}_?).
		\end{equation*}
		Other than for updating the V-function, now an action \( \vec{a}_? \) has to be selected to compute the TD-error \eqref{eq:tdQError}. There are two common methods for estimating \(\vec{a}_?\), both forming separate algorithms:
		\begin{description}[leftmargin=3cm]
			\item[Q-Learning] \( \vec{a}_? = \arg\max_{\vec{a}} \, Q(\vec{s}_i', \vec{a}) \)
				\begin{itemize}
					\item Estimates the Q-function of the optimal policy.
					\item Generates off-policy samples, i.e. \( \vec{a}_? \neq \vec{a}_i' \).
					\item Equivalent to learning the value function.
					\item See \autoref{alg:tabularQLearning} for an algorithmic description.
				\end{itemize}
			\item[SARSA]      \( \vec{a}_? = \vec{a}_i' \), where \( \vec{a}_i' \sim \pi(\cdot \given \vec{s}_i') \), so quintuples \( (\vec{s}_i, \vec{a}_i, r_i, \vec{s}_i', \vec{a}_i') \) are used for learning
				\begin{itemize}
					\item Estimates the Q-function of the exploration policy.
					\item Generates on-policy samples.
					\item \emph{SARSA} stands for \emph{state-action-reward-state-action}.
				\end{itemize}
		\end{description}
		Note that, as the policy depends on the Q-function, the policy is non-stationary.

		\begin{algorithm}  \DontPrintSemicolon
			\( Q^\ast(\vec{s}, \vec{a}) \gets 0 \) for all \( \vec{s} \in \mathcal{S} \) and \( \vec{a} \in \mathcal{A} \) \;
			\( i \gets 0 \) \;
			\Repeat{convergence of \( Q^\ast \)}{
				\( i \gets i + 1 \) \;
				\If{episode finished}{
					Start new episode. \;
				}
				Observe a transition \( (\vec{s}_i, \vec{a}_i, r_i, \vec{s}_i') \). \;
				\eqmakebox[algTabQLearn][r]{\( \vec{a}_? \gets \)} \eqmakebox[algTabQLearnR][l]{\( \arg\max_{\vec{a}} \, Q^\ast(\vec{s}_i', \vec{a}) \)}
				\quad\tcp{Compute the exploration action.}
				\eqmakebox[algTabQLearn][r]{\( \hat{Q}^\ast(\vec{s}_i, \vec{a}_i) \gets \)} \eqmakebox[algTabQLearnR][l]{\( r_i + \gamma Q^\ast(\vec{s}_i', \vec{a}_?) \)}
				\quad\tcp{Compute the one-step prediction.}
				\eqmakebox[algTabQLearn][r]{\( \delta_i \gets \)} \eqmakebox[algTabQLearnR][l]{\( \hat{Q}^\ast(\vec{s}_i, \vec{a}_i) - Q^\ast(\vec{s}_i, \vec{a}_i) \)}
				\quad\tcp{Compute the TD-error.}
				\eqmakebox[algTabQLearn][r]{\( Q^\ast(\vec{s}_i, \vec{a}_i) \gets \)} \eqmakebox[algTabQLearnR][l]{\( Q^\ast(\vec{s}_i, \vec{a}_i) + \alpha \delta_i \)}
				\quad\tcp{Update the Q-Function.}
			}

			\caption{Tabular Q-Learning for Discrete States}
			\label{alg:tabularQLearning}
		\end{algorithm}
	% end

	\section{Approximate Temporal Difference Learning}
		If the states are continuous, it is no longer possible to tabulate them\footnote{Of course discretizing is an option, but the state space becomes really big really quick.}. To keep it simple, use a linear function approximator for the value function:
		\begin{equation*}
			V(\vec{s}) \approx V_{\vec{w}}(\vec{s}) = \vec{\phi}^T(\vec{s}) \vec{w}
		\end{equation*}
		The optimal weights \(\vec{w}\) can again be found using temporal difference learning! With \emph{bootstrapping}, that is using the old approximation values to measure the new approximation error as for tabular TD learning, the goal is to minimize the mean-squared error
		\begin{equation*}
			\MSE(\vec{w}) \approx \MSE_\mathrm{BS}(\vec{w}) = \sum_{i = 1}^{N} \big( \hat{V}^\pi(\vec{s}_i) - V_{\vec{w}}(\vec{s}_i) \big)^2  \label{eq:approxTdMse}
		\end{equation*}
		for samples \( \big\{ (\vec{s}_i, \vec{a}_i, r_i, \vec{s}_i') \big\}_{i = 1}^{N} \) of \(\vec{w}\) with
		\begin{equation*}
			\hat{V}^\pi(\vec{s}_i) = \E_{\vec{a} \sim \pi(\cdot \subgiven \vec{s}_i)} \Big[ r(\vec{s}_i, \vec{a}) + \E_{\vec{s}' \sim p(\cdot \subgiven \vec{s}_i, \vec{a})}\big[ V_{\vec{w}_\mathrm{old}}(\vec{s}') \big] \Big] \approx r_i + \gamma V_{\vec{w}_\mathrm{old}}(\vec{s}_i').
		\end{equation*}
		This objective can be optimized with (stochastic) gradient descent (see \autoref{subsubsec:sgd}) by approximating the bootstrapped objective \eqref{eq:approxTdMse} with a single sample \( (\vec{s}_i, \vec{a}_i, r_i, \vec{s}_i') \):
		\begin{equation}
			\MSE_\mathrm{BS}(\vec{w})
			\approx \big( \hat{V}^\pi(\vec{s}_i) - V_{\vec{w}}(\vec{s}_i) \big)^2
			\approx \big( r_i + \gamma V_{\vec{w}_\mathrm{old}}(\vec{s}_i') - V_{\vec{w}}(\vec{s}_i) \big)^2  \label{eq:approxValueError}
		\end{equation}
		The gradient descent step is then given as
		\begin{align*}
			\vec{w}_{i + 1}
			 & = \vec{w}_i + \tilde{\alpha}_i \pdv{\MSE_\mathrm{BS}}{\vec{w}} \bigg\vert_{\vec{w} \,=\, \vec{w}_i,\, \vec{w}_\mathrm{old} \,=\, \vec{w}_i}        \\
			 & = \vec{w}_i + \alpha_i \underbrace{\big( r_i + \gamma V_{\vec{w}_i}(\vec{s}_i') - V_{\vec{w}_i}(\vec{s}_i) \big)}_{\delta_i} \vec{\phi}(\vec{s}_i) \\
			 & = \vec{w}_i + \alpha_i \delta_i \vec{\phi}(\vec{s}_i)
		\end{align*}
		for a linear approximation \( V_{\vec{w}}(\vec{s}) = \vec{\phi}^T(\vec{s}) \vec{w} \) and the derivative
		\begin{equation}
			\pdv{\MSE_\mathrm{BS}}{\vec{w}}
			= 2 \big( r_i + \gamma V_{\vec{w}_\mathrm{old}}(\vec{s}_i') - V_{\vec{w}}(\vec{s}_i) \big) \pdv{V_{\vec{w}}(\vec{s}_i)}{\vec{w}}
			= 2 \big( r_i + \gamma V_{\vec{w}_\mathrm{old}}(\vec{s}_i') - V_{\vec{w}}(\vec{s}_i) \big) \vec{\phi}(\vec{s}_i)  \label{eq:approxValueErrorPrime}
		\end{equation}
		of the function approximator\footnote{The multiplication with two is absorbed into \( \alpha_i = 2 \tilde{\alpha}_i \).}. Of course this can be readily extended for deep function approximators like neural networks by replacing the derivative \( \pdv{V_{\vec{w}}(\vec{s}_i)}{\vec{w}} \) in \eqref{eq:approxValueErrorPrime} with e.g. an autograd engine or backpropagation. As for tabular TD methods, similar update rules can be achieved for the Q-function, enabling approximate Q-learning and SARSA with \( Q_{\vec{w}}(\vec{s}, \vec{a}) = \vec{\phi}^T(\vec{s}, \vec{a}) \vec{w} \):
		\begin{equation*}
			\vec{w}_{i + 1} = \vec{w}_i + \alpha \big( r_i + \gamma Q_{\vec{w}_i}(\vec{s}_i', \vec{a}_?) - Q_{\vec{w}_i}(\vec{s}_i, \vec{a}_i) \big) \vec{\phi}(\vec{s}_i, \vec{a}_i)
		\end{equation*}
		Note that, even though an index \(t\) is used here, it is possible (and often needed) to train over multiple episodes by resetting the environment once an episode is done. This process of resetting the environment, observing samples, and training the V-/Q-function is repeated until convergence. A procedural description of approximate Q-learning is shown in \autoref{alg:approxQLearning}

		\begin{algorithm}  \DontPrintSemicolon
			\( \vec{w}_0 \gets \vec{0} \) \;
			\( t \gets 0 \) \;
			\Repeat{convergence of \( \vec{w}_i \)}{
			\( t \gets t + 1 \) \;
			\If{episode finished}{
				Start new episode. \;
			}
			Observe a transition \( (\vec{s}_i, \vec{a}_i, r_i, \vec{s}_i') \). \;
			\eqmakebox[algApproxQLearn][r]{\( \vec{a}_? \gets \)} \eqmakebox[algApproxQLearnR][l]{\( \arg\max_{\vec{a}} \, Q_{\vec{w}}(\vec{s}_i', \vec{a}) \)}
			\quad\tcp{Compute the exploration action.}
			\eqmakebox[algApproxQLearn][r]{\( \hat{Q}_{\vec{w}}(\vec{s}_i, \vec{a}_i) \gets \)} \eqmakebox[algApproxQLearnR][l]{\( r_i + \gamma Q_{\vec{w}}(\vec{s}_i', \vec{a}_?) \)}
			\quad\tcp{Compute the one-step prediction.}
			\eqmakebox[algApproxQLearn][r]{\( \delta_i \gets \)} \eqmakebox[algApproxQLearnR][l]{\( \hat{Q}_{\vec{w}}(\vec{s}_i, \vec{a}_i) - Q_{\vec{w}}(\vec{s}_i, \vec{a}_i) \)}
			\quad\tcp{Compute the TD-error.}
			\eqmakebox[algApproxQLearn][r]{\( \vec{w}_{i + 1} \gets \)} \eqmakebox[algApproxQLearnR][l]{\( \vec{w}_i + \alpha_i \delta_i \vec{\phi}(\vec{s}_i, \vec{a}_i) \)}
			\quad\tcp{Update the Q-Function.}
			}

			\caption{Approximate Q-Learning for Continuous States}
			\label{alg:approxQLearning}
		\end{algorithm}

		\paragraph{Remarks on Approximate TD Learning}
			\begin{itemize}
				\item When using a tabular encoding for \( \vec{\phi}(\vec{s}) \), the function approximation is equivalent with discretizing the state-space and using tabular TD learning methods.
				\item Approximate TD learning is not proper stochastic gradient descent!
					\begin{itemize}
						\item Firstly, the target values \( \hat{V}^\pi(\vec{s}) \) change after a parameter update due to bootstrapping. This actually introduces a bias and changes the optimization objective such that not the mean-squared error is optimized anymore.
						\item Secondly, the samples are not statistically independent if draw from a sequence of states and actions. Instead, a \emph{replay memory} should be built that holds samples from multiple trajectories and in each iterative step, one sample should be drawn from the memory.
					\end{itemize}
				\item It is possible to draw multiple samples at once and to a combined GD (mini-batch) step to improve convergence.
				\item Due to e.g. off-policy updates, convergence is not guaranteed every time. For linear approximations, however, certain convergence properties hold given that the convergence properties of SGD (see \autoref{subsubsec:sgd}) are fulfilled.
				\item Temporal difference learning is fast in computation time (i.e. fast convergence), but it is not sample-efficient as every sample is only used once!
			\end{itemize}
		% end
	% end

	\section{Batch Reinforcement Learning Methods}
		One major problem of online temporal difference methods is that they are typically highly data-inefficient as every sample is only used once. This property can be improved by using a batch of data to increase the data-efficiency. This section will look into two exemplary algorithms,
		\begin{itemize}
			\item Least-Squares Temporal Differences (LSTD), and
			\item Fitted Q-Iteration,
		\end{itemize}
		that make use of this technique. But compared to TD methods, batch methods are usually computationally much more expensive, so there is a tradeoff here.

		\subsection{Least-Squares Temporal Differences (LSTD)}
			For \emph{least-squares temporal differences}, the bootstrapped mean-squared error \eqref{eq:approxTdMse} is solved analytically for \(N\) samples. Let
			\begin{align*}
				\vec{r} \coloneqq
				\begin{bmatrix}
					r_1    \\
					r_2    \\
					\vdots \\
					r_N
				\end{bmatrix}
				 &  &
				\vec{\Phi}^T \coloneqq
				\begin{bmatrix}
					\vec{\phi}^T(\vec{s}_1) \\
					\vec{\phi}^T(\vec{s}_2) \\
					\vdots                  \\
					\vec{\phi}^T(\vec{s}_N)
				\end{bmatrix}
				 &  &
				\vec{\Phi}_+^T \coloneqq
				\begin{bmatrix}
					\vec{\phi}^T(\vec{s}_1') \\
					\vec{\phi}^T(\vec{s}_2') \\
					\vdots                   \\
					\vec{\phi}^T(\vec{s}_N')
				\end{bmatrix}
			\end{align*}
			be the rewards, steps and next steps, respectively. Then the bootstrapped objective \eqref{eq:approxTdMse} can be rewritten as
			\begin{align*}
				\MSE_\mathrm{BS}(\vec{w})
				 & \approx \sum_{i = 1}^{N} \big( r_i + \gamma V_{\vec{w}_\mathrm{old}}(\vec{s}_i') - V_{\vec{w}}(\vec{s}_i) \big)^2                                                                    \\
				 & = \sum_{i = 1}^{N} \big( r_i + \gamma \vec{\phi}^T(\vec{s}_i') \vec{w}_\mathrm{old} - \vec{\phi}^T(\vec{s}_i) \vec{w} \big)^2                                                        \\
				 & = \big( \vec{r} + \gamma \mat{\Phi}_+^T \vec{w}_\mathrm{old} - \mat{\Phi}^T \vec{w} \big)^T \big( \vec{r} + \gamma \mat{\Phi}_+^T \vec{w}_\mathrm{old} - \mat{\Phi}^T \vec{w} \big).
			\end{align*}
			By taking the derivative of the mean-squared error and setting it to zero\footnote{\( -\frac{1}{2} \pdv{\vec{w}} \big\lVert \vec{r} + \gamma \mat{\Phi}_+^T \vec{w}_\mathrm{old} - \mat{\Phi}_+^T \vec{w} \big\rVert_2^2 = \mat{\Phi} \big( \vec{r} + \gamma \mat{\Phi}_+^T \vec{w}_\mathrm{old} - \mat{\Phi}^T \vec{w} \big) = \mat{\Phi} \big( \vec{r} + \gamma \mat{\Phi}_+^T \vec{w}_\mathrm{old} \big) - \mat{\Phi} \mat{\Phi}^T \vec{w} \overset{!}{=} \vec{0} \)}, the least-squares solution is
			\begin{equation}
				\vec{w} = \big( \mat{\Phi} \mat{\Phi}^T \big)^{-1} \mat{\Phi} \big( \vec{r} + \gamma \mat{\Phi}_+^T \vec{w}_\mathrm{old} \big).  \label{eq:lstdLeastSquaresSol}
			\end{equation}
			However, this equation still contains \( \vec{w}_\mathrm{old} \). For convergence, \( \vec{w} = \vec{w}_\mathrm{old} \) should hold. Plugging this into the least-squares solution \eqref{eq:lstdLeastSquaresSol} yields the solution to the complete least-squares temporal differences algorithm:
			\begin{align}
				                                                                                                                                                        &                                                                                                                                        &
				\vec{w}                                                                                                                                                 & = \big( \mat{\Phi} \mat{\Phi}^T \big)^{-1} \mat{\Phi} \big( \vec{r} + \gamma \mat{\Phi}_+^T \vec{w}_\mathrm{old} \big)                 & \nonumber               \\
				\overset{\mathclap{\raisebox{1pt}{\scriptsize \( \vec{w}_\mathrm{old} \,\overset{!}{=}\, \vec{w} \)}}}{\quad\implies}                                   &                                                                                                                                        &
				\vec{w}                                                                                                                                                 & = \big( \mat{\Phi} \mat{\Phi}^T \big)^{-1} \mat{\Phi} \big( \vec{r} + \gamma \mat{\Phi}_+^T \vec{w} \big)                              & \nonumber               \\
				\iff                                                                                                                                                    &                                                                                                                                        &
				\vec{w}                                                                                                                                                 & = \big( \mat{\Phi} \mat{\Phi}^T \big)^{-1} \mat{\Phi} \vec{r} + \gamma \big( \mat{\Phi} \mat{\Phi}^T \big)^{-1} \mat{\Phi}_+^T \vec{w} & \nonumber               \\
				\iff                                                                                                                                                    &                                                                                                                                        &
				\Big( \mat{I} - \gamma \big( \mat{\Phi} \mat{\Phi}^T \big)^{-1} \mat{\Phi}_+^T \Big) \vec{w}                                                            & = \big( \mat{\Phi} \mat{\Phi}^T \big)^{-1} \mat{\Phi} \vec{r}                                                                          & \nonumber               \\
				\iff                                                                                                                                                    &                                                                                                                                        &
				\Big(\! \big( \mat{\Phi} \mat{\Phi}^T \big)^{-1} \mat{\Phi} \mat{\Phi}^T - \big( \mat{\Phi} \mat{\Phi}^T \big)^{-1} \gamma \mat{\Phi}_+^T \Big) \vec{w} & = \big( \mat{\Phi} \mat{\Phi}^T \big)^{-1} \mat{\Phi} \vec{r}                                                                          & \nonumber               \\
				\iff                                                                                                                                                    &                                                                                                                                        &
				\big( \mat{\Phi} \mat{\Phi}^T \big)^{-1} \mat{\Phi} \big( \mat{\Phi}^T - \gamma \mat{\Phi}_+^T \big) \vec{w}                                            & = \big( \mat{\Phi} \mat{\Phi}^T \big)^{-1} \mat{\Phi} \vec{r}                                                                          & \nonumber               \\
				\iff                                                                                                                                                    &                                                                                                                                        &
				\mat{\Phi} \big( \mat{\Phi}^T - \gamma \mat{\Phi}_+^T \big) \vec{w}                                                                                     & = \mat{\Phi} \vec{r}                                                                                                                   & \nonumber               \\
				\iff                                                                                                                                                    &                                                                                                                                        &
				\vec{w}                                                                                                                                                 & = \Big(\! \mat{\Phi} \big( \mat{\Phi}^T - \gamma \mat{\Phi}_+^T \big) \!\Big)^{-1} \mat{\Phi} \vec{r}                                  & \label{eq:lstdSolution}
			\end{align}
			Equation \eqref{eq:lstdSolution} is the solution of LSTD. By analyzing the convergence properties of TD-learning it can be seen that this is the same solution as the fixed point of TD-learning. Hence, with enough samples, it is possible to calculate the V-function approximation weights in one shot without iterations. As for TD learning, it is also possible to adapt this for learning the Q-function by altering the matrices \( \mat{\Phi} \) and \( \mat{\Phi}_+ \) accordingly. The algorithm is then called least-squares Q-learning (LSQ) and is used for least-squares policy iteration (LSPI).

			However, as opposed to TD learning, LSTD is computationally much more expensive!
		% end

		\subsection{Fitted Q-Iteration}
			When using batch RL it is also easier to use nonlinear function approximators as many of them only work in a batch setup, e.g. regression trees. Also using just a single sample could lead to catastrophic forgetting, e.g. for neural networks that do not "remember" the states seen previously. \emph{Fitted Q-iteration} is a method that uses some nonlinear regression model \( \mathrm{Regress}(\mat{X}, \vec{y}) \) with input data \(\mat{X}\) and target values \(\vec{y}\) for approximate value iteration. A procedural representation is shown in \autoref{alg:fittedQIteration}. Invoking the regression model performs the required expectation for the Q-function implicitly. But is it still hard to solve the maximization operator on \autoref{algline:fittedQIterationMaxOpt} for continuous actions. See for example \href{https://proceedings.neurips.cc/paper/2008/hash/f79921bbae40a577928b76d2fc3edc2a-Abstract.html}{Neumann and Peters: "Fitted Q-Iteration by Advantage Weighted Regression", NIPS (2008)}.

			\begin{algorithm}  \DontPrintSemicolon
				\KwIn{Dataset \( \mathcal{D} = \big\{ (\vec{s}_i, \vec{a}_i, r_i, \vec{s}_i') \big\}_{i = 1}^{N} \)}
				\eqmakebox[fittedQIterationI][r]{\( \mat{X} \gets \)} \(
				\begin{bmatrix}
					\vec{s}_1 & \vec{s}_2 & \cdots & \vec{s}_N \\
					\vec{a}_1 & \vec{a}_2 & \cdots & \vec{a}_N
				\end{bmatrix}^T
				\) \;
				\eqmakebox[fittedQIterationI][r]{\( Q(\vec{s}, \vec{a}) \gets \)} \( 0 \) for all \( \vec{s} \in \mathcal{S} \) and \( \vec{a} \in \mathcal{A} \) \;
				\Repeat{convergence of \(Q\)}{
					\eqmakebox[fittedQIteration][r]{\( q_i \gets \)} \eqmakebox[fittedQIterationR][l]{\( r_i + \gamma \max_{\vec{a}'} \, Q(\vec{s}_i', \vec{a}') \)}  \label{algline:fittedQIterationMaxOpt}
					\quad\tcp{Generate target values for each \( i = \dotsrange{1}{N} \).}
					\eqmakebox[fittedQIteration][r]{\( Q(\vec{s}, \vec{a}) \gets \)} \eqmakebox[fittedQIterationR][l]{\( \mathrm{Regress}(\mat{X}, \vec{q}) \)}
					\quad\tcp{Learn new Q-Function.}
				}

				\caption{Fitted Q-Iteration with Regression Model \texorpdfstring{\( \mathrm{Regress}(\mat{X}, \vec{y}) \)}{Regress(X, y)}}
				\label{alg:fittedQIteration}
			\end{algorithm}
		% end
	% end

	\section{Wrap-Up}
		This chapter covered value function methods with have been the driving reinforcement learning approach in the 90s. They can be applied to lots of problems, e.g. chess at professional level, robot soccer, etc. However, they are not always the method of choice as the state-action space has to be filled up with a sufficient amount of samples (the amount of data needed grows exponentially with the dimension) and errors in the value function approximation can cause drastic failures that can be very hard to control.

		For a comparison with the other discussed reinforcement learning methods (model-based, see \autoref{c:modelBasedRL} and policy search, see \autoref{c:policySearch}), the following list gives an overview over the main benefits and drawbacks of \emph{value function methods}:
		\begin{description}
			\item[Model Complexity] OK
				\begin{itemize}
					\item Only the Q-function \( Q : \R^{\lvert \mathcal{S} \rvert + \lvert \mathcal{A} \rvert} \to \R \) is learned.
					\item But small function approximation errors can have a big effect on the policy.
				\end{itemize}
			\item[Scalability] Poor (with some positive exceptions)
				\begin{itemize}
					\item Approximating the Q-function in high dimensions is difficult.
					\item Policy is hard to obtain in high-dimensional action spaces due to the maximization.
				\end{itemize}
			\item[Data Efficiency] OK (online TD learning) to good (batch methods)
				\begin{itemize}
					\item Online: every transition is only used once.
					\item Batch: every transition is reused.
				\end{itemize}
			\item[Other Limitations] Policy update is unbounded, might lead to oscillations.
		\end{description}
	% end
% end

\chapter{Policy Search Methods}
	\label{c:policySearch}

	This chapter covers \emph{policy search methods} which do not use value functions or similar as a intermediate step for finding an optimal policy but rather search the policy directly. This is motivated by the problems that arise with value function methods as they do not handle continuous actions well (due to the required maximization of the Q-function) and the amount of samples needed grows exponentially with the dimensionality of the state-space. Also, an error in the value function propagates and arbitrarily distorts the policy update. Many of these problems can be fixed by using parametric policies and policy search, i.e. finding the policy directly instead of using a value function. The policies are improved upon demonstrations and it is even possible to use task-appropriate policies or to train on the real system.

	\section{Categorization of Policy Search}
		In value-based methods, the action to execute is often chosen using an \(\epsilon\)-greedy or soft-max approach, both requiring to either maximize w.r.t. or sum over the actions, both of which is difficult for continuous state spaces. Policy search methods instead use parameterized policies for action selection, for example Gaussian policies
		\begin{equation*}
			\pi(\vec{a} \given \vec{s}; \vec{\theta}) = \mathcal{N}(\vec{a} \given \vec{f}_{\vec{w}}(\vec{s}),\, \sigma^2 \mat{I}),\quad \vec{\theta} = \{ \vec{w}, \sigma^2 \}
		\end{equation*}
		with a learnable mean function \( \vec{f}_{\vec{w}}(\vec{s}) \). The problem of policy search is to find parameters \( \vec{\theta} \) that constitute a good policy. Model-free approaches can be broken down into three main steps:
		\begin{enumerate}
			\item \eqmakebox[modelFreePolicySearch][l]{Exploration:} Sample trajectories \( \vec{\tau}^{[i]} \) following the current policy \( \pi_k \).
			\item \eqmakebox[modelFreePolicySearch][l]{Policy Evaluation:} Assess the quality of the trajectories, either \emph{step-based} or \emph{episode-based}.
			\item \eqmakebox[modelFreePolicySearch][l]{Policy Improvement:} Compute the next policy \( \pi_{k + 1} \) from the trajectories and evaluations.
			\item If not converged, go to 1.
		\end{enumerate}
		There are two types of policy evaluation: \emph{step-based} and \emph{episode-based}, which will be explained further in the following sections.

		\subsection{Episode-Based Policy Search}
			\label{subsec:episodeBasedPolicySearch}

			In \emph{episode-based} policy evaluation, the quality of parameters \( \vec{\theta}^{[i]} \) is directly assessed using the (undiscounted) total reward
			\begin{equation*}
				R^{[i]} = \sum_{t = 1}^{T} r_t^{[i]}.
			\end{equation*}
			The data for the policy update is
			\begin{equation*}
				\mathcal{D} = \Big\{ \vec{\theta}^{[i]}, R^{[i]} \Big\}_{i \,=\, \subdotsrange{1}{N}}.
			\end{equation*}
			But this causes a high variance in \( R^{[i]} \) as it is the sum of \(T\) random variables, but works well for a moderate amount of parameters. Other properties of the episode-based strategy are:
			\begin{itemize}
				\item Explores in the parameter space (the data to learn from contains the parameters).
				\item Allows more sophisticated exploration strategies.
				\item Often efficient for a small amount of parameters.
				\item Generalization and multi-task learning, e.g. for policies like DMPs.
				\item Structure-less black-box optimization of the policy.
			\end{itemize}

			For episodic policy search, an upper-level policy \( \pi(\vec{\theta}; \vec{\omega}) \) over the parameters of the low-level control policy \( \pi(\vec{a} \given \vec{s}; \vec{\theta}) \) is learned. This can, for example, be a Gaussian \( \pi(\vec{\theta}; \vec{\omega}) = \mathcal{N}(\vec{\theta} \given \vec{\mu}, \mat{\Sigma}) \) with the parameters \( \vec{\omega} = \{ \vec{\mu}, \mat{\Sigma} \} \). To reduce the variance in the returns, the upper-level policy is often chosen to be deterministic. Then a policy \( \pi(\vec{\theta}; \vec{\omega}) \) is searched that maximizes the expected reward
			\begin{equation}
				J(\vec{\omega})
				= \E_{\pi(\vec{\theta}; \vec{\omega})}\big[ R_{\vec{\theta}} \big]
				= \int\! \pi(\vec{\theta}; \vec{\omega}) R_{\vec{\theta}} \dd{\vec{\theta}}  \label{eq:upperLevelReward}
			\end{equation}
			with the expected long-term reward
			\begin{equation*}
				R_{\vec{\theta}} = \E_{p, \pi}\Bigg[ \sum_{t = 1}^{T} r_t \Bigggiven \vec{\theta} \Bigg].
			\end{equation*}
			The exploration is encoded in the upper-level policy \( \pi(\vec{\theta}; \vec{\omega}) \) from which parameters are sampled to generate the trajectories in the exploration step of policy search.
		% end

		\subsection{Step-Based Policy Search}
			In \emph{step-based} policy evaluation, the quality of single state-action pairs is assessed using the (undiscounted) reward to come
			\begin{equation}
				Q_t^{[i]} = \sum_{\tau = t}^{T} r_\tau^{[i]}.  \label{eq:policyGradientRewardToCome}
			\end{equation}
			The data for the policy update is
			\begin{equation*}
				\mathcal{D} = \Big\{ \vec{s}_t^{[i]}, \vec{a}_t^{[i]}, Q_t^{[i]} \Big\}_{t \,=\, \subdotsrange{1}{T}}^{i \,=\, \subdotsrange{1}{N}}.
			\end{equation*}
			This causes less variance in \( Q_t^{[i]} \) as it is the sum of only \( T - t + 1 \) random variables. Other properties of the step-based strategy are:
			\begin{itemize}
				\item Explores in the action space (the data to learn from contains the actions).
				\item Less variance in the quality assessment compared to episode-based evaluation.
				\item More data points to fit the policy (one for each time step and episode).
				\item Less likely to create unstable policies.
				\item Uses the structure of the RL problem.
				\item Decomposed into single time steps.
			\end{itemize}

			For step-based policy search, the lower-level policy \( \pi(\vec{a} \given \vec{s}; \vec{\theta}) \) is learned directly. This is done by maximizing the expected reward
			\begin{equation}
				J(\vec{\theta})
				= \E_{p(\vec{\tau}; \vec{\theta})}\big[ R_{\vec{\tau}} \big]
				= \int\! p(\vec{\tau}; \vec{\theta}) R_{\vec{\tau}} \dd{\vec{\tau}}  \label{eq:policyGradientStepBasedObjective}
			\end{equation}
			with the trajectory distribution
			\begin{equation}
				p(\vec{\tau}; \vec{\theta}) = p(\vec{s}_1) \prod_{t = 1}^{T - 1} p(\vec{s}_{t + 1} \given \vec{s}_t, \vec{a}_t) \pi(\vec{a}_t \given \vec{s}_t; \vec{\theta})  \label{eq:policyGradientTrajectoryDistribution}
			\end{equation}
			and the return for a single trajectory \( R_{\vec{\tau}} = \sum_{t = 1}^{T} r_t \). The exploration has to be encoded directly into the lower-level policy \( \pi(\vec{a} \given \vec{s}; \vec{\theta}) \) in contrast to episode-based policy search.
		% end

		\subsection{Exploration vs. Exploitation}
			The exploration vs. exploitation tradeoff describes the tradeoff between continuing to explore in the next iteration (exploration) and maximizing the reward on the existing samples (exploitation). The fundamental question for policy search is how to control this tradeoff. This requires a quantification of the difference of two policies which will be discussed in more detail in \autoref{subsec:policySearchMetrics}. Usually this leads to limiting the distance between two subsequent policies for the policy update as greedy updates
			\begin{equation*}
				\vec{\theta}^\mathrm{new} = \arg\max_{\tilde{\vec{\theta}}} \, \E_{p, \pi_{\tilde{\vec{\theta}}}}\big[ Q^\pi(\vec{s}, \vec{a}) \big]
			\end{equation*}
			will usually lead to an unstable learning process with large jumps.
		% end
	% end

	\section{Policy Gradient Methods}
		This section covers \emph{policy gradient methods}, a class of policy search methods that use the gradient of the expected long-term reward w.r.t. parameters for updating the policy. The first subsection focuses on plain gradient methods and the second covers more sophisticated methods using relative entropy and natural gradients.

		\subsection{Policy Gradients}
			The basic idea of policy gradients is to optimize the policy in a gradient descent-fashion using an estimate of the gradient \( \grad_{\!\vec{\theta}} J(\vec{\theta}) \) of the expected long-term reward. This splits policy gradient methods into two steps: computing the gradient and subsequently improving the policy.

			\subsubsection{Gradient Computation}
				Computing (and approximating) the gradient \( \grad_{\!\vec{\theta}} J(\vec{\theta}) \) is an important part of policy gradient methods. This section covers two methods of computing the gradient: finite differences and likelihood ratio gradients. Additionally, baselines are introduces that reduce the variance in the gradient to improve learning.

				\paragraph{Finite Differences}
					Finite difference gradient approximation is the most straightforward way of approximating the derivative using a first-order Taylor approximation around \(\vec{\theta}\):
					\begin{align*}
						                                                                              &                                                                                                                                                              &
						J(\vec{\theta} + \vec{\delta}_\theta)                                         & \approx J(\vec{\theta}) + \pdv{J(\vec{\theta})}{\vec{\theta}} \vec{\delta}_\theta                                                                            & \\
						\iff                                                                          &                                                                                                                                                              &
						\pdv{J(\vec{\theta})}{\vec{\theta}} \vec{\delta}_\theta                       & \approx J(\vec{\theta} + \vec{\delta}_\theta) - J(\vec{\theta})                                                                                              & \\
						\iff                                                                          &                                                                                                                                                              &
						\pdv{J(\vec{\theta})}{\vec{\theta}} \vec{\delta}_\theta \vec{\delta}_\theta^T & \approx \vec{\delta}_\theta^T \big( J(\vec{\theta} + \vec{\delta}_\theta) - J(\vec{\theta}) \big)                                                            & \\
						\iff                                                                          &                                                                                                                                                              &
						\pdv{J(\vec{\theta})}{\vec{\theta}}                                           & \approx \vec{\delta}_\theta^T \big( \vec{\delta}_\theta \vec{\delta}_\theta^T \big)^{-1} \big( J(\vec{\theta} + \vec{\delta}_\theta) - J(\vec{\theta}) \big) &
					\end{align*}
					This can be used to update a single parameter estimate, e.g. the mean. A large class of algorithms uses this technique: Kiefer-Wolfowitz Procedure, Robbins-Monroe, Simultaneous Perturbation Stochastic Approximation (SPSA), Random Search, \dots
				% end

				\paragraph{Episode-Based Likelihood Ratio Gradient}
					For an upper-level policy \( \pi(\vec{\theta}; \vec{\omega}) \) and the expected long-term reward \( R_{\vec{\theta}} \) for a single \(\vec{\theta}\), it is possible to approximate the gradient of the expected reward \eqref{eq:upperLevelReward} using the log-ratio trick. That is, for an arbitrary function \( f(\vec{x}) \), the following holds:
					\begin{equation*}
						\grad_{\!\vec{x}} \ln f(\vec{x}) = \frac{1}{f(\vec{x})} \grad_{\!\vec{x}} f(\vec{x}) \quad\implies\quad \grad_{\!\vec{x}} f(\vec{x}) = f(\vec{x}) \grad_{\!\vec{x}} \ln f(\vec{x})
					\end{equation*}
					BY applied this to the gradient of the expected return, the gradient turns into an expectation that can be approximated using Monte-Carlo integration
					\begin{align}
						\grad_{\!\vec{\omega}} J(\vec{\omega})
						 & = \grad_{\!\vec{\omega}} \!\int\! \pi(\vec{\theta}; \vec{\omega}) R_{\vec{\theta}} \dd{\vec{\theta}}
						= \int\! \grad_{\!\vec{\omega}} \pi(\vec{\theta}; \vec{\omega}) R_{\vec{\theta}} \dd{\vec{\theta}}  \nonumber                                                                                                                       \\
						 & = \int\! \pi(\vec{\theta}; \vec{\omega}) R_{\vec{\theta}} \grad_{\!\vec{\omega}} \ln \pi(\vec{\theta}; \vec{\omega}) \dd{\vec{\theta}}  \nonumber                                                                                \\
						 & \approx \frac{1}{N} \sum_{i = 1}^{N} R^{[i]} \, \grad_{\!\vec{\omega}} \ln \pi\big(\vec{\theta}^{[i]}; \vec{\omega}\big) \eqqcolon \grad_{\!\vec{\omega}}^\mathrm{MC} J(\vec{\omega})  \label{eq:episodicPolicyGradientGradient}
					\end{align}
					with \(N\) samples \( \vec{\theta}^{[i]} \sim \pi(\cdot; \vec{\omega}) \) and the respective rewards \( R^{[i]} \coloneqq R_{\vec{\theta}^{[i]}} \). This gradient is called the \emph{Parameter Exploring Policy Gradient} (PGPE). The log-ratio trick is needed and Monte-Carlo estimation can not directly be applied to the expectation as that would require differentiation of the sampling which is not possible.
				% end

				\paragraph{Step-Based Likelihood Ratio Policy Gradient}
					As mentioned in \autoref{subsec:episodeBasedPolicySearch}, the total undiscounted reward \( R_{\vec{\theta}} \) in episode-based evaluation exhibits a very high variance. This variance can be reduced by using step-based evaluation and the reward to come \( Q_t^{[i]} \) as defined in \eqref{eq:policyGradientRewardToCome} instead. This requires maximizing the expected reward \( J(\vec{\theta}) \) as defined in \eqref{eq:policyGradientStepBasedObjective} instead as step-based policy gradient methods to not work with upper- and lower-level policies but rather optimize the policy \( \pi(\vec{a} \given \vec{s}; \vec{\theta}) \) directly. By again using the log-ratio trick, the gradient \( \grad_{\!\vec{\theta}} J(\vec{\theta}) \) can be approximated as
					\begin{align}
						\grad_{\!\vec{\theta}} J(\vec{\theta})
						 & = \grad_{\!\vec{\theta}} \!\int\! p(\vec{\tau}; \vec{\theta}) R_{\vec{\tau}} \dd{\vec{\tau}}
						= \int\! \grad_{\!\vec{\theta}} p(\vec{\tau}; \vec{\theta}) R_{\vec{\tau}} \dd{\vec{\tau}}  \nonumber                                                                                                                           \\
						 & = \int\! p(\vec{\tau}; \vec{\theta}) R_{\vec{\tau}} \grad_{\!\vec{\theta}} \ln p(\vec{\tau}; \vec{\theta}) \dd{\vec{\tau}}  \nonumber                                                                                        \\
						 & \approx \frac{1}{N} \sum_{i = 1}^{N} R^{[i]} \grad_{\!\vec{\theta}} \ln p\big(\vec{\tau}^{[i]}; \vec{\theta}\big) \eqqcolon \grad_{\!\vec{\omega}}^\mathrm{MC} J(\vec{\omega})  \label{eq:policyGradientStepBasedMonteCarlo}
					\end{align}
					with the trajectory distribution \( p(\vec{\tau}; \vec{\theta}) \) and \(N\) samples \( \vec{\tau}^{[i]} \sim p(\cdot; \vec{\theta}) \) and the respective rewards \( R^{[i]} \coloneqq R_{\vec{\tau}^{[i]}} \). What remains is to calculate the gradient of logarithm of the trajectory distribution. Applying the logarithm to the distribution \eqref{eq:policyGradientTrajectoryDistribution} yields
					\begin{equation*}
						\ln p(\vec{\tau}; \vec{\theta}) = \sum_{t = 1}^{T - 1} \ln \pi(\vec{a}_t \given \vec{s}_t; \vec{\theta}) + \underbrace{\ln p(\vec{s}_1) + \sum_{t = 1}^{T - 1} \ln p(\vec{s}_{t + 1} \given \vec{s}_t, \vec{a}_t)}_{\text{Constant}}
					\end{equation*}
					where the last terms are constant w.r.t. \(\vec{\theta}\). Hence, these terms vanish when taking the gradient w.r.t. \(\vec{\theta}\). Plugging this result into the Monte-Carlo approximation \eqref{eq:policyGradientStepBasedMonteCarlo} yields
					\begin{equation*}
						\grad_{\!\vec{\omega}}^\mathrm{MC} J(\vec{\omega})
						= \frac{1}{N} \sum_{i = 1}^{N} \sum_{t = 1}^{T - 1} R^{[i]} \, \grad_{\!\vec{\theta}} \ln \pi\big(\vec{a}_t^{[i]} \Biggiven \vec{s}_t^{[i]}; \vec{\theta}\big)
					\end{equation*}
					which is called the REINFORCE algorithm. But this still used the total rewards with high variance! As the rewards in the past are not correlated with the actions in the future, i.e.
					\begin{equation*}
						\E_{p(\vec{\tau})}\big[ r_t \grad_{\!\vec{\theta}} \ln \pi(\vec{a}_h \given \vec{s}_h; \vec{\theta}) \big]
						\,\oversetfootnotemark{=}\, \E_{p(\vec{\tau})}[r_t] \, \underbrace{\E_{p(\vec{\tau}; \vec{\theta})}\big[ \grad_{\!\vec{\theta}} \ln \pi(\vec{a}_h \given \vec{s}_h; \vec{\theta}) \big]}_{\mathclap{=\, \grad_{\!\vec{\theta}} \E_{p(\vec{\tau}; \vec{\theta})}[1] \,=\, \grad_{\!\vec{\theta}} 1 \,=\, 0}}
						= 0
					\end{equation*}
					holds for all \( h > t \), replacing the total reward with the reward to come only scales the gradient. This is called the \emph{policy gradient theorem}. The result gradient then is
					\begin{equation*}
						\grad_{\!\vec{\omega}}^\mathrm{MC} J(\vec{\omega})
						\propto \grad_{\!\vec{\theta}}^\mathrm{PG} J(\vec{\theta})
						= \frac{1}{N} \sum_{i = 1}^{N} \sum_{t = 1}^{T - 1} Q_t^{[i]} \, \grad_{\!\vec{\theta}} \ln \pi\big(\vec{a}_t^{[i]} \Biggiven \vec{s}_t^{[i]}; \vec{\theta}\big).
					\end{equation*}

					\footnotetext{As \(r_t\) and \(\vec{a}_h\) are independent for \(h > t\).}
				% end

				\paragraph{Baselines}
					A big problem with likelihood ratio policy gradients is that they exhibit a high variance. But this variance can be reduced by subtracting a baseline \(b_i\) from the reward:
					\begin{equation}
						\grad_{\!\vec{\omega}}^\mathrm{MC} J(\vec{\omega})
						= \frac{1}{N} \sum_{i = 1}^{N} \big(R^{[i]} - b^{[i]}\big) \grad_{\!\vec{\omega}} \ln \pi\big(\vec{\theta}^{[i]}; \vec{\omega}\big)  \label{eq:policyGradientWithBaseline}
					\end{equation}
					If the baseline is good, this trick can lower the variance of the estimate. It is still an unbiased estimator as the expectation of the baseline \(b\) vanishes:
					\begin{align*}
						\E_{p(\vec{\theta}; \vec{\omega})}\big[ \grad_{\!\vec{\omega}} \ln p(\vec{\theta}; \vec{\omega}) b \big]
						 & = \int\! p(\vec{\theta}; \vec{\omega}) \grad_{\!\vec{\omega}} \ln p(\vec{\theta}; \vec{\omega}) b \dd{\vec{\theta}}
						= b \int\! p(\vec{\theta}; \vec{\omega}) \grad_{\!\vec{\omega}} \ln p(\vec{\theta}; \vec{\omega}) \dd{\vec{\theta}}    \\
						 & = b \int\! \grad_{\!\vec{\omega}} p(\vec{\theta}; \vec{\omega}) \dd{\vec{\theta}}
						= b \grad_{\!\vec{\omega}} \!\int\!  p(\vec{\theta}; \vec{\omega}) \dd{\vec{\theta}}
						= b \grad_{\!\vec{\omega}} 1
						= b \cdot 0
						= 0
					\end{align*}
					A usually good baseline is the average reward, but there also exists an optimal baseline for each algorithm that minimizes the variance of the gradient estimate. The principle of subtracting baselines can analogously be applied to step-based gradients
					\begin{equation*}
						\grad_{\!\vec{\theta}}^\mathrm{PG} J(\vec{\theta})
						= \frac{1}{N} \sum_{i = 1}^{N} \sum_{t = 1}^{T - 1} \big( Q_t^{[i]} - b_i\big(\vec{s}_t^{[i]}\big) \!\big) \grad_{\!\vec{\theta}} \ln \pi\big(\vec{a}_t^{[i]} \given \vec{s}_t^{[i]}; \vec{\theta}\big).
					\end{equation*}
					where the baseline might even vary with the state. As long as the baseline does not change with the action, the expectation vanishes and the estimator is unbiased.
				% end
			% end
		% end

		\subsection{Step Sizes, Metrics, Relative Entropy and Natural Gradient} % 10.34, 10.35, 10.41, 10.42, 10.43
			\label{subsec:policySearchMetrics}

			An important factor when using policy gradient methods is the step size \(\alpha\) in the policy update
			\begin{align*}
				\vec{\omega}_{k + 1} = \vec{\omega}_k + \alpha \grad_{\!\vec{\omega}} J(\vec{\omega})
				 &  &
				\vec{\theta}_{k + 1} = \vec{\theta}_k + \alpha \grad_{\!\vec{\theta}} J(\vec{\theta})
			\end{align*}
			for episode- or step-based evaluation, respectively. The simplest (naive) idea is to constrain the policy update using a distance metric in the parameter space, for example the Euclidean distance \( L_2(\pi_{k + 1}, \pi_k) \) defined as
			\begin{align*}
				L_2(\pi_{k + 1}, \pi_k) \coloneqq \lVert \vec{\omega}_{k + 1} - \vec{\omega}_k \rVert
				 &  &
				L_2(\pi_{k + 1}, \pi_k) \coloneqq \lVert \vec{\theta}_{k + 1} - \vec{\theta}_k \rVert
			\end{align*}
			for the episode- and step-based evaluation, respectively. With a constraint
			\begin{equation*}
				L_2(\pi_{k + 1}, \pi_k) \leq \epsilon
			\end{equation*}
			on the policy update, the step size is given as \( \alpha_k = \lVert \grad{J} \rVert^{-1} \epsilon \). But the Euclidean metric has the big problem if not being invariant to scaling of the variables! Take for example the policy \( \pi(a \given \vec{s}; \vec{\theta}) = \mathcal{N}(a \given \vec{s}^T \vec{\theta}, \sigma^2) \) with \( \vec{s} \in [0, 1] \times [0, 1000] \) and the different parameters \( \vec{\theta}_1 = [1, 1]^T \), \( \vec{\theta}_2 = [1.1, 1]^T \) and \( \vec{\theta}_3 = [1, 1.1] \). Then the distances \( \lVert \vec{\theta}_1 - \vec{\theta}_2 \rVert \) and \( \lVert \vec{\theta}_1 - \vec{\theta}_3 \rVert \) are equivalent, but the policies \( \pi(a \given \vec{s}; \vec{\theta}_1) \) and \( \pi(a \given \vec{s}; \vec{\theta}_2) \) are less different than \( \pi(a \given \vec{s}; \vec{\theta}_1) \) and \( \pi(a \given \vec{s}; \vec{\theta}_3) \) due to the different scaling in \(\vec{s}\).

			Hence, a metric that is invariant to transformations of the parameters would be better. Let \(\mat{M}\) be a matrix that captures the "influence" of the parameters of the policy, then \(\mat{M}\) can be used to define a new metric that incorporates this influence:
			\begin{equation*}
				L_M(\pi_{k + 1}, \pi_k) \coloneqq (\vec{\theta}_{k + 1} - \vec{\theta}_k)^T \mat{M} (\vec{\theta}_{k + 1} - \vec{\theta}_k)
			\end{equation*}
			By choosing a good matrix \(\mat{M}\), different influence like in the example above can be captured. Using this metric can be incorporated into the gradient ascent with a separate optimization problem for the ascent direction \( \vec{\delta}_\theta \):
			\begin{equation}
				\begin{aligned}
					\vec{\delta}_\theta^\ast = \arg\max_{\vec{\delta}_\theta} \, & \big( \grad_{\!\vec{\theta}} J(\vec{\theta}) \big)^T \vec{\delta}_\theta \\
					\mathrm{s.t.} \quad                                          &
					\begin{aligned}[t]
						\vec{\delta}_\theta^T \mat{M} \vec{\delta}_\theta & \leq \epsilon
					\end{aligned}
				\end{aligned}  \label{eq:policyGradientAscentOpt}
			\end{equation}
			This optimization problem encodes that the actual ascent direction should stay as close as possible to the steepest ascent direction, i.e. the gradient, but should not create jumps in the policy. The solution to this optimization problem is
			\begin{equation*}
				\vec{\delta}_\theta^\ast = \lambda \mat{M}^{-1} \grad_{\!\vec{\theta}} J(\vec{\theta}) \propto \mat{M}^{-1} \grad_{\!\vec{\theta}} J(\vec{\theta})
			\end{equation*}
			with the Lagrangian multiplier \(\lambda\).

			\subsubsection{Relative Entropy and Natural Gradient}
				An alternative way of measuring the distance between two policies is to use the "distance" of the distributions. A well-known measure for this is \emph{relative entropy}, also known as the Kullback-Leibler divergence
				\begin{equation*}
					D_\mathrm{KL}(p \Vert q) = \int\! p(\vec{x}) \ln \frac{p(\vec{x})}{q(\vec{x})} \dd{\vec{x}}
				\end{equation*}
				introduced in \autoref{sec:klDivergence}. An important property of the KL divergence is that is that the distance of two parametric distributions \( p_{\vec{\theta} + \vec{\delta}_\theta} \) and \( p_{\vec{\theta}} \) can be approximated by
				\begin{equation*}
					D_\mathrm{KL}(p_{\vec{\theta} + \vec{\delta}_\theta} \Vert p_{\vec{\theta}}) \approx \vec{\delta}_\theta^T \mat{G}(\vec{\theta}) \vec{\delta}_\theta
				\end{equation*}
				with the Fisher information matrix \( \mat{G}(\vec{\theta}) \) (see \autoref{subsec:fisherInformationMatrix} for a thorough introduction). The \emph{natural gradient} uses the fisher information matrix as a metric, i.e. \( \mat{M} = \mat{G}(\vec{\theta}) \), in the optimization problem \eqref{eq:policyGradientAscentOpt}. As every parameter has the same influence under this metric, the natural gradient is invariant to linear transformations of the parameter space! The ascent direction computed with this optimization problem corresponds to the steepest ascent in the policy space and not in the parameter space, hence the name \emph{natural gradient}. It also guarantees converges to a local minimum!

				\paragraph{Computing the Natural Gradient}
					In the episode-based setting, it is easy to introduce the natural Gradient if the upper-level policy is a Gaussian. First the gradient \( \grad_{\!\vec{\omega}}^\mathrm{MC} J(\vec{\omega}) \) and subsequently the natural gradient \( \grad_{\!\vec{\omega}}^\mathrm{NG} J(\vec{\omega}) = \mat{G}^{-1}(\vec{\omega}) \grad_{\!\vec{\omega}} J(\vec{\omega}) \) is computed, where the latter can be done in closed form for Gaussians. The natural gradient is then used for updating the parameters
					\begin{equation*}
						\vec{\omega}_{k + 1} = \vec{\omega}_k + \eta \grad_{\!\vec{\omega}}^\mathrm{NG} J(\vec{\omega})
					\end{equation*}
					with a step size \(\eta\). For the step-based setting, this is not so easy and will be covered in \autoref{subsubsec:compatibleFunctionApproximation}.
				% end
			% end

			\subsubsection{Compatible Function Approximation} % 10.45, 10.46, 10.47, 10.48, 10.49, 10.50, 10.51
				\label{subsubsec:compatibleFunctionApproximation}

				For the step-based policy gradient with a baseline
				\begin{equation*}
					\grad_{\!\vec{\theta}}^\mathrm{PG} J(\vec{\theta})
					= \frac{1}{N} \sum_{i = 1}^{N} \sum_{t = 1}^{T - 1} \big(Q_t^{[i]} - b_i\big(\vec{s}_t^{[i]}\big)\big) \grad_{\!\vec{\theta}} \ln \pi\big(\vec{a}_t^{[i]} \given \vec{s}_t^{[i]}; \vec{\theta}\big),
				\end{equation*}
				the gradient estimate can be improved by estimating the reward to come with a function approximator
				\begin{equation*}
					f_{\vec{w}}(\vec{s}, \vec{a}) = \vec{\psi}^T(\vec{s}, \vec{a}) \vec{w} \approx Q_t^{[i]} - b_i\big(\vec{s}_t^{[i]}\big).
				\end{equation*}
				This approximator works only on the states and actions and does not depend on the time step or the sample sequence. The function approximation (FA) gradient is
				\begin{equation*}
					\grad_{\!\vec{\theta}}^\mathrm{FA} J(\vec{\theta})
					\coloneqq \frac{1}{N} \sum_{i = 1}^{N} \sum_{t = 1}^{T - 1} f_{\vec{w}}\big( \vec{s}_t^{[i]}, \vec{a}_t^{[i]} \big) \, \grad_{\!\vec{\theta}} \ln \pi\big(\vec{a}_t^{[i]} \given \vec{s}_t^{[i]}; \vec{\theta}\big),
				\end{equation*}
				which is still unbiased for the features \( \vec{\psi}(\vec{s}, \vec{a}) = \grad_{\!\vec{\theta}} \ln \pi(\vec{a} \given \vec{s}; \vec{\theta}) \).

				\todo{Continue compatible function approximation!}

				\paragraph{Episodic Natural Actor-Critic} % 10.52, 10.53, 10.54
					\todo{Content}
				% end
			% end
		% end
	% end

	\section{Probabilistic Policy Search}
		\emph{Probabilistic policy search} (a subset of episodic policy search methods) addresses the problems of policy gradient methods, namely the high amount of samples that are needed and the fact that the learning rate still has to be tuned. It is based on the \emph{success matching principle} by Arrow, 1958: "When learning from a set of their own trials in iterated decision problems, humans attempt to match not the best taken action but the reward-weighted frequency of their actions and outcomes." This means that not only the reward, but also the frequency of this reward and the action is relevant. If a state is not very common, it might not be worth it to optimize the policy here.

		This idea can be formalized as \emph{episode-based success matching}. In each iteration \(k\), first sample parameters \( \vec{\theta}^{[i]} \sim \pi(\cdot; \vec{\omega}_k) \) and evaluate them: \( R^{[i]} = \sum_{t = 1}^{T} r_t^{[i]} \). Then somehow compute the \emph{success probability}
		\begin{equation*}
			w^{[i]} = f\big( R^{[i]} \big)
		\end{equation*}
		of each sample. This is essentially a transformation of the reward to a non-negative weight (an improper probability distribution). Next, compute the success-weighted policy
		\begin{equation*}
			p\big( \vec{\theta}^{[i]} \big) \propto w^{[i]} \pi\big( \vec{\theta}^{[i]}; \vec{\omega}_k \big)
		\end{equation*}
		of each sample. This is only a proportionality to indicate that the distribution has to be normalized. Finally, fit a new policy \( \pi\big( \vec{\theta}^{[i]}; \vec{\omega}_{k + 1} \big) \) that best approximates \( p\big( \vec{\theta}^{[i]} \big) \). The two open issues in this algorithm are:
		\begin{itemize}
			\item how to compute \( w^{[i]} = f\big( R^{[i]} \big) \) and
			\item how to fit the policy \( \pi\big( \vec{\theta}^{[i]}; \vec{\omega}_{k + 1} \big) \).
		\end{itemize}

		\subsection{Policy Fitting by Weighted Maximum Likelihood}
			A first idea to find the policy \( \pi\big(\vec{\theta}^{[i]}; \vec{\omega}_{k + 1}\big) \) that best fits the distribution \( p\big(\vec{\theta}^{[i]}\big) \propto w^{[i]} \pi\big(\vec{\theta}^{[i]}; \vec{\omega}_{k + 1}\big) \) is to minimize the KL divergence between the two:
			\begin{align}
				\vec{\omega}_{k + 1}
				 & = \arg\min_{\vec{\omega}}\, D_\mathrm{KL}\big( p(\vec{\theta}) \Vert \pi(\vec{\theta}; \vec{\omega}) \big)
				= \arg\min_{\vec{\omega}}\, \int\! p(\vec{\theta}) \ln \frac{p(\vec{\theta})}{\pi(\vec{\theta}; \vec{\omega})} \dd{\vec{\theta}}  \nonumber                                                                                        \\
				 & = \arg\min_{\vec{\omega}}\, \underbrace{\int\! p(\vec{\theta}) \ln p(\vec{\theta}) \dd{\vec{\theta}}}_\text{constant w.r.t. \(\pi\)} - \int\! p(\vec{\theta}) \ln \pi(\vec{\theta}; \vec{\omega}) \dd{\vec{\theta}}  \nonumber  \\
				 & = \arg\max_{\vec{\omega}}\, \int\! p(\vec{\theta}) \ln \pi(\vec{\theta}; \vec{\omega}) \dd{\vec{\theta}}
				= \arg\max_{\vec{\omega}}\, \int\! \pi(\vec{\theta}; \vec{\omega}) \frac{p(\vec{\theta})}{\pi(\vec{\theta}; \vec{\omega})} \ln \pi(\vec{\theta}; \vec{\omega}) \dd{\vec{\theta}}  \nonumber                                        \\
				 & \approx \arg\max_{\vec{\omega}}\, \frac{1}{N} \sum_{i = 1}^{N} \underbrace{\frac{p\big(\vec{\theta}^{[i]}\big)}{\pi\big(\vec{\theta}^{[i]}; \vec{\omega}\big)}}_{=\, w^{[i]}} \ln \pi\big(\vec{\theta}^{[i]}; \vec{\omega}\big)
				= \arg\max_{\vec{\omega}} \frac{1}{N} \sum_{i = 1}^{N} w^{[i]} \ln \pi\big(\vec{\theta}^{[i]}; \vec{\omega}\big)  \label{eq:weightedMaxLikelihoodObjective}
			\end{align}
			This estimate of \( \pi^\mathrm{new}(\vec{\theta}) \) is called the \emph{weighted maximum likelihood estimate}. For a Gaussian policy \( \pi(\vec{\theta}) = \pi(\vec{\theta}; \vec{\omega}) = \mathcal{N}(\vec{\theta} \given \vec{\mu}, \mat{\Sigma}) \) with \( \vec{\omega} = \{ \vec{\mu}, \mat{\Sigma} \} \), the estimates can be computed in closed form
			\begin{align*}
				\vec{\mu}_{k + 1} = \frac{1}{W} \sum_{i = 1}^{N} w^{[i]} \vec{\theta}^{[i]}
				 &  &
				\mat{\Sigma}_{k + 1} = \frac{1}{W} \sum_{i = 1}^{N} w^{[i]} \big( \vec{\theta}^{[i]} - \vec{\mu}_{k + 1} \big) \big( \vec{\theta}^{[i]} - \vec{\mu}_{k + 1} \big)^T
			\end{align*}
			with \( W \coloneqq \sum_{i = 1}^{N} w^{[i]} \). Closed-form solutions also exist for more complicated policies like Gaussian mixture models and Gaussian processes. With closed-form solutions, no learning rates are needed!

			\subsubsection{Gradient Ascent for Non-Closed-Forms and Similarity to Policy Gradients}
				If no closed-form solution exists for a parametric policy \( \pi(\vec{\theta}; \vec{\omega}) \), it is always possible to take the gradient of the objective \eqref{eq:weightedMaxLikelihoodObjective} as
				\begin{equation*}
					\grad_{\!\vec{\omega}}\Bigg(\! \frac{1}{N} \sum_{i = 1}^{N} w^{[i]} \ln \pi\big(\vec{\theta}^{[i]}; \vec{\omega}\big) \!\!\Bigg)
					= \frac{1}{N} \sum_{i = 1}^{N} w^{[i]} \grad_{\!\vec{\omega}} \ln \pi\big(\vec{\theta}^{[i]}; \vec{\omega}\big)
				\end{equation*}
				and maximize the objective with gradient ascent. The form of the gradient is very similar to the gradient to \eqref{eq:episodicPolicyGradientGradient}, the gradient of the episodic policy gradient method. The difference is that instead of the total reward \( R^{[i]} \), the success probabilities \( w^{[i]} \) are used for weighting the samples. If the rewards are directly used for the weights (e.g. if the reward already is an improper probability distribution), policy gradient and weighted maximum likelihood are actually equivalent.
			% end

			\subsubsection{Computing the Weights}
				The next problem is to compute the weights (success probabilities) \( w^{[i]} = f\big(R^{[i]}\big) \). The simplest way is to use an exponential transformation
				\begin{equation*}
					w^{[i]} = \exp{ \big( R^{[i]} - R^{[i]}_\mathrm{max} \big) \beta }
				\end{equation*}
				with a \emph{temperature} \( \beta \). This temperature is usually set by heuristics, e.g. \( \beta = 10 / \big( R^{[i]}_\mathrm{max} - R^{[i]}_\mathrm{min} \big) \). Different algorithms emerged from this idea, for example:
				\begin{itemize}
					\item \eqmakebox[probPolicySearchWeights][l]{EM-Algorithms:}   PoWER, Reward-Weighted Regression
					\item \eqmakebox[probPolicySearchWeights][l]{Optimal Control:} PI2
					\item Relative Entropy Policy Search (REPS)
				\end{itemize}
				A downside of the exponential transformation is that in a stochastic environment, the no longer the expected reward is maximized as
				\begin{equation*}
					\E_{p(\vec{\tau})}\Big[\! \exp\big\{R(\vec{\tau})\big\} \Big] \neq \exp\Big\{ \E_{p(\vec{\tau})}\big[ R(\vec{\tau}) \big] \Big\}.
				\end{equation*}
				This has the effect that the objective gets "risk attracted". However, it still works well for moderately stochastic environments.

				\paragraph{Notes on Expectation Maximization}
					The expectation-maximization (EM) algorithm is a method for maximum likelihood estimation in the presence of latent variables. In this setting, the observed variables are the reward and the unobserved variables are the trajectories (or parameters) that lead to the reward. The algorithm is split into two steps:
					\begin{description}[leftmargin=2cm]
						\item[E-Step] Estimate the new desired distribution.
						\item[M-Step] Estimate new (policy) parameters from weighted samples.
					\end{description}
					Algorithms that are based on the EM-principle are Reward-Weighted Regression (Peters, 2007) and PoWER\footnote{PoWER stands for "Policy Learning by Weighting Exploration with the Returns".} (Kober, 2008).
				% end
			% end
		% end

		\subsection{Relative Entropy Policy Search (REPS)}
			Choosing the next policy inherently requires choosing a got temperature \(\beta\). A too low temperature would converge too slow and a too high temperature would directly jump to a single sample and does not explore enough. This is a variant of the exploration-exploitation tradeoff. As for vanilla policy gradients, this can again be addressed using a metric to control the jump in the policy. The metric of choice for \emph{relative entropy policy search} (REPS) is, as obvious from the name, the KL divergence
			\begin{equation*}
				D_\mathrm{KL}\big( \pi(\vec{\theta}) \,\Vert\, q(\vec{\theta}) \big) \leq \epsilon
			\end{equation*}
			constraining the information loss between the new policy\footnote{Note REPS does not assume the policy to be parametric. In the following, an upper-level policy is optimized, but it is also possible to directly optimize a policy that produces actions dependent on states. See Peters, Mlling, Altn: "Relative Entropy Policy Search" (2010).} \(\pi(\vec{\theta})\) and the old policy \(q(\vec{\theta})\) to be at most \(\epsilon\). The REPS optimization problem is:
			\begin{equation}
				\begin{aligned}
					\max_\pi \,         & \int\! \pi(\vec{\theta}) R(\vec{\theta}) \dd{\vec{\theta}} \\
					\mathrm{s.t.} \quad &
					\begin{aligned}[t]
						D_\mathrm{KL}\big( \pi(\vec{\theta}) \,\Vert\, q(\vec{\theta}) \big) & \leq \epsilon \\
						\int\! \pi(\vec{\theta}) \dd{\vec{\theta}}                           & = 1
					\end{aligned}
				\end{aligned}
			\end{equation}
			The analytical solution to this optimization problem is
			\begin{equation*}
				\pi(\vec{\theta}) \propto q(\vec{\theta}) \exp{ \frac{R(\vec{\theta})}{\eta} }
			\end{equation*}
			where \( R(\vec{\theta}) \) is the total reward for a parameter \( \vec{\theta} \) and \(\eta\) is the Lagrangian multiplier for the KL constraint. This the same result as for the weighted maximum likelihood approach using the exponential transformation with temperature \( \beta = 1/\eta \). The scaling factor \(\eta\) is determined by the optimization problem and specified by the KL bound \(\epsilon\). To get \(\eta\), the dual objective (approximated with Monte-Carlo integration, \( \vec{\theta}_i \sim q(\cdot) \))
			\begin{equation*}
				\eta\epsilon + \ln \int\! q(\vec{\theta}) \exp{\frac{R(\vec{\theta})}{\eta}} \dd{\vec{\theta}}
				\approx \eta\epsilon + \ln \Bigg(\! \frac{1}{N} \sum_{i = 1}^{N} \exp{\frac{R(\vec{\theta}_i)}{\eta}} \!\Bigg)
			\end{equation*}
			has to be minimized (subject to \( \eta \geq 0 \)). This can be done with standard numerical optimization algorithms like trust-region methods or similar.

			For discrete \(\vec{\theta}\), the optimization problem is
			\begin{equation}
				\begin{aligned}
					\max_\pi \,         & \sum_{\vec{\theta}} \pi(\vec{\theta}) R(\vec{\theta}) \\
					\mathrm{s.t.} \quad &
					\begin{aligned}[t]
						D_\mathrm{KL}\big( \pi(\vec{\theta}) \,\Vert\, q(\vec{\theta}) \big) & \leq \epsilon \\
						\sum_{\vec{\theta}} \pi(\vec{\theta})                                & = 1
					\end{aligned}
				\end{aligned}
			\end{equation}
			where now the KL divergence is discrete too.

			\subsubsection{Derivation of REPS}
				With the multipliers \( \lambda \) and \( \eta \), the Lagrangian of this problem is
				\begin{align}
					L[\pi, \lambda, \eta] & =
					\int\! \pi(\vec{\theta}) R(\vec{\theta}) \dd{\vec{\theta}}
					+ \lambda \bigg(\! 1 - \int\! \pi(\vec{\theta}) \dd{\vec{\theta}} \!\!\bigg)
					+ \eta \big( \epsilon - D_\mathrm{KL}\big( \pi(\vec{\theta}) \,\Vert\, q(\vec{\theta}) \big) \big)  \nonumber                             \\
					                      & =
					\int\! \pi(\vec{\theta}) R(\vec{\theta}) \dd{\vec{\theta}}
					+ \lambda \bigg(\! 1 - \int\! \pi(\vec{\theta}) \dd{\vec{\theta}} \!\!\bigg)
					+ \eta \bigg(\! \epsilon - \int\! \pi(\vec{\theta}) \ln \frac{\pi(\vec{\theta})}{q(\vec{\theta})} \dd{\vec{\theta}} \!\!\bigg)  \nonumber \\
					                      & = (\lambda + \eta \epsilon)
					+ \int\! \pi(\vec{\theta}) R(\vec{\theta}) \dd{\vec{\theta}}
					- \lambda \int\! \pi(\vec{\theta}) \dd{\vec{\theta}}
					- \eta \int\! \pi(\vec{\theta}) \ln \frac{\pi(\vec{\theta})}{q(\vec{\theta})} \dd{\vec{\theta}}  \label{eq:repsLagrangian}                \\
					                      & = (\lambda + \eta\epsilon)
					+ \int\! \pi(\vec{\theta}) \bigg[ R(\vec{\theta}) - \lambda - \eta \ln \frac{\pi(\vec{\theta})}{q(\vec{\theta})} \bigg] \dd{\vec{\theta}}.  \nonumber
				\end{align}
				To maximize this Lagrangian w.r.t. the primal variable \( \pi(\vec{\theta}) \), the term can be discarded as it does not depend on the primal variable. The remaining integral is a function over the primal variable that can be solved using variational calculus. Let
				\begin{equation*}
					\mathcal{L}[\pi, \lambda, \eta] = \pi(\vec{\theta}) \bigg[ R(\vec{\theta}) - \lambda - \eta \ln \frac{\pi(\vec{\theta})}{q(\vec{\theta})} \bigg]
				\end{equation*}
				be the respective Lagrangian. As \(\mathcal{L}\) does not depend on \( \pi'(\vec{\theta}) \), the Euler-Lagrange equation becomes \( \pdv{\mathcal{L}}{\pi(\vec{\theta})} = 0 \). This yields the algebraic equation
				\begin{equation*}
					\pdv{\mathcal{L}}{\pi(\vec{\theta})}
					= R(\vec{\theta}) - \lambda - \eta \ln \frac{\pi(\vec{\theta})}{q(\vec{\theta})} - \eta \pi(\vec{\theta}) \frac{1}{\pi(\vec{\theta})}
					= R(\vec{\theta}) - \eta \ln \pi(\vec{\theta}) + \eta \ln q(\vec{\theta}) - (\eta + \lambda)
					\overset{!}{=} 0
				\end{equation*}
				that is easily solvable for \( \pi(\vec{\theta}) \):
				\begin{align*}
					                           &                                                                                              &
					0                          & = R(\vec{\theta}) - \eta \ln \pi(\vec{\theta}) + \eta \ln q(\vec{\theta}) - (\eta + \lambda) & \\
					\iff                       &                                                                                              &
					\eta \ln \pi(\vec{\theta}) & = R(\vec{\theta}) + \eta \ln q(\vec{\theta}) - (\eta + \lambda)                              & \\
					\iff                       &                                                                                              &
					\ln \pi(\vec{\theta})      & = \frac{R(\vec{\theta})}{\eta} + \ln q(\vec{\theta}) - \frac{\eta + \lambda}{\eta}           & \\
					\iff                       &                                                                                              &
					\pi(\vec{\theta})          & = q(\vec{\theta}) \exp{\frac{R(\vec{\theta})}{\eta}} \exp{-\frac{\eta + \lambda}{\eta}}      &
				\end{align*}
				But the last exponential term is just a normalization factor! Hence, it can also be written as
				\begin{equation*}
					\exp{\frac{\eta + \lambda}{\eta}} = \int\! q(\vec{\theta}) \exp{\frac{R(\vec{\theta})}{\eta}} \dd{\vec{\theta}}
					\quad\iff\quad
					\frac{\eta + \lambda}{\eta} = \ln \int\! q(\vec{\theta}) \exp{\frac{R(\vec{\theta})}{\eta}} \dd{\vec{\theta}}.
				\end{equation*}
				To get the dual objective \( h(\lambda, \eta) \), the solution of the primal variable has to be plugged into the Lagrangian \eqref{eq:repsLagrangian}:
				\begin{align*}
					h(\lambda, \eta)
					 & = L[\pi^\ast, \lambda, \eta]
					= (\lambda + \eta\epsilon)
					+ \int\! \pi^\ast(\vec{\theta}) R(\vec{\theta}) \dd{\vec{\theta}}
					- \lambda \underbrace{\int\! \pi^\ast(\vec{\theta}) \dd{\vec{\theta}}}_{=\, 1}
					- \,\eta \int\! \pi(\vec{\theta}) \ln \frac{\pi^\ast(\vec{\theta})}{q(\vec{\theta})} \dd{\vec{\theta}}                                                                                                                     \\
					 & = \eta\epsilon + \int\! \pi^\ast(\vec{\theta}) \bigg[ R(\vec{\theta}) - \eta \ln \frac{\pi^\ast(\vec{\theta})}{q(\vec{\theta})} \bigg] \dd{\vec{\theta}}                                                                \\
					 & = \eta\epsilon + \int\! \pi^\ast(\vec{\theta}) \bigg[ R(\vec{\theta}) - \eta \ln \frac{q(\vec{\theta}) \exp{\frac{R(\vec{\theta})}{\eta}} \exp{-\frac{\eta + \lambda}{\eta}}}{q(\vec{\theta})} \bigg] \dd{\vec{\theta}} \\
					 & = \eta\epsilon + \int\! \pi^\ast(\vec{\theta}) \bigg[ R(\vec{\theta}) - \eta \ln \bigg(\!\! \exp{\frac{R(\vec{\theta})}{\eta}} \exp{-\frac{\eta + \lambda}{\eta}} \!\bigg) \bigg] \dd{\vec{\theta}}                     \\
					 & = \eta\epsilon + \int\! \pi^\ast(\vec{\theta}) \bigg[ R(\vec{\theta}) - \eta \frac{R(\vec{\theta})}{\eta} + \eta \frac{\eta + \lambda}{\eta} \bigg] \dd{\vec{\theta}}                                                   \\
					 & = \eta\epsilon + \eta \frac{\eta + \lambda}{\eta} \underbrace{\int\! \pi^\ast(\vec{\theta}) \dd{\vec{\theta}}}_{=\, 1}
					= \eta\epsilon + \eta \frac{\eta + \lambda}{\eta}
					= \eta\epsilon + \ln \int\! q(\vec{\theta}) \exp{\frac{R(\vec{\theta})}{\eta}} \dd{\vec{\theta}}
				\end{align*}
				By approximating this integral with Monte-Carlo integration, the Lagrangian multiplier \(\eta\) can be calculated.
			% end
		% end

		\subsection{REPS for Contextual Policy Search}
			In contextual policy search, the task is dependent on a context \(\vec{c}\) that is fixed before executing (e.g. the target location when throwing a ball). This can be introduced to the policy by conditioning the upper-level policy on the context: \( \pi(\vec{\theta} \given \vec{c}) \). The dataset for the policy update is changed to \( \mathcal{D} = \big\{ \vec{\theta}^{[i]}, \vec{c}^{[i]}, R^{[i]} \big\}_{i \,=\, \subdotsrange{1}{N}} \), taking care of the context. The objective is also changed to maximize the expected reward over all parameters and all contexts with the context distribution \( \mu_0(\vec{c}) \) and the reward \( R(\vec{c}, \vec{\theta}) \):
			\begin{equation*}
				J_\pi = \iint\! \mu_0(\vec{c}) \pi(\vec{\theta} \given \vec{c}) R(\vec{c}, \vec{\theta}) \dd{\vec{c}} \dd{\vec{\theta}}
			\end{equation*}
			The REPS optimization problem for contextual policy search directly optimized the joint distribution \( p(\vec{c}, \vec{\theta}) = \mu_0(\vec{c}) \pi(\vec{\theta} \given \vec{c}) \) instead of conditioning the optimization problem:
			\begin{equation*}
				\begin{aligned}
					\max_p \,           & \iint\! p(\vec{c}, \vec{\theta}) R(\vec{c}, \vec{\theta}) \dd{\vec{c}} \dd{\vec{\theta}} \\
					\mathrm{s.t.} \quad &
					\begin{aligned}[t]
						D_\mathrm{KL}\big( p(\vec{c}, \vec{\theta}) \,\Vert\, q(\vec{c}, \vec{\theta}) \big) & \leq \epsilon    \\
						\iint\! p(\vec{c}, \vec{\theta}) \dd{\vec{c}} \dd{\vec{\theta}}                      & = 1              \\
						\forall \vec{c} : \int\! p(\vec{c}, \vec{\theta}) \dd{\vec{\theta}}                  & = \mu_0(\vec{c})
					\end{aligned}
				\end{aligned}
			\end{equation*}
			Some major problems with this approach are that the context distributions cannot be freely chosen by the algorithm, there are an infinite amount of constraints and for each context a lot of parameter samples are needed.

			\subsubsection{Feature Matching}
				Instead of perfectly matching the context distribution which emits an infinite amount of constraints, one idea is to instead match certain features \( \vec{\phi}(\vec{c}) \), for example the first- and second-order moments \( \vec{\phi}^T(c) = \begin{bmatrix} c & c^2 \end{bmatrix} \). This would be equivalent to matching mean and variance which, for Gaussian distributions, is exact (mean and variance are sufficient statistics). With the target features \( \hat{\vec{\phi}} \), the the optimization problem is given as:
				\begin{equation*}
					\begin{aligned}
						\max_p \,           & \iint\! p(\vec{c}, \vec{\theta}) R(\vec{c}, \vec{\theta}) \dd{\vec{c}} \dd{\vec{\theta}} \\
						\mathrm{s.t.} \quad &
						\begin{aligned}[t]
							D_\mathrm{KL}\big( p(\vec{c}, \vec{\theta}) \,\Vert\, q(\vec{c}, \vec{\theta}) \big) & \leq \epsilon      \\
							\iint\! p(\vec{c}, \vec{\theta}) \dd{\vec{c}} \dd{\vec{\theta}}                      & = 1                \\
							\iint\! p(\vec{c}, \vec{\theta}) \vec{\phi}(\vec{c}) \dd{\vec{c}} \dd{\vec{\theta}}  & = \hat{\vec{\phi}}
						\end{aligned}
					\end{aligned}
				\end{equation*}
				This optimization problem can again be solved in closed form
				\begin{equation*}
					p(\vec{c}, \vec{\theta})
					= \mu_0(\vec{c}) \pi(\vec{\theta} \given \vec{c})
					\propto q(\vec{c}, \vec{\theta}) \exp{ \frac{R(\vec{c}, \vec{\theta}) - V(\vec{c})}{\eta} }
				\end{equation*}
				with \( V(\vec{c}) = \vec{\phi}^T(\vec{c}) \vec{v} \) with the Lagrangian multipliers \(\eta\) and \(\vec{v}\). Those are for the KL and the moment matching constraints, respectively. The Lagrangian multipliers are again given as the solution of the dual optimization problem. The dual function is
				\begin{align*}
					h(\eta, \vec{v})
					 & = \eta\epsilon + \hat{\vec{\phi}}^T \vec{v} + \eta \ln \int\! q(\vec{c}, \vec{\theta}) \exp{ \frac{R(\vec{c}, \vec{\theta}) - \vec{\phi}^T(\vec{c}) \vec{v}}{\eta} }                                                         \\
					 & \approx \eta\epsilon + \hat{\vec{\phi}}^T \vec{v} + \eta \ln \Bigg(\! \frac{1}{N} \sum_{i = 1}^{N} q(\vec{c}_i, \vec{\theta}_i) \exp{ \frac{R(\vec{c}_i, \vec{\theta}_i) - \vec{\phi}^T(\vec{c}_i) \vec{v}}{\eta} } \!\Bigg)
				\end{align*}
				with Monte-Carlo samples \( (\vec{c}_i, \vec{\theta}_i) \) drawn from \( q(\vec{c}, \vec{\eta}) \).

				For discrete \(\vec{c}\) and \(\vec{\theta}\), the optimization problem is
				\begin{equation*}
					\begin{aligned}
						\max_p \,           & \sum_{\vec{c}, \vec{\theta}} p(\vec{c}, \vec{\theta}) R(\vec{c}, \vec{\theta}) \\
						\mathrm{s.t.} \quad &
						\begin{aligned}[t]
							D_\mathrm{KL}\big( p(\vec{c}, \vec{\theta}) \,\Vert\, q(\vec{c}, \vec{\theta}) \big) & \leq \epsilon      \\
							\sum_{\vec{c}, \vec{\theta}} p(\vec{c}, \vec{\theta})                                & = 1                \\
							\sum_{\vec{c}, \vec{\theta}} p(\vec{c}, \vec{\theta}) \vec{\phi}(\vec{c})            & = \hat{\vec{\phi}}
						\end{aligned}
					\end{aligned}
				\end{equation*}
				where now the KL divergence is discrete too.
			% end

			\subsubsection{Contextual Policies with Weighted ML}
				It is also possible to get contextual policies using the weighted maximum likelihood approach. For a Gaussian policy
				\begin{equation*}
					\pi_{\vec{\omega}}(\vec{\theta} \given \vec{c}) = \mathcal{N}(\vec{\theta} \given \mat{K} \vec{c} + \vec{k},\, \mat{\Sigma}),\quad \vec{\omega} = \{ \mat{K}, \vec{k}, \mat{\Sigma} \}
				\end{equation*}
				with parameters \(\vec{\omega}\), the solution is given as
				\begin{align*}
					\begin{bmatrix}
						\vec{k}^T \\
						\mat{K}^T
					\end{bmatrix} = \big( \mat{C}^T \mat{D} \mat{C} \big)^{-1} \mat{C}^T \mat{D} \mat{A}
					 &  &
					\mat{\Sigma} = \frac{1}{\sum_{i = 1}^{N} w_i} \sum_{i = 1}^{N} w^{[i]} \big( \vec{\theta}^{[i]} - \vec{\mu}^{[i]} \big) \big( \vec{\theta}^{[i]} - \vec{\mu}^{[i]} \big)
					 &  &
					\vec{\mu}^{[i]} \coloneqq \mat{K} \vec{c}^{[i]} + \vec{k}
				\end{align*}
				where the weights \( w^{[i]} \) are computed some how (e.g. using the exponential transformation) and the samples \( \vec{c}^{[i]} \) and \( \vec{\theta}^{[i]} \) are drawn from the previous distribution. Here, \(\mat{C}\) is the input data matrix (with an appended \(1\) for the bias), \(\mat{D}\) is a diagonal weighting matrix and \(\mat{A}\) is the parameter matrix.
			% end
		% end

		\subsection{Learning Versatile Solutions and Hierarchical REPS (HiREPS)}
			In many motor tasks, multiple solutions to the same problem are possible. One problem with the current formulation is that either only one of the solutions is found or the policy averages over multiple solutions. As these solutions may lie far apart, that creates a rather unstable policy. One idea to solve this is to use a hierarchical approach where an upper-level \emph{gating policy} \( \pi(o \given \vec{c}) \) chooses the option \(o\) that then selects an \emph{option policy} \( \pi(\vec{\theta} \given \vec{c}, o_i) \) to handle the task.

			The naive hierarchical approach is to define the joint distribution \( p(\vec{c}, \vec{\theta}, o) = \mu_0(\vec{c}) \pi(\vec{\theta} \given \vec{c}, o) \pi(o \given \vec{c}) \) and solve the optimization problem
			\begin{equation*}
				\begin{aligned}
					\max_p \,           & \sum_o \iint\! p(\vec{c}, \vec{\theta}, o) R(\vec{c}, \vec{\theta}) \dd{\vec{c}} \dd{\vec{\theta}} \\
					\mathrm{s.t.} \quad &
					\begin{aligned}[t]
						D_\mathrm{KL}\big( p(\vec{c}, \vec{\theta}, o) \,\Vert\, q(\vec{c}, \vec{\theta}, o) \big)    & \leq \epsilon      \\
						\sum_o \iint\! p(\vec{c}, \vec{\theta}, o) \dd{\vec{c}} \dd{\vec{\theta}}                     & = 1                \\
						\sum_o \iint\! p(\vec{c}, \vec{\theta}, o) \vec{\phi}(\vec{c}) \dd{\vec{c}} \dd{\vec{\theta}} & = \hat{\vec{\phi}}
					\end{aligned}
				\end{aligned}
			\end{equation*}
			while still matching the features. But this optimization problem does not enforce versatile solutions, i.e. multiple options are learned, but no separated between them is done. The insight to solve this is that versatile solutions are characterized by a small overlap in the solutions. This can be represented as limiting the entropy of \( p(o \given \vec{c}, \vec{\theta}) \). This leads to the optimization problem underlying \emph{Hierarchical REPS} (HiREPS):
			\begin{equation*}
				\begin{aligned}
					\max_p \,           & \sum_o \iint\! p(\vec{c}, \vec{\theta}, o) R(\vec{c}, \vec{\theta}) \dd{\vec{c}} \dd{\vec{\theta}} \\
					\mathrm{s.t.} \quad &
					\begin{aligned}[t]
						D_\mathrm{KL}\big( p(\vec{c}, \vec{\theta}, o) \,\Vert\, q(\vec{c}, \vec{\theta}, o) \big)                                                                       & \leq \epsilon      \\
						\sum_o \iint\! p(\vec{c}, \vec{\theta}, o) \dd{\vec{c}} \dd{\vec{\theta}}                                                                                        & = 1                \\
						\sum_o \iint\! p(\vec{c}, \vec{\theta}, o) \vec{\phi}(\vec{c}) \dd{\vec{c}} \dd{\vec{\theta}}                                                                    & = \hat{\vec{\phi}} \\
						\sum_o \iint\! p(\vec{c}, \vec{\theta}, o) \big[\! -p(o \given \vec{c}, \vec{\theta}) \ln p(o \given \vec{c}, \vec{\theta}) \big] \dd{\vec{c}} \dd{\vec{\theta}} & \leq \kappa
					\end{aligned}
				\end{aligned}
			\end{equation*}
			Of course this can readily be adapted to discrete states and parameters by swapping the integrals for sums. The solution of this optimization problem actually learns versatile solutions!
		% end

		\subsection{Sequencing Movement Primitives and Sequential REPS}
			For many motor tasks it is also necessary to execute different elementary building blocks in a sequence to fulfill a task. This also means that the context of blocks executed later also depends on previously executed blocks! So the long-term effects have to be learned. Similar to the hierarchical approach, this changes the goal from maximizing the expected reward to maximize the expected reward over \(K\) decision steps, i.e. building blocks. For each block an upper-level policy \( \pi_k(\vec{\theta} \given \vec{c}) \) is learned where the \(k\)-th block has to react to the outcome of block \(k\). The objective function is
			\begin{equation*}
				J_\pi = \sum_{k = 1}^{K} \iint\! \mu_k(\vec{c}) \pi_k(\vec{\theta} \given \vec{c}) R_k(\vec{c}, \vec{\theta}) \dd{\vec{c}} \vec{\theta}
			\end{equation*}
			where the context distributions \( \mu_k(\vec{c}) \) are specified by the previous policies \( \pi_{l < k}(\vec{\theta} \given \vec{c}) \):
			\begin{equation*}
				\mu_k(\vec{c}') = \iint\! \mu_{k - 1}(\vec{c}) \pi_{k - 1}(\vec{\theta} \given \vec{c}) p(\vec{c}' \given \vec{c}, \vec{\theta}) \dd{\vec{c}} \dd{\vec{\theta}}
			\end{equation*}
			With \( p_k(\vec{c}, \vec{\theta}) = \mu_k(\vec{c}) \pi(\vec{\theta} \given \vec{c}) \), the optimization problem of \emph{Sequential REPS} is
			\begin{equation*}
				\begin{aligned}
					\max_p \,           & \sum_{k = 1}^{K} \iint\! p_k(\vec{c}, \vec{\theta}) R_k(\vec{c}, \vec{\theta}) \dd{\vec{c}} \dd{\vec{\theta}} \\
					\mathrm{s.t.} \quad &
					\begin{aligned}[t]
						D_\mathrm{KL}\big( p_k(\vec{c}, \vec{\theta}) \,\Vert\, q_k(\vec{c}, \vec{\theta}) \big)                         & \leq \epsilon,\, \forall k       \\
						\iint\! p_k(\vec{c}, \vec{\theta}) \dd{\vec{c}} \dd{\vec{\theta}}                                                & = 1,\, \forall k                 \\
						\iint\! p_{k - 1}(\vec{c}, \vec{\theta}) p(\vec{c}' \given \vec{c}, \vec{\theta}) \dd{\vec{c}} \dd{\vec{\theta}} & = p_k(\vec{c}'),\, \forall k > 1
					\end{aligned}
				\end{aligned}
			\end{equation*}
			with the solution
			\begin{equation*}
				p_k(\vec{c}, \vec{\theta}) \propto q_k(\vec{c}, \vec{\theta}) \exp{ \frac{R_k(\vec{c}, \vec{\theta}) + \E_{p(\vec{c}' \subgiven \vec{c}, \vec{\theta})}\big[ V_{k + 1}(\vec{c}') \big] - V_k(\vec{c})}{\eta_k} }
			\end{equation*}
			where \( \E_{p(\vec{c}' \subgiven \vec{c}, \vec{\theta})}\big[ V_{k + 1}(\vec{c}') \big] \) encoded the long-term reward.
		% end
	% end

	\section{Wrap-Up}
		Policy search is a powerful and practical alternative to value function and model-based methods as learning a model is really hard and a small error in either the model or the value function can have catastrophic consequences. Policy search gradients have been the go-to methods for policy search for a long time, but they still need lots of samples and the learning rate has to be tuned (learning the learning rate is still an open problem). Newer methods are probabilistic policy search methods.

		They reduce the policy update to a weighted maximum likelihood estimate of the policy parameters. The weights are calculated using an exponential transformation with a temperature \(\beta\). This temperature has to be hand-tuned when using plain weighted ML estimates. The REPS algorithm optimizes the temperature by imposing an upper bound on the KL divergence of the new and old policy. Various extensions of REPS have been discussed like contextual REPS, HiREPS and Sequential REPS.


		For a comparison with the other discussed reinforcement learning methods (model-based, see \autoref{c:modelBasedRL} and value function, see \autoref{c:valueFunctionMethods}), the following list gives an overview over the main benefits and drawbacks of \emph{episode-based policy search methods}:
		\begin{description}
			\item[Model Complexity] None (no approximation errors)
				\begin{itemize}
					\item Need to evaluate total reward \( R \).
				\end{itemize}
			\item[Scalability] Good
				\begin{itemize}
					\item Parameterized policies are compact.
					\item Also allows learning for high-dimensional robots.
					\item Only works for a moderate amount of parameter.
				\end{itemize}
			\item[Data Efficiency] Poor
				\begin{itemize}
					\item Each rollout is just one sample.
					\item High variance in rewards for stochastic environments.
				\end{itemize}
			\item[Other Limitations] Mainly used for learning single trajectories (e.g. DMPs).
		\end{description}
		And for \emph{step-based policy search}:
		\begin{description}
			\item[Model Complexity] None (no approximation errors)
				\begin{itemize}
					\item Need to evaluate reward to come \( Q_t \).
				\end{itemize}
			\item[Scalability] Good
				\begin{itemize}
					\item Parameterized policies are compact.
					\item Also allows learning for high-dimensional robots.
					\item Only works for a moderate amount of parameter.
				\end{itemize}
			\item[Data Efficiency] Poor
				\begin{itemize}
					\item Uses every state-action pair with reward to come.
					\item High variance in reward to come.
				\end{itemize}
			\item[Other Limitations] Mainly used for learning single trajectories (e.g. DMPs).
		\end{description}
	% end
% end

\chapter{Imitation Learning: Behavioral Cloning and Inverse RL}
	Ot is often very hard or even impossible and time-consuming to learn policies from scratch using reinforcement learning, e.g. as the search space is too large to explore it in a reasonable amount of time. A human export, for example a professional table tennis player, takes years for perfecting its policy and the robot can avoid that by cloning his policy. This is called \emph{imitation learning} and is discussed in this chapter. Two different types of imitation learning are discussed: behavioral closing, where the actions are reproduced given the state, and inverse reinforcement learning, which is learning the reward function from demonstrations, given the optimal policy. Both of them rely on demonstrations, so a crucial question is how to demonstrate movements? A few possibilities are
	\begin{itemize}
		\item Teleoperation: \\
			Use a joystick to train an RC car, The steering wheel of a car to train a car, Data cloves for an artificial hand, \dots
		\item Kinesthetic Teach-In: \\
			Take the robot "by the hand" and demonstrate the movement, like a tennis teacher teaches a student.
		\item Sensuits: \\
			Suits with encoders and accelerometers that are attached to humans to capture the movement.
		\item Marker-Based Tracking: \\
			Use markers and a basic skeleton to obtain precise data from the movement (motion capturing suits).
		\item Vision: \\
			Video-based tracking of humans.
	\end{itemize}

	\section{Distribution Matching}
		\subsection{Behavioral Cloning}
			\emph{Behavioral cloning} is the simplest form of imitation learning where demonstrations \( \big\{ \vec{s}_{1:T}^{[i]}, \vec{a}_{1:T}^{[i]} \big\}_{i \,=\, \subdotsrange{1}{N}} \), generated by experts, that follow an optimal policy \( \pi^\ast(\vec{a} \given \vec{s}) \) with the long-term behavior \( \mu^\ast(\vec{s}) \), are used to estimate a policy \( \hat{\pi}(\vec{a} \given \vec{s}) \). This is, in principle, a supervised regression problem.

			Direct behavioral cloning directly applies supervised learning methods (e.g. linear regression, Gaussian processes or neural networks) to the dataset \(\mathcal{D}\) to extract a policy \( \pi(\vec{a} \given \vec{s}) \), boiling down it to a regression problem. This often causes the \emph{clean-up effect}: due to regularization in the ML model, noise in the demonstrations is often not exhibited by the reproduction. Hence, the student often surpassed the quality of the expert/teacher.

			While this is a rather simple approach, it has a few major problems:
			\begin{itemize}
				\item It requires lots of demonstrations to generalize, and these demonstrations often exhibit a high variance.
				\item The estimated policy always has an approximation error and small errors in the policy may lead to large error in the long-term behavior \( \hat{\mu}(\vec{s}) \).
				\item The agent does not know how to recover from previously unseen states, which is problematic as the agent is prone to make mistakes due to the residual error! This problem is called the \emph{covariate shift}.
			\end{itemize}

			\subsubsection{DAGGER: New Samples to Learn to Recover}
				In the \emph{DAGGER} (\emph{Dataset Aggregation}, see Ross, Gordon and Bagnell: "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning" (2010)) algorithm, the robot is given the ability to ask for new demonstrations if it ends up in an unseen states. The teacher then provides new data and the robot aggregates it with the previously seen demonstrations.
			% end

			\subsubsection{DART: Robustness in Imitation Learning}
				In the \emph{DART} (\emph{Disturbances for Augmenting Robot Trajectories}, see Laskey, et al.: "DART: Noise Injection for Robust Imitation Learning" (2017)) method, noise is added to the demonstrations to learn how the teacher adapts. In consequence, the learned policy will be more robust with respect to unexpected variations. This is an improvement over DAGGER as it does not require a teacher to present which is time consuming.
			% end
		% end

		\subsection{Generative Adversarial Learning}
			In \emph{generative adversarial learning}, not the distance between the agents and the experts policy is minimized, but the \emph{occupancy measure}
			\begin{equation*}
				\rho_\pi(\vec{s}, \vec{a}) = \pi(\vec{s} \given \vec{a}) \sum_{t = 1}^{\infty} \gamma^t \Pr(\vec{s}_t = \vec{s} \given \pi)
			\end{equation*}
			is matched. It can be interpreted as the distribution of state-action pairs that the agent encounters when navigating through the environment with policy \(\pi\). The probability \( \Pr(\vec{s}_t = \vec{s} \given \pi) \) is the probability of encountering state \(\vec{s}\) in time step \(t\) when following policy \(\pi\). The matching is done by minimizing the KL divergence between the policy \(\pi_E\) of the expert and the estimated policy \(\hat{\pi}\):
			\begin{equation*}
				\hat{\pi} = \arg\min_{\hat{\pi}'}\, D_\mathrm{KL}\big( \rho_{\pi_E}(\vec{s}, \vec{a}) \,\big\Vert\, \rho_{\hat{\pi}'}(\vec{s}, \vec{a}) \big)
			\end{equation*}
			This is solved by GAIL (Generative Adversarial Imitation Learning) with a minmax game
			\begin{equation*}
				\max_{\hat{\pi}'} \, \min_D \, \E_{\hat{\pi}'}\big[\! \ln D(\vec{s}, \vec{a}) \big] + \E_{\pi_E}\big[\! \ln\big( 1 - D(\vec{s}, \vec{a}) \big) \big] - \lambda H(\hat{\pi}')
			\end{equation*}
			with a discriminator \(D\) and the policy \(\hat{\pi}'\). The algorithm is composed of two steps:
			\begin{enumerate}
				\item The discriminator is trained for a high probability in the experts demonstrations.
				\item The policy is trained for maximize the discriminator, considering the discriminator as the reward.
			\end{enumerate}
		% end
	% end

	\section{Inverse Reinforcement Learning}
		In \emph{inverse reinforcement learning} (IRL), the problem if imitation learning is tackled from a different point of view: assuming the reward function provides the most informative and transferable definition of the task, learn the reward function from demonstrations given a model and a policy and subsequently recover the reward function. This reward function can then be used to further improve the policy or to learn a different policy (apprenticeship learning).

		The problem setup uses demonstrations \( \big\{ \vec{s}_{1:T}^{[i]}, \vec{a}_{1:T}^{[i]} \big\}_{i \,=\, \subdotsrange{1}{N}} \) from an expert that encode the teachers policy \( \pi^\ast(\vec{a} \given \vec{s}) \) and the long-term behavior \( \mu^\ast(\vec{s}) \) and a transition model \( r(\vec{s}_{t + 1} \given \vec{s}_t, \vec{a}_t) \) but no reward function as inputs. The problem of inverse RL is now to recover the reward function \( r(\vec{s}) \) that best explains the policy \( \pi^\ast(\vec{a} \given \vec{s}) \) and the long-term behavior \( \mu^\ast(\vec{s}) \).

		\subsection{Basic Principle}
			The problem can be formulated as a feasibility problem: find a reward function \( r^\ast(\vec{s}) \) such that
			\begin{equation}
				\E_{p, \pi^\ast}\Bigg[ \sum_{t = 1}^{\infty} \gamma^t r^\ast(\vec{s}_t) \Bigg]
				\geq
				\E_{p, \pi}\Bigg[ \sum_{t = 1}^{\infty} \gamma^t r^\ast(\vec{s}_t) \Bigg]
				\label{eq:inverseRLBasicPrinciple}
			\end{equation}
			holds for all policies \(\pi\). While this problem is convex, it still has many challenges:
			\begin{description}[leftmargin=4.5cm]
				\item[Limited Data]         Often only traces from the experts behavior are given, not the entire optimal policy.
				\item[Ill-Posed]            Reward \( r^\ast(\vec{s}) \equiv 0 \) is a solution, there is ambiguity in the reward function.
				\item[Expert Suboptimality] Assumes that the expert is indeed optimal. If this is not the case, the problem becomes infeasible.
				\item[Computation]          Assumes that the policies can be enumerated.
			\end{description}

			\subsubsection{Addressing the "Limited Data" Challenge: Feature-Based Reward Function}
				Assuming the reward function is linear in some features \( \vec{\phi}(\vec{s}) \), i.e. \( r(\vec{s}) = \vec{w}^T \vec{\phi}(\vec{s}) \), the right hand side of \eqref{eq:inverseRLBasicPrinciple} becomes
				\begin{equation*}
					\E_{p, \pi}\Bigg[ \sum_{t = 1}^{\infty} \gamma^t r(\vec{s}_t) \Bigg]
					= \E_{p, \pi}\Bigg[ \sum_{t = 1}^{\infty} \gamma^t \vec{w}^T \vec{\phi}(\vec{s}) \Bigg]
					= \vec{w}^T \underbrace{\E_{p, \pi}\Bigg[ \sum_{t = 1}^{\infty} \gamma^t \vec{\phi}(\vec{s}) \Bigg]}_{\vec{\psi}(\pi) \,\coloneqq}
					\doteq \vec{w}^T \vec{\psi}(\pi)
				\end{equation*}
				with \( \vec{\psi}(\pi) \) being the expected discounted features of policy \(\pi\). The feasibility problem \eqref{eq:inverseRLBasicPrinciple} now is for find some \( \vec{w}^\ast \) such that
				\begin{equation*}
					(\vec{w}^\ast)^T \vec{\psi}(\pi^\ast) \geq (\vec{w}^\ast)^T \vec{\psi}(\pi)
				\end{equation*}
				holds for all policies \(\pi\). This solves the \emph{limited data} challenge as the expectation \( \vec{\psi}(\pi^\ast) \) can be estimated from the sampled trajectories (the distributions). The amount of data obviously scales with the number of features in the reward function, but does not depend on the size of the state space of the complexity of the experts policy!
			% end

			\subsubsection{Addressing the "Ill-Posed" Challenge: (Structured) Max. Margin Solution}
				To deal with the challenge that the problem is ill-posed, it is possible to enforce a predefined reward margin of \(1\) in the optimization problem. The optimization problem can be written as
				\begin{equation*}
					\begin{aligned}
						\min_{\vec{w}} \,   & \vec{w}^T \vec{w} \\
						\mathrm{s.t.} \quad &
						\begin{aligned}
							\forall\pi : \vec{w}^T \vec{\psi}(\pi^\ast) & \geq \vec{w}^T \vec{\psi}(\pi) + 1
						\end{aligned}
					\end{aligned}
				\end{equation*}
				where the optimization objective serves as regularization of the parameters. This enforced margin does not change the problem much as it can be absorbed in the weights/features (e.g. by introducing a bias feature), but does not allow \( \vec{w} = \vec{0} \) as solution anymore. This is called the \emph{max. margin} solution.

				A more sophisticated approach is to use a metric \( m(\pi^\ast, \pi) \) to measure the distance between two policies (e.g. the sum of minimum distances from the path generated by \(\pi^\ast\) for the path generated by \(\pi\)) and use this metric as the minimum reward margin:
				\begin{equation*}
					\begin{aligned}
						\min_{\vec{w}} \,   & \vec{w}^T \vec{w} \\
						\mathrm{s.t.} \quad &
						\begin{aligned}
							\forall\pi : \vec{w}^T \vec{\psi}(\pi^\ast) & \geq \vec{w}^T \vec{\psi}(\pi) + m(\pi^\ast, \pi)
						\end{aligned}
					\end{aligned}
				\end{equation*}
				The justification for this is that the reward margin should be larger for policies that are very different for \(\pi^\ast\).
			% end

			\subsubsection{Addressing the "Expert Suboptimality" Challenge: Slack Variables}
				Instead of enforcing all constraints, it is possible to add slack variables \(\xi\) to the inequality constraints to allow them to be violate a bit:
				\begin{equation*}
					\begin{aligned}
						\min_{\vec{w}, \xi} \, & \vec{w}^T \vec{w} + C \xi \\
						\mathrm{s.t.} \quad    &
						\begin{aligned}
							\forall\pi : \vec{w}^T \vec{\psi}(\pi^\ast) & \geq \vec{w}^T \vec{\psi}(\pi) + m(\pi^\ast, \pi) - \xi
						\end{aligned}
					\end{aligned}
				\end{equation*}
				The altering of the objective ensures that the slack variables do not get too high which could lead allowing improper solutions. This resolved the challenge of expert suboptimality.

				\todo{Slide 13.60, Suboptimal Expert Case}
			% end

			\subsubsection{Addressing the "Computation" Challenge: Constraint Generation}
				One method of dealing with the too much constraints is \emph{constraint generation}. Instead of using all policies \(\pi\) as constraints, only a certain set \( \Pi \) of policies is used as constraints. Policies are added iteratively (with the iteration variable \(k\)) to this set by computing the most-violated constraint
				\begin{equation*}
					\pi^{(k)} = \arg\max_\pi\, \vec{w}^{(k), T} \vec{\psi}(\pi) + m(\pi^\ast, \pi).
				\end{equation*}
				If no violations have been found, the iteration stops.
			% end
		% end

		\subsection{Feature Matching by Max. Entropy}
			The classical approach to statistical modeling is the principle of maximum entropy. The premise is that modeling should be performed with the least commitment possible, i.e. using the distribution with the maximum entropy (subject to certain constraints).

			\subsubsection{Max. Entropy Approach to Inference}
				Given the first- and second-order moments \(m_1\) and \(m_2\), the distribution with the maximum entropy is given as the solution of
				\begin{equation*}
					\begin{aligned}
						\max_p \,           & \int\! p(x) \big[\! -\!\ln p(x) \big] \dd{x} \\
						\mathrm{s.t.} \quad &
						\begin{aligned}[t]
							\int\! p(x) \dd{x}     & = 1   \\
							\int\! p(x) x \dd{x}   & = m_1 \\
							\int\! p(x) x^2 \dd{x} & = m_2
						\end{aligned}
					\end{aligned}
				\end{equation*}
				where the first constraint specifies that \(p(x)\) is actually a distribution. The solution to this problem is
				\begin{equation*}
					p(x) \propto \exp{ \lambda_1 x + \lambda_2 x^2 }
				\end{equation*}
				with the Lagrangian multipliers \(\lambda_{1, 2}\). This is a Gaussian distribution! So the Gaussian is the max. entropy distribution with given mean and variance.
			% end

			\subsubsection{Max. Entropy Approach to Inverse RL}
				Similarly to inference, the max. entropy approach can be applied to inverse RL with a trajectory distribution \( p(\vec{\tau}) \) and certain features \( \vec{\psi}(\vec{\tau}) \) that have to match \( \vec{\psi}(\pi^\ast) \). The optimization problem is
				\begin{equation*}
					\begin{aligned}
						\max_p \,           & \int\! p(\vec{\tau}) \big[\! -\!\ln p(\vec{\tau}) \big] \dd{\vec{\tau}} \\
						\mathrm{s.t.} \quad &
						\begin{aligned}[t]
							\int\! p(\vec{\tau}) \dd{\vec{\tau}}                        & = 1                    \\
							\int\! p(\vec{\tau}) \vec{\psi}(\vec{\tau}) \dd{\vec{\tau}} & = \vec{\psi}(\pi^\ast)
						\end{aligned}
					\end{aligned}
				\end{equation*}
				and has the solution
				\begin{equation*}
					p(\vec{\tau}) \propto \exp{ \vec{w}^T \vec{\psi}(\vec{\tau}) }
				\end{equation*}
				with the Lagrangian multipliers \(\vec{w}\). This is a softmax distribution over the trajectories with the return \( r(\vec{\tau}) = \vec{w}^T \vec{\psi}(\vec{\tau}) \). The problem is that the optimization does not take the transition dynamics into account, i.e. there might be trajectories with a huge return that are unlikely due to the dynamics.
			% end

			\subsubsection{Maximum-Casual-Entropy Inverse RL}
				In contrast for "normal" max. entropy for inverse RL, \emph{maximum-causal-entropy inverse RL} also includes the state dynamics as a constraint:
				\begin{equation*}
					\begin{aligned}
						\max_\pi \,         & \sum_t \iint\! \mu_t(\vec{s}) \pi_t(\vec{a} \given \vec{s}) \ln \pi_t(\vec{a} \given \vec{s}) \dd{\vec{s}} \dd{\vec{a}} \\
						\mathrm{s.t.} \quad &
						\begin{aligned}[t]
							\forall \vec{s} : \int\! \pi(\vec{a} \given \vec{s})                                                                           & = 1                    \\
							\sum_t \int\! \mu_t(\vec{s}) \vec{\phi}(\vec{s})                                                                               & = \vec{\psi}(\pi^\ast) \\
							\iint\! \mu_{t - 1}(\vec{s}) \pi_{t - 1}(\vec{a} \given \vec{s}) p(\vec{s}' \given \vec{s}, \vec{a}) \dd{\vec{s}} \dd{\vec{a}} & = \mu_t(\vec{s}_t')
						\end{aligned}
					\end{aligned}
				\end{equation*}
				As usual, the first constraint specifies that \(\pi\) is actual a distribution. The second and third constraint specify that the features are matched and that the transition dynamics distribution is consistent. The solution to this optimization problem is
				\begin{equation*}
					\pi_t(\vec{a} \given \vec{s}) \propto \exp{ \vec{w}^T \vec{\phi}(\vec{s}) + \E\big[ V_{t + 1}(\vec{s}') \biggiven \vec{s}, \vec{a} \big] }
				\end{equation*}
				where \( \vec{w} \) and \( V_t(\vec{s}) \) are Lagrangian multipliers. If \( r(\vec{s}) = \vec{w}^T \vec{\phi}(\vec{s}) \) is the reward, then this is a softmax over the Q-function:
				\begin{equation*}
					\pi_t(\vec{a} \given \vec{s}) \propto \exp{ Q(\vec{s}, \vec{a}; \vec{w}) }
				\end{equation*}
				As this is still a convex problem, the solution can be obtained by solving the dual relatively efficiently.
			% end
		% end

		\subsection{Reward-Parameterized Policies}
			Alternatively, it is also possible to assume that the policy is a softmax over the Q-function
			\begin{equation*}
				\pi_t(\vec{a} \given \vec{s}; r, \alpha) \propto \exp{ \alpha Q^\ast(\vec{s}, \vec{a}; r) }
			\end{equation*}
			where \(Q^\ast\) is the optimal Q-function for the reward \(r(\vec{s})\). Then the likelihood of a set of state-action pairs can be evaluated as
			\begin{equation*}
				\log p(\mathcal{D} \given r, \alpha) = \sum_i \log \pi(\vec{a}_i \given \vec{s}_i; r, \alpha).
			\end{equation*}
			This is like a "smarter" approach to behavioral cloning. In fact, it can be shown that this is equivalent. It can also be extended to a Bayesian setup by introducing a prior \( p(r, \alpha) \):
			\begin{equation*}
				p(r, \alpha \given \mathcal{D}) \propto p(\mathcal{D} \given r, \alpha) p(r, \alpha)
			\end{equation*}
		% end
	% end

	\section{Behavioral Cloning vs. Inverse RL}
		Behavioral cloning is
		\begin{itemize}
			\item simple to implement
			\item imposes no assumptions on the model, but
			\item it might not be possible to reproduce the long-term behavior.
		\end{itemize}
		For representation a \emph{policy} is used which is hard to generalize and that needs many samples.

		Inverse RL is
		\begin{itemize}
			\item harder to implement and
			\item requires solving a Markov decision process which is hard for many interesting MDPs.
			\item But if successful, reproducing the long-term behavior is possible.
		\end{itemize}
		For representation the reward function is used which is a compact description and easily transfers to new tasks.
	% end
% end

\chapter{Bayesian Reinforcement Learning}
	This chapter covers how to apply Bayesian methods to reinforcement learning. If applicable, they provide a principled way to handle the exploration-exploitation tradeoff by introducing (epistemic) uncertainty on the model or the policy. This can also be seen as regularization on greedy policy updates avoiding aggressive model exploitation. Active learning can also be used for guided exploration with information-theoretic objectives. Bayesian RL can also be used to create risk-sensitive and robust controllers as risk as well as perturbations of certain parameters and be encoded as uncertainty in the model. For MBRL, this was already briefly covered in \autoref{sec:mbrlBayesian}.

	The major problem with Bayesian methods is that they are really complicated. In Bayesian linear regression, for example, the distribution over the output variables is given as the marginalization over the weights
	\begin{equation*}
		p(\vec{y} \given \vec{x}, \mathcal{D}) = \int\! p(\vec{y} \given \vec{x}, \vec{w}, \mathcal{D}) p(\vec{w} \given \mathcal{D}) \dd{\vec{w}}
	\end{equation*}
	with the dataset \(\mathcal{D}\) and the posterior
	\begin{equation*}
		p(\vec{w} \given \mathcal{D}) = \frac{p(\mathcal{D} \given \vec{w}) p(\vec{w})}{\int\! p(\mathcal{D} \given \vec{w}') p(\vec{w}') \dd{\vec{w}'}}.
	\end{equation*}
	These two integrals are often hard to solve as they involve integrating over all dimensions of \(\vec{w}\) and over nonlinear distributions. This motivates the concept of \emph{conjugate priors}: a prior \( p(\vec{w}) \) is \emph{conjugate} to the likelihood \( p(\mathcal{D} \given \vec{w}) \), if both the prior and the posterior belong to the same family of distributions. This allows (per definition) closed form solutions for Bayes rule which has the advantage that an update can be computed in real-time which is important to execute policies online. Some likelihoods and their respective conjugate priors are shown in \autoref{tab:conjugatePrior}.

	Take a Bernoulli likelihood \( \mathrm{Bernoulli}(y \given p) = p^y (1 - p)^{1 - y} \) with \( y \in \{ 0, 1 \} \) for example. The conjugate prior is a Beta distribution \( \mathrm{Beta}(p; \alpha, \beta) \) distribution with the hyperparameters \( (\alpha, \beta) \). The posterior then is the updated Beta distribution \( p(p \given y) = \mathrm{Beta}(p; \alpha + y, \beta - y + 1) \).

	But even when using conjugate priors, being Bayesian is still hard: most interesting Bayesian models include nonlinear transformations requiring approximate inference (e.g. a Gaussian process has no closed-form solution for regression due to the required nonlinear transformation into categorical labels). This problem has already been discussed in \autoref{c:stateEstimation}.

	\begin{table}
		\centering
		\begin{tabular}{c|c|c}
			\textbf{Likelihood} \( p(\mathcal{D} \given \vec{w}) \) & \textbf{Parameter} \( \vec{w} \) & \textbf{Conjugate Prior} \( p(\vec{w}) \) \\ \hline
			Bernoulli                                               & \(p\)                            & Beta                                      \\
			Poisson                                                 & \(\lambda\)                      & Gamma                                     \\
			Gaussian                                                & \(\mu\)                          & Gaussian                                  \\
			Gaussian                                                & \(\sigma^2\)                     & Inverse Gamma
		\end{tabular}
		\caption{Examples for Conjugate Likelihood-Prior-Pairs}
		\label{tab:conjugatePrior}
	\end{table}

	\section{Bandits and Thompson Sampling}
		Bandits are (due to their simplicity) great settings for exploring Bayesian policies and the exploration-exploitation tradeoff. They are essentially stateless MDPs with a reward distribution \( p(r \given a) \) that can be used to model various environments like targeted advertising. The optimization problem is. A \(k\)-armed bandit is a bandit with \(k\) actions \( \dotsrange{a_1}{a_k} \). The idea of \emph{Thompson sampling} is to build a model \( p(r \given a) \) of the reward and use it decision making. In Thompson sampling, the actions are sampled from a \emph{belief}. While it was originally not motivated for a Bayesian setting, it can be used to perform Bayesian RL.

		\subsection{Restaurant Selection}
			Consider three restaurants \(a_1\), \(a_2\), and \(a_3\) and the binary reward \( r \in \{ 0, 1 \} \) where \(1\) encodes that the meal was delicious and \(0\) encodes that it was not. Let the reward be
			\begin{equation*}
				p(r = 1 \given a) =
				\begin{cases}
					0.05 & \text{for } a = a_1 \\
					0.1  & \text{for } a = a_2 \\
					0.85 & \text{for } a = a_3
				\end{cases}
			\end{equation*}
			so every restaurant is capable of serving delicious meals, but with different odds. Three strategies for maximizing the reward are now:
			\begin{enumerate}
				\item Choose randomly every day.
				\item Build an empirical model of \( p(r \given a) \) and use \(\epsilon\)-greedy sampling.
				\item Build a Bayesian model of \( p(r \given a) \) and sample from the posterior \( p(a \given r) \).
			\end{enumerate}
			Obviously the first approach will not maximize the reward. This section focuses on the third approach. For the model for \( p(r \given a) \), simply calculate the frequencies of the past rewards for each restaurant -- this yields a binomial likelihood distribution. Then put a Prior \( \mathrm{Beta}(p_a; \alpha_a, \beta_a) \) on the probabilities \( p_a \) for each restaurant. With \( \alpha_a = \beta_a = 2 \), the prior has a mean and mode of \(0.5\) which facilitates exploration. The posterior \( p(a \given r) = \mathrm{Beta}(a \given r; \tilde{\alpha}_a, \tilde{\beta}_a) \) can then be computed in closed form as the Beta distribution is a conjugate prior for the Binomial likelihood. The new parameters are
			\begin{align*}
				\tilde{\alpha}_a = \alpha_a + \sum_{i = 1}^{n_a} r_a^{[i]}
				 &  &
				\tilde{\beta}_a = \beta_a + n_a - \sum_{i = 1}^{n_a} r_a^{[i]}
			\end{align*}
			where \( (\alpha_a, \beta_a) \) are the parameters of the prior, \(n_a\) are the frequencies and \(r_a^{[i]}\) are the reward for each restaurant \(a\). In practice, it is easier to compute the joint distribution for each restaurant and marginalize as needed.

			This Bayesian approach does not require tuning any hyperparameters and converges to the optimal policy! While \(\epsilon\)-greedy methods can be tuned to have faster convergence, it is suboptimal due to constant exploration. To achieve the same optimality, more sophisticated \(\epsilon\)-greedy methods are needed where the exploration rate decreases over time (e.g. linear annealed exploration).
		% end
	% end

	\section{Model-Based Bayesian RL for Discrete MDPs}
		In model-based Bayesian RL, an MDP with unknown model parameters (the transition and reward functions) are solved. This section will only give a shallow overview on Bayesian MBRL for discrete MDPs and for exploration and exploitation work together to find an optimal policy. An MDP \( \mathcal{M} = (\mathcal{S}, \mathcal{A}, r, P) \) is build from the discrete state space \(\mathcal{S}\), the discrete action space \(\mathcal{A}\), the reward function \( r : \mathcal{S} \times \mathcal{A} \to \R \), and the transition probabilities \( P : \mathcal{S} \times \mathcal{A} \to [0, 1] \) as a distribution over the states. If \(P\) is known, this is a rather easy problem where the value function and the optimal policy can be found using dynamic programming. In the RL setting, \(P\) is unknown, but it can be parameterized by \(\vec{\theta}\): \( P(s' \given s, a; \vec{\theta}) \). For Bayesian RL, the state space is expanded with a \emph{belief} distribution \( b(\vec{\theta}) \) over the transition parameters. The modified MDP -- a Bayes Adaptive MDP (BAMDP) -- has the new state space \( \mathcal{S} \times \mathcal{B} \) where \(\mathcal{S}\) are the MDP states and \(\mathcal{B}\) are the belief states. The transition distribution then becomes \( P(s', b' \given s, b, a) \) and is known! As \(P\) is known, the MDP becomes a "simple" planning problem with a value function \( V : \mathcal{S} \times \mathcal{B} \to \R \).

		The two remaining questions on BAMDPs are:
		\begin{itemize}
			\item What exactly is a belief \(b(\vec{\theta})\)?
			\item Why is the transition distribution \(P(s', b' \given s, b, a)\) known?
		\end{itemize}

		\subsection{Belief}
			The belief \(b(\vec{\theta})\) is a distribution over the parameters \(\vec{\theta}\) of the transition model. For discrete states, the transition distribution is simply a multinomial and the parameters are the transition probabilities
			\begin{equation*}
				\theta_{s, a, s'} = P(s' \given s, a) \in [0, 1].
			\end{equation*}
			As \(\vec{\theta}\) groups all the transitions, its dimensionality is \( \lvert \mathcal{S} \rvert^2 \cdot \lvert \mathcal{A} \rvert \). The distribution \( b(\vec{\theta}) \) can be chosen freely, but as the transition distribution is a multinomial, it is wise to choose the conjugate prior, a Dirichlet distribution, for the belief. A Dirichlet distribution \( \mathrm{Dir}(\vec{\alpha}) \) is parameterized by \( \alpha_i > 0 \), \( i = \dotsrange{1}{k} \), \( k \geq 2 \) and the probability density function is
			\begin{equation*}
				\mathrm{Dir}(\vec{\theta}; \vec{\alpha}) \propto \prod_{s, a, s'} \theta_{s, a, s'}^{\alpha_{s, a, s'} - 1}.
			\end{equation*}
			When choosing a Dirichlet distribution as the prior for \( \vec{\theta} \), i.e. the belief \( b(\vec{\theta}) = \mathrm{Dir}(\vec{\theta}; \vec{\alpha}) \), the parameters \( \vec{\alpha} \) are hyperparameters.
		% end

		\subsection{State Transition Model}
			The state transition model \( P(s', b' \given s, b, a) \) decomposes into a belief and a physical model:
			\begin{equation*}
				P(s', b' \given s, b, a) = \underbrace{P(b' \given s, b, a, s')}_\text{Belief Model} \underbrace{P(s' \given s, b, a)}_\text{Physical Model}
			\end{equation*}
			The physical model is known as the parameters \(\vec{\theta}\) of the transition model \( P(s' \given s, a; \vec{\theta}) \) can be integrated out over the belief \( b(\vec{\theta}) \):
			\begin{equation*}
				P(s' \given s, b, a) = \E_{b(\vec{\theta})}\big[ P(s' \given s, a; \vec{\theta}) \big] = \int\! b(\vec{\theta}) P(s' \given s, a; \vec{\theta}) \dd{\vec{\theta}}
			\end{equation*}
			This leaves the belief model to be unknown. But as \(b(\vec{\theta})\) is a conjugate prior for the likelihood \( P(s' \given s, a; \vec{\theta}) \), it is possible to just set the believe model to one if the next belief equals the posterior belief:
			\begin{equation*}
				P(b' \given s, b, a, s') =
				\begin{cases}
					1 & \text{if } b'(\vec{\theta}) = b(\vec{\theta} \given s, a, s') \\
					0 & \text{otherwise}
				\end{cases}
			\end{equation*}

		% end

		\subsection{Optimal Value Function for BAMDPs}
			The Bellman equation for BAMDPs is
			\begin{equation*}
				V^\ast(s, b) = \max_a\, \Big(\! r(s, a) + \gamma \!\sum_{\substack{s' \,\in\, \mathcal{S} \\ b' \,\in\, \mathcal{B}}} P(s' \given s, b, a) V^\ast(s', b') \!\Big),
			\end{equation*}
			for which all components are known decently! Hence, the BE can be solved using dynamic programming. This maximization selects actions that maximize the reward and actions that further explore the environment in a natural way by incorporating the belief in the objective. If only the current belief \(b\) would be used in the maximization, pure exploitation would be done as the change in the belief would be taken into account.

			The BAMDP can be recast as a POMDP with the belief as the hidden variable. In practice, these problems do not scale well with the number of states, making direct implementations infeasible for most problems. Instead different ways are used to approximate the value function and solving the Bellman equation, for example in the BEETLE algorithm\footnote{Poupart et al.: "An Analytic Solution to Discrete Bayesian Reinforcement Learning" (2006)}.
		% end
	% end

	\section{Continuous MDPs and Dual Control}
		In continuous optimal control, the idea of trading off exploration and exploitation in a principled way is older than for discrete MDPs. Active learning was already covered in \autoref{c:modelLearning} for optimal exploration, and this section covers the combination of optimal control and optimal exploration: \emph{dual control}. Learning from sequential interactions is also known as \emph{continual learning}. Similar to discrete MDPs, the dynamics model \( p(\vec{s}_{t + 1} \given \vec{s}_t, \vec{a}_t) \) is parameterized, making it possible to put a prior on the parameters \(\vec{w}\):
		\begin{equation*}
			p(\vec{s}_{t + 1} \given \vec{s}_t, \vec{a}_t) = \int\! p(\vec{s}_{t + 1} \given \vec{s}_t, \vec{a}_t, \vec{w}) p(\vec{w}) \dd{\vec{w}}
		\end{equation*}
		By also adding a prior \(p(\vec{s}_t)\) on the state, it is possible to integrate the state out too (where \( \vec{z}_t \coloneqq (\vec{s}_t, \vec{w}) \)):
		\begin{equation*}
			p(\vec{s}_{t + 1})
			= \iint\! p(\vec{s}_{t + 1} \given \vec{s}_t, \vec{a}_t, \vec{w}) p(\vec{s}_t) p(\vec{w}) \dd{\vec{s}_t} \dd{\vec{w}}
			\doteq \int\! p(\vec{s}_{t + 1} \given \vec{z}_t, \vec{a}_t) p(\vec{z}_t) \dd{\vec{z}_t}
		\end{equation*}
		This way incremental learning can be incorporated into the dynamics model
		\begin{equation*}
			p(\vec{w} \given \vec{s}_{t + 1}) \propto p(\vec{s}_{t + 1} \given \vec{w}) p(\vec{w})
		\end{equation*}
		where now \( \vec{w} \) changes with the state, so it becomes time-dependent too: \( \vec{w} \to \vec{w}_t \), \( \vec{z}_t = (\vec{s}_t, \vec{w}) \). Maximizing the reward whilst anticipating the future \( p(\vec{w}_t) \) then allows optimal exploration. Unfortunately, dual control is rather intractable\dots

		\subsection{One-Dimensional Linear Gaussian Dual Control}
			Assume a one-dimensional stationary LQR problem with the transition distribution
			\begin{equation*}
				p(s_{t + 1} \given s_t, a_t) = \mathcal{N}\big( s_{t + 1} \biggiven A s_t + B a_t,\, \sigma_\eta^2 \big)
			\end{equation*}
			and the reward function\footnote{The reward gain \(R\) was omitted as a one-dimensional reward function can be normalized w.r.t. \(R\) by dividing both errors by \(R\).} \( r(s_{t + 1}, a_t) = -s_{t + 1}^2 - H a_t^2 \). The optimal one-step action can be derived by maximizing \( r(s_{t + 1}, a_t) \):
			\begin{align*}
				                                                                                 &                                                                                        & \pdv{\E_{s_{t + 1}}\big[ r(s_{t + 1}, a_t) \biggiven a_t \big]}{a_t}
				                                                                                 & = -\pdv{a_t} \Big( \E_{s_{t + 1}}\big[ s_{t + 1}^2 \biggiven a_t \big] + H a_t^2 \Big)
				= -\pdv{a_t} \Big(\! \big( A s_t + B a_t \big)^2 + \sigma_\eta^2 + H a_t^2 \Big) &                                                                                                                                                                                                                                    \\
				                                                                                 &                                                                                        &                                                                      & = -\pdv{a_t} \Big(\! \big( A s_t + B a_t \big)^2 + H a_t^2 \Big)
				\propto (A s_t + B a_t) B + H a_t \overset{!}{=} 0                               &                                                                                                                                                                                                                                    \\
				                                                                                 &                                                                                        & \implies a_t^\ast                                                    & = -\frac{A B s_t}{H + B^2}                                       &
			\end{align*}
			Considering a Bayesian time-varying uncertainty model \( B_t \sim \mathcal{N}(\mu_{B, t}, \sigma_{B, t}^2) \), the "certainty equivalent" optimal one-step control is:
			\begin{align*}
				 &                                                                                             & \pdv{\E_{s_{t + 1}, B_t}\big[ r(s_{t + 1}, a_t) \biggiven a_t \big]}{a_t}
				 & = -\pdv{a_t} \Big( \E_{s_{t + 1}, B_t}\big[ s_{t + 1}^2 \biggiven a_t \big] + H a_t^2 \Big) &                                                                                                                                                                                                                       \\
				 &                                                                                             &                                                                           & = -\pdv{a_t} \Big( \E_{\eta, B_t}\big[ (A s_t + B_t a_t + \eta)^2 \biggiven a_t \big] + H a_t^2 \Big)                                   & \\
				 &                                                                                             &                                                                           & \oversetfootnotemark{=} -\pdv{a_t} \Big(\! \big( A s_t + \mu_{B, t} a_t \big)^2 + \sigma_{B, t}^2 a_t^2 + \sigma_\eta^2 + H a_t^2 \Big) & \\
				 &                                                                                             &                                                                           & \propto (A s_t + \mu_{B, t} a_t) \mu_{B, t} + \sigma_{B, t}^2 a_t + H a_t \overset{!}{=} 0                                              & \\
				 &                                                                                             & \implies a_t^\mathrm{CE}                                                  & = -\frac{A \mu_{B, t} s_t}{H + \mu_{B, t}^2 + \sigma_{B, t}^2}
			\end{align*}
			This result has the interesting property that as the uncertainty \( \sigma_{B, t}^2 \) increases, the control input decreases! This is the uncertainty-based regularization that was introduced earlier on and is usually known as the "caution" or "turn-off phenomenon".
			\footnotetext{
				The expectation can be computed by considering the joint distribution \( \mathcal{N}(\vec{z} \given \vec{\mu}_z, \mat{\Sigma}_z) \) with \( \vec{z} \coloneqq \begin{bmatrix} B_t & \eta \end{bmatrix}^T \), \( \vec{\mu}_z \coloneqq \begin{bmatrix} \mu_{B, t} & 0 \end{bmatrix}^T \) and \( \mat{\Sigma} = \diag\!\big(\sigma_{B, t}^2, \sigma_\eta^2\big) \) and then computing the expectation \( \E\big[ (\vec{z} - \vec{s})^T \mat{M} (\vec{z} - \vec{s}) \big] \) with \( \vec{s} \coloneqq \begin{bmatrix} 0 & -As_t \end{bmatrix}^T \) and \( \mat{M} \coloneqq \begin{bmatrix} a_t^2 & a \\ a & 1 \end{bmatrix} \) using equation (380) from the matrix cookbook.
			}

			% TODO: Finish 1D LQR Dual Control! Slides: 14.47, 14.48, 14.49, 14.50
		% end

		\subsection{Practical Dual Control}
			Achieving dual control in practice is an open research topic due to the quasi-intractable nature of dual control. One current approaches are to perform certainty equivalence actions while computing the local uncertainties in an inner loop, and approximating dual control in an outer loop (in a MPC fashion). Another approach is to relax the continual learning requirement by focusing on the episodic setting, which is closer to a planning problem with active learning.
		% end
	% end

	\section{Wrap-Up}
		This chapter covered Bayesian reinforcement learning methods which have a lot to offer, especially regarding exploration, regularization and robustness of policies. But while theoretically appealing, these methods are hard to implement in practice. In a discrete setting, the MDP can be modified into an BAMDP, a kind of POMDP, with a belief over the model parameters. This makes it possible to solve the MDP in a Bayesian fashion, but is computationally very expensive. In continuous control, the problem of exploring and planning simultaneously is older and is known as dual control. But this can not be achieved in closed form\dots

		A common method for Bayesian exploration is Thompson sampling which samples from the posterior policy to achieve said optimal exploration.
	% end
% end

\chapter{Outlook}
	This "summary" covered lots of topics in the intersection of robotics and machine learning. Separated by chapter, the key terms that should now be known are:
	\begin{description}[leftmargin=5cm]
		\item[Robotics] Kinematics, Dynamics, DH Parameters, Inverse Kinematics, Feedback, Feedforward, PID Control, Splines, Task-Space Control, Null Space, Singularities
		\item[Machine Learning] Bayes Rule, Regression, Classification, Maximum Likelihood, Bias, Variance, Gaussian Processes, Neural Networks, Backpropagation, Gradient Estimation
		\item[Optimal Control] Dynamic Programming, Bellman Equation, Policy Iteration, Value Iteration, Policy Evaluation, Linear Quadratic Regulator, Riccati Equation
		\item[Approximate OC] Differential Dynamic Programming, Gauss-Newton Approximation, Line Search, Approximate Dynamic Programming
		\item[State Estimation] % TODO
		\item[Model Learning] White-Box, Black-Box, Gray-Box, System Identification, Impulse Response, Active Learning, Linear Gaussian Dynamical System, Differentiable Physics
		\item[Policy Representations] Out-of-the-Box Policies, Dynamical Systems, Movement Primitives, Stability, Stochastic Dynamical Systems
		\item[Model-Based RL] Sample Efficiency, Optimism, Aleatoric Uncertainty, Epistemic Uncertainty, Model-Predictive Control
		\item[Value Function Methods] Temporal Difference, Q-Learning, SARSA, Batch RL, Least Squares Temporal Differences (LSTD), Fitted Q-Iteration
		\item[Policy Gradient] Step-Based, Episode-Based, Finite Differences, Likelihood Ratio, Baselines, REINFORCE, Fisher Information Matrix, Natural Gradient
		\item[Probabilistic Policy Search] Weighted Maximum Likelihood, Relative Entropy, Contexts, Hierarchy
		\item[Imitation Learning] Distribution Matching, Covariate Shift, Behavioral Cloning, Inverse Reinforcement Learning
		\item[Bayesian RL] Bandits, Thompson Sampling, Belief Space, Dual Control, Continual Learning, Caution, Turn-Off Phenomenon, Certainty Equivalence
	\end{description}

	But still, there are a lot of open research questions, for example:
	\begin{itemize}
		\item How to derive robot learning algorithms with guarantees (e.g. convergence, stability, optimality)?
		\item How to make interpretable robot learning algorithms (explainable AI)?
		\item How to combine and balance model-free and model-based methods?
		\item How to incorporate robustness into machine learning methods?
		\item What are good inductive biases and prior?
		\item What are good tasks for benchmarking robot learning algorithms?
		\item Will dual control ever be tractable?
	\end{itemize}
% end

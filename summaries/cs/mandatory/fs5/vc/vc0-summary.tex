\documentclass[a4paper, 11pt, accentcolor = tud3b]{tudreport}

% Core packages.
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
% Other packages.
\usepackage[german, ruled, vlined, linesnumbered]{algorithm2e}
\usepackage{amsthm}
\usepackage{caption}
\usepackage{csquotes}
\usepackage{enumitem}
\usepackage[mathcal]{euscript} % Get readable mathcal font.
\usepackage{float}
\usepackage{pgfplots}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{siunitx}
\usepackage{qtree}
\usepackage{stmaryrd}
\usepackage{tabto}
\usepackage{tikz}
\usepackage[disable]{todonotes}
\usetikzlibrary{arrows.meta, shapes, backgrounds, angles, calc, decorations.markings, positioning}

% Basic information.
\title{Visual Computing}
\subtitle{Zusammenfassung \\ Fabian Damken}
\author{Fabian Damken}
\date{\today}

% Description-list styling.
\SetLabelAlign{parright}{\parbox[t]{\labelwidth}{\raggedleft#1}}
\setlist[description]{style = multiline, leftmargin = 4cm, align = parright}

\colorlet{colorDensity}{tud1b}

\tikzset{density/.style = { colorDensity, line width = 2pt }}
\tikzset{declare function = { gaussian(\x,\m,\S) = 1/sqrt(2*pi*\S) * e^(-1/(2*\S) * (\x-\m)^2); }}
\tikzset{> = { Latex[length = 2.5mm] }}
\tikzstyle{every path} = [ very thick ]

\MakeOuterQuote{"}

% New commands.
\DeclareMathOperator{\total}{d}
\DeclareMathOperator{\Rang}{Rang}
\DeclareMathOperator{\const}{const}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\sinc}{sinc}
\newcommand{\dif}[1]{\,\total#1}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\qef}{\hfill \( \square \)}
\newcommand{\given}{\,\vert\,}
% Matrix/Vector notation.
\makeatletter
% TODO: This does not make all symbols bold (e.g. 'v').
\newcommand{\mat}[1]{\boldsymbol{#1}}
% TODO: This adds crazy symbols for symbols that cannot be drawn bold (e.g. \dot{v} becomes \underline{v}).
\renewcommand{\vec}[1]{\boldsymbol{\mathbf{#1}}}
\makeatother
% Abbreviations.
\renewcommand{\dh}{d.\,h.~}
\newcommand{\bzw}{bzw.~}
\newcommand{\ca}{ca.~}
\newcommand{\bspw}{bspw.~}
\newcommand{\bzgl}{bzgl.~}
\newcommand{\gdw}{gdw.~}
\newcommand{\zB}{z.\,B.~}
\newcommand{\iA}{i.\,A.~}
\newcommand{\ggf}{ggf.~}
\newcommand{\mglw}{mglw.~}
\newcommand{\vs}{vs.~}
\newcommand{\DIRKIN}{DIR\,KIN~}
\newcommand{\INVKIN}{INV\,KIN~}
\newcommand{\DIRJAC}{DIR\,JAC~}
\newcommand{\INVJAC}{INV\,JAC~}
\newcommand{\DIRDYN}{DIR\,DYN~}
\newcommand{\INVDYN}{INV\,DYN~}

% https://tex.stackexchange.com/a/333383
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
	\hskip -\arraycolsep
	\let\@ifnextchar\new@ifnextchar
	\array{#1}}
\makeatother

\begin{document}
	\maketitle
	\tableofcontents
	\listoftodos

	\chapter{Einführung} % 1.1, 1.10
		\todo{Content}

		\section{Visual Computing} % 1.1, 1.18, 1.30, 1.31, 1.32, 1.33, 1.34, 1.50
			\todo{Content}

			\subsection{3D-Internet} % 1.35, 1.36, 1.37
				\todo{Content}
			% end

			\subsection{Skalierbare Objektmodellierung/-erkennung} % 1.38, 1.39, 1.40, 1.41, 1.42, 1.43
				\todo{Content}
			% end

			\subsection{Big Data, Visual Analytics} % 1.44, 1.45, 1.46
				\todo{Content}
			% end

			\subsection{Scene Understanding} % 1.47, 1.48, 1.49
				\todo{Content}
			% end
		% end

		\section{Generalisierte Dokumente} % 1.51, 1.52, 1.53, 1.54, 1.55, 1.56, 1.68
			\todo{Content}

			\subsection{Retro-Digitalisierung, Digital Creation} % 1.57, 1.58, 1.59
				\todo{Content}
			% end

			\subsection{Generative Modeling Language} % 1.62, 1.63, 1.64, 1.65, 1.66, 1.67
				\todo{Content}
			% end
		% end
	% end

	\chapter{Wahrnehmung}
		\section{Human-Computer-Interaction}
			Abbildung~\ref{fig:hci} zeigt den klassischen Zyklus der \emph{Human-Computer-Interaction} (HCI), \dh der Interaktion zwischen Mensch und Maschine. Dabei dient insbesondere die visuelle Interaktion und Kommunikation über das Auge eine große Rolle.
		
			\begin{figure}
				\centering
				\begin{tikzpicture}[->, every node/.style = { align = center }]
					\node [draw, rectangle] (human) {Human};
					\node [draw, rectangle, right = 4 of human] (computer) {Computer};
					
					\draw (human) to[bend right] node[below]{Explizite Eingabe, \\ Implizite Interaktion} (computer);
					\draw (computer) to[bend right] node[above]{Visuelle Ausgabe, \\ Andere Ausgabemodalitäten} (human);
				\end{tikzpicture}
				\caption{Klassischer Zyklus der Human-Computer-Interaction (HCI).}
				\label{fig:hci}
			\end{figure}
		% end

		\section{Überblick}
			Der Mensch hat fünf grundlegende Sinne: Sehen, Hören, Fühlen, Schmecken und Riechen, wobei das Sehen, Hören und Fühlen derzeit dominant sind. Der heute sicherlich relevanteste Sinn ist dabei das Sehen und das menschliche Auge. Da die meisten erzeugten Bilder der Kommunikation von und zum Menschen dienen sollen, ist es gut, das menschliche visuelle System zu kennen, um den Informationstransfer optimal zu gestalten (der Monitorausgang ist nicht das Ende des Informationsflusses).
			
			Hören und Fühlen sind dabei relevant für die Informationsaufnahme und Interaktion mit der realen Welt (außerhalb der Mensch-Maschine-Interaktion).
			
			Bei der Gestaltung von Kommunikation gibt es zwei große Probleme:
			\begin{itemize}
				\item Die Wahrnehmung ist nicht objektiv.
				\item Das visuelle System ist stark nichtlinear (es ist keine einfache Interpolation oder Extrapolation von Versuchsergebnissen möglich).
			\end{itemize}

			\subsection{Menschliche Informationsverarbeitung}
				Abbildung~\ref{fig:human_info_processing} zeigt die drei Stufen der menschlichen Informationsverarbeitung:
				\begin{itemize}
					\item \emph{Wahrnehmung} von Eindrücken durch die Sinne,
					\item \emph{Entscheidung}sfindungs im Gehirn und
					\item \emph{Reaktion} durch den Körper.
				\end{itemize}
				Dabei verhält sich die Ausführungszeit additiv und die Funktionen werden durch neurologisch getrennte Gehirnteile ausgeführt, die "elektronisch" verbunden sind.
				
				\begin{figure}
					\centering
					\begin{tikzpicture}[->, every node/.style = { draw, rectangle, minimum height = 0.8cm, minimum width = 5cm }]
						\node (a) {Wahrnehmung (Sensorik)};
						\node [below = 0.5 of a] (b) {Entscheidung (Kognition)};
						\node [below = 0.5 of b] (c) {Reaktion (Motorik)};
						
						\draw (a) -- (b);
						\draw (b) -- (c);
					\end{tikzpicture}
					\caption{Modulares Drei-Stufenmodell der menschlichen Informationsverarbeitung.}
					\label{fig:human_info_processing}
				\end{figure}
				
				Dabei braucht jede Bearbeitung in den einzelnen Stufen unterschiedlich lange und die benötigten Zeiten können verwendet werden, um die Performanz abzuschätzen, \bzw vorherzusagen (\bspw für die Bildfrequenz von Filmen, die maximale Morserate, \dots). Typische Zeiten sind in Tabelle~\ref{fig:human_processing_times} abgebildet.
				
				\begin{table}
					\centering
					\begin{tabular}{l|l|l}
						\textbf{Untersystem}     & \textbf{Durchschnitt}   & \textbf{Bereich}                 \\ \hline
						Wahrnehmung (Perzeption) & \SI{100}{\milli\second} & \SIrange{50}{200}{\milli\second} \\
						Entscheidung (Kognition) & \SI{70}{\milli\second}  & \SIrange{25}{170}{\milli\second} \\
						Reaktion (Motorik)       & \SI{70}{\milli\second}  & \SIrange{30}{100}{\milli\second}
					\end{tabular}
					\caption{Typische Bearbeitungszeiten der Untersysteme der menschlichen Informationsverarbeitung.}
					\label{fig:human_processing_times}
				\end{table}

				\subsubsection{Eingabe (Wahrnehmung)}
					Die Untersysteme der Wahrnehmung,
					\begin{itemize}
						\item Visuell (Sehen)
						\item Akustisch (Hören)
						\item Haptisch (Fühlen)
					\end{itemize}
					können dabei (theoretisch) parallel arbeiten.
				
					\paragraph{Klangwahrnehmung}
						Die Hauptkomponenten von Klängen sind
						\begin{itemize}
							\item Klangfarbe,
							\item Tonlage und
							\item Lautstärke.
						\end{itemize}
						Diese werden durch verschiedene Mechanismen wahrgenommen und Informationen (\zB der Ursprung eines Geräuschs) extrahiert.
					% end

					\paragraph{Berührungswahrnehmung}
						Die Hauptkomponenten der Haptik sind
						\begin{itemize}
							\item Fühl- und Tastsinn (Temperatur, Schmerz, Druck, Oberflächen) und
							\item Propriozeption (Wahrnehmung der Bewegung und Lage der eigenen Körperglieder).
						\end{itemize}
						Dabei interagiert die Haptik stark mit Sehen und Hören, was bei sich widersprechenden Informationen Illusionen hervorrufen kann. Ein User-Interface-Designer nutzt Illusionen dabei geziehlt aus, um bestimmte Informationen zu vermitteln.
					% end
				% end

				\subsubsection{Ausgabe (Reaktion)}
					Die Untersysteme der Reaktion,
					\begin{itemize}
						\item Artikulation (Sprechen)
						\item Motorisch (Bewegen)
					\end{itemize}
					können dabei (theoretisch) parallel arbeiten.
					
					Die motorische Ausgabe kann dabei auf verschiedene Weisen angewandt werden:
					\begin{itemize}
						\item Diskret (Schalter) oder
						\item Kontinuierlich (Heben).
					\end{itemize}
					Sie ist dabei beschränkt durch Geschwindigkeit, Stärke, Koordinationsvermögen, Wendigkeit, \dots. Neurologisch ist die motorische Ausgabe dabei mit dem haptischen System verbunden (Reflexe).
					
					Das \emph{Muskelgedächtnis} hilft dabei, relevante Positionen im Raum (\zB die Gangschaltung im Auto) zu lernen.
				% end
			% end
		% end

		\section{Wahrnehmung}
			\subsection{Das Auge}
				\subsubsection{Reiz und Licht}
					Einer äußerer, visueller Reiz (Licht) erzeugt beim Menschen eine physikalische Rezeption des äußeren Reizes (Input). Dies geschieht durch einen Sensor (\bspw das Auge) und die Reizung produziert ein neuro-physiologisches Signal. Dieses wird anschließend verarbeitet und interpretiert.
					
					Physikalisch ist ein solcher Reiz elektromagnetische Strahlung. Dabei wird monochromatisches, \dh einfarbiges, Licht durch die Angabe der Frequenz \(v\), \bzw der Wellenlänge \(\lambda\), beschrieben. Diese beiden Größen sind durch die Beziehung
					\begin{equation*}
						v \lambda = c, \quad c \approx \SI{3e8}{\meter\per\second}
					\end{equation*}
					miteinander verknüpft, wobei \(c\) die Ausbreitungsgeschwindigkeit des Lichts ist.
					
					Das menschliche Auge kann dabei Frequenzen im Wellenlängenbereich \( \SIrange{380}{750}{\nano\meter} \) wahrnehmen. Kleinere Wellenlängen haben \zB Ultraviolett-Licht, Röntgen- und \(\gamma\)-Strahlung. Darüber liegende Wellenlängen haben \zB Infrarot-Licht und Rundfunk-Wellen.
				% end

				\subsubsection{Das visuelle System}
					Das menschliche Auge ist aufgebaut aus:
					\begin{itemize}
						\item Hornhaut (Kornea)
						\item Linse (zur Scharfstellung)
						\item Iris (Blendenmechanismus)
						\item Retina (Netzhaut)
							\begin{itemize}
								\item Blinder Fleck: Hier geht der Sehnerv ab.
								\item Fovea Centralis (Gelber Fleck): Bereich mit der höchsten Auflösung.
							\end{itemize}
					\end{itemize}
				% end

				\subsubsection{Photorezeptoren}
					Die Photorezeptoren (welche auf der Retina platziert sind), bestehen aus:
					\begin{itemize}
						\item Stäbchen
							\begin{itemize}
								\item Hauptsächlich außerhalb der Fovea.
								\item Das Empfindlichkeitsmaximum liegt bei \SI{498}{\nano\meter} ("grün").
							\end{itemize}
						\item Zapfen
							\begin{itemize}
								\item Vor allem in der Fovea platziert.
								\item Es gibt drei Zapfentypen für Farbsehen.
								\item Das Empfindlichkeitsmaximum dieser Zapfen liegt bei \SI{420}{\nano\meter} ("blau"), \SI{534}{\nano\meter} ("grün") und \SI{564}{\nano\meter} ("rot").
							\end{itemize}
					\end{itemize}
				% end

				\subsubsection{Skotopisches und Photopisches Sehen}
					\begin{itemize}
						\item Nachtsehen (skotopisch): Dominanz der Stäbchen.
						\item Tagsehen (photopisch): Dominanz der Zapfen.
					\end{itemize}
				% end

				\subsubsection{Zapfenverteilung} % 2.38, 2.39, 2.40, 2.41, 2.42, 2.43, 2.44
					\todo{Content}
				% end
			% end

			\subsection{Vorverarbeitung visueller Informationen}
				\subsubsection{Signalverarbeitung in der Retina}
					Neben den Photorezeptoren gibt es noch weitere Zellen zur Signalverarbeitung in der Retina:
					\begin{itemize}
						\item Horizontale Zellen \\ Kombination von mehreren Rezeptoren einer Region.
						\item Amakrin-Zellen \\ Zeitliche Verarbeitung.
						\item Bipolar-Zellen \\ Informationsfilter (Sammeln, Gewichten und Weiterleiten).
						\item Ganglien-Zellen \\ Integration Informationen (\zB Kontrastwahrnehmung).
					\end{itemize}
				% end

				\subsubsection{Helligkeit}
					\begin{itemize}
						\item \emph{Helligkeit} (\emph{brightness}) entspricht der wahrgenommenen Menge an Licht, das von einer selbstleuchtenden Lichtquelle ausgeht.
						\item \emph{Helligkeit} (\emph{lightness}) entspricht der wahrgenommenen Menge an Licht, das von einer reflektierenden Oberfläche ausgeht.
							\begin{itemize}
								\item Dies ist keine absolute Wahrnehmungsgröße und abhängig von
									\begin{itemize}
										\item Reizstärke (Leuchtdichte)
										\item Vorherige Leuchtdichte (Adaption)
										\item Umgebungsleuchtdichte
										\item Größe (Fläche) des Reizes
									\end{itemize}
								\item Somit subjektiv!
							\end{itemize}
						\item Dies wirft einige nicht so einfach zu beantwortende Fragen auf, z.\,B.: Was ist weiß? Was ist schwarz? Was ist mittelgrau?
						\item Der Hell-Dunkel-Kontrast ist dabei eine wichtige Empfindungsgröße zum Form- und Objektsehen. Daher muss der Unterschied groß genug sein (für kleine Details mindestens \(3:1\), besser \(10:1\)).
					\end{itemize}

					\paragraph{Kontrast als Reizverhältnis}
						Für den Kontrast gibt es verschiedene Definitionen, \zB (dabei ist \(L\) stets die Leuchtdichte):
						\begin{equation*}
							m = k = \frac{L_\text{max} - L_\text{min}}{L_\text{max} + L_\text{min}}
						\end{equation*}
						oder
						\begin{equation*}
							K = \frac{L_R - L_H}{L_H} = \frac{\delta L}{L_H}
						\end{equation*}
						wobei \( L_R \) die Leuchtdichte des Vordergrunds und \(L_H\) die Leuchtdichte des Hintergrunds darstellt.
						
						\todo{Weber-Fechnersches Gesetz, Stevensches Gesetz; 2.65}
					% end
				% end

				\subsubsection{Erkennung von Details}
					Die Erkennung kleiner Details ist begrenzt durch
					\begin{itemize}
						\item Optische Eigenschaften des Auges, \zB Beugungserscheinungen,
						\item Abtastung durch Rezeptoren und
						\item nervöse Verarbeitung.
					\end{itemize}
					Zwei mögliche Maße zur "Erkennbarkeit" sind:
					\begin{itemize}
						\item Kontrastempfindlichkeit
						\item Schwellenkontrast
					\end{itemize}

					\paragraph{Kontrastempfindlichkeit}
						Die Kontrastempfindlichkeit ist die Auflösung des menschlichen Auges im Frequenzraum. Veränderliche Intensität kann dabei mit Sinus-förmigen Mustern gemessen werden.
					% end
				% end

				\subsubsection{Frühe Wahrnehmung}
					Das Auge nimmt einige Veränderungen der Umgebung schneller wahr als andere. Um die Aufmerksamkeit auf etwas zu lenken, können beispielsweise
					\begin{itemize}
						\item Farbe,
						\item Richtung,
						\item Bewegung,
						\item Größe,
						\item Beleuchtung/Schattierung
					\end{itemize}
					variiert werden.
				% end
			% end

			\subsection{Informationsextraktion}
				Ein reiner Reiz ist noch keine \emph{Wahrnehmung}. Dazu kommen noch andere Faktoren wie Kontext, Erwartungen, Adaption. Das Messen der tatsächlichen Wahrnehmung ist leider sehr schwierig, weshalb häufig nur statistische Aussagen auf Basis von User-Tests getätigt werden können.
				
				Dabei wird erschwert, dass die Wahrnehmung nicht immer der Realität entspricht. Es wird hingegen das Bild durch einen Wahrnehmungsprozess im Gehirn produziert. Dabei wird die menschliche Wahrnehmung adaptiert, \bspw dreht sich das Bild bei einem Kopfstand.

				\subsubsection{Raumwahrnehmung}
					Die Wahrnehmung des Raums (Raumwahrnehmung) enthält unter anderem
					\begin{itemize}
						\item Tiefenwahrnehmung,
						\item Entfernungs- und Distanzwahrnehmung und
						\item Ausrichtung des Körpers im Raum.
					\end{itemize}
					Daran sind viele Wahrnehmungssysteme beteiligt:
					\begin{itemize}
						\item Vestibuläres System (im Innenohr)
						\item Haptisch-somatisches System (Tasten und Berühren)
						\item Auditives Sehen (Gehört)
						\item Propriozeptives System (Eigenwahrnehmung)
						\item Visuelles System
					\end{itemize}
				
					Dabei ist die Raumwahrnehmung auch mit einem Auge (Monokular) möglich (tatsächlich sind \SIrange{5}{10}{\percent} aller Menschen stereoblind und \SI{20}{\percent} haben eine Stereo-Schwäche).
					
					Tatsächlich ist die Raumwahrnehmung ein sehr komplexer Prozess, der auch heute nur zu Teilen verstanden wird. Dabei fließen noch viele weitere Phänomene ein, \zB Größenkonstanz, Annahme starrer Körper oder Vektion. Letzteres ist dabei die scheinbare Eigenbewegung bei einem statischen Vordergrund als Referenzrahmen und einem bewegtem Hintergrund.
				% end

				\subsubsection{Depth Cue Theorie}
					Die Annahme der \emph{Depth Cue Theorie} ist, dass die Raumwahrnehmung des visuellen Systems auf Hinweisreizen (sogenannten \emph{Depth Cues}) basiert. Diese werden in drei Kategorien eingeteilt:
					\begin{enumerate}
						\item Binokulare Depth Cues (mit zwei Augen)
							\begin{itemize}
								\item Disparität/Parallaxe
								\item Akkomodation (Krümmung der Augenlinsen)
								\item Konvergenz (die Augen nach innen drehen)
							\end{itemize}
						\item Pictoiral Depth Cues (mit einem Auge)
							\begin{itemize}
								\item Linearperspektive
								\item Verdeckung
								\item Texturgradient
								\item Fokus und Blur
								\item Atmosphärische Tiefe
								\item Vertraute Größe
								\item Höhe im Gesichtsfeld
								\item Beleuchtung
								\item Schattenwurf
								\item Luminanzänderung
								\item Transluzenz
								\item Schattierung
							\end{itemize}
						\item Dynamische Depth Cues (Animation)
							\begin{itemize}
								\item Bewegungsparallaxe
								\item Kinetischer Tiefeneffekt
								\item Interposition
								\item Bewegung von Highlights
							\end{itemize}
					\end{enumerate}

					\paragraph{Stereoskopie}
						Bei der Stereoskopie nehmen beide Augen ein leicht unterschiedliches Bild wahr, woraus die Entfernung zu einem Objekt berechnet werden kann.
					% end

					\paragraph{Pictorial Depth Cues}
						\subparagraph{Linearperspektive} % 2.90
							\todo{Content}
						% end

						\subparagraph{Texturgradient}
							Sind als parallel angenommene Linien nicht mehr parallel, so ergibt sich eine scheinbare Tiefe (als wenn kariertes Papier um einen Ball gerollt und von oben betrachtet wird).
						% end

						\subparagraph{Fokus und Blur}
							Das Auge fokussiert einen Punkt und produziert somit eine Tiefenschärfe. Daran kann erahnt werden, welche Objekte im Vorder- oder Hintergrund sind.
						% end

						\subparagraph{Atmosphärische Tiefe}
							Anhand der Atmosphäre (\zB durch Nebel ausgelöst) wird erkannt, was vermutlich im Hintergrund liegt. So kann zum Beispiel bei einem Foto von einem Berg geschätzt werden, dass der Boden niedriger ist, wenn Wolken über diesem hängen.
						% end

						\subparagraph{Schattenwurf}
							Annahme: Beleuchtung von oben und Vorhandensein einer Grundebene. Dann kann durch den Abstand von Schatten zum Objekt erahnt werden, wie weit dieses vom Boden entfernt ist.
						% end
					% end

					\paragraph{Dynamische Depth Cues}
						\subparagraph{Motion Parallax} % 2.98
							\todo{Content}
						% end

						\subparagraph{Raumwahrnehmung durch Bewegung}
							Wird \zB eine schaukelnde Vase von oben betrachtet, so bewegt sich die Öffnung charakteristisch, sodass eine Wahrnehmung der Tiefe entsteht.
						% end

						\subparagraph{Kinetic Depth Effect, Structure from Motion} % 2.100
							\todo{Content}
						% end
					% end

					\paragraph{Auswertung von Depth Cues}
						Unterschiedliche Depth Cues haben im Allgemeinen einen unterschiedliche Informationsgehalt. Dabei sind sie nicht redundant, sondern additiv. Durch ein kompliziertes Zusammenspiel (flexible Gewichtung, Dominanz eines Depth Cue) bildet sich das Gehirn ein Bild. Dabei bildet es sich allerdings kein tatsächliches 3D-Modell, sondern verwendet sie unterschiedlichen Cues für verschiedene Aufgaben. Diese können \zB sein:
						\begin{itemize}
							\item Einschätzen von Objektgrößen
							\item Einschätzen von Entfernungen
							\item Verfolgung von Pfaden
							\item Navigation
							\item Einschätzen der Eigenbewegung
							\item Abschätzung der Kollisionszeit
						\end{itemize}
					% end
				% end
			% end
		% end

		\section{Aufmerksamkeit}
			\subsection{Limitierung der Wahrnehmung}
				Die initiale Reizaufnahme hat viele Limitieren, sodass nur ein Bruchteil des äußeren Reizes zur kognitiven Verarbeitung zur Verfügung steht, Dabei sind Aufmerksamkeit und externe Faktoren wichtige Einflüsse auf die tatsächliche Wahrnehmung. Die Wahrnehmung ist dabei eher eine partielle Hypothese, die auf Basis unvollständiger Informationen generiert wurde. Es wird dabei periodisch aktualisiert aufgrund von Beobachtungen, \dh die Hypothese wird gegen sensorische Daten getestet. Durch eine dynamische Suche des visuellen Systems wird nach der besten Hypothese/Interpretation/Modell gesucht.
			% end

			\subsection{Das Gedächtnis und "Gateway to Memory"}
				Das Gehirn kann sich auf bestimmte Dinge fokussieren und den Rest ignorieren. Dabei gibt es drei verschiedene Arten der Aufmerksamkeit:
				\begin{itemize}
					\item \emph{Gewählte Aufmerksamkeit} (selective): Zwischen mehreren Möglichkeiten wird eine zu fokussierende Sache aktiv ausgewählt.
						\begin{itemize}
							\item Das Auge folgt den Objekten von Interesse.
							\item Der Kopf folgt den Klängen von Interesse.
							\item Es gibt nur einen einzigen "Ort der Aufmerksamkeit".
						\end{itemize}
					\item \emph{Geteilte Aufmerksamkeit} (divided): Ein Versuch durch "Multitasking" mehrere Dinge zu fokussieren.
						\begin{itemize}
							\item Entweder "gleichzeitig" der durch schnelles Umschalte (time multiplexing).
							\item Dies wirkt sich negativ auf die Verarbeitung aus, wenn die Aufgaben überfordernd sind.
							\item Die Aufgaben beeinträchtigen sich gegenseitig.
						\end{itemize}
					\item \emph{Erfasste Aufmerksamkeit} (captured): Ein äußerer Reiz zieht alle Aufmerksamkeit auf sich.
						\begin{itemize}
							\item Im Gegensatz zur gewählten Aufmerksamkeit wird der "Ort" nicht aktiv ausgewählt.
							\item Dies geschieht \zB wenn man von einem Tier angefallen wird.
						\end{itemize}
				\end{itemize}
			
				Das menschliche Gedächtnis ist in mehrere "Teilgedächtnisse" aufgeteilt. Voran steht das \emph{Arbeitsgedächtnis}, auf das ein schneller Zugriff (\ca \SI{70}{\milli\second}) möglich ist, welches aber einen schnellen Verfall hat (nach \ca \SI{200}{\milli\second}). Nach wenigen Sekunden wird der Inhalt jedoch an das Langzeitgedächtnis weitergegeben. Es stellt sozusagen das "Schmierblatt" des Gehirns da.
				
				Das Langzeitgedächtnis ist langsamer (\ca \SI{100}{\milli\second}), dafür aber auch sehr viel größer (die genaue Größe ist unbekannt). Das Langzeitgedächtnis hat dabei drei Hauptaufgaben:
				\begin{itemize}
					\item Informationen speichern und sich an diese erinnern,
					\item Informationen abrufen und
					\item Informationen vergessen.
				\end{itemize}
			% end
		% end
	% end

	\chapter{Computer Vision: Objekterkennung und Bayes}
		Die \emph{Computer Vision} beschäftigt sich mit dem maschinellen Sehen, \dh der Suche nach einem Modell des menschlichen Sehens. Anwendungsgebiete sind \bspw Autos, die Fußgänger erkennen, medizinische Bildverarbeitung, Überwachung, Unterhaltung, Computergraphik, \dots.

		\section{Computer Vision}
			Das einfachste Standardmodell einer Lochkamera ist ein Kasten mit einem kleinen Loch. Um ein digitales Bild eines solchen Kameramodells zu erhalten, wird das Bild rasterisiert. Demnach ist ein Graustufenbild eine Matrix an Pixeln mit jeweils einem Wert (die "Grauigkeit" des Pixels).
			
			Die Computer Vision beschäftigt sich nun damit, aus einem solchen Bild Informationen zu extrahieren. Bei der Objekterkennung ist es wichtig, eine gute lokale Beschreibung/Merkmale zu haben (\zB Augen, Mund, Nase) und eine globale Anordnung der lokalen Merkmale (\zB relative Positionen, relative Größen). Es ist aber auch eine schnelle Generierung guter Hypothesen, Segmentierung der Bildbereiche und kennen des Szenenkontextes wichtig.
			
			Nach Fischler und Elschlager hat das Modell eines Bildes zwei Komponenten: Teile (2D Bildfragmente) und den Aufbau (die Anordnung der Teile). Mit diesem abstrakten Modell lassen sich viele Dinge (\zB ein Gesicht) charakterisieren.
		% end

		\section{Bayesian Decision Theory}
			Beispiel: Buchstabenerkennung. Es soll ein neu aufgenommener Buchstabe so klassifiziert werden, dass die Wahrscheinlichkeit der Fehlklassifikation minimiert wird.

			\subsection{Konzepte und Bayes Theorem}
				\paragraph{Vorbemerkung: Wahrscheinlichkeitsdichte und Wahrscheinlichkeit}
					Ist \( p(x) \) eine Wahrscheinlichkeitsdichte, so ist die Wahrscheinlichkeit, dass \(x\) im Intervall \( (x_0, x_y) \) liegt, gegeben durch:
					\begin{equation*}
						P(x_0 < x < x_1) = \int_{x_0}^{x_1} \! p(\tau) \dif{\tau}
					\end{equation*}
					Da für die Wahrscheinlichkeit, dass \( x \) im Intervall \( (x, x + \Delta x) \) mit \( \Delta x \to 0 \) gilt:
					\begin{equation*}
						\lim\limits_{\Delta x \to 0} P(x) = \lim\limits_{\Delta x \to 0} P(x < t < x + \Delta x) = p(x) \cdot \Delta x
					\end{equation*}
					kann Wahrscheinlichkeitsdichte und Wahrscheinlichkeit in den meisten Fällen gegeneinander ausgetauscht werden.
				% end
			
				\paragraph{1. Konzept: A-Priori Wahrscheinlichkeit (Prior)}
					Die \emph{a-priori Wahrscheinlichkeit} (Prior) enthält die Information, wie wahrscheinlich eine beliebige Messung der Klasse zugehört (\dh die "Klassenhäufigkeit"). Ist \( C_k \) eine Klasse, so ist \( P(C_k) \) der Prior \bzgl der Klasse \( C_k \) (analog für \( p(C_k) \)).
				% end
				
				\paragraph{2. Konzept: Bedingte Wahrscheinlichkeit (Likelihood)}
					Ist \(\vec{x}\) der Merkmalsvektor (Feature), welcher Eigenschaften der Messung beschreibt (Anzahl schwarzer Pixel, Höhe/Breite, \dots) und \( C_k \) eine Klasse, so ist \( P(\vec{x} \given C_k) \) die \emph{Likelihood}, \dh die Wahrscheinlichkeit, dass \(\vec{x}\) für einen Buchstaben der Klasse \( C_k \) gemessen wird (analog für \( p(\vec{x} \given X_k) \)).
				% end
				
				\paragraph{3. Konzept: A-Posteriori Wahrscheinlichkeit (Posterior), Bayes Theorem}
					Die \emph{a-posteriori Wahrscheinlichkeit} (Posterior) ist die Wahrscheinlichkeit, dass ein Merkmalsvektor \(\vec{x}\) einer Klasse \( C_k \) angehört, \dh \( P(C_k \given \vec{x}) \). Dieser Posterior kann durch Bayes Theorem gefunden werden:
					\begin{equation*}
						P(C_k \given \vec{x}) = \frac{P(\vec{x} \given C_k) \cdot P(C_k)}{P(\vec{x})}
					\end{equation*}
					Oder Namentlich:
					\begin{equation*}
						\text{Posterior} = \frac{\text{Likelihood} \times \text{Prior}}{\text{Normalisierung}}
					\end{equation*}
				% end
			% end

			\subsection{Problemstellung}
				Abbildung~\ref{fig:likelihoodPriorPosterior} zeigt die Likelihood, Prior und den Posterior auf. Die Zielstellung eines Bayesian Classifier ist nun, die Wahrscheinlichkeit der Fehlklassifikation zu minimieren und somit eine Entscheidungsgrenze zu bestimmen. Die Wahrscheinlichkeit eines Fehlers ist gegeben durch:
				\begin{align*}
					P(\text{Fehler}) &= P(x \in R_2, C_1) + P(x \in R_1, C_2) \\
						&= P(x \in R_2 \given C_1) P(C_1) + P(x \in R_1 \given C_2) P(C_2) \\
						&= \int_{R_2} \! p(x \in R_2 \given C_1) P(C_1) \dif{x} + \int_{R_2} \! p(x \in R_2 \given C_2) P(X_2) \dif{x}
				\end{align*}
				Dabei ist \( P(x \in R_i, C_j) \) die Wahrscheinlichkeit, dass \(x\) zu Klasse \(R_i\) gehört, aber als Klasse \(C_j\) klassifiziert wurde (für \( i \neq j \) entspricht dies einer Fehlklassifikation).

				\begin{figure}
					\centering
					\begin{tikzpicture}
						\begin{axis}[
									name = likelihood,
									domain = 0:15,
									xmin = 0,
									xmax = 15,
									ymin = 0,
									ymax = 0.5,
									xlabel = \(x\),
									width = 12cm,
									height = 7cm,
									legend style = { xshift = -0.5cm, yshift = -0.5cm },
									xticklabels = {,,}
								]
							\addplot [density, tud9b, smooth] { gaussian(x, 5, 2) }; \addlegendentry{\( p(x \given a) \)};
							\addplot [density, tud1b, smooth] { gaussian(x, 11, 2) }; \addlegendentry{\( p(x \given b) \)};
						\end{axis}
						\begin{axis}[
									name = likelihoodPrior,
									domain = 0:15,
									xmin = 0,
									xmax = 15,
									ymin = 0,
									ymax = 0.5,
									xlabel = \(x\),
									width = 12cm,
									height = 7cm,
									at = (likelihood.right of south east),
									anchor = north,
									xshift = -5.23cm,
									yshift = -0.5cm,
									legend style = { xshift = -0.5cm, yshift = -0.5cm },
									xticklabels = {,,}
								]
							\addplot [density, tud9b, smooth] { gaussian(x, 5, 2) * 0.8 }; \addlegendentry{\( p(x \given a) p(a) \)};
							\addplot [density, tud1b, smooth] { gaussian(x, 11, 2) * 0.2 }; \addlegendentry{\( p(x \given b) p(b) \)};
						\end{axis}
						\begin{axis}[
									name = posterior,
									domain = 0:15,
									xmin = 0,
									xmax = 15,
									ymin = 0,
									ymax = 1.1,
									xlabel = \(x\),
									width = 12cm,
									height = 7cm,
									at = (likelihoodPrior.right of south east),
									anchor = north,
									xshift = -5.23cm,
									yshift = -0.5cm,
									legend style = { xshift = -0.5cm, yshift = -0.5cm }
								]
							\addplot [density, tud9b, smooth] { (gaussian(x, 5, 2) * 0.8) / (gaussian(x, 5, 2) * 0.8 + gaussian(x, 11, 2) * 0.2) }; \addlegendentry{\( p(a \given x) \)};
							\addplot [density, tud1b, smooth] { (gaussian(x, 11, 2) * 0.2) / (gaussian(x, 5, 2) * 0.8 + gaussian(x, 11, 2) * 0.2) }; \addlegendentry{\( p(b \given x) \)};
						\end{axis}
					\end{tikzpicture}
					\caption{Likelihood, \( \text{Likelihood} \times \text{Prior} \) und Posterior.}
					\label{fig:likelihoodPriorPosterior}
				\end{figure}
			% end

			\subsection{Entscheidungsregel}
				Durch die Minimierung des Erwartungswertes des Fehlers kann die Entscheidungsregel, wann \(x\) in eine Klasse einsortiert wird, hergeleitet werden. Dabei soll \(x\) genau dann in Klasse \(C_1\) sortiert werden, wenn
				\begin{equation*}
					P(C_1 \given x) > P(C_2 \given x)
				\end{equation*}
				Da die Posteriors im Allgemeinen nicht bekannt sind, werden die über Bayes Theorem berechnet:
				\begin{align*}
					               &  & P(C_1 \given x)                         & > P(C_2 \given x)                     &  \\
					\quad\iff\quad &  & \frac{P(x \given C_1) P(C_1)}{P(x)}     & > \frac{P(c \given C_2) P(C_2)}{P(x)} &  \\
					\quad\iff\quad &  & P(x \given C_1) P(C_1)                  & > P(c \given C_2) P(C_2)              &  \\
					\quad\iff\quad &  & \frac{P(x \given C_1)}{P(x \given C_2)} & > \frac{P(C_1)}{P(C_2)}               &
				\end{align*}
				Dies wird auch \emph{Likelihood Ratio Test} genannt.
				
				Dieser Test kann sich für mehr als zwei Klassen verallgemeinern lassen: Wähle Klasse \( k \) genau dann, wenn
				\begin{equation*}
					P(C_k \given x) > P(C_j \given x) \quad\forall j \neq k
				\end{equation*}
				gilt. Äquivalent zu dem zwei-Klassen-Fall kann dies in einen Likelihood Ratio Test umgeformt werden:
				\begin{equation*}
					\frac{P(x \given C_k)}{P(x \given C_j)} > \frac{P(C_j)}{P(C_k)} \quad\forall j \neq k
				\end{equation*}
			% end

			\subsection{Naive Bayes Classifier}
				Bei mehr als zwei Merkmalen (\zB Höhe und Breite) werden \( P(x_1, x_2 \given C_k) \) und \( P(x_1, x_2) \) mehrdimensional und eine Schätzung der Dichte ist nicht immer möglich. Daher nimmt ein \emph{Naive Bayes Classifier} an, dass die Merkmale statistisch unabhängig sind. Damit gilt:
				\begin{align*}
					P(x_1, x_2 \given C_k) & = P(x_1 \given C_k) P(x_2 \given C_k) \\
					P(x_1, x_2)            & = P(x_1) P(x_2)
				\end{align*}
				In der Realität ist diese Annahme oft nicht korrekt, liefert aber häufig gute Ergebnisse und ist somit eine gute Basis zum Vergleich.
			% end
		% end

		\section{Probability Density Estimation}
			Bisher wurden die Wahrscheinlichkeiten \( P(x \given C_k) \) und \( P(C_k) \) als bekannt vorausgesetzt. In der Realität ist dies oft nicht der Fall, weshalb die Wahrscheinlichkeitsdichte geschätzt werden muss. Siehe hierzu auch Vorlesung \href{https://projects.frisp.org/documents/26}{Statistical Machine Learning}.
		% end

		\section{Gesichtsdetektion}
			Bei \emph{Appearance-Bases Methods} wird ein Erscheinungsmodell aus (üblicherweise) großen Mengen von Bildern gelernt. Dabei wird am häufigsten der Sliding Window Ansatz genutzt (siehe~\ref{sec:sliding_window}). Dabei sind vor allem drei Aspekte relevant:
			\begin{enumerate}
				\item Repräsentation des Objektes (lokale Merkmale, globale Anordnung)
				\item Trainingsdaten (positive und negative Beispiele)
				\item Klassifikator und Lernmethode
			\end{enumerate}

			\subsection{Sliding Window Ansatz}
				\label{sec:sliding_window}
				
				Bei dem \emph{Sliding Window Ansatz} wird ein Bild in Ein-Pixel-Schritten horizontal und vertikal gescannt. Nach jedem Durchlauf wird das Bild immer wieder verkleinert, bis das Bild zu klein ist. So können auch mit einem Klassifikator, der nur Bilder einer Größe entgegen nehmen kann, große Bilder durchsucht werden.
			% end

			\subsection{Beispiel: Gesichtsdetektion}
				\begin{enumerate}
					\item Repräsentation des Objekts
						\begin{itemize}
							\item Die Bilder werden in Wavelets zerlegt, \dh die Gesichtsmerkmale werden mit Frequenzen und deren Ort und Orientierung dargestellt.
							\item Lokale Merkmale: Wavelet Koeffizienten (Frequenzen von \zB Auge und Mund).
							\item Globale Merkmale: Absolute Position der Frequenzen im Bild.
						\end{itemize}
					\item Trainingsdaten
						\begin{itemize}
							\item Positive Beispiele
								\begin{itemize}
									\item Möglichst vielfältig.
									\item Jedes Bild eines Gesichts wird manuell an den Rändern abgeschnitten und auf eine Größe normalisiert.
									\item Zusätzlich werden virtuelle Beispiele erstellt (\zB durch Spiegelung).
								\end{itemize}
							\item Negative Beispiele
								\begin{itemize}
									\item Beliebige Bilder, die keine Gesichter enthalten.
									\item Teilbilder von großen Bildern.
								\end{itemize}
						\end{itemize}
					\item Klassifikator und Lernmethode
						\begin{itemize}
							\item Naive Bayes Classifier
							\item Merkmale \(x_i\): Wavelet Koeffizienten an einer bestimmten Position.
							\item Zwei-Klasse-Problem:
								\begin{itemize}
									\item \(C_1\): Gesichter
									\item \(C_2\): Alles andere (keine Gesichter)
								\end{itemize}
							\item Das "Lernen" entspricht dem Schätzen der Wahrscheinlichkeiten der Wavelet-Koeffizienten.
							\item Durch Diskretisierung von Koeffizienten und Positionen gibt es eine diskrete und endliche Anzahl von \(x_i\).
							\item Schätzen: Zählen, wie häufig jedes \(x_i\) in Bilder mit und ohne Gesichtern vorkommt.
							\item Dann wird ein Likelihood Ration Test verwendet.
						\end{itemize}
				\end{enumerate}
			
				Um Bilder aus verschiedenen Perspektiven zu erkennen, wird für jede Ansicht ein eigener Detektor verwendet (jeder für eine Ansicht) und diese kombiniert.
			% end

			\subsection{Erkennungsarten}
				Eine Gesichtserkennung zählt zu den biometrischen Verfahren und werden \bspw in sicherheitstechnischen, kriminalistischen und forensischen Gebieten eingesetzt. Der Zweck ist dir Identifikation und Verifikation natürlicher Personen.
				\begin{itemize}
					\item Verifikation: Die Person muss dem System ihren Namen oder User-ID mitteilen und das System entscheidet, ob die Person dazu gehört.
					\item Identifikation: Die Person offenbart ausschließlich ihre biometrischen Merkmale und das System ermittelt daraus den Namen oder die User-ID.
				\end{itemize}
			% end
		% end
	% end

	\chapter{Fouriertheorie}
		Bei der Beugung an einem einfachen Spalt der breite \(a\) ergibt sich auf dem Schirm ein Beugungsmuster, welches im Zentrum ein Intensitätsmaximum und nach außen hin immer wieder Intensitätsminima und -maxima hat. Der Spalt kann durch eine Rechteckfunktion
		\begin{equation*}
			\text{Rect}(x) =
				\begin{cases}
					1 & -1 \leq x \leq 1 \\
					0 & \text{sonst}
				\end{cases}
		\end{equation*}
		beschrieben werden. Das sich ergebende Beugungsmuster, \bzw die zeitlich gemittelte Intensität \(I\), hat dann die Form
		\begin{equation*}
			I(\theta) = I_0 \, \Bigg( \frac{\sin(\theta)}{\theta} \Bigg)^2 = I_0 \cdot \sinc^2(\theta)
		\end{equation*}
		mit der \(\sinc\)-Funktion \( \sinc(\theta) = \sin(\theta) / \theta \). Dabei stellt \(\theta\) den Ausfallwinkel des Lichts aus dem Spalt hinaus dar.
		
		Dieser Zusammenhang zwischen der Gestalt des beugenden Objekts (hier der Spalt) und der Amplitudenfunktion \( I(\theta) \) ist durch eine \emph{Fourier-Transformation} gegeben.

		\section{Mathematische Grundlagen}
			\subsection{Vektorraum}
				Ein \emph{Vektorraum} ist eine algebraische Struktur über einen Zahlenbereich mit Operationen wie Addition und Multiplikationen mit einem Skalar. Alle Operationen müssen dabei Elemente des Vektorraums wieder auf selbigen abbilden. Die Elemente eines solchen Raums sind \emph{Vektoren}.
				
				\subparagraph{Beispiel}
					Ein Beispiel ist er euklidische Vektorraum über den reellen Zahlen. Dabei repräsentieren Vektoren Verschiebungen und es lassen sich Längen und Winkel messen (rechtwinkliges, kartesisches Koordinatensystem). Es ist außerdem ein Skalarprodukt definiert:
					\begin{gather*}
						\langle \vec{v}, \vec{w} \rangle = \sum_{i = 1}^{n} v_i w_i \in \R \\
						\langle \vec{v}, \vec{w} \rangle = v_1 w_2 + v_2 w_2 = \lVert \vec{v} \rVert \cdot \lVert \vec{w} \rVert \cos \big( \angle(\vec{v}, \vec{w}) \big)
					\end{gather*}
					Die letztere Eigenschaft gilt nur für \( n = 2 \) (\iA lassen sich solche Winkel aber auch mit beliebigem \(n\) definieren). In der euklidischen Ebene \( \R^2 \) lassen sich Vektoren durch Ortsvektoren (Pfeile) darstellen.
				% end
			% end

			\subsection{Basis eines Vektorraums}
				Jeder Satz (Menge) an linear unabhängigen Vektoren eines Vektorraums kann als Basis verwendet werden. Zwei Vektoren \(\vec{v}\), \(\vec{w}\) sind genau dann linear unabhängig, wenn \( \big\lvert \langle \vec{v}, \vec{w} \rangle \big\rvert < \lVert \vec{v} \rVert \cdot \lVert \vec{w} \rVert \) gilt.
			
				\subparagraph{Beispiel}
					In der euklidischen Ebene \( \R^2 \) ist eine Basis durch
					\begin{equation*}
						\vec{e}_1 = \begin{bmatrix} 1 \\ 0 \end{bmatrix} \quad\quad \vec{e}_2 = \begin{bmatrix} 0 & 1 \end{bmatrix}
					\end{equation*}
					gegeben, wobei \(\vec{e}_1\) und \(\vec{e}_2\) orthogonal aufeinander stehen (\( \langle \vec{v}, \vec{w} \rangle = 0 \)) und somit linear unabhängig sind. Alle \( \vec{v} \in \R^2 \) lassen sich dann als \emph{Linearkombination} der Basisvektoren darstellen (mit geeigneten \( a_1, a_2 \in \R \)):
					\begin{equation*}
						\vec{v} = a_1 \vec{e}_1 + a_2 \vec{e}_2
					\end{equation*}
				% end
			% end

			\subsection{Krummlinige Koordinatensysteme}
				Gerade in physikalischen Anwendungen kann es von Vorteil sein, keine kartesischen Koordinaten (mit \(x\)- und \(y\)-Wert) zu nutzen, sondern auf \emph{krummlinige Koordinaten} umzusteigen. Ein typisches krummliniges Koordinatensystem sind \zB Polarkoordinaten. Dabei wird ein Punkt in der Ebene durch den Abstand \(r\) vom Ursprung und durch den Winkel \(\varphi\) mit der \(x\)-Achse beschrieben. Die Koordinaten lassen sich durch
				\begin{align*}
					x(r, \varphi) &= r \cdot \cos(\varphi) \\
					y(r, \varphi) &= r \cdot \sin(\varphi)
				\end{align*}
				in kartesische Koordinaten umrechnen.
				
				Weitere krummlinige Koordinatensysteme sind \zB Kugel- oder Zylinderkoordinaten.
			% end

			\subsection{Andere Räume}
				Es ist auch möglich, dass die Elemente eines Vektorraums Funktionen sind (Funktionenräume). Auch kann ein Raum unendlich-dimensional sein.
				
				Die \emph{Fourier-Theorie} beschäftigt sich mit der Frage, ob es möglich ist, Basisfunktionen zu finden, mit denen sich beliebige Funktionen \bzgl dieser Basen darstellen lassen.
			% end

			\subsection{Komplexe Zahlen} % 4.20, 4.48
				Komplexe Zahlen haben zwei Komponenten: Einen Real- und einen Imaginärteil. Dabei können sie als kartesische Koordinaten in einer zwei-dimensionalen Ebene (der komplexen Ebene) aufgefasst werden und entsprechen dargestellt werden (mit der imaginären Zahl \( i \) mit der Eigenschaft \( i^2 = -1 \)):
				\begin{equation*}
					z = a + bi
				\end{equation*}
				Oder als Polarkoordinaten (in einer zwei-dimensionalen Ebene) mit der Darstellung
				\begin{equation*}
					z = r e^{\varphi i}
				\end{equation*}
				wobei sich kartesische und Polardarstellung wie bei Polarkoordinaten ineinander umrechnen lassen.
				
				Die Äquivalenz der beiden Darstellung geht auf die Euler-Identität
				\begin{equation*}
					e^{i \varphi} = \cos(\varphi) + i \sin(\varphi)
				\end{equation*}
				zurück, wobei hier \( r = 1 \) gilt. Aus aus dieser folgt (für \( \lvert z \rvert = 1 \)) ebenfalls:
				\begin{align}
					a = \cos(\varphi) &= \frac{1}{2} \big( e^{i \varphi} + e^{-i \varphi} \big) \\
					b = \sin(\varphi) &= \frac{1}{2i} \big( e^{i \varphi} - e^{-i \varphi} \big)
				\end{align}
			% end

			\subsection{Gerade/Ungerade Funktionen}
				Für eine gerade Funktion gilt
				\begin{equation*}
					f(x) = f(-x)
				\end{equation*}
				für eine ungerade Funktion gilt
				\begin{equation*}
					f(x) = -f(-x)
				\end{equation*}
				für jeweils alle \(x\).
			% end
		% end

		\section{Fourier-Reihe}
			\subsection{Dirichlet-Bedingungen}
				Jede Funktion, die die \emph{Dirichlet-Bedingungen} erfüllt:
				\begin{enumerate}
					\item Die Anzahl Unstetigkeiten innerhalb einer Periode ist endlich.
					\item Die Anzahl Maxima und Minima innerhalb einer Periode ist endlich.
					\item Die Funktion ist in jeder Periode integrierbar (\dh die Fläche unter dem Betrag der Funktion ist endlich).
				\end{enumerate}
				Kann durch eine Summe von Kosinus- und Sinusfunktionen dargestellt werden.
			% end

			\subsection{\(2\pi\)-periodische Funktion}
				Ist \( f(x) \) eine periodische Funktion mit der Periodenlänge \( 2\pi \) (\dh die wiederholt sich alle \(2\pi\)), die die Dirichlet-Bedingungen erfüllt, so gilt
				\begin{equation*}
					f(x) = \sum_{n = 0}^{\infty} \big( a_n \cos(nx) + b_n \sin(nx) \big)
				\end{equation*}
				mit geeigneten \emph{Fourier-Koeffizienten} \( a_n \) und \( b_n \).
			% end

			\subsection{Skalarprodukt, Orthogonale Basis}
				Sei \(H\) der Raum aller \(2\pi\)-periodischen reellen Funktionen, die die Dirichlet-Bedingungen erfüllen. Dann wird durch
				\begin{equation*}
					\langle f, g \rangle \coloneqq \int_{-\pi}^{\pi} \! f(\tau) g(\tau) \dif{\tau}
				\end{equation*}
				ein Skalarprodukt definiert.
				
				Die Funktionen
				\begin{align*}
					u_n(x) &= \cos(nx) \\
					v_n(x) &= \sin(nx)
				\end{align*}
				bilden dann eine orthogonale Funktionenfolge in \(H\):
				\begin{align*}
					\langle u_n, u_m \rangle &=
						\begin{cases}
							0    & m \neq n  \\
							2\pi & m = n = 0 \\
							\pi  & m = n > 0
						\end{cases} \\
					\langle v_n, v_m \rangle &=
						\begin{cases}
							0   & m \neq n  \\
							0   & m = n = 0 \\
							\pi & m = n > 0
						\end{cases} \\
					\langle u_n, v_m \rangle = \langle v_m, u_n \rangle &= 0
				\end{align*}
				
				Durch diese Darstellung kann die allgemeine Fourier-Reihe mit \( u_n = u_n(x) \) und \( v_n = v_n(x) \) auch geschrieben werden als:
				\begin{equation*}
					f(x) = \sum_{n = 0}^{\infty} \big( a_n u_n + b_n v_n \big)
				\end{equation*}
			% end

			\subsection{Berechnung der Koeffizienten \(a_m\), \(b_m\)}
				Um die Koeffizienten \( a_m \), \( m = 1, 2, \cdots \) zu bestimmen, wird das Skalarprodukt zwischen \(f\) und \( u_m \) gebildet:
				\begin{equation*}
					\langle f, u_m \rangle
						= \Bigg\langle \sum_{n = 0}^{\infty} \big( a_n u_n + b_n v_n \big),\, u_m \Bigg\rangle
						= \Big\langle \big( a_m u_m + b_m v_m \big),\, u_m \Big\rangle
						= \langle a_m u_m,\, u_m \rangle
						= a_m \langle u_m, u_m \rangle
						= a_m \pi
				\end{equation*}
				Umstellen nach \( a_m \) liefert die Werte der Fourier-Koeffizienten:
				\begin{equation*}
					a_m = \frac{1}{\pi} \langle f, u_m \rangle = \frac{1}{\pi} \int_{-\pi}^{\pi} \! f(x) \cos(mx) \dif{x}
				\end{equation*}
				
				Analog für \( a_0 \) mit \( u_0 \):
				\begin{equation*}
					\langle f, u_0 \rangle
						= \Bigg\langle \sum_{n = 0}^{\infty} \big( a_n u_n + b_n v_n \big),\, u_0 \Bigg\rangle
						= \Big\langle \big( a_0 u_0 + b_0 v_0 \big),\, u_0 \Big\rangle
						= \langle a_0 u_0,\, u_0 \rangle
						= a_0 \langle u_0, u_0 \rangle
						= a_0 2\pi
				\end{equation*}
				Umstellen nach \( a_0 \):
				\begin{equation*}
					a_0 = \frac{1}{2\pi} \langle f, u_0 \rangle = \frac{1}{2\pi} \int_{-\pi}^{\pi} \! f(x) \cos(0x) \dif{x} = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(x) \dif{x}
				\end{equation*}
				
				Analog für \( b_m \), \( m = 1, 2, \cdots \):
				\begin{equation*}
					\langle f, v_m \rangle
						= \Bigg\langle \sum_{n = 0}^{\infty} \big( a_n u_n + b_n v_n \big),\, v_m \Bigg\rangle
						= \Big\langle \big( a_m u_m + b_m v_m \big),\, v_m \Big\rangle
						= \langle b_m v_m, v_m \rangle
						= b_m \langle v_m, v_m \rangle
						= b_m \pi
				\end{equation*}
				Umstellen nach \( b_m \):
				\begin{equation*}
					b_m = \frac{1}{\pi} \langle f, v_m \rangle = \frac{1}{\pi} \int_{-\pi}^{\pi} \! f(x) \sin(mx) \dif{x}
				\end{equation*}
				Da \( \sin(0x) = \sin(0) = 0 \) ist, muss \( b_0 \) nicht berechnet werden.
			% end

			\subsection{Beispiel: Rechteck-Schwingung}
				Sei eine Rechteck-Schwingung
				\begin{equation*}
					f(x) =
						\begin{cases}
							-k & -\pi < x < 0 \\
							k  & 0 < x < \pi
						\end{cases},\quad f(x) = f(x + 2\pi)
				\end{equation*}
				gegeben. Für diese lauten die Fourier-Koeffizienten:
				\begin{align*}
					a_0 &= 0 \\
					a_n &= 0 \\
					b_n &= \frac{4k}{n\pi} \text{ für ungerade } n
				\end{align*}
				Daraus ergibt sich die Fourier-Reihe:
				\begin{equation*}
					f(x) = \frac{4k}{\pi} \sum_{n = 0}^{\infty} \frac{1}{2k + 1} \sin\big((2k + 1) x\big)
				\end{equation*}
				
				Die Rechteck-Schwingung ist dabei eine ungerade Funktion. Allgemein gilt:
				\begin{itemize}
					\item Für gerade Funktionen sind alle \( b_n = 0 \).
					\item Für ungerade Funktionen sind all \( a_n = 0 \).
				\end{itemize}
			% end
		% end

		\section{Fourier-Transformation}
			Mit der Fourier-Transformation wird versucht, eine ähnliche Darstellung wie die Fourier-Reihe für Funktionen zu finden, die nicht \(2\pi\)-periodisch sind.
			
			Durch die Euler-Identität kann die allgemeine Fourier-Reihe umgeformt werden:
			\begin{align*}
				f(x)
					&= a_0 + \sum_{n = 1}^{\infty} \big( a_n \cos(nx) + b_n \sin(nx) \big) \\
					&= a_0 + \sum_{n = 1}^{\infty} \Bigg( a_n \frac{e^{inx} + e^{-inx}}{2} + b_n \frac{e^{inx} - e^{-inx}}{2i} \Bigg) \\
					&= a_0 + \sum_{n = 1}^{\infty} \Bigg( a_n \frac{e^{inx} + e^{-inx}}{2} - b_n i \frac{e^{inx} - e^{-inx}}{2} \Bigg) \\
					&= a_0 + \sum_{n = 1}^{\infty} \Bigg( \frac{a_n - ib_n}{2} e^{inx} + \frac{a_n + ib_n}{2} e^{-inx} \Bigg) \\
					&= a_0 + \sum_{n = 1}^{\infty} \frac{a_n - ib_n}{2} e^{inx} + \sum_{n = 1}^{\infty} \frac{a_n + ib_n}{2} e^{-inx} \\
					&= a_0 + \sum_{n = 1}^{\infty} \frac{a_n - ib_n}{2} e^{inx} + \sum_{n = -\infty}^{-1} \frac{a_{-n} + ib_{-n}}{2} e^{inx} \\
					&= c_0 + \sum_{n = 1}^{\infty} c_n e^{inx} + \sum_{n = -\infty}^{-1} c_n e^{inx} \\
					&= \sum_{n = -\infty}^{\infty} c_n e^{inx}
			\end{align*}
			woraus sich eine äquivalente Formulierung der Fourier-Reihe mit den komplexen Koeffizienten
			\begin{equation*}
				c_n = \frac{a_n - ib_n}{2} e^{inx}, \quad n = 1, 2, \cdots \quad\quad\quad\quad c_n = \frac{a_{-n} + ib_{-n}}{2}, \quad n = -1, -2, \cdots \quad\quad\quad\quad c_0 = a_0
			\end{equation*}
			ergibt. Nun werden zunächst Funktionen \( f_L(x) \) mit einer beliebigen Periode \( 2L \) betrachtet:
			\begin{align*}
				f_L(x) &= \sum_{n = -\infty}^{\infty} c_n e^{in \frac{2\pi}{2L} x} \\
					 &= \sum_{n = -\infty}^{\infty} c_n e^{in \frac{\pi}{L} x} \\
				\intertext{Einsetzen der Koeffizienten \(c_n\):}
					&= \sum_{n = -\infty}^{\infty} \Bigg( \frac{1}{2L} \int_{-L}^{L} \! f(\tau) e^{-in \frac{\pi}{L} \tau} \dif{\tau} \Bigg) e^{in \frac{\pi}{L} x}
			\end{align*}
			Nun wird der Übergang \( L \to \infty \), \dh zu nicht-periodischen Funktionen, betrachtet:
			\begin{align*}
				\lim\limits_{L \to \infty} f(x)
					&= \lim\limits_{L \to \infty} \sum_{n = -\infty}^{\infty} \Bigg( \frac{1}{2L} \int_{-L}^{L} \! f(\tau) e^{-in \frac{\pi}{L} \tau} \dif{\tau} \Bigg) e^{in \frac{\pi}{L} x} \\
					&= \lim\limits_{L \to \infty} \sum_{n = -\infty}^{\infty} \frac{1}{2L} \int_{-L}^{L} \! f(\tau) e^{-in \frac{\pi}{L} (\tau - x)} \dif{\tau} \\
					&= \lim\limits_{L \to \infty} \sum_{n = -\infty}^{\infty} \int_{-L}^{L} \! \frac{1}{2L} f(\tau) e^{-in \frac{\pi}{L} (\tau - x)} \dif{\tau} \\
					&= \lim\limits_{L \to \infty} \int_{-L}^{L} \sum_{n = -\infty}^{\infty} \frac{1}{2L} f(\tau) e^{-in \frac{2\pi}{2L} (\tau - x)} \dif{\tau} \\
					&= \lim\limits_{L \to \infty} \int_{-L}^{L} \! f(\tau) \int_{-\infty}^{\infty} \! e^{-2\pi i u (\tau - x)} \dif{u} \dif{\tau} \\
					&= \int_{-\infty}^{\infty} \! f(\tau) \int_{-\infty}^{\infty} \! e^{-2\pi i u (\tau - x)} \dif{u} \dif{\tau}
			\end{align*}
			Dieser Übergang lässt sich als "Superposition" auffassen mit:
			\begin{align*}
				f(x) &= \int_{-\infty}^{\infty} \! F(u) e^{ 2\pi i u x} \dif{u} \\
				F(u) &= \int_{-\infty}^{\infty} \! f(x) e^{-2\pi i u x} \dif{x}
			\end{align*}
			
			Dabei heißt der Übergang \( f(x) \to F(u) \) \emph{Fourier-Transformation} und der Übergang \( F(u) \to f(x) \) \emph{Inverse Fourier-Transformation}. Dabei ist \(F(u)\) oft komplex und \(f(x)\) ist reell.

			\subsection{Beispiel: Rechteckimpuls}
				Für einen Rechteckimpuls
				\begin{equation*}
					f(x) =
						\begin{cases}
							1 & -1 < x < 1 \\
							0 & \text{sonst}
						\end{cases}
				\end{equation*}
				ergibt sich die Fourier-Transformation
				\begin{equation*}
					F(u) = \int_{-1}^{1} \! e^{-2\pi i u \tau} \dif{\tau} = \frac{1}{2\pi i u} \big[ e^{-2\pi i u \tau} \big]_{-1}^{1} = \frac{1}{\pi u} \cdot \frac{e^{2\pi i u} - e^{-2\pi i u}}{2i} = 2 \frac{\sin(2\pi u)}{2\pi u} = 2 \sinc(2\pi u)
				\end{equation*}
				wie erwartet ein Vielfaches der \( \sinc \)-Funktion.
			% end

			\subsection{Transformationspaare}
				Die Fourier-Transformation zerlegt eine Funktion in ihre Frequenzbestandteile! Beispielhafte Fourier-Transformationspaare sind:
				\begin{itemize}
					\item \( \cos(0) = 1 \): \tabto{2.5cm} Delta-Funktion bei \( u = 0 \)
					\item \( \cos(kx) \):    \tabto{2.5cm} Delta-Funktion bei \( u = \pm k \)
					\item \( \sin(kx) \):    \tabto{2.5cm} Delta-Funktion bei \( u = \pm ik \)
				\end{itemize}
			% end
			
			\subsection{2D-Fourier-Transformation}
				Für eine zweidimensionale Funktion \( f(x, y) \) lautet die Fourier-Transformation:
				\begin{align*}
					f(u, v) &= \iint_{-\infty}^{\infty} \! f(x, y) e^{ 2\pi i (xu + vy)} \dif{u} \dif{v} \\
					F(u, v) &= \iint_{-\infty}^{\infty} \! f(x, y) e^{-2\pi i (ux + vy)} \dif{x} \dif{y}
				\end{align*}
				Die Fourier-Transformierte ist entspricht also zweidimensionalen Funktionen (Real- und Imaginärteil), die als Graustufenbilder visualisiert werden können. Meistens wird dabei aber nur das sogenannte Amplituden-Spektrum betrachtet, welches die Amplituden der Fourier-Transformation visualisiert. Dabei entspricht der Pixelwert an der Stelle \( (u, v) \) der Amplitude, \dh dem Betrag, der Frequenzen \( \big\lvert F(u, v) \big\rvert \).
			% end
		% end

		\section{Faltung}
			Werden zwei Funktionen \( F(u) \), \( G(u) \) im Frequenzraum multipliziert:
			\begin{align*}
				F(u) \cdot G(u)
					&= \int_{-\infty}^{\infty} \! f(\tau) e^{-2\pi i u \tau} \dif{\tau} \cdot \int_{-\infty}^{\infty} \! g(t) e^{-2\pi i u t} \dif{t} \\
					&= \int_{-\infty}^{\infty} \! f(\tau) e^{-2\pi i u \tau} \int_{-\infty}^{\infty} \! g(t - \tau) e^{-2\pi i u (t - \tau)} \dif{\tau} \dif{t} \\
					&= \int_{-\infty}^{\infty} \! e^{-2\pi i u t} \underbrace{\int_{-\infty}^{\infty} \! f(\tau) g(t - \tau) \dif{\tau}}_{h(t) \coloneqq} \dif{t} \\
					&= \int_{-\infty}^{\infty} \! h(t) e^{-2\pi i u t} \dif{t} \\
					&= H(t)
			\end{align*}
			Das Integral \( h(t) = \int_{-\infty}^{\infty} \! h(t) e^{-2\pi i u t} \dif{t} \eqqcolon f(t) \ast g(t) \) ist das sogenannte \emph{Faltungsintegral} der Funktionen \(f\) und \(g\). Eine Faltung im Ortsraum entspricht somit einer Multiplikation im Frequenzraum!
			
			Eine Faltung \( f(t) \ast g(t) \) kann als Mittelwertbildung der Werte von \(f\) mit Gewichten \(g\) verstanden werden. So kann \bspw analytisch ein gleitender Durchschnitt (mit einer Kastenfunktion \(g\)) erstellt werden.
			
			\subsection{Anwendung: Filter} % 4.68
				\todo{Content}
			% end
		% end

		\section{Abtastung}
			Ist eine kontinuierliche Funktion, \bzw ein analoges Signal, gegeben, so muss dieses für eine diskrete Repräsentation \emph{abgetastet} werden, \dh es müssen Messungen an einzelnen Stellen durchgeführt werden. Eine solche diskrete Abtastung kann durch die Funktion
			\begin{equation*}
				\hat{f}(x) = f(x) \cdot \sum_{n = -\infty}^{\infty} \delta(x - n \cdot \Delta x)
			\end{equation*}
			\dh als Produkt einer Funktion \( f(x) \) und einer Kamm-Funktion beschrieben werden. Die Fourier-Transformierte \( \hat{F}(u) \) der abgetasteten Funktion entspricht dann der Fourier-Transformierten \( F(u) \) der nicht abgetasteten Funktion, wird aber periodisch mit der Periode \( 1/\Delta x \) wiederholt und mit \( 1/\Delta x \) skaliert.

			\subsection{Abtasttheorie}
				Sei die Funktion \( f(x) \) bandbegrenzt durch eine Maximalfrequenz \( u_G \), \dh \( F(u) = 0 \) für \( \lvert u \rvert > u_G \).
				
				Gilt nun \( 2u_G < 1 / \Delta x \), so überlappen sich die Fouriertransformierten nicht, \dh die Spektren von \( F(u) \) und \( \hat{F}(u) \) stimmen auf dem Intervall \( [-u_G, u_G] \) (bis auf die Skalierung \( 1 / \Delta x \)) überein. Das Frequenzspektrum von \( F(u) \) kann somit vollständig aus dem Abtastsignal und den Abtastwerten berechnet werden.
				
				Gilt nun \( 2u_G > 1 / \Delta x \), so überlappen sich die Fouriertransformierten und in den Überschneidungsbereichen bilden sich Summen. Damit ist es unmöglich, das originale Frequenzspektrum von \( F(u) \) zu bestimmen (\emph{Aliasing}).
			% end

			\subsection{Abtasttheorem von Whittaker-Shannon}
				Aus den vorherigen Überlegungen ergibt sich das \emph{Abtasttheorem von Whittaker-Shannon}: Existiert für eine Funktion \( f(x) \) eine Grenzfrequenz \( u_G < \infty \), sodass \( F(u) = 0 \) für \( \lvert u \rvert > u_G \) gilt, dann ist \( f(x) \) fehlerfrei rekonstruierbar, sofern die Abtastfrequenz \( 1 / \Delta x \) mindestens doppelt so hoch wie \( u_G \) ist:
				\begin{equation*}
					\frac{1}{\Delta x} > 2 u_G
				\end{equation*}
			% end
		% end
	% end

	\chapter{Bilder}
		\section{Bildverbesserung}
			Bei der Bildverbesserung wird versucht, die Bildinformationen so aufzubereiten, dass die für den Betrachter verbessert sind/wirken. Dafür gibt es (leider) keine allgemeine Theorie, sondern die möglichen Verbesserungen sind sehr Anwendungsspezifisch und abhängig von Bild und Betrachter. Typische Anwendungen sind dabei der Ausgleich von nicht-Linearitäten der Kamera, Anpassung von Helligkeit und Kontrast und Hervorhebung von Bildbereichen.
			
			Es wird unterschieden zwischen Methoden im Ortsraum (die direkt Pixelwerte manipulieren) und Methoden im Frequenzraum, bei denen das Bild zunächst durch eine Fourier-Transformation in seine Frequenzen zerlegt, manipuliert und rücktransformiert wird.

			\subsection{Histogramm}
				Das Histogramm eines Bildes ist die graphische Darstellung der Häufigkeitsverteilung von bestimmten Merkmalen (\zB von bestimmten Grauwerten). Histogramme von Bildern können viele Aussagen treffen, \zB über:
				\begin{itemize}
					\item Dynamik (Bereich reeller Lichtintensitäten, er auf der Grauwertskala abgebildet wird)
					\item Kontrast (Bereich der Grauwertskala, der zur Darstellung ausgenutzt wird)
					\item Helligkeit (Beleuchtungsstärke (der Grauwert))
				\end{itemize}
				Dabei entspricht die Helligkeit eines Grauwertbildes dem Mittelwert aller Grauwerte und der Bildkontrast der Varianz aller Grauwerte.
			% end

			\subsection{Pixeloperationen}
				Bei Pixeloperationen wird ein Pixel unabhängig von seiner Nachbarschaft modifiziert. Beispiele für solche Operationen sind:
				\begin{itemize}
					\item Negativ
					\item Binärisierung/Thresholding
					\item Fensterung
					\item Kontrastspreizung
					\item Dynamikkompression
					\item Gammakorrektur (Bildschirm)
					\item Helligkeit
					\item Histogrammausgleich
					\item Differenz
					\item Mittelung
				\end{itemize}

				\subsubsection{Bildnegativ}
					Bei einem Bildnegativ wird der Wert eines Pixels von der Maximal möglichen Intensität abgezogen und dieser Wert als neuer Pixelwert verwendet:
					\begin{equation*}
						g[m, n] = f_\text{max} - f[m, n]
					\end{equation*}
				% end

				\subsubsection{Binärisierung/Thresholding}
					Bei der Binärisierung wird ein Schwellwert \(\tau\) sowie zwei Werte \(f_\text{max}\) und \(f_\text{min}\) festgelegt und das Bild wie folgt manipuliert:
					\begin{equation*}
						g[m, n] =
							\begin{cases}
								f_\text{max} & f[m, n] > \tau \\
								f_\text{min} & f[m, n] \leq \tau
							\end{cases}
					\end{equation*}
					Für den Spezialfall der Binärisierung gilt \( f_\text{max} = 1 \) und \( g_\text{min} = 0 \).
				% end

				\subsubsection{Grauwertfensterung}
					Bei der Grauwertfensterung wird ein bestimmtes Intensitätsintervall hervorgehoben (gespreizt) und alles andere wird auf einen fixen Wert gesetzt.
				% end
				
				\subsection{Kontrastspreizung}
					Bei der Kontrastspreizung wird der Grauwert auf eine neue Grauwertskala anhand einer einwertigen oder monotonen Funktion abgebildet.
				% end
				
				\subsection{Histogrammausgleich}
					Bei einem Histogrammausgleich wird die Grauwertskala anhand der Kurve der Summenwahrscheinlichkeiten, \dh anhand der kumulierten Wahrscheinlichkeiten bis zu einem bestimmten Wert, transformiert:
					\begin{equation*}
						p(g) = \max(\text{Intensität}) \cdot \sum_{i = 0}^{g} p(i)
					\end{equation*}
					Ein solcher Histogrammausgleich ist verlustbehaftet und nicht umkehrbar!
				% end
				
				\subsection{Mittelung}
					Ein unkorreliertes Rauschen im Bild kann durch Mittelung über \(k\) Aufnahmen des gleichen Motivs unterdrückt werden:
					\begin{equation*}
						g[m, n] = \frac{1}{k} \sum_{i = 0}^{k - 1} f_i[m, n]
					\end{equation*}
				% end
			% end
		% end

		\section{Bildfilterung}
			Zur Bildfilterung gibt es zwei grundlegende Vorgehensweise:
			\begin{itemize}
				\item Filterung im Ortsraum durch direkte Manipulation der Pixel und
				\item Filterung im Frequenzraum durch vorherige Fourier-Transformation und Rücktransformation.
			\end{itemize}

			\subsection{Ortsraum}
				Filter im Ortsraum werden durch \emph{Filtermasken} beschrieben. Dabei wird ein Pixel in Abhängigkeit von seiner Nachbarschaft modifiziert. Eine Filtermaske wird durch eine \( k \times l \)-Matrix (mit \(k\), \(l\) ungerade) beschrieben, die die Gewichtung der umliegenden Pixel beschreibt. Dies entspricht einer linearen Filterung (Faltung) im Ortsraum:
				\begin{equation*}
					(f \ast w)[m, n] = \sum_{i = -\lfloor k/2 \rfloor}^{\lfloor k/2 \rfloor} \sum_{j = -\lfloor l/2 \rfloor}^{\lfloor l/2 \rfloor} w_{ij} f[m + i, n + j]
				\end{equation*}
				Dabei entspricht \( w \in \R^{k \times l} \) der Filtermaske.

				\subsubsection{Tiefpass-Filter}
					\begin{itemize}
						\item Die Koeffizienten (\dh die Einträge der Filtermaske) sind allesamt positiv und normalisiert, sodass die Summe \num{1} ergibt.
						\item Dadurch werden nur positive Werte produziert.
						\item Es kommt zu Randeffekten, da am Rand die Nachbarn eines Pixels nicht definiert sind.
						\item Typische Vertreter dieser Kategorie sind Mittelwert- und Gauß-Filter.
					\end{itemize}

					\paragraph{Mittelwert-Filter}
						\( 3 \times 3 \)-Mittelwertfilter ("Boxfilter"):
						\begin{equation*}
							\frac{1}{9}
							\begin{bmatrix}
								1 & 1 & 1 \\
								1 & 1 & 1 \\
								1 & 1 & 1
							\end{bmatrix}
						\end{equation*}
						\indent \( 5 \times 5 \)-Mittelwertfilter:
						\begin{equation*}
							\frac{1}{25}
							\begin{bmatrix}
								1 & 1 & 1 & 1 & 1 \\
								1 & 1 & 1 & 1 & 1 \\
								1 & 1 & 1 & 1 & 1 \\
								1 & 1 & 1 & 1 & 1 \\
								1 & 1 & 1 & 1 & 1
							\end{bmatrix}
						\end{equation*}
						
						Je größer der Filter gewählt wird, desto mehr wird das Bild "verwischt".
					% end
	
					\paragraph{Gauß-Filter}
						Bei einem Gauß-Filter werden die umliegenden Pixel durch eine diskrete Approximation der Funktion
						\begin{equation*}
							G(x, y) = \frac{1}{2\pi\sigma^2} \exp \Bigg\{ -\frac{x^2 + y^2}{2\sigma^2} \Bigg\}
						\end{equation*}
						gewichtet. Für \( 3 \times 3 \) ("Binomialfilter") und \( 5 \times 5 \) (\( \sigma = 1 \)) ergeben such folgende Approximationen:
						\begin{equation*}
							\frac{1}{16}
							\begin{bmatrix}
								1 & 2 & 1 \\
								2 & 4 & 2 \\
								1 & 2 & 1
							\end{bmatrix}
							\quad\quad\quad\quad
							\frac{1}{273}
							\begin{bmatrix}
								1 & 4  & 7  & 4  & 1 \\
								4 & 16 & 26 & 16 & 4 \\
								7 & 26 & 41 & 26 & 7 \\
								4 & 16 & 26 & 16 & 4 \\
								1 & 4  & 7  & 4  & 1
							\end{bmatrix}
						\end{equation*}
					% end
	
					\paragraph{Median-Filter}
						Der Median-Filter ist ein nichtlinearer Filter und kann daher nicht durch eine Faltung ausgedrückt werden. Bei ihm wird jeder Pixel durch den Medianwert seiner Nachbarschaft ersetzt. Dadurch werden keine Grautöne interpoliert, isolierte Punkte und Rauschen wird minimiert und dir Schärfe der Kanten bleibt erhalten. Jedoch ist dieser Filter aufgrund der Sortierung sehr rechenintensiv.
					% end
				% end

				\subsubsection{Hochpass-Filter}
					\begin{itemize}
						\item Die Koeffizienten können sowohl negativ und positiv sein und sind normalisiert, sodass die Summe \num{0} ergibt.
						\item Dadurch werden positive und negative Werte produziert.
						\item Typischer Vertreter dieser Kategorie sind Ableitungen und Differenzfilter.
					\end{itemize}
				
					\paragraph{Diskretisierung von Ableitungen}
						Eine Ableitung \( \partial f / \partial x \) einer Funktion kann durch Rückwärtsdifferenzen approximiert werden:
						\begin{equation*}
							\frac{\partial f}{\partial x}(x) = \lim\limits_{h \to 0} \frac{f(x) - f(x - h)}{h} \overset{\,h \,=\, 1\,}{\approx} \frac{f(x) - f(x - 1)}{1} = f(x) - f(x - 1)
						\end{equation*}
						Dies gilt ebenfalls für die zweite Ableitung \( \partial^2 f / \partial x^2 \) (diesmal durch Vorwärtsdifferenzen):
						\begin{align*}
							\frac{\partial^2 f}{\partial x^2}(x)
								=&\, \lim\limits_{h \to 0} \frac{\frac{\partial f}{\partial x}(x + h) - \frac{\partial f}{\partial x}(x)}{h} \\
								\approx&\, \lim\limits_{h \to 0} \frac{f(x + h) - f(x + h - 1) - f(x) + f(x - 1)}{h} \\
								\approx&\, \frac{f(x + 1) - f(x) - f(x) + f(x - 1)}{1} \\
								=&\, f(x + 1) - 2f(x) + f(x - 1)
						\end{align*}
					% end
					
					\paragraph{Laplacian-Filter}
						Durch den Laplacian-Filter wird der Laplace-Operator
						\begin{equation*}
							\Delta f(x, y) = \nabla^2 f(x, y) = \frac{\partial^2 f(x, y)}{\partial x^2} + \frac{\partial^2 f(x, y)}{\partial y^2}
						\end{equation*}
						approximiert. Für \( 3 \times 3 \) ergibt sich \bspw die folgende diskrete Approximation:
						\begin{equation*}
							\begin{bmatrix}
								0 & 1  & 0 \\
								1 & -4 & 1 \\
								0 & 1  & 0
							\end{bmatrix}
						\end{equation*}
						
						Eine alternative Approximation mit Parametern stellt
						\begin{equation*}
							\frac{1}{\beta}
							\begin{bmatrix}
								\alpha     & 1 - \alpha & \alpha     \\
								1 - \alpha & -4         & 1 - \alpha \\
								\alpha     & 1 - \alpha & \alpha
							\end{bmatrix}
						\end{equation*}
						da, wobei
						\begin{equation*}
							\beta =
								\begin{cases}
									4             & 0 \leq \alpha \leq 1 \\
									4(q - \alpha) & -1 \leq \alpha < 0
								\end{cases}
						\end{equation*}
						gilt (der Parameter \( \alpha \) ist frei wählbar).
					% end
					
					\paragraph{Laplacian of Gaussian Filter}
						Oftmals wird zunächst ein Gauß- und danach ein Laplacian-Filter angewandt. Dieser Filter wird "Laplacian of Gaussian Filter", "Marr-Hildreth-Operator", "Mexican Hat Filter" oder "Sombrerofilter" genannt. Dabei wird die Funktion
						\begin{equation*}
							\Delta G(x, y) = \frac{\partial^2 G(x, y)}{\partial x^2} + \frac{\partial^2 G(x, y)}{\partial y^2},\quad G(x, y) = \frac{1}{2\pi\sigma^2} \exp \Bigg\{ -\frac{x^2 + y^2}{2\sigma^2} \Bigg\}
						\end{equation*}
						approximiert.
					% end
				% end
				
				\subsubsection{Bilaterale Filter}
					Bei der bilateralen Filterung wird versucht, die Bilder weichzuzeichnen, aber scharfe Kanten zu erhalten. Dabei fließen Pixelfarben aus der Nachbarschaft nicht nur in Abhängigkeit von der Entfernung, sondern auch vom Farbabstand in die Berechnung ein.
				% end
			% end

			\subsection{Frequenzraum}
				Durch eine Multiplikation jeder Frequenz-Komponente \( F(u, v) \) anhand einer Gewichtungsfunktion (Filter), können bestimmte Komponenten erhöht oder verringert werden. Durch eine inverse Fouriertransformation werden die Veränderungen sichtbar. Eine solche selektive Beseitigung von Frequenz-Komponenten heißt \emph{Fourier-Filterung}. Filter werden \bspw eingesetzt, um den Einfluss von Datenfehlern oder Störsignalen zu verringern, hoch-/niederfrequente Signale zu trennen oder bestimmte Frequenzen hervorzuheben.
				
				Es werden dabei drei grundlegende Filtertypen unterschieden:
				\begin{itemize}
					\item Hochpass-Filter \\ Tiefe Frequenzen \( \lvert \omega \rvert < D_0 \) werden abgeschnitten und es können nur hohe Frequenzen passieren. Dadurch werden scharfe Übergange deutlicher.
					\item Tiefpass-Filter \\ Hohe Frequenzen \( \lvert \omega \rvert > D_0 \) werden abgeschnitten und es können nur niedrige Frequenzen passieren. Dadurch wird Rauschen eliminiert, das Bild aber etwas unschärfer.
					\item Bandpass-Filter \\ Es können nur Frequenzen aus dem Band \( D_0 < \omega < D_1 \) passieren.
				\end{itemize}

				\subsubsection{Idealer Tiefpass-Filter}
					Bei einem idealen Tiefpass-Filter werden alle Frequenzen jenseits einer Grenzfrequenz \( D_0 \) abgeschnitten und der "Kegel" ist radialsymmetrisch zum Ursprung. Der Filter hat dann die Form
					\begin{align*}
						H(u, v) &=
							\begin{cases}
								1 & D \leq D_0 \\
								0 & D > 0
							\end{cases} \\
						D(u, v) &= \sqrt{u^2 + v^2}
					\end{align*}
					Dieser Filter ist aber so physikalisch nicht realisierbar (dies liegt an der unendlich langen Impulsantwort, \zB bei einer Rechteck-Funktion)!
				% end

				\subsubsection{Gaußscher Tiefpass-Filter}
					Stattdessen wird ein Gaußscher Tiefpass-Filter eingesetzt. Da die Fourier-Transformation einer Gauß-Glocke wieder eine Gauß-Glocke ist, ist dieser Filter realisierbar.
				% end

				\subsubsection{Idealer Hochpass-Filter}
					Ein idealer Hochpass-Filter schneidet alle Frequenzen unter einer Grenzfrequenz \( D_0 \) ab:
					\begin{align*}
						H(u, v) &=
							\begin{cases}
								0 & D \leq D_0 \\
								1 & D > 0
							\end{cases} \\
						D(u, v) &= \sqrt{u^2 + v^2}
					\end{align*}
					Ebenso wie der ideale Tiefpass-Filter ist auch dieser Filter physikalisch nicht realisierbar.
				% end
			% end

			\subsection{Vergleich: Orts- und Frequenzraum-Filter}
				\begin{itemize}
					\item Frequenzraum-Filter können schnell berechnet werden (Fast Fourier-Transform), Ortsraumfilter sind meistens aber noch schneller.
					\item Einfache Handhabung (Das Filterdesign im Frequenzraum ist intuitiv).
					\item Ortsraumfilter sind nur eine Approximation der Frequenzraum-Filter (es sind keine unendlich breiten Filter möglich) und Abschneiden führt zu Artefakten.
				\end{itemize}
			% end
		% end

		\section{Bildkompression}
			Die Rasterung und Abtastung einer Intensitätsfunktion von Licht erzeugt eine große Menge an Daten, was unpraktisch zur Speicherung und Übertragung ist. Es ist somit eine kompaktere Darstellung gewünscht (ohne oder mit zumutbarem Qualitätsverlust).
			
			Bildkompression versucht dabei die Menge an Daten zur Repräsentation zu reduzieren:
			\begin{itemize}
				\item Eliminierung redundanter Daten
				\item Kodierungen
				\item Nachbarschaftsbeziehungen (räumlich, zeitlich)
				\item Psychovisuelle Effekte (Wahrnehmung des Menschen, Farbauflösung des Auges)
			\end{itemize}
			Die Kompressionsverfahren werden dabei in zwei Klassen eingeteilt:
			\begin{itemize}
				\item Verlustfreie Kompression, z.\,B.:
					\begin{itemize}
						\item Variable-Length-Coding (Huffman Code, Arithmesischer Code)
						\item Bit-Plane Coding (Bit-PLane Slicing, Run-Length Coding)
						\item Predictive Coding
						\item Lempel-Ziv-Welch-Algorithmus (LZW; GIF, TIFF, Kombination von Variable-Length und Run-Length Coding)
					\end{itemize}
				\item Verlustbehaftete Kompression
					\begin{itemize}
						\item Die Bildinformationen werden so komprimiert, dass nicht alle Eigenschaften berücksichtigt werden und eine exakte Rekonstruktion \ggf nicht mehr möglich ist.
						\item Viele Verfahren erlauben dem Anwender das Qualitäts-Kompressions-Verhältnis einzustellen (\zB JPEG oder PNG).
						\item Häufig werden Modelle der menschlichen Wahrnehmung verwenden (zur Identifizierung von für den Betrachter irrelevanten Bildeigenschaften, die nicht kodiert werden müssen).
					\end{itemize}
			\end{itemize}
		
			Beispiele für Kompressionsverfahren:
			\begin{itemize}
				\item Audio
					\begin{itemize}
						\item Unkomprimiert: AIFF, WAV, \dots
						\item Verlustlos: MPEG-4-ALC, Apple Lossless (ALAC), WMA Lossless, \dots
						\item Verlustbehaftet: MP3, Ogg Vorbis, MPEG-Audio, AAC (iTunes), WMA, \dots
					\end{itemize}
				\item Bilder
					\begin{itemize}
						\item Unkomprimiert: BMP, RAW, \dots
						\item Verlustlos: TIFF, GIF, PNG, \dots
						\item Verlustbehaftet: JPEG, JPEG2000, \dots
					\end{itemize}
				\item Video
					\begin{itemize}
						\item Unkomprimiert: Nicht praktikabel.
						\item Verlustlos: Nicht praktikabel.
						\item Verlustbehaftet: H.264 (DivX, QuickTime), MPEG-4 part 2 (Xvid, DivX), WMV, \dots
					\end{itemize}
			\end{itemize}

			\subsection{Harmonische Transformation}
				Bei einer Kompression durch harmonische Transformation werden die Daten in verschiedene Frequenzanteile zerlegt, \zB durch Fourier-Transformation oder Wavelet-Transformation. Ein typischer Vertreter ist die das JPEG-Kompressionsverfahren.

				\subsubsection{JPEG}
					JPEG ist eine Familie von Algorithmen zur Kompression in Echtfarbqualität (dabei gibt es verlustfreie und verlustbehaftete Verfahren). Die verlustbehafteten Prozesse sind für fotografische Aufnahmen mit fließenden Farbübergängen optimiert und daher nicht so gut für Text oder ähnlichen Bilddaten mit harten Kontrasten geeignet.
					
					Durch JPEG sind Kompressionsraten bis zu \( 1:20 \) bis \( 1:35 \) erreichbar, wobei diese in den Hauptanwendungsgebieten verlustbehaftet sind. Dabei basiert JPEG auf einer diskreten Kosinustransformation.

					\paragraph{Schritt 1: Umwandlung in den YCbCr-Farbraum}
						Im ersten Schritt werden die Farben als
						\begin{itemize}
							\item \(Y\) Helligkeitswert
							\item \(C_B\) Abweichung von Grau in Richtung Blau
							\item \(C_R\) Abweichung von Grau in Richtung Rot
						\end{itemize}
						kodiert:
						\begin{equation*}
							\begin{bmatrix}
								Y \\
								C_B \\
								C_R
							\end{bmatrix}
							\approx
							\begin{bmatrix}
								0 \\
								128 \\
								128
							\end{bmatrix}
							+
							\begin{bmatrix}
								0.299     & 0.587     & 0.114     \\
								-0.168736 & -0.331264 & 0.5       \\
								0.5       & -0.418688 & -0.081312
							\end{bmatrix}
							\cdot
							\begin{bmatrix}
								R \\
								G \\
								B
							\end{bmatrix}
						\end{equation*}
					% end

					\paragraph{Schritt 2: Farb-Subsampling}
						Die Farben werden verlustbehaftet Komprimiert (dies ist aufgrund der höheren Genauigkeit des Auges im grünen Bereich möglich). Dabei wird für ein kleines Gebiet (üblicherweise \(2 \times 2\) Pixel) die Farbdifferenzwerte \(C_R\) und \(C_B\) gemittelt und für das gesamte Gebiet zusammen angegeben.
					% end

					\paragraph{Schritt 3: Diskrete Kosinustransformation}
						In diesem Schritt werden die Bildinformationen in den Frequenzbereich zerlegt. Dazu wird zunächst jede Komponente \( (Y, C_B, C_R) \) in \( 8 \cdot 8 = 64 \) Bildblöcke gerastert und diese anschließend einer diskreten Fourier-Transformation unterzogen. Dabei wird nur der Kosinus-Teil berechnet, da dadurch die Berechnung einfacher wird.
						
						Das Ziel ist, die Informationen in eine Darstellung zu überführen, die besser für die folgenden Schritte geeignet ist.
						
						Vorteil: Wenn sich benachbarte Bildpunkte kaum unterscheiden, \dh das Bild keine scharfen Kanten hat, sind die meisten Koeffizienten gleich Null.
					% end

					\paragraph{Schritt 4: Quantisierung}
						Bei der Quantisierung werden die Informationsanteile beseitigt, die das Auge nicht oder nur schlecht wahrnimmt.
					% end

					\paragraph{Schritt 5: Kodierung der Koeffizienten}
						Aus den entstehenden Blöcken wird ein sequentieller Bitstrom erzeugt und die Koeffizienten werden als Differenzen zum vorhergehenden Koeffizienten kodiert (durch die Kohärenz ergeben sich hier kleine Werte). Die Koeffizienten werden dabei entlang einer Zick-Zack-Kurve kodiert (ähnliche wie bei Cantors Diagonalargument). Da hohe Frequenzen oft sehr klein sind, entsteht so eine für die Kompression günstige Reihenfolge.
						
						Bisher wurde noch nichts wirklich komprimiert, sondern nur grob Transformiert. Der entstehende Bitstrom kann nun aber durch typische Kompressionstechniken (Huffman-Algorithmus, Arithmetisches Kodieren) komprimiert werden.
					% end
				% end
			% end
		% end
	% end

	\chapter{Bildverarbeitung, Deblurring}
		Beim Deblurring wird versucht, eine vorhandene Verwischung (Blurring) eines Bildes zu entfernen.
	
		Sei bekannt, dass das Bild \(g\) die mit einer Faltung \(a\) verwischte Version (Blurring) eines Bildes \(f\) ist, \dh \( g = a(f) \) (oftmals ist \(a\) eine Gauß-Glocke). Im Fourier-Raum ergibt sich dann \( G = A \cdot F \) und die Rekonstruktion des Bildes \(f\), \bzw der Frequenzen \(F\), scheint mit \( F = A^{-1} \cdot G \) sehr einfach. Bei dieser Rekonstruktion treten jedoch mehrere Probleme auf:
		\begin{enumerate}
			\item Der Blurring-Kernel \(A\) kann unendlich klein werden, sodass es beinah zu einer Division durch Null kommt. Dadurch werden Rauschen und numerische Fehler verstärkt.
			\item Es gibt immer Rauschen \(n\): \( g = a(f) + n \)
		\end{enumerate}
		Für das erste Problem kann verwendet werden, dass \(A\) \iA komplex ist. Es gilt dann mit der komplex konjugierten Matrix \( A^\ast \):
		\begin{equation*}
			G = A \cdot F \quad\implies\quad A^\ast \cdot A \cdot F = \lvert A \rvert^2 \cdot F
		\end{equation*}
		Und mit \( \lvert A \rvert^2 > 0 \) kann die Rekonstruktion umgeformt werden:
		\begin{equation*}
			F = \frac{1}{A} G = \frac{A^\ast}{A^\ast A} G = \frac{A^\ast}{\lvert A \rvert^2} G
		\end{equation*}
		Dadurch hat das rekonstruierte Bild nun keine reellen Zahlen mehr.

		\section{Korrekt gestellte Probleme}
			Nach Jacques Hadamard ist ein mathematisches Modell \emph{korrekt gestellt}, wenn:
			\begin{itemize}
				\item Eine Lösung existiert,
				\item diese eindeutig ist und
				\item die Lösung in einer vernünftigen Topologie kontinuierlich von den Daten abhängt.
			\end{itemize}
			Ansonsten ist das Problem nicht korrekt gestellt.
			
			Als Konsequenz daraus folgt, dass Blurring korrekt gestellt, Deblurring aber nicht korrekt gestellt ist. Daher ist eine Regularisierung notwendig, \dh es werden zusätzliche Annahmen (Glätte, Informationen zum Rauschen) hinzugenommen.
		% end

		\section{Einschrittverfahren}
			\subsection{Wiener Filter}
				Das zweite Problem von Deblurring (Rauschen) kann durch eine Regularisierung des Filter im Fourierraum:
				\begin{equation*}
					F = \frac{A^\ast}{\lvert A \rvert^2 + R^2} G
				\end{equation*}
				reduziert werden (dies wird als \emph{Wieder Filter} bezeichnet). Dabei ist \(R\) das Verhältnis von Rauschen zum Signal. Der Parameter entscheidet dabei, das verstärkt wird.
				\begin{itemize}
					\item Ist \(R\) zu groß, so verhält sich der Filter wie ein Tiefpass-Filter, d.\,h.:
						\begin{itemize}
							\item Grobe Struktur bleibt erhalten.
							\item Kanten werden verwischt.
							\item Rauschen wird entfernt.
						\end{itemize}
					\item Ist \(R\) zu klein, so verhält sich der Filter wie ein Hochpass-Filter, d.\,h.:
						\begin{itemize}
							\item Grobe Strukturen werden entfernt.
							\item Kanten werden entfernt.
							\item Rauschen wird verstärkt.
						\end{itemize}
					\item Ist \(R\) optimal, so verhält sich der Filter wie ein Bandpass-Filter, d.\,h.:
						\begin{itemize}
							\item Grobe Struktur bleibt erhalten.
							\item Kanten werden verstärkt.
							\item Rauschen wird entfernt.
						\end{itemize}
				\end{itemize}
			
				\textbf{Vorteile:}
				\begin{itemize}
					\item Schnell
					\item Häufig verwendet
					\item Beliebt (dadurch viel Know-How vorhanden)
					\item Leicht zu implementieren
				\end{itemize}
				\textbf{Nachteile:}
				\begin{itemize}
					\item Nur ein Filter für das gesamte Bild
					\item Keine lokalen, spezifischen Verbesserungen
					\item Nur ein Wert für \(R\)
				\end{itemize}
				Der Wiener-Filter kann \zB durch lokale Verfeinerungen (Mehrkomponentenverfahren) oder iterative Verfeinerungen (Mehrschrittverfahren) verbessert werden.
			% end

			\subsection{Mehrkomponentenverfahren}
				\subsubsection{Scale-Space-Ansatz}
					Zum Schärfen wird der Laplace-Operator, multipliziert mit einer unabhängigen Konstante \(t\), vom Bild abgezogen:
					\begin{equation*}
						L_\text{schärfer} = L_0 - t \underbrace{\big( L_{xx} + L_{yy} \big)}_{= \Delta L} = L_0 - t \cdot \Delta L
					\end{equation*}
					Durch Hinzufügen von zusätzlichen Termen (mit Ableitungen höherer Ordnung) kann das Ergebnis weiter verfeinert werden:
					\begin{equation*}
						L_\text{schärfer} = L_0 - t \big( L_{xx} + L_{yy} \big) + \frac{1}{2} t^2 \big( L_{xxxx} + 2L_{xxyy} + L_{yyyy} \big) - \frac{1}{6} t^3 \big( L_{xxxxxx} + 3L_{xxxxyy} + 3L_{xxyyyy} + L_{yyyyyy} \big)
					\end{equation*}
					Diese Sequenz ist eine Taylorreihe der partiellen Differentialgleichung
					\begin{equation*}
						\frac{\partial L}{\partial t} = \frac{\partial^2 L}{\partial x^2} + \frac{\partial^2 L}{\partial y^2}
					\end{equation*}
					in \(-t\). Die Veränderung in einem Bild über eine gewisse Zeit ist also durch eine partielle Differentialgleichung zweiter Ordnung definiert.
				% end
			% end
		% end

		\section{Mehrschrittverfahren (Iterative Methoden)}
			\subsection{Energie und Variationsableitung}
				Sei \(E\) die \emph{Energie} eines Bildes \(L\):
				\begin{equation*}
					E(L) = \frac{1}{2} \iint_{x, y} \! L^2 \dif{x} \dif{y}
				\end{equation*}
				Diese "Energie" sagt aus, wie viel Intensität in den Pixeln vorhanden ist. Eine Minimierung der Intensität führt dann zum optimalen Bild \bzgl der definierten Energie. Dieses Minimum kann durch Variationsrechnung oder iterative Prozesse gefunden werden.
			
				Die Variationsableitung \( \delta E(L) \) eine eine Verallgemeinerung der normalen Ableitung, wobei dieses Prinzip hier nicht näher beleuchtet werden soll. Für die Energie
				\begin{equation*}
					E(L) = \frac{1}{2} \iint_{x, y} \! L^2 \dif{x} \dif{y}
				\end{equation*}
				gilt \( \delta E(L) = L \) und das Minimum liegt bei \( \delta E(L) \overset{!}{=} 0 \). In diesem Fall liegt das Minimum also bei \( L = 0 \), \dh \( E(L) = 0 \).
				
				Für die Energie
				\begin{equation*}
					E(L) = \frac{1}{2} \iint_{x, y} \! L_x^2 + L_y^2 \dif{x} \dif{y}
				\end{equation*}
				folgt \( \delta E(L) = -\big( L_{xx} + L_{yy} \big) = -\Delta L \). Hier ist es weniger trivial das Minimum zu finden, weshalb das System in eine partielle Differentialgleichung
				\begin{equation*}
					L_t = -\delta E(L)
				\end{equation*}
				überführt wird und das Minimum iterativ gesucht wird.
			% end

			\subsection{Alternativen}
				Da mit den bisherigen Energien keine guten interessanten Bilder generiert werden können, gibt es noch andere Energien, z.\,B.:
				\begin{itemize}
					\item Perona-Malik
						\begin{itemize}
							\item Rauschen wird verwischt
							\item Kanten werden verstärkt
							\item Smart Energy Term und Stoppzeit
						\end{itemize}
					\item Totale Variation
						\begin{itemize}
							\item Rauschen wird verwischt
							\item Kanten werden verstärkt
							\item Smart Energy Term und Distance Penalty
						\end{itemize}
					\item uvm.
				\end{itemize}
			% end

			\subsection{Perona-Malik}
				Die Heat-Equation \( L_t = L_{xx} + L_{yy} = \Delta L \) wird modifiziert zu
				\begin{equation*}
					\partial_t L = \nabla \circ \big( c \cdot \nabla L \big)
				\end{equation*}
				wobei \(c\) der \emph{Conductivity Coefficient} ist, durch den die Diffusion an lokale Bildstrukturen anpassbar ist (\dh \( c = c(L, L_x, L_xx, \cdots) \)). Mit \( c = 1 \) fällt die Methode zurück zum gaußschen Scale-Space.

				\subsubsection{Die Perona-Malik-Gleichung}
					Nach Perona und Malik ist \( c(\cdot) \) eine Funktion der Gradientenstärke, welche die Diffusion dort reduziert, wo Kanten sind (\(c\) nahe Null) und in flachen Bereichen erhöht. Dies eingesetzt ergibt:
					\begin{equation*}
						\partial_t L = \nabla \circ \Big( c\big( \lvert \nabla L \rvert^2 \big) \cdot \nabla L \Big)
					\end{equation*}
					Für \(c\) gibt es im Grunde zwei Möglichkeiten, die beide mehr oder weniger das gleiche Verhalten erzeugen:
					\begin{equation*}
						c_1 = \exp \Bigg\{ -\frac{\lvert \nabla L \rvert^2}{k^2} \Bigg\} \quad\quad\quad\quad c_2 = \Bigg( 1 + \frac{\lvert \nabla L \rvert^2}{k^2} \Bigg)^{-1}
					\end{equation*}
					Dabei bestimmt \(k\) den Einfluss der Kantenstärke. Bei einem großen \(k\) bleiben nur die größten Gradienten (starke Kanten) übrig, bei einem kleinen \(k\) bleiben (fast) alle Gradienten (Kanten, Rauschen) übrig.
				% end

				\subsubsection{Implementierung}
					Für die Lösung der Perona-Malik-Gleichung gibt es keine analytischen Methoden, weshalb eine iterative Methode eingesetzt wird:
					\begin{equation*}
						L^{(t + 1)} = L^{(t)} + \Delta t \cdot \bigg( \nabla \circ \Big( c\big(\lvert \nabla L \rvert^2\big) \cdot \nabla L \Big) \bigg)
					\end{equation*}
					dabei ist:
					\begin{itemize}
						\item \( L^{(0)} \) das originale Bild und
						\item \( \Delta t \) ein kleiner Zeitschritt.
					\end{itemize}
					"Irgendwann" muss die Iteration beendet werden, \zB nach \(n\) Schritten oder wenn \( L^{(t + 1)} \) "gut aussieht".
				% end

				\subsubsection{Stoppzeit}
					Während der Iteration steige das Signal-Rausch-Verhältnis \iA an und fällt danach wieder ab (das Bild konvergiert gegen eine gleichmäßig graue Fläche). Das heißt die Iteration stoppt nicht bei der optimal Lösung und es wird eine \emph{Stoppzeit} benötigt.
				% end
			% end

			\subsection{Eingeschränkte Evolution: Totale Variation}
				Wird sichergestellt, dass die Iterationen gegen die optimale Lösung konvergieren, so ist keine benutzerdefinierte Stoppzeit nötigt. Daher wird versucht, eine klug gewählte Energie zu minimieren und ein \emph{Distance Penalty} hinzuzufügen.

				\subsubsection{Distance Penalty}
					Bei einem Distance Penalty wird zusätzlich zum Modell des Bildes ein Rauschmodell angenommen (\zB gaußsches Zufallsrauschen), sodass es zusätzliche Bedingungen an die Lösung \(L\) gibt:
					\begin{align*}
						\iint_{x, y} \! \big( g - a(L) \big) \dif{x} \dif{y} &= 0 \\
						\iint_{x, y} \! \big( g - a(L) \big)^2 \dif{x} \dif{y} &= \sigma^2
					\end{align*}
					Diese Beschränkungen können zur Energie hinzugefügt werden, um das Konvergenzverhalten zu verbessern.
				% end

				\subsubsection{Totale Variation}
					Bei der Methode der totalen Variation wird die Energie
					\begin{equation*}
						E(L) = \iint_{x, y} \! \Big( \lvert \nabla L \rvert + \lambda\big( g - a(L) \big)^2 \Big) \dif{x} \dif{y}
					\end{equation*}
					minimiert, wobei \(\lambda\) ein vom Rauschen abhängiger Parameter ist. Da die totale Variation gegen die optimale Lösung konvergiert, wird keine Stoppzeit benötigt.
				% end

				\subsubsection{Erweiterungen}
					\begin{itemize}
						\item Schwierigere Funktionen/Energien (Statistik, Niveaumengen für Segmentierung, \dots)
						\item Andere Rauschstatistiken (\zB Multiplikatives Rauschen)
						\item Andere Steuerungsmechanismen für den Conductive Coefficient \(c\) (\zB Kantenverstärkende Diffusion, Kohärenzverstärkende Diffusion, \dots)
					\end{itemize}
				% end
			% end
		% end
	% end

	\chapter{Grafikpipeline}
		\section{Hardware}
			Abbildung~\ref{fig:hardware_and_paradigms} zeigt einen Überblick über Computer-Paradigmen. Die Überschneidungen sind dabei Bereiche, in denen mehrere Paradigmen zusammenlaufen.
		
			\begin{figure}
				\centering
				\begin{tikzpicture}[->, every node/.style = { align = center }]
					\node [draw, circle, minimum width = 5cm] (p1) at (0, 1) {P1: Large Scale};
					\node [draw, circle, minimum width = 5cm] (p4) at (0, 4) {P4: Network};
					\node [draw, circle, minimum width = 5cm] (p2) at (-2, 7) {P2: Personal};
					\node [draw, circle, minimum width = 5cm] (p3) at (2, 7) {P3: Mobile};
					
					\path (p1) -- coordinate(a) (p4);
					\path (p4) -- coordinate(b) (p2);
					\path (p4) -- coordinate(c) (p3);
					\path (p2) -- coordinate(d) (p3);
					\coordinate (e) at (intersection of b--p3 and c--p2);
					
					\node [left  = 3 of b, yshift = -1cm] (B) {Public-Personal};
					\node [right = 3 of c, yshift = -1cm] (C) {Virtual Reality, \\ Ambient, Invisible};
					\node [above = 5 of e]                (E) {Collaborative, \\ Wearable, Uniquitous};
					
					\draw (B) -- (b);
					\draw (C) -- (c);
					\draw (E) -- (e);
				\end{tikzpicture}
				\caption{Hardware und Paradigmen}
				\label{fig:hardware_and_paradigms}
			\end{figure}
		
			\subsection{P1: Large-Scale-Computing}
				Die ursprünglichen Mainframe-Computer waren sehr große Rechenmaschinen, die als Hosts angesteuert wurden. Der Zugriff auf diese fand über externe, mit Tastaturen ausgerüstete, alphanumerische, Terminals statt (Host- und Terminal-System).
			% end

			\subsection{P2: Personal/Desktop Computing}
				Computer stehen nun Zuhause und sind sehr viel kleiner.
			% end

			\subsection{P3: Networked Computing}
				Computer kommunizieren untereinander.
			% end

			\subsection{P4: Mobile Computing}
				Es gibt viele verschiedene Arten von mobilen Geräten (Laptops, Tablets, Smartphones, \dots).
			% end

			\subsection{ZP1: Collaborative Computing}
				Zum Beispiel Multi-Touch-Tables.
			% end

			\subsection{ZP2: Virtual Reality}
				Virtuelle Realität lässt den Menschen in virtuelle Welten eintauchen. Dabei gibt es zwei Arten:
				\begin{itemize}
					\item Nicht-immersive Umgebungen (Bildschirm- und Zeigerbasiert, 3D-Anzeige, \mglw haptisches Feedback).
					\item Immersive Umgebungen (es wird tatsächlich der Eindruck erweckt, in einer Welt aus virtuellen Objekten zu sein).
				\end{itemize}
			% end

			\subsection{Augmented Reality}
				Augmented Reality beschreibt die Integration von virtuellen und realen Objekten, wobei die Wahrnehmung des Benutzers erweitert und verbessert werden soll. Die Ein- und Ausgabegeräte sind \zB Heads-Up-Displays (HUDs) oder Head-Mounted-Displays (HMDs).
			% end

			\subsection{Ambient/Invisible}
				Zum Beispiel Freiraum-Gestenerkennung.
			% end

			\subsection{Wearable/Ubiquitous}
				Die Geräte müssen nicht mehr explizit gehalten werden, sondern sie sind "anziehbar", \zB eine Smartwatch.
			% end
		% end

		\section{Computergrafik}
			Abbildung~\ref{fig:computer_graphics} zeigt eine stark vereinfachte Pipeline der Computergrafik.
		
			\begin{figure}
				\centering
				\begin{tikzpicture}[->, every node/.style = { draw, rectangle, align = center }]
					\node (a) {3D-Objekte};
					\node [right = 2 of a] (b) {3D-Modelle \\ (Szene, Geometrie, \\ Material, Beleuchtung) \\ Notwendig zur \\ Rasterisierung};
					\node [right = 2 of b] (c) {Bilder \\ (Interaktion, \\ Animation)};
					
					\draw (a) -- (b);
					\draw (b) -- (c);
				\end{tikzpicture}
				\caption{Einfache Pipeline der Computergrafik.}
				\label{fig:computer_graphics}
			\end{figure}
		
			\subsection{Geschichte} % N/A
				\todo{Content}

				\subsubsection{Die Anfänge} % 7.26, 7.27
					\todo{Content}
				% end

				\subsubsection{60er Jahre} % 7.28
					\todo{Content}
				% end

				\subsubsection{70er Jahre} % 7.29
					\todo{Content}
				% end

				\subsubsection{80er Jahre} % 7.31
					\todo{Content}
				% end

				\subsubsection{90er Jahre} % 7.32
					\todo{Content}
				% end

				\subsubsection{2000 bis 2005} % 7.33
					\todo{Content}
				% end

				\subsubsection{2006 bis 2020} % 7.34, 7.35
					\todo{Content}
				% end
			% end

			\subsection{Uncanny Valley}
				Das \emph{Uncanny Valley} beschreibt eine Beziehung zwischen der Ähnlichkeit eines Objekts zum Menschen und der emotionalen Reaktion darauf. Dabei tritt kurz vor einer absoluten Ähnlichkeit ein Tief auf, welches als Uncanny Valleys bezeichnet wird. In diesem Bereich werden Objekte besonders gruselig wahrgenommen (siehe \zB \href{fig:computer_graphics}{hier} für eine Demonstration dieses Effekts).
			% end
		% end

		\section{Grafikpipeline}
			Abbildung~\ref{fig:graphics_pipeline} zeigt die typische Grafikpipeline von der Anwendung zur Ausgabe. Dabei geschehen folgende Schritte:
			\begin{itemize}
				\item Anwendung \\ Dieser Schritt produziert eine Modellierung der Daten.
				\item Geometrieverarbeitung \\ Dieser Schritt produziert 2D-Koordinaten und zusätzliche Daten (z-Buffer Werte, Farbwerte pro Knoten/Primitiv).
				\item Rasterisierung \\ Dieser Schritt produziert ein Rasterbild (Bildspeicher).
				\item Ausgabe \\ Speichern/Anzeigen des Bilds.
			\end{itemize}
		
			\begin{figure}
				\centering
				\begin{tikzpicture}[block/.style = { draw, rectangle, minimum height = 1cm, minimum width = 3cm, align = center }]
					\node [block, label = above:CPU] (a) {Anwendung};
					\node [block, right = 1.5 of a] (b) {Geometrie- \\ verarbeitung};
					\node [block, right = 1 of b] (c) {Rasterisierung};
					\node [block, right = 1.5 of c, label = above:{\zB Monitor}] (d) {Ausgabe};
					
					\coordinate [above left  = 0.5 of b.north west] (A);
					\coordinate [below left  = 0.5 of b.south west] (B);
					\coordinate [below right = 0.5 of c.south east] (C);
					\coordinate [above right = 0.5 of c.north east] (D);
					\path (D) -- coordinate(X) (C);
					\path (A) -- coordinate(Y) (B);
					
					\path (A) -- node[above]{GPU} (D);
					
					\draw (A) -| (C) -| cycle;
					
					\draw [->] (a) -- (Y);
					\draw [->] (b) -- (c);
					\draw [->] (X) -- (d);
				\end{tikzpicture}
				\caption{Typische Grafikpipeline}
				\label{fig:graphics_pipeline}
			\end{figure}
		% end

		\section{Anwendung}
			Die Anwendung beschäftigt sich mit der Eingabe von grafischen Daten sowie deren Repräsentation.
		
			\subsection{Eingabe grafischer Daten}
				Grafische Daten können \bspw durch die Generierung von 3D-Modellen oder der Abtastung realer Objekte eingegeben werden.
			% end
	
			\subsection{Repräsentation von 3D-Daten}
				\subsubsection{Grafische Primitive}
					Grafische Primitive, aus denen ein größeres Modell zusammengesetzt werden kann, sind z.\,B.:
					\begin{itemize}
						\item Punkte
						\item Linien
						\item Dreiecke
					\end{itemize}
				% end
	
				\subsubsection{Transformationen}
					Siehe Kapitel~\ref{c:transformations}.
				% end
			% end

			\subsection{Räumliche Datenstrukturen}
				Räumliche Datenstrukturen werden \bspw zum View Frustum Cullung (feststellen der Sichtbarkeit eines Objekts im Sichtvolumen), Occlusion Culling/z-Buffer (Verdeckung) oder Kollisionserkennung genutzt. Sie werden in folgende Klassen eingeteilt:
				\begin{itemize}
					\item Hüllkörperhierarchie (\bspw Bounding Sphere Hierarchy)
					\item Raumunterteilung (Gitter, Hierarchisch (k-d Tree, Quadtree, Octree, Binary Space Partition))
				\end{itemize}
	
				\subsubsection{Hüllkörper/-hierarchien}
					Ein \emph{Hüllkörper} ist eine einfache Form um den eigentlichen Körper herum, sodass sich Schnitttests mit anderen Primitiven einfach durchführen lassen. Typische Primitive sind \zB Kugeln, Rechtecke oder rotierte Rechtecke. Eine \emph{Hüllkörperhierarchie} wird dann aus mehreren solchen Hüllkörpern zusammengesetzt.
				% end
	
				\subsubsection{Raumunterteilung}
					\paragraph{Achsenparallele Gitter (Grids)}
						Der Raum wird in ein Gitter eingeteilt, wobei Objekte in mehreren Zellen enthalten sein können (Redundanz). Die Aufteilung kann sich demnach nicht der Geometrie anpassen und ist sehr speicherintensiv. Dennoch ist sie sehr effizient traversierbar und es ist ein schneller Zugriff auf Nachbarn möglich.
					% end
	
					\paragraph{Quadtree/Octree}
						Bei einem \emph{Quadtree} wird der Raum in ein Gitter aufgeteilt und aus diesem ein Baum berechnet (siehe \href{https://projects.frisp.org/documents/29}{GdR Zusammenfassung}, Abschnitt "Approximative Zellzerlegung").
						
						Ein \emph{Octree} entspricht dann einem Quadtree im dreidimensionalen.
					% end
	
					\paragraph{Binary Space Partition}
						Bei einer \emph{Binary Space Partition} (BSP) wird der Raum binär unterteilt, wobei immer an den durch Polygone induzierten Ebenen geteilt wird. Dann entspricht jeder Knoten einer Unterteilungsebene, die den Raum in zwei Halbräume unterteilt.
					% end
				% end
			% end
		% end

		\section{Geometrieverarbeitung}
			\subsection{Modelltransformation (Orientierung, Position)} % 7.62, 7.63
				\todo{Content}
			% end

			\subsection{Simulation der Beleuchtung}
				Um die Beleuchtung eines Objekts zu simulieren, müssen zunächst die Leuchtdichten eines Primitivs bestimmt werden (in der Praxis findet die Bestimmung der Leuchtdichte pro Pixel erst während der Rasterisierung statt). Zur Bestimmung der Beleuchtung gibt es unterschiedliche Ansätze:
				\begin{itemize}
					\item Flat Shading
						\begin{itemize}
							\item Die Normale des Primitivs ergibt eine einheitliche Helligkeit.
						\end{itemize}
					\item Gouraud Shading
						\begin{itemize}
							\item Die Normalen in den Eckpunkten ergeben die Helligkeitswerte für die Eckpunkte.
							\item Die Helligkeitswerte der Eckpunkte werden linear interpoliert.
						\end{itemize}
					\item Phong Shading
						\begin{itemize}
							\item Die Normalen in den Eckpunkten werden für jeden Punkt linear interpoliert und normiert.
							\item Der Helligkeitswert ergibt sich aus der interpolierten Normalen.
						\end{itemize}
				\end{itemize}

				\subsubsection{Phong-Beleuchtungsmodell}
					Im Phong-Beleuchtungsmodell setzt sich die Gesamtbeleuchtung
					\begin{equation*}
						I_\textit{total} = I_\textit{amb} + I_\textit{diff} + I_\textit{spec}
					\end{equation*}
					aus
					\begin{itemize}
						\item Ambienter Reflexion \( I_\textit{amb} \) (global modelliert),
						\item Diffuser Reflexion \( I_\textit{diff} \) (lokal modelliert) und
						\item Spiegelnder Reflexion \( I_\textit{spec} \) (lokal modelliert).
					\end{itemize}
					zusammen. Die ambiente Komponente ist richtungsunabhängig und abhängig von dem Umgebungslicht \(C\) sowie der Materialkonstanten \(k\):
					\begin{equation*}
						I_\textit{amb} = k_\textit{amb} C_\textit{amb}
					\end{equation*}
					Die diffuse Reflexion ist abhängig von der Richtung des Lichts, der Oberflächennormalen \( \vec{N} \) und der Richtung zum Licht \( \vec{L} \):
					\begin{equation*}
						I_\textit{diff} = k_\textit{diff} C_\textit{light} \big( \vec{N} \circ \vec{L} \big)
					\end{equation*}
					Die spiegelnde Reflexion ist abhängig von der Richtung der Reflexion \( \vec{R} \), der Betrachtungsrichtung \( \mat{V} \) und der "Rauheit" \(m\) des Materials:
					\begin{equation*}
						I_\textit{spec} = k_\textit{spec} C_\textit{light} \big( \vec{R} \circ \vec{V} \big)^m
					\end{equation*}
				% end
			% end

			\subsection{Perspektivische Transformation und Clipping (Abschneiden)}
				Liegen mehrere Objekte voreinander, so ist der dem Auge nächste Punkt sichtbar (es sei denn, das Objekt ist transparent, dann wird auch der dahinterliegende Punkt sichtbar, usw.). \emph{Clipping} bezeichnet nun das Abschneiden von Objekten am Rand des gewünschten Bildausschnitts.

				\subsubsection{Painters Algorithmus}
					Bei diesem Algorithmus werden die Primitive "wie von einem Maler" gezeichnet: Es wird mit dem tiefsten z-Wert begonnen und die Objekte mit aufsteigendem z-Wert darüber gezeichnet. So verdecken sich hintereinander liegende Polygone "automatisch". Jedoch sind transparente Objekte sowie "im Kreis überdeckende" Objekte nicht korrekt darstellbar.
				% end
			% end

			\subsection{Culling (Verdeckungsrechnung im Objektraum)}
				Üblicherweise machen die Rückseiten von Objekten \ca die Hälfte der vorkommenden Flächen aus, können aber nicht gesehen werden. Durch \emph{Culling} werden die Rückseiten berechnet und beim Rendering explizit ausgeschlossen, um Rechenleistung zu sparen.
				
				Eine Fläche ist immer dann eine Rückseite, wenn das Skalarprodukt von Sehstrahl \( \vec{s} \) und Normale \( \vec{n} \) positiv ist: \( \vec{n} \circ \vec{s} > 0 \).
			% end

			\subsection{Projektion}
				Siehe~\ref{sec:projection}.
			% end
		% end

		\section{Rasterisierung}
			Bei der \emph{Rasterisierung} werden die Primitive (Linien, Polygone) in Pixel zerlegt und zusätzlich pro Pixel eine Verdeckungsrechnung und Shading durchgeführt.

			\subsection{Scan-Konvertierung}
				\subsubsection{Rasterisierung von Linien (Bresenham-Algorithmus)}
					Der Bresenham-Algorithmus~\ref{alg:bresenham} ist ein Algorithmus zum Zeichnen von Linien von Anfangspunkt \( (x_1, y_1) \) mit Endpunkt \( (x_2, y_2) \). Mit \( \Delta x \coloneqq x_2 - x_1 \geq 0 \) und \( \Delta y \coloneqq y_2 - y_1 \geq 0 \) (die Bedingung \( \geq 0 \) kann durch geschickte Vertauschung immer eingehalten werden), muss der Algorithmus genau \( \max \big\{ \Delta x, \Delta y \big\} + 1 \) Pixel zeichnen.
					
					\begin{algorithm}
						\DontPrintSemicolon
						
						\KwIn{Startpunkt \( (x_1, y_1) \), Endpunkt \( (x_2, y_2) \)}
						
						\( \delta x \gets x_2 - x_1 \)\;
						\( \delta y \gets y_2 - y_1 \) \;
						\( x \gets x_1 \) \;
						\( y \gets y_1 \) \;
						
						
						Setze Pixel \( (x, y) \) \;
						\( \xi \gets \delta x / 2 \) \;
						
						\While{\( x < x_2 \)}{
							\( x \gets x + 1 \) \;
							\( \xi \gets \xi - \delta y \) \;
							
							\If{\( \xi < 0 \)}{
								\( y \gets y + 1 \) \;
								\( \xi \gets \xi + \delta x \)
							}
						
							Setze Pixel \( (x, y) \);
						}
					
						\caption{Bresenham-Algorithmus zum Rastern einer Linie.}
						\label{alg:bresenham}
					\end{algorithm}
				% end
	
				\subsubsection{Rasterisierung von Polygonen (Scanline Algorithmus)}
					Polygone können \bspw mit dem \emph{Scanline Algorithmus} gerastert werden. Dieser scannt die Pixelebene von oben nach unten mit einer "Scan Line" durch und findet alle Schnittpunkte mit den Kanten des Polygons. Anschließend werden die Schnittpunkt nach \(x\)-Koordinaten sortiert und die Pixel zwischen Paaren aufeinanderfolgender Schnittpunkte gefüllt. Dabei wird eine Parität mitgeführt, die in jedem Schritt um Eins erhöht wird (beginnend von Null). Ist die Parität ungerade, wird der Pixel gesetzt, sonst nicht.
				% end
			% end

			\subsection{Verdeckungsrechnung}
				\subsubsection{z-Buffer-Algorithmus}
					\begin{itemize}
						\item Zu jedem Bildpunkt wird noch ein z-Wert gespeichert.
						\item Initialisierung: Der Bildspeicher wird auf die Hintergrundfarbe gesetzt, der z-Speicher auf den maximalen Wert.
						\item Anschließend werden alle Objekte der Szene nacheinander gerastert, wobei keine besondere Reihenfolge notwendig ist: \\
								Für jeden Punkt \( (x, y) \) eines Polygons wird \( z(x, y) \) berechnet. Aufgrund der perspektivischen Transformation ist eine lineare Interpolation nicht mehr einfach möglich! Ist \( z(x, y) \) kleiner als der bereits gespeicherte Wert, so wird \( z(x, y) \) gespeichert und der zugehörige Farbwert in den Bildspeicher geschrieben.
						\item Nach der Behandlung aller Objekte steht im Objektspeicher das Bild der gewünschten (Teil-)~Flächen.
					\end{itemize}
				
					\textbf{Vorteile:}
					\begin{itemize}
						\item Jede Szene mit jeder Art von Objekten kann behandelt werden.
						\item Die Komplexität ist unabhängig von der Tiefenkomplexität.
						\item In eine fertige Szene können nachträglich Objekte eingefügt werden.
						\item Spezielle Objekte (\zB ein 3D-Cursor) können in der Szene mit korrekter Verdeckung dargestellt werden.
						\item Leicht in Hardware zu realisieren.
					\end{itemize}
					\textbf{Nachteile:}
					\begin{itemize}
						\item Für jeden Bildpunkt wird nur ein Objekt gespeichert (dies führt zu Abtastfehlern).
						\item Transparenz ist prinzipiell nicht realisierbar.
						\item Die Genauigkeit des z-Buffers ist beschränkt (getrennte Objekte erhalten den selben z-Wert, die Farbe wird dann von der Objektreihenfolge bei der Rasterung bestimmt).
					\end{itemize}
				% end
			% end
		% end
	% end

	\chapter{Transformationen}
		\label{c:transformations}
		
		Aus Sicht der Grafikpipeline gibt es viele unterschiedliche Koordinaten:
		\begin{itemize}
			\item Objektkoordinaten (Festlegung der lokalen Lage von 3D-Objekten)
			\item Weltkoordinaten (Beschreibung der gesamten Szene in 3D)
			\item Projektionskoordinaten (nach der Anwendung der Projektionstransformation)
			\item Bildschirmkoordinaten (Darstellung der Szene in einem Fenster mit gewählter Position und Größe)
		\end{itemize}
		Abbildung~\ref{fig:pipeline_coordinates} zeigt die Umwandlung der Koordinaten innerhalb der Grafikpipeline.
	
		\begin{figure}
			\centering
			\begin{tikzpicture}[ every node/.style = { align = center }, block/.style = { draw, rectangle }]
				\coordinate (a);
				\node [block, right = 1.5 of a] (b) {Modell};
				\node [block, right = 1 of b] (c) {Projektions- \\ Transformation};
				\node [block, right = 1 of c] (d) {Normierung};
				\node [block, right = 1 of d] (e) {Viewport \\ Transformation};
				\coordinate [right = 1.5 of e] (f);
				
				\draw [->] (a) -- coordinate[label = above:Vertex](A) (b);
				\draw [->] (b) -- coordinate(B) (c);
				\draw [->] (c) -- coordinate(C) (d);
				\draw [->] (d) -- coordinate(D) (e);
				\draw [->] (e) -- coordinate(E) (f);
				
				\node [below = 2 of A] (aA) {Objekt- \\ koordinaten};
				\node [below = 2 of B] (bB) {Welt- \\ koordinaten};
				\node [below = 2 of C] (cC) {Projektions- \\ koordinaten};
				\node [below = 2 of D] (dD) {Normierte \\ Koordinaten};
				\node [below = 2 of E] (eE) {Bildschirm- \\ koordinaten};
				
				\draw [<-] (aA) -- (A);
				\draw [<-] (bB) -- (B);
				\draw [<-] (cC) -- (C);
				\draw [<-] (dD) -- (D);
				\draw [<-] (eE) -- (E);
			\end{tikzpicture}
			\caption{Änderung der Koordinaten innerhalb der Grafikpipeline.}
			\label{fig:pipeline_coordinates}
		\end{figure}

		\section{Affine Abbildungen}
			\subsection{Eigenschaften}
				\emph{Affine Abbildungen} (Translation, Rotation, Skalierung, Scherung) haben folgende Eigenschaften:
				\begin{itemize}
					\item Geraden werden auf Gerade abgebildet.
					\item Beschränkte Objekte bleiben beschränkt.
					\item Verhältnisse von Längen, Flächen, Volumen bleiben erhalten.
					\item Parallele Objekte bleiben parallel.
				\end{itemize}
			
				Mathematischer formuliert: Eine Abbildung \( \Phi : \R^n \to \R^n \) heißt \emph{affin}, \gdw \( \Phi \) in der Form
				\begin{equation*}
					\Phi(\vec{v}) = A(\vec{v}) + I(\vec{b})
				\end{equation*}
				mit einer linearen Abbildung \( A : \R^n \to \R^n \), der Identitätsabbildung \( I : \R^n \to \R^n \) und \( \vec{v}, \vec{b} \in \R^n \) darstellbar ist. Eine affine Abbildung setzt sich also aus einer linearen Abbildung und einer Translation zusammen.
				
				Eine Abbildung \( A : \R^n \to \R^n \) heißt \emph{linear}, \gdw
				\begin{equation*}
					A(\lambda \vec{u} + \mu \vec{v}) = \lambda \, A(\vec{u}) + \mu \, A(\vec{v})
				\end{equation*}
				für alle \( \vec{u}, \vec{v} \in \R^n \) und \( \lambda, \mu \in \R \) gilt.
			% end

			\subsection{Homogene Koordinaten}
				Anstelle der aufwendigen Notation \( A\vec{v} + \vec{b} \) für affine Abbildungen können homogene Koordinaten verwendet werden (sei dazu im folgenden \( n = 3 \)). Dann wird eine Äquivalenzklasse nach \( \R^4 \) definiert durch:
				\begin{equation*}
					\begin{bmatrix}
						x \\
						y \\
						z
					\end{bmatrix}
					\to
					\begin{bmatrix}
						x \\
						y \\
						z \\
						1
					\end{bmatrix}
					\quad\quad\quad\quad
					\begin{bmatrix}
						x \\
						y \\
						z \\
						w
					\end{bmatrix}
					\to
					\begin{bmatrix}
						x/w \\
						y/w \\
						z/w
					\end{bmatrix}
				\end{equation*}
				Wobei \( w \neq 0 \) einen Skalierungsfaktor darstellt (dieser ist meistens \num{1}).
				
				Eine reine Translation kann dann in homogenen Koordinaten ausgedrückt werden als:
				\begin{equation*}
					\begin{bmatrix}
						x' \\
						y' \\
						z' \\
						1
					\end{bmatrix}
					=
					\begin{bmatrix}
						1 & 0 & 0 & x_0 \\
						0 & 1 & 0 & y_0 \\
						0 & 0 & 0 & z_0 \\
						0 & 0 & 0 & 1
					\end{bmatrix}
					\cdot
					\begin{bmatrix}
						x \\
						y \\
						z \\
						1
					\end{bmatrix}
					=
					\begin{bmatrix}
						x + x_0 \\
						y + y_0 \\
						z + z_0 \\
						1
					\end{bmatrix}
				\end{equation*}
				
				Allgemein ergibt sich für homogene Koordinaten die Transformationsmatrix
				\begin{equation*}
					\begin{bmatrix}
						a_{11} & a_{12} & a_{13} & x_0 \\
						a_{21} & a_{22} & a_{23} & y_0 \\
						a_{31} & a_{32} & a_{33} & z_0 \\
						0      & 0      & 0      & 1
					\end{bmatrix}
				\end{equation*}
				was einer Anwendung der Matrix \( \mat{A} \) und eine Translation um den Vektor \( \begin{bmatrix} x_0 & y_0 & z_0 \end{bmatrix}^T \) entspricht (dies ist leicht nachzurechnen).
				
				Diese einheitliche und kompakte Darstellung von Rotationen/Skalierungen/\dots und Translationen als eine Matrixmultiplikation erlaubt eine einfache Implementierung und eine Hintereinanderausführung mehrerer Operationen entspricht einer reinen Multiplikation von Matrizen.
			% end
		% end

		\section{Skalierung, Scherung, Rotation}
			Die affinen Abbildung der Skalierung, Scherung und Rotation lassen den Ursprung invariant, \dh der Vektor \( \begin{bmatrix} 0 & 0 & 0 \end{bmatrix}^T \) wird nicht verschoben. Hierfür während theoretisch normale \( (3 \times 3) \)-Matrizen ausreichend.

			\subsection{Skalierung}
				Eine Skalierung wird durch eine Diagonalmatrix
				\begin{equation*}
					\begin{bmatrix}
						s_1 & 0   & 0   \\
						0   & s_2 & 0   \\
						0   & 0   & s_3
					\end{bmatrix}
					\to
					\underbrace{
						\begin{bmatrix}
							s_1 & 0   & 0   & 0 \\
							0   & s_2 & 0   & 0 \\
							0   & 0   & s_3 & 0 \\
							0   & 0   & 0   & 1
						\end{bmatrix}
					}_\text{Homogene Koordinaten}
				\end{equation*}
				beschrieben. Dabei wird die \(x\)-Achse um \(s_1\) skaliert, die \(y\)-Achse um \(s_2\) und die \(z\)-Achse um \(s_3\). Gilt \( s_1 = s_2 = s_3 \), so werden alle Koordinaten gleichermaßen skaliert.
			% end

			\subsection{Scherung}
				Eine Scherung wird durch eine Matrix
				\begin{equation*}
					\begin{bmatrix}
						1   & s_2 & s_5 \\
						s_1 & 1   & s_6 \\
						s_3 & s_4 & 1
					\end{bmatrix}
					\to
					\underbrace{
						\begin{bmatrix}
							1   & s_2 & s_5 & 0 \\
							s_1 & 1   & s_6 & 0 \\
							s_3 & s_4 & 1   & 0 \\
							0   & 0   & 0   & 1
						\end{bmatrix}
					}_\text{Homogene Koordinaten}
				\end{equation*}
				beschrieben.
			% end

			\subsection{Rotation}
				Eine Basisrotation (um eine der Koordinatenachsen) wird durch folgende drei Matrizen beschrieben (von oben nach unten jeweils um die \(x\)-, \(y\)- und \(z\)-Achse):
				\begin{align*}
					\mat{R}(x; \alpha) &=
						\begin{bmatrix}
							1 & 0            & 0             \\
							0 & \cos(\alpha) & -\sin(\alpha) \\
							0 & \sin(\alpha) & \cos(\alpha)
						\end{bmatrix} \\
					\mat{R}(y; \alpha) &=
						\begin{bmatrix}
							\cos(\alpha)  & 0 & \sin(\alpha) \\
							0             & 1 & 0            \\
							-\sin(\alpha) & 0 & \cos(\alpha)
						\end{bmatrix} \\
					\mat{R}(z; \alpha) &=
						\begin{bmatrix}
							\cos(\alpha) & -\sin(\alpha) & 0 \\
							\sin(\alpha) & \cos(\alpha)  & 0 \\
							0            & 0             & 1
						\end{bmatrix}
				\end{align*}
				Der Kompaktheit halber wurden hier die homogenen Formulierungen weg gelassen, sie werden aber analog zu denen der Skalierung und Scherung gebildet.

				\subsubsection{Rotation um beliebige Achse}
					Zur Rotation um einen beliebigen, normierten Vektor \( \vec{r} \) um den Winkel \(\alpha\) muss zunächst das körperfeste Koordinatensystem des zu rotierenden Körpers koinzident zu den Ursprungsachsen gedreht werden (durch eine Rotationsmatrix \( \mat{R} \)) und anschließend um eine der Achsen (\zB der \(x\)-Achse) rotiert werden und anschließend wieder zurück in das körperfeste Koordinatensystem gedreht werden:
					\begin{equation*}
						\mat{R}_{\vec{r}}(\alpha) = \mat{R}^{-1} \cdot \mat{R}(x; \alpha) \cdot \mat{R}
					\end{equation*}
					Dazu muss zunächst eine orthonormale Basis \( (\vec{r}, \vec{s}, \vec{t}) \) bestimmt werden. Der erste Basisvektor ist die Drehachse \(\vec{r}\), die anderen beiden Vektoren werden wie folgt berechnet:
					\begin{align*}
						\vec{s} &=
							\begin{cases}
								\frac{\vec{r} \times \vec{e}_y}{\lVert \vec{r} \times \vec{e}_y \rVert} & \text{falls } r \parallel \vec{e}_x \\
								\frac{\vec{r} \times \vec{e}_x}{\lvert \vec{r} \times \vec{e}_x \rVert} & \text{sonst}
							\end{cases} \\
						\vec{t} = \vec{r} \times \vec{s}
					\end{align*}
					Daraus ergibt sich mit \( \mat{R} = \begin{bmatrix} \vec{r} & \vec{s} & \vec{t} \end{bmatrix} \) die benötigte Transformationsmatrix.
				% end

				\subsubsection{Rotation um beliebigen Punkt}
					Soll ein Objekt um einen anderen Punkt als den Ursprung gedreht werden, so muss das Rotationszentrum zunächst in den Ursprung verschoben, dann die Rotation durchgeführt und anschließend das Rotationszentrum wieder zurückgeschoben werden.
					
					\subparagraph{Beispiel}
					Es oll eine Transformationsmatrix erstellt werden, die eine Rotation um die \(i\)-Achse (\( i \in \{\, x, y, z \,\} \)) im Punkt beschrieben durch \( \vec{r} \) durchführt. Die Transformationsmatrix lautet dann allgemein:
					\begin{equation*}
						\mat{T} = \text{Trans}(\vec{r}) \cdot \text{Rot}(i; \alpha) \cdot \text{Trans}(-\vec{r})
					\end{equation*}
				% end
			% end

			\subsection{Nicht-Kommutativität von Transformationen}
				Die Reihenfolge von Transformationen darf \iA nicht vertauscht werden (insbesondere ist die Matrixmultiplikation nicht kommutativ).
			% end

			\subsection{Rechenaufwand}
				Bei vielen nacheinander ausgeführten Transformationen ist es sinnvoller, einmal die gesamte Transformationsmatrix zu berechnen, statt oftmals Matrix-Vektor-Multiplikationen durchzuführen.
			% end
		% end

		\section{Projektion}
			\label{sec:projection}
			
			Zur Projektion von 3D-Elementen gibt es viele unterschiedliche Möglichkeiten:
			\begin{itemize}
				\item Aufriss (Frontansicht)
				\item Kabinett-/Kavallierperspektive
				\item Allgemeine Parallelprojektion
				\item Isometrische Perspektive
				\item Zentralperspektive
				\item Vogelperspektive
			\end{itemize}
		
			Solche projektiven Abbildungen können durch homogene Transformationen beschrieben werden, wobei jedoch die Winkel verändert werden! Außerdem geht die Parallelität von Linien oft verloren (\dh Parallelen schneiden sich in Fluchtpunkten). Dies ist der allgemeine Unterschied zwischen perspektivischer und paralleler Projektion:
			\begin{itemize}
				\item Bei einer perspektivischen Projektion treffen sich die Strahlen im Augenpunkt (Projektionszentrum) und Winkel werden verändert.
				\item Bei der parallelen Projektionen sind die Projektionsstrahlen parallel und die Winkel bleiben erhalten.
			\end{itemize}

			\subsection{Perspektive Projektion}
				\begin{itemize}
					\item Vergleichbar mit einem fotografischen System.
					\item Entspricht der natürlichen Wahrnehmung.
					\item Der Abstand zwischen Objekten und Projektionsebene geht ein.
					\item Längenverhältnisse ändern sich.
					\item Winkel ändern sich.
					\item Parallele Geraden bleiben nicht parallel.
				\end{itemize}
			
				Das heißt perspektivische Projektionen sind im allgemeinen keine affinen Abbildungen! Insbesondere werden vom Blickpunkt weiter entfernte Objekte kleiner dargestellt.
				
				Ist der zu projizierende Punkt \( P = (x, y) \) und der Augpunkt \( A = (-x_0, 0) \) gegeben, so gilt für den Bildpunkt \( B = (0, y_0) \):
				\begin{equation*}
					\frac{y_0}{y} = \frac{x_0}{x + x_0}
				\end{equation*}
				Allgemein lautet mit \( y_0 = y x_0 / (x + x_0) \) die Abbildung wie folgt:
				\begin{equation*}
					\begin{bmatrix}
						x \\
						y
					\end{bmatrix}
					\mapsto
					\begin{bmatrix}
						0 \\
						\frac{y x_0}{x_0 + x}
					\end{bmatrix}
				\end{equation*}
				Daraus ergibt sich die homogene \( (3 \times 3) \)-Matrix:
				\begin{equation*}
					\begin{bmatrix}
						0 \\
						\frac{y x_0}{x_0 + x} \\
						1
					\end{bmatrix}
					\simeq
					\begin{bmatrix}
						0 \\
						y x_0 \\
						x_0 + x
					\end{bmatrix}
				\end{equation*}
				Und somit die perspektivische Transformation:
				\begin{equation*}
					\begin{bmatrix}
						x' \\
						y' \\
						1
					\end{bmatrix}
					=
					\begin{bmatrix}
						0 & 0   & 0   \\
						0 & x_0 & 0   \\
						1 & 0   & x_0
					\end{bmatrix}
					\cdot
					\begin{bmatrix}
						x \\
						y \\
						1
					\end{bmatrix}
					\quad\simeq\quad
					\frac{1}{x_0}
					\begin{bmatrix}
						x' \\
						y' \\
						1
					\end{bmatrix}
					=
					\begin{bmatrix}
						0 & 0 & 0 \\
						0 & 1 & 0 \\
						1/x_0 & 0 & 1
					\end{bmatrix}
					\cdot
					\begin{bmatrix}
						x \\
						y \\
						1
					\end{bmatrix}
				\end{equation*}

				\subsubsection{Allgemeine perspektivische Transformation}
					Mit dem Fluchtpunkt in \( (x_0, y_0, z_0) \) wird eine allgemeine perspektivische Transformation beschrieben durch:
					\begin{equation*}
						\begin{bmatrix}
							x' \\
							y' \\
							z' \\
							w'
						\end{bmatrix}
						=
						\begin{bmatrix}
							1     & 0     & 0     & 0 \\
							0     & 1     & 0     & 0 \\
							0     & 0     & 0     & 1 \\
							1/x_0 & 1/y_0 & 1/z_0 & 1
						\end{bmatrix}
						\cdot
						\begin{bmatrix}
							x \\
							y \\
							z \\
							w
						\end{bmatrix}
					\end{equation*}
				% end
			% end

			\subsection{Parallele Projektion}
				\begin{itemize}
					\item Ist weniger realistisch.
					\item Winkel ändern sich \iA nicht.
					\item Parallele Geraden bleiben parallel.
				\end{itemize}
			% end
			
			\subsection{Kanonisches Sichtvolumen}
				Die perspektivische Projektion wird in zwei Abbildungen zerlegt:
				\begin{itemize}
					\item Die perspektivische Transformation und
					\item eine anschließende Parallelprojektion.
				\end{itemize}
				Nach der perspektivischen Transformation ist das Sichtvolumen ein Würfel und durch eine Rotation in den Augpunkt wird erreicht, dass der Würfel achsenparallel wird:
				\begin{equation*}
					\underbrace{
						\begin{bmatrix}
						0     & 0 & 0 \\
						0     & 1 & 0 \\
						1/x_0 & 0 & 1
						\end{bmatrix}
					}_\text{Perspektivische Projektion}
					=
					\underbrace{
						\begin{bmatrix}
						0 & 0 & 0 \\
						0 & 1 & 0 \\
						0 & 0 & 1
						\end{bmatrix}
					}_\text{Parallelprojektion}
					\cdot
					\underbrace{
						\begin{bmatrix}
						1     & 0 & 0 \\
						0     & 1 & 0 \\
						1/x_0 & 0 & 1
						\end{bmatrix}
					}_\text{Perspektivische Transformation}
				\end{equation*}
				
				Ist die Kamera im Unendlichen (Parallelprojektion), wird das Sichtvolumen ein Einheitswürfel. Bei einer perspektivischen Projektion ist das Sichtvolumen eine Pyramide, nach der perspektivischen Transformation hingegen wieder ein Würfel!
			% end
		% end

		\section{3D-Interaktion}
			Bei der 3D-Interaktion ist nicht immer klar, welche Bewegung der Nutzer ausgeführt haben möchte. Ansätze hierzu sind
			\begin{itemize}
				\item Desktop,
				\item Multi-Window (Mehrfachauswahl),
				\item Direktes 2D-Maus-Mapping oder
				\item Manipulatoren.
			\end{itemize}

			\subsection{Manipulatoren}
				Im zweidimensionalen werden Manipulatoren häufig eingesetzt (Manipulatoren sind dabei zum Beispiel kleine "Rädchen" um ein Objekt, mit welchen dieses gedreht werden kann). Auch werden \zB Kästchen eingesetzt, mit denen skaliert und verschoben werden kann (Drag-and-Drop). Dabei sind sie im zweidimensionalen sehr einfach zu implementieren, da eine Eins-zu-Eins Abbildung der Mauszeigerposition zum Manipulator erstellt werden kann. Außerdem gibt es keine Probleme bei der perspektivischen Abbildung.
				
				Immer häufiger werden Manipulatoren auch im dreidimensionalen, \zB zur Navigation der Kamera eingesetzt. Hier ist die Implementierung jedoch sehr viel schwieriger, da es unendliche viele Möglichkeiten gibt, eine Cursorposition auf einer geraden Linie im 3D-Raum abzubilden. Dennoch stellen Manipulatoren eines der besten momentan verfügbaren Werkzeuge zur 3D-Interaktion dar.
			% end
		% end
	% end

	\chapter{3D-Visualisierung}
		\section{(Gewinnung) 3D-Daten}
			Bei 3D-Daten werden die Messwerte dreidimensional im Raum verteilt und jeder Wert hat drei Koordinaten \( (x, y, z) \). Dabei können die Werte gleichmäßig oder unterschiedlich verteilt sein und ein Wert kann skalar oder höherdimensional sein (\zB ein Vektor in einem Strömungsfeld).

			\paragraph{Terrain}
				Zum Scannen eines Terrains wird an beliebigen Positionen \( (x, y) \) die Höhe \(z\) gemessen, wodurch sich eine 3D-Position ergibt. Oberflächeninformationen (\zB Vegetation) können durch Satellitenbilder gewonnen werden.
			% end

			\paragraph{Laser Scanning}
				Beim Laser Scanning wird ein Laserstrahl auf eine Oberfläche projiziert und das rückstrahlende Licht gemessen. Aus dem Abstand zwischen Laser und Kamera kann dann die Distanz durch Triangulation berechnet werden. Dies ergibt eine unstrukturierte Punktwolke.
			% end

			\paragraph{Range Images}
				Aus einem Range Image \( r(u, v) \) ergibt sich die Pixelinformation als 3D-Punkt \( \big(u, v, r(u, v)\big) \).
			% end

			\paragraph{Medizinische Bilddaten}
				In der Medizin werden viele bildgebende Geräte verwendet, um physikalische Eigenschaften zu messen (MRI, CT, Ultraschall, \dots). Diese produzieren einen "Stapel" von parallelen Scheiben (Slices), die jeweils einem regulären 2D-Gitter entsprechen. Ein solcher Scan produziert jedoch riesige Datenmengen!
			% end

			\paragraph{Wetter}
				Die Parameter des Wetters (\zB Temperatur, Druck, Niederschlag, Windrichtung, \dots) sind für bestimmte Regionen auf verschiedenen Höhen unterschiedlich und sowohl vektoriell als auch skalar. Zur Messung dieser Daten wird die Erde in viele Zellen einer bestimmten Größe (meist einige Kilometer) aufgeteilt.
			% end
		% end

		\section{Triangulation von Punktwolken}
			Eine unstrukturierten Punktmenge \( s_i = (x_i, y_i, z_i) \) auf einer Oberflächen \(S\) wird als \emph{Punktwolke} bezeichnet. Für einfache Oberflächen (ohne Falten) können Punkte auf eine Ebene projiziert und in 2D trianguliert werden (planare Triangulation). Dieses 2D-Netz wird dann entsprechend der \(z_i\)-Werte deformiert.

			\subsection{Ideal Triangulation}
				Bei einer idealen Triangulation haben alle Dreiecke die Innenwinkel \( (\SI{60}{\deg}, \SI{60}{\deg}, \SI{60}{\deg}) \), \dh die Dreiecke sind gleichschenklig. Dies führt zu einer numerischen Stabilität und vereinfacht das Post-Processing.
			% end

			\subsection{Voronoi-Diagramm}
				Statt einer idealen Triangulation, die schwer zu berechnen ist, kann ein Voronoi-Diagramm eingesetzt werden. Dabei wird für jeden (in 2D projizierten) Punkt \( \bar{s}_i \) eine \emph{Voronoi-Zelle} definiert, die alle Punkte enthält, die näher an \( \bar{s}_i \) als an allen anderen Orten (andere \( \bar{s}_j \)) liegt. Die Kante einer Voronoi-Zelle liegt dann auf den Punkten mit dem gleichen Abstand zu den zwei nächsten Orten und der Knoten einer Voronoi-Zelle auf einem Punkt, der den gleichen Abstand zu drei anderen Orten hat. Dadurch wird die 2D-Fläche "parkettiert".
			% end
			
			\subsection{Delaunay-Triangulation}
				Durch Betrachtung des dualen Graph zu einem Voronoi-Diagramm wird eine \emph{Delaunay-Triangulation} beschrieben (es sind jedoch \mglw Korrekturen nötig). Dabei ist ein Dreiecksnetz nur dann eine Delaunay-Triangulation, wenn alle Umkreise von allen Dreiecken leer sind, \dh es liegt kein Ort in ihnen. Dies kann durch das "Umdrehen" einer Kante korrigiert werden.
			% end
		% end

		\section{Indirekte Volumenvisualisierung}
			Eine Menge von Volumendaten enthält viele Informationen, was das Rendering verlangsamt, obwohl vieles (\zB verdeckte) Elemente gar nicht angezeigt wird. Deshalb wird \iA nicht das gesamte Volumen, sondern nur ein Teil angezeigt.

			\subsection{3D-Volumen und Nachbarschaft}
				In einem normalen 3D-Raster wird ein Volumenelement (Würfel) als \emph{Voxel} bezeichnet. Die Dicke eines Slices (\dh die Breite eines Würfels) ist dabei oftmals größer als die Pixelabstände (anisotropische Volumen). Die Eine Rasterposition wird dabei durch einen Index \( (i, j, k) \) beschrieben.
				
				Ein Voxel ist dabei adjazent zu einem Referenzvoxel:
				\begin{itemize}
					\item Im zweidimensionalen sind dies über \num{4} Kanten und \num{4} Ecken insgesamt \num{8} Nachbarvoxel.
					\item Im dreidimensional sind dies über \num{6} Flächen, \num{12} Kanten und \num{8} Ecken insgesamt \num{24} Nachbarvoxel.
				\end{itemize}
			% end

			\subsection{2D: Konturlinien}
				Eine \emph{Konturlinie} ist eine Linie entlang der selben Höhe, \dh der Wert ist entlang einer Konturlinie konstant (\dh ein Kontur-Diagramm entspricht einer Höhenkarte). Dabei ist die Ausrichtung des Gefälles orthogonal zu den Konturlinien.
			% end

			\subsection{3D: Isoflächen}
				Eine Trennung zwischen verschiedenen Strukturen führt zu einer Eingrenzung von Strukturen, wodurch ebendiese Strukturen erkannt werden könne. Die Voxel an einer solchen Eingrenzung bilden, wenn die die gleiche Intensität haben, sogenannte \emph{Isoflächen}. Eine Isofläche ist dabei eine implizite Fläche
				\begin{equation*}
					i(x) = V(x) - \tau = 0
				\end{equation*}
				wobei \( V(x) \) der Voxelwert und \( \tau = \const \) ein festgesetzter Isowert ist. Die Datenmenge wird dann aufgeteilt in innen (\( i(x) > 0 \)) und außen (\( i(x) < 0 \)) liegende Flächen. Die Definition eines solchen Isowerts entspricht also einem Thresholding der Daten.
			% end

			\subsection{2D: Marching Squares}
				Der \emph{Marching Squares Algorithmus} versucht, gegeben einem Fixen Isowert \( \tau \) die Isolinie \( s(x) = 0 \) zu finden, um so die Fläche in innen und außen zu unterteilen. Dabei sei jede Bildzelle durch ihre vier umgebenden Pixel definiert. Danach wird jede Bildzelle abgelaufen, um sie einem der in Abbildung~\ref{fig:marching_squares} aufgezeigten Fälle zuzuordnen und die Isolinie entsprechend zu ziehen. Ein Pixel hat dabei den "Set Pixel"-Zustand, wenn dieser größer oder gleich \( \tau \) ist.
				
				Werden Rotationen und Symmetrien beachtet, so reduzieren sich die Fälle auf fünf Fälle.
				
				\begin{figure}
					\centering
					\begin{tikzpicture}[xscale = 1/2, yscale = 1/2, set/.style = { draw, circle, fill = black }, reset/.style = { draw, circle }, alt/.style = { dashed }]
						\node [reset] (ref) {};
						\node [reset, right = 0.5 of ref] (b) {};
						\node [reset, below = 0.5 of b] (c) {};
						\node [reset, left = 0.5 of c] (d) {};
						\coordinate [right = 0.25 of ref] (A);
						\coordinate [below = 0.25 of b] (B);
						\coordinate [left = 0.25 of c] (C);
						\coordinate [above = 0.25 of d] (D);
						\path (c) -- node[below, yshift = -0.2cm]{Fall 0} (d);
						\draw (ref) -- (b) -- (c) -- (d) -- (ref);
						
						\node [reset, right = 1 of b] (a) {};
						\node [reset, right = 0.5 of a] (b) {};
						\node [reset, below = 0.5 of b] (c) {};
						\node [  set, left = 0.5 of c] (d) {};
						\coordinate [right = 0.25 of a] (A);
						\coordinate [below = 0.25 of b] (B);
						\coordinate [left = 0.25 of c] (C);
						\coordinate [above = 0.25 of d] (D);
						\path (c) -- node[below, yshift = -0.2cm]{Fall 1} (d);
						\draw (a) -- (b) -- (c) -- (d) -- (a);
						\draw (C) -- (D);
						
						\node [reset, right = 1 of b] (a) {};
						\node [reset, right = 0.5 of a] (b) {};
						\node [  set, below = 0.5 of b] (c) {};
						\node [reset, left = 0.5 of c] (d) {};
						\coordinate [right = 0.25 of a] (A);
						\coordinate [below = 0.25 of b] (B);
						\coordinate [left = 0.25 of c] (C);
						\coordinate [above = 0.25 of d] (D);
						\path (c) -- node[below, yshift = -0.2cm]{Fall 2} (d);
						\draw (a) -- (b) -- (c) -- (d) -- (a);
						\draw (B) -- (C);
						
						\node [reset, right = 1 of b] (a) {};
						\node [reset, right = 0.5 of a] (b) {};
						\node [  set, below = 0.5 of b] (c) {};
						\node [  set, left = 0.5 of c] (d) {};
						\coordinate [right = 0.25 of a] (A);
						\coordinate [below = 0.25 of b] (B);
						\coordinate [left = 0.25 of c] (C);
						\coordinate [above = 0.25 of d] (D);
						\path (c) -- node[below, yshift = -0.2cm]{Fall 3} (d);
						\draw (a) -- (b) -- (c) -- (d) -- (a);
						\draw (B) -- (D);
						
						\node [reset, right = 1 of b] (a) {};
						\node [  set, right = 0.5 of a] (b) {};
						\node [reset, below = 0.5 of b] (c) {};
						\node [reset, left = 0.5 of c] (d) {};
						\coordinate [right = 0.25 of a] (A);
						\coordinate [below = 0.25 of b] (B);
						\coordinate [left = 0.25 of c] (C);
						\coordinate [above = 0.25 of d] (D);
						\path (c) -- node[below, yshift = -0.2cm]{Fall 4} (d);
						\draw (a) -- (b) -- (c) -- (d) -- (a);
						\draw (A) -- (B);
						
						\node [reset, right = 1 of b] (a) {};
						\node [  set, right = 0.5 of a] (b) {};
						\node [reset, below = 0.5 of b] (c) {};
						\node [  set, left = 0.5 of c] (d) {};
						\coordinate [right = 0.25 of a] (A);
						\coordinate [below = 0.25 of b] (B);
						\coordinate [left = 0.25 of c] (C);
						\coordinate [above = 0.25 of d] (D);
						\path (c) -- node[below, yshift = -0.2cm]{Fall 5} (d);
						\draw (a) -- (b) -- (c) -- (d) -- (a);
						\draw [alt] (A) -- (B);
						\draw [alt] (C) -- (D);
						\draw (A) -- (D);
						\draw (B) -- (C);
						
						\node [reset, right = 1 of b] (a) {};
						\node [  set, right = 0.5 of a] (b) {};
						\node [  set, below = 0.5 of b] (c) {};
						\node [reset, left = 0.5 of c] (d) {};
						\coordinate [right = 0.25 of a] (A);
						\coordinate [below = 0.25 of b] (B);
						\coordinate [left = 0.25 of c] (C);
						\coordinate [above = 0.25 of d] (D);
						\path (c) -- node[below, yshift = -0.2cm]{Fall 6} (d);
						\draw (a) -- (b) -- (c) -- (d) -- (a);
						\draw (A) -- (C);
						
						\node [reset, right = 1 of b] (a) {};
						\node [  set, right = 0.5 of a] (b) {};
						\node [  set, below = 0.5 of b] (c) {};
						\node [  set, left = 0.5 of c] (d) {};
						\coordinate [right = 0.25 of a] (A);
						\coordinate [below = 0.25 of b] (B);
						\coordinate [left = 0.25 of c] (C);
						\coordinate [above = 0.25 of d] (D);
						\path (c) -- node[below, yshift = -0.2cm]{Fall 7} (d);
						\draw (a) -- (b) -- (c) -- (d) -- (a);
						\draw (A) -- (D);
						
						
						\node [  set, below = 2 of ref] (a) {};
						\node [reset, right = 0.5 of a] (b) {};
						\node [reset, below = 0.5 of b] (c) {};
						\node [reset, left = 0.5 of c] (d) {};
						\coordinate [right = 0.25 of a] (A);
						\coordinate [below = 0.25 of b] (B);
						\coordinate [left = 0.25 of c] (C);
						\coordinate [above = 0.25 of d] (D);
						\path (c) -- node[below, yshift = -0.2cm]{Fall 8} (d);
						\draw (a) -- (b) -- (c) -- (d) -- (a);
						\draw (A) -- (D);
						
						\node [  set, right = 1 of b] (a) {};
						\node [reset, right = 0.5 of a] (b) {};
						\node [reset, below = 0.5 of b] (c) {};
						\node [  set, left = 0.5 of c] (d) {};
						\coordinate [right = 0.25 of a] (A);
						\coordinate [below = 0.25 of b] (B);
						\coordinate [left = 0.25 of c] (C);
						\coordinate [above = 0.25 of d] (D);
						\path (c) -- node[below, yshift = -0.2cm]{Fall 9} (d);
						\draw (a) -- (b) -- (c) -- (d) -- (a);
						\draw (A) -- (C);
						
						\node [  set, right = 1 of b] (a) {};
						\node [reset, right = 0.5 of a] (b) {};
						\node [  set, below = 0.5 of b] (c) {};
						\node [reset, left = 0.5 of c] (d) {};
						\coordinate [right = 0.25 of a] (A);
						\coordinate [below = 0.25 of b] (B);
						\coordinate [left = 0.25 of c] (C);
						\coordinate [above = 0.25 of d] (D);
						\path (c) -- node[below, yshift = -0.2cm]{Fall 10} (d);
						\draw (a) -- (b) -- (c) -- (d) -- (a);
						\draw [alt] (A) -- (D);
						\draw [alt] (B) -- (C);
						\draw (A) -- (B);
						\draw (C) -- (D);
						
						\node [  set, right = 1 of b] (a) {};
						\node [reset, right = 0.5 of a] (b) {};
						\node [  set, below = 0.5 of b] (c) {};
						\node [  set, left = 0.5 of c] (d) {};
						\coordinate [right = 0.25 of a] (A);
						\coordinate [below = 0.25 of b] (B);
						\coordinate [left = 0.25 of c] (C);
						\coordinate [above = 0.25 of d] (D);
						\path (c) -- node[below, yshift = -0.2cm]{Fall 11} (d);
						\draw (a) -- (b) -- (c) -- (d) -- (a);
						\draw (A) -- (B);
						
						\node [  set, right = 1 of b] (a) {};
						\node [  set, right = 0.5 of a] (b) {};
						\node [reset, below = 0.5 of b] (c) {};
						\node [reset, left = 0.5 of c] (d) {};
						\coordinate [right = 0.25 of a] (A);
						\coordinate [below = 0.25 of b] (B);
						\coordinate [left = 0.25 of c] (C);
						\coordinate [above = 0.25 of d] (D);
						\path (c) -- node[below, yshift = -0.2cm]{Fall 12} (d);
						\draw (a) -- (b) -- (c) -- (d) -- (a);
						\draw (B) -- (D);
						
						\node [  set, right = 1 of b] (a) {};
						\node [  set, right = 0.5 of a] (b) {};
						\node [reset, below = 0.5 of b] (c) {};
						\node [  set, left = 0.5 of c] (d) {};
						\coordinate [right = 0.25 of a] (A);
						\coordinate [below = 0.25 of b] (B);
						\coordinate [left = 0.25 of c] (C);
						\coordinate [above = 0.25 of d] (D);
						\path (c) -- node[below, yshift = -0.2cm]{Fall 13} (d);
						\draw (a) -- (b) -- (c) -- (d) -- (a);
						\draw (B) -- (C);
						
						\node [  set, right = 1 of b] (a) {};
						\node [  set, right = 0.5 of a] (b) {};
						\node [  set, below = 0.5 of b] (c) {};
						\node [reset, left = 0.5 of c] (d) {};
						\coordinate [right = 0.25 of a] (A);
						\coordinate [below = 0.25 of b] (B);
						\coordinate [left = 0.25 of c] (C);
						\coordinate [above = 0.25 of d] (D);
						\path (c) -- node[below, yshift = -0.2cm]{Fall 14} (d);
						\draw (a) -- (b) -- (c) -- (d) -- (a);
						\draw (C) -- (D);
						
						\node [  set, right = 1 of b] (a) {};
						\node [  set, right = 0.5 of a] (b) {};
						\node [  set, below = 0.5 of b] (c) {};
						\node [  set, left = 0.5 of c] (d) {};
						\coordinate [right = 0.25 of a] (A);
						\coordinate [below = 0.25 of b] (B);
						\coordinate [left = 0.25 of c] (C);
						\coordinate [above = 0.25 of d] (D);
						\path (c) -- node[below, yshift = -0.2cm]{Fall 15} (d);
						\draw (a) -- (b) -- (c) -- (d) -- (a);
					\end{tikzpicture}
					\caption{Möglichkeiten zur Isolinien-Legung in den Bildzellen beim Marching Squares Algorithmus. Dabei bedeutet ein schwarzer Pixel, dass der Pixel im Zustand "Set Pixel" ist, weiß bedeutet dementsprechend "Reset Pixel". In den Fällen \num{5} und \num{11} ist die Legung der Linie nicht eindeutig und es kann wahlweise die gestrichelte Version oder die durchgezogene Version gewählt werden.}
					\label{fig:marching_squares}
				\end{figure}
			% end

			\subsection{3D: Marching Cubes}
				Der \emph{Marching Cubes Algorithmus} erweitert die Idee von Marching Squares auf drei Dimensionen, wobei eine Volumenzelle durch ihre acht umgebenden Voxel definiert ist. Es gibt, ohne Beachtung von Symmetrien, \num{256} verschiedene Kombination von "Set Pixel" und "Reset Pixel" Zuständen. Unter Einbeziehung von Symmetrien werden diese auf \num{15} Klassen reduziert.
			% end

			\subsection{Große Polygonmodelle und Performanz}
				Der Marching Cubes Algorithmus erzeugt sehr viele Millionen Dreiecke, was zu einem hohen Berechnungsaufwand führt. Daher muss das entstehende Mesh vor dem Rendern noch reduziert werden.

				\subsubsection{Culling von Geometrie}
					Eine Möglichkeit stellt das Cullung von Geometrie dar, bei dem unsichtbare Polygone aus der Rendering-Pipeline entfernt werden:
					\begin{itemize}
						\item Backface-Culling: Rückseiten werden nicht gezeichnet.
						\item View-Frustum-Culling: Polygone, die sich ganz oder teilweise außerhalb des View-Frustums befinden, werden nicht (oder nur teilweise) gezeichnet.
						\item Occlusion-Culling: Polygone werden nach der tiefe sortiert und nur gerendert, wenn sie nicht vollständig verdeckt sind (Transparenz muss beachtet werden!).
						\item uvm.
					\end{itemize}
				% end

				\subsubsection{Meshreduktion}
					Bei der Meshreduktion wird die Anzahl der Polygone verringert, wobei die "Größe" der Vereinfachung stark vom Szenario abhängt (Genauigkeit \vs Zeitbegrenzung).
				% end

				\subsubsection{Mesh-Glättung}
					Das Ziel der Mesh-Glättung ist die Bereitstellung einer guten Visualisierung sowie der Artefakt-Reduzierung und Entfernung von "Löchern". Die Herausforderung ist hierbei, dass Volumen zu erhalten.
					
					Laplacian Glättung:
					\begin{itemize}
						\item Es wird eine "Regenschirm"-Region betrachtet (\dh ausgehend von einem Vertex alle durch eine Kante verbundenen Vertexe sowie deren Verbindungen).
						\item Anschließend werden hochfrequente Oberflächeninformationen reduziert.
						\item Dies sorgt für eine Reduktion von Krümmungen.
					\end{itemize}
				% end
			% end
		% end

		\section{Direkte Volumenvisualisierung}
			Im Gegensatz zur indirekten Volumenvisualisierung, bei der zunächst eine Zwischendarstellung generiert wird und die Komplexität von der Anzahl an Polygonen abhängig ist, wird bei der direkten Volumenvisualisierung die Voxel direkt ohne Zwischendarstellung visualisiert. Dabei ist die Komplexität von der Anzahl der Voxel und der Auflösung der Anzeigefläche abhängig.

			\subsection{Density Emitter Model}
				\begin{itemize}
					\item Es werden nur Emission und Absorption betrachtet.
					\item Jeder Voxel in der Datenmenge ist eine kleine Lichtquelle.
					\item Das Licht wird schwächer, wenn es durch die Volumendatenmenge wandert.
					\item Das Medium ist eine homogene Dichtewolke.
				\end{itemize}
			
				Dadurch wird die Bestrahlungsstärke \( I(s) \) eines Voxels \(s\) beschrieben durch:
				\begin{equation}
					I(s) = I_{s_0} \cdot \exp \Bigg\{ -\int_{s_0}^{s} \! \tau(t) \dif{t} \Bigg\} + \int_{s_0}^{s} \! Q(\tilde{s}) \cdot \exp \Bigg\{ -\int_{\tilde{s}}^{s} \tau(t) \dif{t} \Bigg\} \dif{\tilde{s}}  \label{eqn:volume_rendering}
				\end{equation}
				wobei \( I_{s_0} \) die Beleuchtungsstärke des Hintergrunds und \( Q(\tilde{s}) \) die aktive Emission des Voxels \( \tilde{s} \) ist. Mit \( t_i \coloneqq \exp\Big\{ -\tau(i \cdot \Delta t) \cdot \Delta t \Big\} \) kann die Volumen-Rendering-Gleichung~\ref{eqn:volume_rendering} diskretisiert werden:
				\begin{equation}
					I(s) = I_0 \prod_{k = 0}^{n - 1} t_k + \sum_{k = 0}^{n - 1} \Bigg( Q(k \cdot \Delta s) \cdot \Delta s \prod_{j = k + 1}^{n - 1} t_j \Bigg)  \label{eqn:volume_rendering_discrete}
				\end{equation}
			% end

			\subsection{Volumen-Rendering-Pipeline}
				In der Volumen-Rendering-Pipeline gibt es drei grundlegende Schritte:
				\begin{enumerate}
					\item Abtastung (Sampling)
					\item Klassifizierung und Beleuchtung
					\item Komposition
				\end{enumerate}
				Die nacheinander durchlaufen werden
	
				\paragraph{Abtastung}
					Es werden Voxelwerte an bestimmten Orten angesammelt, wobei die Position dieser Orte durch die Abtastdistanz \( \Delta s \) festgelegt ist (diese sollte kleiner als die Hälfte der Rasterauflösung sein, Shannons Abtasttheorem). Dabei befinden sich die Abtastpositionen meistens zwischen den Rasterpostionen.
					
					Anschließend werden die Werte Interpoliert, \bspw mit Nearest Neighbor, Trilinear oder B-Spline Modellen.
				% end
	
				\paragraph{Klassifikation und Beleuchtung}
					Für jeden Abtastpunkt wird nun der Anteil (Farbwert) \( Q_k = Q(k \cdot \Delta s) \) sowie der Abschwächungsfaktor (Transparenz) \( t_i \) bei jedem Abtastpunkt berechnet. Dadurch wird der beleuchtete Anteil berechnet, wobei Volumenabtastungen als gerichtete Lichtquellen betrachtet werden (Shading).
				% end
	
				\paragraph{Komposition}
					Die abgetasteten, klassifizierten und beleuchteten Objekte werden nun zusammengeführt (akkumuliert), wobei die Volumen-Rendering-Gleichung numerisch approximiert wird (Gleichung~\ref{eqn:volume_rendering_discrete}). Diese Akkumulation wird in zwei Unterschritte geteilt:
					\begin{enumerate}
						\item Back-to-Front-Komposition
						\item Front-to-Back-Komposition
					\end{enumerate}
	
					\subparagraph{Back-to-Front-Komposition}
						Bei der Back-to-Front-Komposition wird mit der Abtastposition am Ende des Volumens mit der Zusammensetzung begonnen und in die Richtung des Sichtpunktes fortgefahren. Dabei wird Iterativ die Farbe \( C_k \) sowie die Intensität \( T_k \) an der aktuellen Abtastposition \( k \) berechnet:
						\begin{align*}
							C_k &= Q(k \cdot \Delta s) \cdot \Delta s \\
							I_k &= I_{k - 1} \cdot t_k + C_k
						\end{align*}
					% end
	
					\subparagraph{Front-to-Back-Komposition}
						Es wird mit der Abtastposition am Anfang des Volumens begonnen und in Richtung des Endes fortgefahren. Dabei werden iterativ Intensität \( I_{k - 1} \), Transparenz \( \tau_{k - 1} \) berechnet:
						\begin{align*}
							I_{n - 1} &= C_{n - 1} \\
							\tau_{n - 1} &= t_{n - 1} \\
							I_{k - 1} &= T_k + C_k \cdot \tau_k \\
							\tau_{k - 1} &= t_k \cdot \tau_k
						\end{align*}
						Es müssen immer zwei akkumulierte Werte (Intensität und Transparenz) berechnet werden, um \zB Nebel darzustellen (bei diesem sind Lichtquellen ab einem bestimmten Punkt irrelevant).
						
						Die Zusammensetzung wird gestoppt, sobald die akkumulierte Transparenz zu klein wird (Early Ray Termination). Dadurch müssen nicht alle Positionen entlang des Strahls betrachtet werden und die Rendering-Geschwindigkeit wird erhöht. Üblicherweise ist dies durch ein Transparenz-Threshold definiert.
					% end
				% end
	
				\subsubsection{Transferfunktion}
					Die \emph{Transferfunktion} bildet gemessene und abgetastete Werte auf optische Eigenschaften ab (der Farbwert \(Q\) durch eine Farbtransferfunktion und die Transparenz \(t\) durch eine Opazitätstransferfunktion). Dabei bilden die Voxelwerte den Definitionsbereich und die optischen Eigenschaften den Bildbereich einer eindimensionalen Transferfunktion:
					\begin{equation*}
						\textit{tf}_i : V \to O_i
					\end{equation*}
					
					Bei Grauwertabbildungen werden nur Grauwerte betrachtet und die Intensität wird gemäß den Abtastwerten zugewiesen (niedrigster Wert schwarz, höchster Wert weiß, die Werte dazwischen werten linear interpoliert). Dunklen Werten wird dabei eine geringe Opazität (Transparenz) zugewiesen.
					
					Spezifikation:
					\begin{itemize}
						\item Es kann \zB ein Histogramm verwendet werden.
						\item Die Referenzeinstellungen für Opazität und Farbe werden für eine begrenze Anzahl von Werten im Definitionsbereich spezifiziert.
						\item Zwischen den Werten wird dann linear interpoliert.
						\item Oftmals werden Höchstwerte gewählt, um die Opazität zu erhöhen und die Farbe zu ändern.
						\item Meistens sind fotorealistische Renderings das Ziel.
						\item Den verschiedenen Strukturen werden demnach "echte" Farben zugewiesen.
						\item Problem: Zwei Strukturen mit dem gleichen Grauwert haben \mglw verschiedene echte Farben.
					\end{itemize}
				% end
			% end
		% end
	% end

	\chapter{Szenengraphen am Beispiel X3DOM} % 10.1, 10.2, 10.3, 10.40
		\todo{Content}

		\section{Strukturierung von 3D-Szenendaten} % 10.4, 10.5, 10.6
			\todo{Content}
		% end

		\section{Szenengraph} % 10.10, 10.11, 10.12, 10.14
			\todo{Content}
		% end

		\section{X3DOM} % 10.16, 10.17, 10.19
			\todo{Content}
		% end
	% end

	\chapter{Informationsvisualisierung} % 11.1, 11.2, 11.15, 11.16, 11.17, 11.18, 11.19, 11.20
		\todo{Content}

		\section{Informationsdesign} % 11.23, 11.24, 11.25, 11.34, 11.56, 11.57
			\todo{Content}

			\subsection{Referenzmodell von Card} % 11.26, 11.27, 11.28, 11.29, 11.30, 11.31, 11.32, 11.33
				\todo{Content}
			% end
		% end

		\section{Datentypen} % N/A
			\todo{Content}

			\subsection{1D-Daten, Zeitreihen} % 11.62, 11.63
				\todo{Content}
			% end

			\subsection{2D-Daten} % 11.73
				\todo{Content}
			% end

			\subsection{mD-Daten (multidimensional)} % 11.77, 11.78
				\todo{Content}
			% end

			\subsection{Hierarchien} % 11.91
				\todo{Content}
			% end

			\subsection{Graphen/Netzwerke} % 11.155
				\todo{Content}
			% end
		% end

		\section{Kuchendiagramm (1D)} % 11.64, 11.65
			\todo{Content}
		% end

		\section{Balkendiagramm (1D)} % 11.68
			\todo{Content}
		% end

		\section{Liniendiagramm (Zeitreihe)} % 11.67, 11.68, 11.69
			\todo{Content}

			\paragraph{Problem: Viele Zeitreihen} % 11.70, 11.71
				\todo{Content}
			% end

			\paragraph{Problem: Länge} % 11.72
				\todo{Content}
			% end
		% end

		\section{Scatterplot (2D, 3D)} % 11.74, 11.75
			\todo{Content}

			\paragraph{Problem: Overplotting} % 11.76
				\todo{Content}
			% end
		% end

		\section{Scatterplotmatrix (nD)} % 11.79, 11.80, 11.81
			\todo{Content}
		% end

		\section{Parallele Koordinaten (3D, nD)} % 11.82, 11.83, 11.84, 11.85, 11.86, 11.87, 11.88
			\todo{Content}

			\paragraph{Problem: Overplotting} % 11.89
				\todo{Content}
			% end

			\paragraph{Problem: Viele Dimensionen} % 11.90
				\todo{Content}
			% end
		% end

		\section{Node-Link-Diagramm (Hierarchien, Graphen)} % 11.92, 11.156
			\todo{Content}

			\paragraph{Problem: Layout} % 11.93, 11.157, 11.158
				\todo{Content}
			% end

			\paragraph{Problem: Viele Knoten} % 11.94, 11.159
				\todo{Content}
			% end
		% end

		\section{Treemap (Hierarchien)} % 11.95, 11.96, 11.97, 11.98, 11.99, 11.100, 11.101, 11.102
			\todo{Content}

			\paragraph{Problem: Überlappung} % 11.105
				\todo{Content}
			% end

			\paragraph{Problem: Größendarstellung} % 11.106, 11.107
				\todo{Content}
			% end
		% end

		\section{Zusammenfassung} % 11.160
			\todo{Content}
		% end
	% end

	\chapter{Farbe} % 12.1, 12.2, 12.3, 12.6, 12.6
		\todo{Content}

		\section{Dimensionalität} % 12.8, 12.9
			\todo{Content}
		% end

		\section{Wahrnehmungskorrelate} % 12.10, 12.11, 12.12
			\todo{Content}
		% end

		\section{Reproduktion} % 12.13
			\todo{Content}
		% end

		\section{Berechnung von Farbattributen} % 12.14
			\todo{Content}

			\subsection{Das Auge} % 12.15, 12.16, 12.17
				\todo{Content}
			% end

			\subsection{Spektrale Charakterisierung des Auges} % 12.18, 12.19, 12.20, 12.21, 12.22
				\todo{Content}
			% end

			\subsection{Spektralwertfunktion} % 12.23, 12.24
				\todo{Content}
			% end

			\subsection{Cone Fundamentals} % 12.25
				\todo{Content}
			% end
		% end

		\section{Objektfarben, Lichtmatrix und XIEXYZ-Farbraum} % 12.26, 12.27, 12.28, 12.29
			\todo{Content}
		% end

		\section{Metamerie} % 12.30, 12.31, 12.32, 12.33
			\todo{Content}
		% end

		\section{Gegenfarbtheorie} % 12.35
			\todo{Content}
		% end

		\section{Stevenssche Potenzfunktion} % 12.36
			\todo{Content}
		% end

		\section{CIELAB Farbraum} % 12.37, 12.38
			\todo{Content}
		% end

		\section{Technische Farbräume} % 12.39
			\todo{Content}

			\subsection{Geräte RGB} % 12.40
				\todo{Content}
			% end

			\subsection{Geräteunabhängige RGB} % 12.40
				\todo{Content}
			% end

			\subsection{YCbCr} % 12.41
				\todo{Content}
			% end

			\subsection{HSI/HSV/HSL} % 12.41
				\todo{Content}
			% end

			\subsection{CMY/CMYK} % 12.42
				\todo{Content}
			% end
		% end

		\section{Komplexität von Farbe} % 12.43
			\todo{Content}

			\subsection{Chromatische Adaptation} % 12.45, 12.46
				\todo{Content}

				\subsubsection{Modellbildung} % 12.47, 12.48
					\todo{Content}
				% end
			% end

			\subsection{Farbwahrnehmungsphänomene} % 12.49
				\todo{Content}

				\subsubsection{Simultankontrast} % 12.49
					\todo{Content}
				% end

				\subsubsection{Crispening Effekt} % 12.50
					\todo{Content}
				% end

				\subsubsection{Stevens Effekt} % 12.51
					\todo{Content}
				% end

				\subsubsection{Hunt Effekt} % 12.52
					\todo{Content}
				% end
			% end

			\subsection{Farbwahrnehmungsmodelle} % 12.54
				\todo{Content}

				\subsubsection{CIECAM02} % 12.55, 12.56, 12.57
					\todo{Content}
				% end
			% end

			\subsection{Kontrastsensitivität} % 12.58, 12.59, 12.60
				\todo{Content}

				\subsubsection{S-CIELAB (Spacial-CIELAB)} % 12.61, 12.62, 12.63
					\todo{Content}
				% end

				\subsubsection{iCAM (Image Color Appearance Model)} % 12.64, 12.65, 12.66
					\todo{Content}
				% end
			% end
		% end
	% end

	\chapter{User Interfaces} % 13a.1, 13a.64
		\todo{Content}

		\section{Interaktion} % 13a.7
			\todo{Content}

			\subsection{Möglichkeiten} % 13a.8
				\todo{Content}

				\subsubsection{Kommandozeile} % 13a.9
					\todo{Content}
				% end

				\subsubsection{Menüs} % 13a.10, 13a.11
					\todo{Content}
				% end

				\subsubsection{Formulare} % 13a.12
					\todo{Content}
				% end

				\subsubsection{Fragen und Antworten} % 13a.13
					\todo{Content}
				% end

				\subsubsection{Direkte Manipulation} % 13a.14, 13a.15
					\todo{Content}
				% end

				\subsubsection{3D-Umgebungen} % 13a.17
					\todo{Content}
				% end

				\subsubsection{Natürliche Sprache} % 13a.18
					\todo{Content}
				% end

				\subsubsection{Gesten} % 13a.19
					\todo{Content}
				% end
			% end

			\subsection{Designprozess} % 13a.20, 13a.21, 13a.22
				\todo{Content}

				\subsubsection{Wasserfallmodell} % 13a.24
					\todo{Content}
				% end

				\subsubsection{Spiralmodell} % 13a.25
					\todo{Content}
				% end

				\subsubsection{V-Modell} % 13a.26
					\todo{Content}
				% end

				\subsubsection{Dynamic Systems Development Method (DSDM)} % 13a.27
					\todo{Content}
				% end

				\subsubsection{Design Process Model} % 13a.28
					\todo{Content}
				% end
			% end
		% end

		\section{GUI: Benutzeroberflächen} % 13a.30
			\todo{Content}

			\subsection{Das WIMP-Interface} % 13a.31
				\todo{Content}

				\subsubsection{Fenster-Komponenten} % 13a.32
					\todo{Content}

					\paragraph{Multiple Document Interface (MDI)} % 13a.33
						\todo{Content}
					% end

					\paragraph{Single Document Interface (SDI)} % 13a.34
						\todo{Content}
					% end

					\paragraph{Tabbed Document Interface} % 13a.35
						\todo{Content}
					% end
				% end

				\subsubsection{Dialogboxen} % 13a.36
					\todo{Content}

					\paragraph{Checkboxen} % 13a.37
						\todo{Content}
					% end

					\paragraph{Radio Buttons} % 13a.38
						\todo{Content}
					% end

					\paragraph{Listboxen} % 13a.39
						\todo{Content}
					% end

					\paragraph{Comboboxen} % 13a.40
						\todo{Content}
					% end

					\paragraph{Spinner} % 13a.41
						\todo{Content}
					% end

					\paragraph{Slider} % 13a.42
						\todo{Content}
					% end

					\paragraph{Weiteres} % 13a.43
						\todo{Content}
					% end
				% end
			% end

			\subsection{Menübasierte Programme} % 13a.44, 13a.45, 13a.46
				\todo{Content}

				\subsubsection{Untermenüs} % 13a.47
					\todo{Content}
				% end

				\subsubsection{Auswahl (if-then-else-Struktur)} % 13a.48, 13a.49
					\todo{Content}

					\paragraph{Verschachtelte Entscheidungsstrukturen} % 13a.50
						\todo{Content}
					% end
				% end

				\subsubsection{Die case-Struktur} % 13a.51
					\todo{Content}
				% end

				\subsubsection{Modularisierung} % 13a.52
					\todo{Content}
				% end
			% end

			\subsection{GUI-Anwendungen und Event-basiertes Programmieren} % 13a.53, 13a.54
				\todo{Content}

				\subsubsection{Graphical User Interfaces (GUIs)} % 13a.55
					\todo{Content}
				% end

				\subsubsection{Event-Handler} % 13a.56, 13a.57
					\todo{Content}
				% end

				\subsubsection{Nutzerinteraktionen} % 13a.58
					\todo{Content}
				% end

				\subsubsection{Das Delegationsmodell} % 13a.59
					\todo{Content}
				% end
			% end
		% end

		\section{3D-Interaktion} % 13a.61, 13a.62
			\todo{Content}
		% end
	% end

	\chapter{Multimedia Information Retrieval} % 13b.1, 13b.7, 13b.8, 13b.64
		\todo{Content}

		\section{Inhaltsbasierte Suche} % 13b.10, 13b.11, 13b.14, 13b.17
			\todo{Content}

			\subsection{Mathematische Beschreibung} % 13b.15
				\todo{Content}
			% end

			\subsection{Retrieval Ergebnis} % 13b.16
				\todo{Content}
			% end
		% end

		\section{Distanzmaße} % 13b.19, 13b.20, 13b.21, 13b.22, 13b.23, 13b.24
			\todo{Content}
		% end

		\section{Query-Modalitäten} % 13b.25, 13b.30, 13b.31
			\todo{Content}

			\subsection{Text} % 13b.32, 13b.33, 13b.34, 13b.35, 13b.36, 13b.37
				\todo{Content}
			% end

			\subsection{Example} % 13b.38, 13b.39, 13b.40, 13b.41, 13b.42
				\todo{Content}
			% end

			\subsection{Example-Bilder} % 13b.43
				\todo{Content}
			% end

			\subsection{Sketch} % 13b.44, 13b.45, 13b.46, 13b.47, 13b.48, 13b.49, 13b.50, 13b.51, 13b.52, 13b.53
				\todo{Content}
			% end
		% end

		\section{Explorative Suche} % 13b.54, 13b.55
			\todo{Content}

			\subsection{Research Data} % 13b.56, 13b.57, 13b.58, 13b.59, 13b.60
				\todo{Content}
			% end
		% end
	% end
\end{document}

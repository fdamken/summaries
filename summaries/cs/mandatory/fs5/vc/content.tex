\chapter{Einführung}
	\section{Visual Computing}
		\emph{Visual Computing} ist die Kombination mehrerer Bereiche der Informatik, in denen im Wesentlichen mit Bildern und Modellen gearbeitet wird. Dabei gibt es \zB folgende Themenbereiche:
		\begin{itemize}
			\item 3D-Internet
			\item Skalierbare Objektmodellierung und -erkennung
			\item Big Data, Visual Analytics
			\item Scene Understanding (Verstehen und Analysieren von 3D-/4D-Szenen)
		\end{itemize}
		Die Fachgebiete Computergraphik (Bildgebung) und Computer Vision (Bilderkennung) sollten nicht länger getrennt voneinander betrachtet werden, insbesondere die Schnittstelle beider Gebiete verdient besondere Beachtung.

		\subsection{3D-Internet}
			Das 3D-Internet ist eine Erweiterung des bislang größtenteils textuell aufgefassten Internets. Dabei gibt es vielfältige Anwendungsmöglichkeiten:
			\begin{itemize}
				\item Modellerzeugung
				\item Medical Computing
				\item uvm.
			\end{itemize}
		% end

		\subsection{Skalierbare Objektmodellierung/-erkennung}
			Heutige visuelle Objekterkennungsalgorithmen sind auf wenige hundert Kategorien limitiert, für die semantische Beschreibung der Umwelt werden aber mehrere zehntausend benötigt. Außerdem müssen neue Kategorien anhand weniger Beispiele erlernbar werden! Dazu werden 3D-Objektdatenbanken benötigt, um ähnliche Objekte zu bestimmen.

			Häufig werden Deep-Learning-Verfahren eingesetzt, die gigantische Berechnungen und intelligente (stochastische) Optimierungsalgorithmen benötigen.
		% end

		\subsection{Big Data, Visual Analytics}
			Die geeignete Analyse und explorative Erschließung von abstrakten, heterogenen Datenmengen ist in vielen Anwendungen sehr hilfreich. Der Bereich der Visual Analytics kombiniert viele Methoden der automatischen Datenverarbeitung und des maschinellen Lernens sowie neuartige Ansätze der Visualisierung und Mensch-Maschine-Interaktion.
		% end

		\subsection{Scene Understanding}
			Scene Unterstanding beschäftigt sich \zB damit, einmal erkannte Personen in einem Video auch nicht zu verlieren, wenn diese verdeckt werden. Dazu wird Personendetektion und -verfolgung integriert.

			Das allgemeine Ziel ist die vollständige Modellierung und Darstellung einer erfassten Szene sowie die semantische Beschreibung dieser.
		% end
	% end

	\section{Generalisierte Dokumente}
		Der technische Fortschritt führt zu einer Überflutung an Informationen, sodass effiziente Such- und Retrieval-Algorithmen immer mehr benötigt werden. Ein \emph{generalisiertes Dokument} kann dabei ein Bild, Text, Video, 3D-Modell, \dots sein.

		\subsection{Retro-Digitalisierung, Digital Creation}
			Die Digitalisierung von 3D-Objekten ist momentan noch zu aufwendig und zeitintensiv. Des weiteren fehlen Werkzeuge zur Massendigitalisierung. Es ist somit das Ziel, effektive 3D-Massendigitalisierungs-Werkzeuge und Workflows zu erstellen (\zB Cultlab3D).
		% end

		\subsection{Generative Modeling Language}
			Die \emph{Generative Modeling Language} (GML) ist eine Programmiersprache zur Beschreibung von dreidimensionalen Formen. Dabei folgt sie einem generativen Ansatz, \dh es werden nicht Objekt-Listen, sondern Objekt-erzeugende Operationen zur Datenrepräsentation verwendet.
		% end
	% end
% end

\chapter{Wahrnehmung}
	\section{Human-Computer-Interaction}
		Abbildung~\ref{fig:hci} zeigt den klassischen Zyklus der \emph{Human-Computer-Interaction} (HCI), \dh der Interaktion zwischen Mensch und Maschine. Dabei spielt insbesondere die visuelle Interaktion und Kommunikation über das Auge eine große Rolle.

		\begin{figure}
			\centering
			\begin{tikzpicture}[->, every node/.style = { align = center }]
				\node [draw, rectangle] (human) {Human};
				\node [draw, rectangle, right = 4 of human] (computer) {Computer};

				\draw (human) to[bend right] node[below]{Explizite Eingabe, \\ Implizite Interaktion} (computer);
				\draw (computer) to[bend right] node[above]{Visuelle Ausgabe, \\ Andere Ausgabemodalitäten} (human);
			\end{tikzpicture}
			\caption{Klassischer Zyklus der Human-Computer-Interaction (HCI).}
			\label{fig:hci}
		\end{figure}
	% end

	\section{Überblick}
		Der Mensch hat fünf grundlegende Sinne: Sehen, Hören, Fühlen, Schmecken und Riechen, wobei das Sehen, Hören und Fühlen derzeit dominant sind. Der heute sicherlich relevanteste Sinn ist dabei das Sehen und das menschliche Auge. Da die meisten erzeugten Bilder der Kommunikation von und zum Menschen dienen sollen, ist es gut, das menschliche visuelle System zu kennen, um den Informationstransfer optimal zu gestalten (der Monitorausgang ist nicht das Ende des Informationsflusses).

		Hören und Fühlen sind dabei relevant für die Informationsaufnahme und Interaktion mit der realen Welt (außerhalb der Mensch-Maschine-Interaktion).

		Bei der Gestaltung von Kommunikation gibt es zwei große Probleme:
		\begin{itemize}
			\item Die Wahrnehmung ist nicht objektiv.
			\item Das visuelle System ist stark nichtlinear (es ist keine einfache Interpolation oder Extrapolation von Versuchsergebnissen möglich).
		\end{itemize}

		\subsection{Menschliche Informationsverarbeitung}
			Abbildung~\ref{fig:humaninfoprocessing} zeigt die drei Stufen der menschlichen Informationsverarbeitung:
			\begin{itemize}
				\item \emph{Wahrnehmung} von Eindrücken durch die Sinne,
				\item \emph{Entscheidung}sfindung im Gehirn und
				\item \emph{Reaktion} durch den Körper.
			\end{itemize}
			Dabei verhält sich die Ausführungszeit additiv und die Funktionen werden durch neurologisch getrennte Gehirnteile ausgeführt, die "elektronisch" verbunden sind.

			\begin{figure}
				\centering
				\begin{tikzpicture}[->, every node/.style = { draw, rectangle, minimum height = 0.8cm, minimum width = 5cm }]
					\node (a) {Wahrnehmung (Sensorik)};
					\node [below = 0.5 of a] (b) {Entscheidung (Kognition)};
					\node [below = 0.5 of b] (c) {Reaktion (Motorik)};

					\draw (a) -- (b);
					\draw (b) -- (c);
				\end{tikzpicture}
				\caption{Modulares Drei-Stufen-Modell der menschlichen Informationsverarbeitung.}
				\label{fig:humaninfoprocessing}
			\end{figure}

			Dabei braucht jede Bearbeitung in den einzelnen Stufen unterschiedlich lange und die benötigten Zeiten können verwendet werden, um die Performanz abzuschätzen, \bzw vorherzusagen (\bspw für die Bildfrequenz von Filmen, die maximale Morserate, \dots). Typische Zeiten sind in Tabelle~\ref{fig:humanprocessingtimes} abgebildet.

			\begin{table}
				\centering
				\begin{tabular}{l|l|l}
					\textbf{Untersystem}     & \textbf{Durchschnitt}   & \textbf{Bereich}                 \\ \hline
					Wahrnehmung (Perzeption) & \SI{100}{\milli\second} & \SIrange{50}{200}{\milli\second} \\
					Entscheidung (Kognition) & \SI{70}{\milli\second}  & \SIrange{25}{170}{\milli\second} \\
					Reaktion (Motorik)       & \SI{70}{\milli\second}  & \SIrange{30}{100}{\milli\second}
				\end{tabular}
				\caption{Typische Bearbeitungszeiten der Untersysteme der menschlichen Informationsverarbeitung.}
				\label{fig:humanprocessingtimes}
			\end{table}

			\subsubsection{Eingabe (Wahrnehmung)}
				Die Untersysteme der Wahrnehmung,
				\begin{itemize}
					\item Visuell (Sehen)
					\item Akustisch (Hören)
					\item Haptisch (Fühlen)
				\end{itemize}
				können dabei (theoretisch) parallel arbeiten.

				\paragraph{Klangwahrnehmung}
					Die Hauptkomponenten von Klängen sind
					\begin{itemize}
						\item Klangfarbe,
						\item Tonlage und
						\item Lautstärke.
					\end{itemize}
					Diese werden durch verschiedene Mechanismen wahrgenommen und Informationen (\zB der Ursprung eines Geräuschs) extrahiert.
				% end

				\paragraph{Berührungswahrnehmung}
					Die Hauptkomponenten der Haptik sind
					\begin{itemize}
						\item Fühl- und Tastsinn (Temperatur, Schmerz, Druck, Oberflächen) und
						\item Propriozeption (Wahrnehmung der Bewegung und Lage der eigenen Körperglieder).
					\end{itemize}
					Dabei interagiert die Haptik stark mit Sehen und Hören, was bei sich widersprechenden Informationen Illusionen hervorrufen kann. Ein User-Interface-Designer nutzt Illusionen dabei gezielt aus, um bestimmte Informationen zu vermitteln.
				% end
			% end

			\subsubsection{Ausgabe (Reaktion)}
				Die Untersysteme der Reaktion,
				\begin{itemize}
					\item Artikulation (Sprechen)
					\item Motorisch (Bewegen)
				\end{itemize}
				können dabei (theoretisch) parallel arbeiten.

				Die motorische Ausgabe kann dabei auf verschiedene Weisen angewandt werden:
				\begin{itemize}
					\item Diskret (Schalter) oder
					\item Kontinuierlich (Heben).
				\end{itemize}
				Sie ist dabei beschränkt durch Geschwindigkeit, Stärke, Koordinationsvermögen, Wendigkeit, \dots. Neurologisch ist die motorische Ausgabe dabei mit dem haptischen System verbunden (Reflexe).

				Das \emph{Muskelgedächtnis} hilft dabei, relevante Positionen im Raum (\zB die Gangschaltung im Auto) zu lernen.
			% end
		% end
	% end

	\section{Wahrnehmung}
		\subsection{Das Auge}
			\subsubsection{Reiz und Licht}
				Ein äußerer, visueller Reiz (Licht) erzeugt beim Menschen eine physikalische Rezeption des äußeren Reizes (Input). Dies geschieht durch einen Sensor (\bspw das Auge) und die Reizung produziert ein neurophysiologisches Signal. Dieses wird anschließend verarbeitet und interpretiert.

				Physikalisch ist ein solcher Reiz elektromagnetische Strahlung. Dabei wird monochromatisches, \dh einfarbiges, Licht durch die Angabe der Frequenz \(v\), \bzw der Wellenlänge \(\lambda\), beschrieben. Diese beiden Größen sind durch die Beziehung
				\begin{equation*}
					v \lambda = c, \quad c \approx \SI{3e8}{\meter\per\second}
				\end{equation*}
				miteinander verknüpft, wobei \(c\) die Ausbreitungsgeschwindigkeit des Lichts ist.

				Das menschliche Auge kann dabei Frequenzen im Wellenlängenbereich \( \SIrange{380}{750}{\nano\meter} \) wahrnehmen. Kleinere Wellenlängen haben \zB Ultraviolett-Licht, Röntgen- und \(\gamma\)-Strahlung. Darüber liegende Wellenlängen haben \zB Infrarot-Licht und Rundfunk-Wellen.
			% end

			\subsubsection{Das visuelle System}
				Das menschliche Auge ist aufgebaut aus:
				\begin{itemize}
					\item Hornhaut (Kornea)
					\item Linse (zur Scharfstellung)
					\item Iris (Blendenmechanismus)
					\item Retina (Netzhaut)
						\begin{itemize}
							\item Blinder Fleck: Hier geht der Sehnerv ab.
							\item Fovea Centralis (Gelber Fleck): Bereich mit der höchsten Auflösung.
						\end{itemize}
				\end{itemize}
			% end

			\subsubsection{Photorezeptoren}
				Die Photorezeptoren (welche auf der Retina platziert sind), bestehen aus:
				\begin{itemize}
					\item Stäbchen
						\begin{itemize}
							\item Hauptsächlich außerhalb der Fovea.
							\item Das Empfindlichkeitsmaximum liegt bei \SI{498}{\nano\meter} ("grün").
						\end{itemize}
					\item Zapfen
						\begin{itemize}
							\item Vor allem in der Fovea platziert.
							\item Es gibt drei Zapfentypen für Farbsehen.
							\item Das Empfindlichkeitsmaximum dieser Zapfen liegt bei \SI{420}{\nano\meter} ("blau", \emph{S}hort-Zapfen), \SI{534}{\nano\meter} ("grün", \emph{M}edium-Zapfen) und \SI{564}{\nano\meter} ("rot", \emph{L}ong-Zapfen).
						\end{itemize}
				\end{itemize}
			% end

			\subsubsection{Skotopisches und Photopisches Sehen}
				\begin{itemize}
					\item Nachtsehen (skotopisch): Dominanz der Stäbchen.
					\item Tagsehen (photopisch): Dominanz der Zapfen.
				\end{itemize}
			% end

			\subsubsection{Zapfenverteilung} % 2.38, 2.39, 2.40, 2.41, 2.42, 2.43, 2.44
				\todo{Auge: Zapfenverteilung}
			% end
		% end

		\subsection{Vorverarbeitung visueller Informationen}
			\subsubsection{Signalverarbeitung in der Retina}
				Neben den Photorezeptoren gibt es noch weitere Zellen zur Signalverarbeitung in der Retina:
				\begin{itemize}
					\item Horizontale Zellen \\ Kombination von mehreren Rezeptoren einer Region.
					\item Amakrin-Zellen \\ Zeitliche Verarbeitung.
					\item Bipolar-Zellen \\ Informationsfilter (Sammeln, Gewichten und Weiterleiten).
					\item Ganglien-Zellen \\ Integrations-Informationen (\zB Kontrastwahrnehmung).
				\end{itemize}
			% end

			\subsubsection{Helligkeit}
				\begin{itemize}
					\item \emph{Helligkeit} (\emph{brightness}) entspricht der wahrgenommenen Menge an Licht, das von einer selbstleuchtenden Lichtquelle ausgeht.
					\item \emph{Helligkeit} (\emph{lightness}) entspricht der wahrgenommenen Menge an Licht, das von einer reflektierenden Oberfläche ausgeht.
						\begin{itemize}
							\item Dies ist keine absolute Wahrnehmungsgröße und abhängig von
								\begin{itemize}
									\item Reizstärke (Leuchtdichte)
									\item Vorheriger Leuchtdichte (Adaption)
									\item Umgebungsleuchtdichte
									\item Größe (Fläche) des Reizes
								\end{itemize}
							\item Somit subjektiv!
						\end{itemize}
					\item Dies wirft einige nicht so einfach zu beantwortende Fragen auf, \zB: Was ist weiß? Was ist schwarz? Was ist mittelgrau?
					\item Der Hell-Dunkel-Kontrast ist dabei eine wichtige Empfindungsgröße zum Form- und Objektsehen. Daher muss der Unterschied groß genug sein (für kleine Details mindestens \(3:1\), besser \(10:1\)).
				\end{itemize}

				\paragraph{Kontrast als Reizverhältnis}
					Für den Kontrast gibt es verschiedene Definitionen, \zB (dabei ist \(L\) stets die Leuchtdichte):
					\begin{equation*}
						m = k = \frac{L_\text{max} - L_\text{min}}{L_\text{max} + L_\text{min}}
					\end{equation*}
					oder
					\begin{equation*}
						K = \frac{L_R - L_H}{L_H} = \frac{\delta L}{L_H}
					\end{equation*}
					wobei \( L_R \) die Leuchtdichte des Vordergrunds und \(L_H\) die Leuchtdichte des Hintergrunds darstellt.

					\todo{Wahrnehmung: Weber-Fechnersches Gesetz, Stevensches Gesetz; 2.65}
				% end
			% end

			\subsubsection{Erkennung von Details}
				Die Erkennung kleiner Details ist begrenzt durch
				\begin{itemize}
					\item Optische Eigenschaften des Auges, \zB Beugungserscheinungen,
					\item Abtastung durch Rezeptoren und
					\item nervöse Verarbeitung.
				\end{itemize}
				Zwei mögliche Maße zur "Erkennbarkeit" sind:
				\begin{itemize}
					\item Kontrastempfindlichkeit
					\item Schwellenkontrast
				\end{itemize}

				\paragraph{Kontrastempfindlichkeit}
					Die Kontrastempfindlichkeit ist die Auflösung des menschlichen Auges im Frequenzraum. Veränderliche Intensität kann dabei mit Sinus-förmigen Mustern gemessen werden.
				% end
			% end

			\subsubsection{Frühe Wahrnehmung}
				Das Auge nimmt einige Veränderungen der Umgebung schneller wahr als andere. Um die Aufmerksamkeit auf etwas zu lenken, können beispielsweise
				\begin{itemize}
					\item Farbe,
					\item Richtung,
					\item Bewegung,
					\item Größe,
					\item Beleuchtung/Schattierung
				\end{itemize}
				variiert werden.
			% end
		% end

		\subsection{Informationsextraktion}
			Ein reiner Reiz ist noch keine \emph{Wahrnehmung}. Dazu kommen noch andere Faktoren wie Kontext, Erwartungen, Adaption. Das Messen der tatsächlichen Wahrnehmung ist leider sehr schwierig, weshalb häufig nur statistische Aussagen auf Basis von User-Tests getätigt werden können.

			Dabei erschwert, dass die Wahrnehmung nicht immer der Realität entspricht. Es wird hingegen das Bild durch einen Wahrnehmungsprozess im Gehirn produziert. Dabei wird die menschliche Wahrnehmung adaptiert, \bspw dreht sich das Bild bei einem Kopfstand.

			\subsubsection{Raumwahrnehmung}
				Die Wahrnehmung des Raums (Raumwahrnehmung) enthält unter anderem
				\begin{itemize}
					\item Tiefenwahrnehmung,
					\item Entfernungs- und Distanzwahrnehmung und
					\item Ausrichtung des Körpers im Raum.
				\end{itemize}
				Daran sind viele Wahrnehmungssysteme beteiligt:
				\begin{itemize}
					\item Vestibuläres System (im Innenohr)
					\item Haptisch-somatisches System (Tasten und Berühren)
					\item Auditives Sehen (Gehör)
					\item Propriozeptives System (Eigenwahrnehmung)
					\item Visuelles System
				\end{itemize}

				Dabei ist die Raumwahrnehmung auch mit einem Auge (Monokular) möglich (tatsächlich sind \SIrange{5}{10}{\percent} aller Menschen stereoblind und \SI{20}{\percent} haben eine Stereo-Schwäche).

				Tatsächlich ist die Raumwahrnehmung ein sehr komplexer Prozess, der auch heute nur zu Teilen verstanden wird. Dabei fließen noch viele weitere Phänomene ein, \zB Größenkonstanz, Annahme starrer Körper oder Vektion. Letzteres ist dabei die scheinbare Eigenbewegung bei einem statischen Vordergrund als Referenzrahmen und einem bewegten Hintergrund.
			% end

			\subsubsection{Depth Cue Theorie}
				Die Annahme der \emph{Depth Cue Theorie} ist, dass die Raumwahrnehmung des visuellen Systems auf Hinweisreizen (sogenannten \emph{Depth Cues}) basiert. Diese werden in drei Kategorien eingeteilt:
				\begin{enumerate}
					\item Binokulare Depth Cues (mit zwei Augen)
						\begin{itemize}
							\item Disparität/Parallaxe
							\item Akkommodation (Krümmung der Augenlinsen)
							\item Konvergenz (die Augen nach innen drehen)
						\end{itemize}
					\item Pictoiral Depth Cues (mit einem Auge)
						\begin{itemize}
							\item Linearperspektive
							\item Verdeckung
							\item Texturgradient
							\item Fokus und Blur
							\item Atmosphärische Tiefe
							\item Vertraute Größe
							\item Höhe im Gesichtsfeld
							\item Beleuchtung
							\item Schattenwurf
							\item Luminanzänderung
							\item Transluzenz
							\item Schattierung
						\end{itemize}
					\item Dynamische Depth Cues (Animation)
						\begin{itemize}
							\item Bewegungsparallaxe
							\item Kinetischer Tiefeneffekt
							\item Interposition
							\item Bewegung von Highlights
						\end{itemize}
				\end{enumerate}

				\paragraph{Stereoskopie}
					Bei der Stereoskopie nehmen beide Augen ein leicht unterschiedliches Bild wahr, woraus die Entfernung zu einem Objekt berechnet werden kann.
				% end

				\paragraph{Pictorial Depth Cues}
					\subparagraph{Linearperspektive} % 2.90
						\todo{Depth Cues: Linearperspektive}
					% end

					\subparagraph{Texturgradient}
						Sind als parallel angenommene Linien nicht mehr parallel, so ergibt sich eine scheinbare Tiefe (als wenn kariertes Papier um einen Ball gerollt und von oben betrachtet wird).
					% end

					\subparagraph{Fokus und Blur}
						Das Auge fokussiert einen Punkt und produziert somit eine Tiefenschärfe. Daran kann erahnt werden, welche Objekte im Vorder- oder Hintergrund sind.
					% end

					\subparagraph{Atmosphärische Tiefe}
						Anhand der Atmosphäre (\zB durch Nebel ausgelöst) wird erkannt, was vermutlich im Hintergrund liegt. So kann zum Beispiel bei einem Foto von einem Berg geschätzt werden, dass der Boden niedriger ist, wenn Wolken über diesem hängen.
					% end

					\subparagraph{Schattenwurf}
						Annahme: Beleuchtung von oben und Vorhandensein einer Grundebene. Dann kann durch den Abstand von Schatten zum Objekt erahnt werden, wie weit dieses vom Boden entfernt ist.
					% end
				% end

				\paragraph{Dynamische Depth Cues}
					\subparagraph{Motion Parallax} % 2.98
						\todo{Depth Cues: Motion Parallax}
					% end

					\subparagraph{Raumwahrnehmung durch Bewegung}
						Wird \zB eine schaukelnde Vase von oben betrachtet, so bewegt sich die Öffnung charakteristisch, sodass eine Wahrnehmung der Tiefe entsteht.
					% end

					\subparagraph{Kinetic Depth Effect, Structure from Motion} % 2.100
						\todo{Depth Cues: Kinetic Depth Effect}
					% end
				% end

				\paragraph{Auswertung von Depth Cues}
					Unterschiedliche Depth Cues haben im Allgemeinen einen unterschiedlichen Informationsgehalt. Dabei sind sie nicht redundant, sondern additiv. Durch ein kompliziertes Zusammenspiel (flexible Gewichtung, Dominanz eines Depth Cues) bildet sich das Gehirn ein Bild. Dabei bildet es sich allerdings kein tatsächliches 3D-Modell, sondern verwendet die unterschiedlichen Cues für verschiedene Aufgaben. Diese können \zB sein:
					\begin{itemize}
						\item Einschätzen von Objektgrößen
						\item Einschätzen von Entfernungen
						\item Verfolgung von Pfaden
						\item Navigation
						\item Einschätzen der Eigenbewegung
						\item Abschätzung der Kollisionszeit
					\end{itemize}
				% end
			% end
		% end
	% end

	\section{Aufmerksamkeit}
		\subsection{Limitierung der Wahrnehmung}
			Die initiale Reizaufnahme hat viele Limitierungen, sodass nur ein Bruchteil des äußeren Reizes zur kognitiven Verarbeitung zur Verfügung steht. Dabei sind Aufmerksamkeit und externe Faktoren wichtige Einflüsse auf die tatsächliche Wahrnehmung. Die Wahrnehmung ist dabei eher eine partielle Hypothese, die auf Basis unvollständiger Informationen generiert wurde. Sie wird periodisch aktualisiert aufgrund von Beobachtungen, \dh die Hypothese wird gegen sensorische Daten getestet. Durch eine dynamische Suche des visuellen Systems wird nach der besten Hypothese/Interpretation/Modell gesucht.
		% end

		\subsection{Das Gedächtnis und "Gateway to Memory"}
			Das Gehirn kann sich auf bestimmte Dinge fokussieren und den Rest ignorieren. Dabei gibt es drei verschiedene Arten der Aufmerksamkeit:
			\begin{itemize}
				\item \emph{Gewählte Aufmerksamkeit} (selective): Zwischen mehreren Möglichkeiten wird eine zu fokussierende Sache aktiv ausgewählt.
					\begin{itemize}
						\item Das Auge folgt den Objekten von Interesse.
						\item Der Kopf folgt den Klängen von Interesse.
						\item Es gibt nur einen einzigen "Ort der Aufmerksamkeit".
					\end{itemize}
				\item \emph{Geteilte Aufmerksamkeit} (divided): Ein Versuch durch "Multitasking" auf mehrere Dinge zu fokussieren.
					\begin{itemize}
						\item Entweder "gleichzeitig" durch schnelles Umschalten (time multiplexing).
						\item Dies wirkt sich negativ auf die Verarbeitung aus, wenn die Aufgaben überfordernd sind.
						\item Die Aufgaben beeinträchtigen sich gegenseitig.
					\end{itemize}
				\item \emph{Erfasste Aufmerksamkeit} (captured): Ein äußerer Reiz zieht alle Aufmerksamkeit auf sich.
					\begin{itemize}
						\item Im Gegensatz zur gewählten Aufmerksamkeit wird der "Ort" nicht aktiv ausgewählt.
						\item Dies geschieht \zB wenn man von einem Tier angefallen wird.
					\end{itemize}
			\end{itemize}

			Das menschliche Gedächtnis ist in mehrere "Teilgedächtnisse" aufgeteilt. Voran steht das \emph{Arbeitsgedächtnis}, auf das ein schneller Zugriff (\ca \SI{70}{\milli\second}) möglich ist, welches aber einen schnellen Verfall hat (nach \ca \SI{200}{\milli\second}). Es stellt sozusagen das "Schmierblatt" des Gehirns da. Nach wenigen Sekunden wird der Inhalt jedoch an das Langzeitgedächtnis weitergegeben.

			Das Langzeitgedächtnis ist langsamer (\ca \SI{100}{\milli\second}), dafür aber auch sehr viel größer (die genaue Größe ist unbekannt). Das Langzeitgedächtnis hat dabei drei Hauptaufgaben:
			\begin{itemize}
				\item Informationen speichern und sich an diese erinnern,
				\item Informationen abrufen und
				\item Informationen vergessen.
			\end{itemize}
		% end
	% end
% end

\chapter{Computer Vision: Objekterkennung und Bayes}
	Die \emph{Computer Vision} beschäftigt sich mit dem maschinellen Sehen, \dh der Suche nach einem Modell des menschlichen Sehens. Anwendungsgebiete sind \bspw Autos, die Fußgänger erkennen, medizinische Bildverarbeitung, Überwachung, Unterhaltung, Computergraphik, \dots.

	\section{Computer Vision}
		Das einfachste Standardmodell einer Lochkamera ist ein Kasten mit einem kleinen Loch. Um ein digitales Bild eines solchen Kameramodells zu erhalten, wird das Bild rasterisiert. Demnach ist ein Graustufenbild eine Matrix an Pixeln mit jeweils einem Wert (die "Grauigkeit" des Pixels).

		Die Computer Vision beschäftigt sich nun damit, aus einem solchen Bild Informationen zu extrahieren. Bei der Objekterkennung ist es wichtig, eine gute lokale Beschreibung/Merkmale zu haben (\zB Augen, Mund, Nase) und eine globale Anordnung der lokalen Merkmale (\zB relative Positionen, relative Größen). Es ist aber auch eine schnelle Generierung guter Hypothesen, Segmentierung der Bildbereiche und Kennen des Szenenkontextes wichtig.

		Nach Fischler und Elschlager hat das Modell eines Bildes zwei Komponenten: Teile (2D Bildfragmente) und den Aufbau (die Anordnung der Teile). Mit diesem abstrakten Modell lassen sich viele Dinge (\zB ein Gesicht) charakterisieren.
	% end

	\section{Bayesian Decision Theory}
		Beispiel: Buchstabenerkennung. Es soll ein neu aufgenommener Buchstabe so klassifiziert werden, dass die Wahrscheinlichkeit der Fehlklassifikation minimiert wird.

		\subsection{Konzepte und Bayes Theorem}
			\paragraph{Vorbemerkung: Wahrscheinlichkeitsdichte und Wahrscheinlichkeit}
				Ist \( p(x) \) eine Wahrscheinlichkeitsdichte, so ist die Wahrscheinlichkeit, dass \(x\) im Intervall \( (x_0, x_1) \) liegt, gegeben durch:
				\begin{equation*}
					P(x_0 < x < x_1) = \int_{x_0}^{x_1} \! p(\tau) \dd{\tau}
				\end{equation*}
				Da für die Wahrscheinlichkeit, dass \( x \) im Intervall \( (x, x + \Delta x) \) mit \( \Delta x \to 0 \) gilt:
				\begin{equation*}
					\lim\limits_{\Delta x \to 0} P(x) = \lim\limits_{\Delta x \to 0} P(x < t < x + \Delta x) = p(x) \cdot \Delta x
				\end{equation*}
				kann Wahrscheinlichkeitsdichte und Wahrscheinlichkeit in den meisten Fällen gegeneinander ausgetauscht werden.
			% end

			\paragraph{1. Konzept: A-Priori Wahrscheinlichkeit (Prior)}
				Die \emph{a-priori Wahrscheinlichkeit} (Prior) enthält die Information, wie wahrscheinlich eine beliebige Messung der Klasse zugehört (\dh die "Klassenhäufigkeit"). Ist \( C_k \) eine Klasse, so ist \( P(C_k) \) der Prior \bzgl der Klasse \( C_k \) (analog für \( p(C_k) \)).
			% end

			\paragraph{2. Konzept: Bedingte Wahrscheinlichkeit (Likelihood)}
				Ist \(\vec{x}\) der Merkmalsvektor (Feature), welcher Eigenschaften der Messung beschreibt (Anzahl schwarzer Pixel, Höhe/Breite, \dots) und \( C_k \) eine Klasse, so ist \( P(\vec{x} \given C_k) \) die \emph{Likelihood}, \dh die Wahrscheinlichkeit, dass \(\vec{x}\) für einen Buchstaben der Klasse \( C_k \) gemessen wird (analog für \( p(\vec{x} \given X_k) \)).
			% end

			\paragraph{3. Konzept: A-Posteriori Wahrscheinlichkeit (Posterior), Bayes Theorem}
				Die \emph{a-posteriori Wahrscheinlichkeit} (Posterior) ist die Wahrscheinlichkeit, dass ein Merkmalsvektor \(\vec{x}\) einer Klasse \( C_k \) angehört, \dh \( P(C_k \given \vec{x}) \). Dieser Posterior kann durch Bayes Theorem gefunden werden:
				\begin{equation*}
					P(C_k \given \vec{x}) = \frac{P(\vec{x} \given C_k) \cdot P(C_k)}{P(\vec{x})}
				\end{equation*}
				Oder namentlich:
				\begin{equation*}
					\text{Posterior} = \frac{\text{Likelihood} \times \text{Prior}}{\text{Normalisierung}}
				\end{equation*}
			% end
		% end

		\subsection{Problemstellung}
			Abbildung~\ref{fig:likelihoodPriorPosterior} zeigt die Likelihood, Prior und den Posterior auf. Die Zielstellung eines Bayesian Classifier ist nun, die Wahrscheinlichkeit der Fehlklassifikation zu minimieren und somit eine Entscheidungsgrenze zu bestimmen. Die Wahrscheinlichkeit eines Fehlers ist gegeben durch:
			\begin{align*}
				P(\text{Fehler}) & = P(x \in R_2, C_1) + P(x \in R_1, C_2)                                                                     \\
				                 & = P(x \in R_2 \given C_1) P(C_1) + P(x \in R_1 \given C_2) P(C_2)                                           \\
				                 & = \int_{R_2} \! p(x \in R_2 \given C_1) P(C_1) \dd{x} + \int_{R_2} \! p(x \in R_2 \given C_2) P(X_2) \dd{x}
			\end{align*}
			Dabei ist \( P(x \in R_i, C_j) \) die Wahrscheinlichkeit, dass \(x\) zu Klasse \(R_i\) gehört, aber als Klasse \(C_j\) klassifiziert wurde (für \( i \neq j \) entspricht dies einer Fehlklassifikation).

			\begin{figure}
				\centering
				\begin{tikzpicture}
					\begin{axis}[
							name = likelihood,
							domain = 0:15,
							xmin = 0,
							xmax = 15,
							ymin = 0,
							ymax = 0.5,
							xlabel = \(x\),
							width = 12cm,
							height = 7cm,
							legend style = { xshift = -0.5cm, yshift = -0.5cm, fill=\thepagecolor, draw=fgcolor },
							xticklabels = {,,}
						]
						\addplot [density, TUDa-9\IfDarkModeTF{a}{b}, smooth] { gaussian(x, 5, 2) }; \addlegendentry{\( p(x \given a) \)};
						\addplot [density, TUDa-1\IfDarkModeTF{a}{b}, smooth] { gaussian(x, 11, 2) }; \addlegendentry{\( p(x \given b) \)};
					\end{axis}
					\begin{axis}[
							name = likelihoodPrior,
							domain = 0:15,
							xmin = 0,
							xmax = 15,
							ymin = 0,
							ymax = 0.5,
							xlabel = \(x\),
							width = 12cm,
							height = 7cm,
							at = (likelihood.right of south east),
							anchor = north,
							xshift = -5.23cm,
							yshift = -0.5cm,
							legend style = { xshift = -0.5cm, yshift = -0.5cm, fill=\thepagecolor, draw=fgcolor },
							xticklabels = {,,}
						]
						\addplot [density, TUDa-9\IfDarkModeTF{a}{b}, smooth] { gaussian(x, 5, 2) * 0.8 }; \addlegendentry{\( p(x \given a) p(a) \)};
						\addplot [density, TUDa-1\IfDarkModeTF{a}{b}, smooth] { gaussian(x, 11, 2) * 0.2 }; \addlegendentry{\( p(x \given b) p(b) \)};
					\end{axis}
					\begin{axis}[
							name = posterior,
							domain = 0:15,
							xmin = 0,
							xmax = 15,
							ymin = 0,
							ymax = 1.1,
							xlabel = \(x\),
							width = 12cm,
							height = 7cm,
							at = (likelihoodPrior.right of south east),
							anchor = north,
							xshift = -5.23cm,
							yshift = -0.5cm,
							legend style = { xshift = -0.5cm, yshift = -0.5cm, fill=\thepagecolor, draw=fgcolor },
						]
						\addplot [density, TUDa-9\IfDarkModeTF{a}{b}, smooth] { (gaussian(x, 5, 2) * 0.8) / (gaussian(x, 5, 2) * 0.8 + gaussian(x, 11, 2) * 0.2) }; \addlegendentry{\( p(a \given x) \)};
						\addplot [density, TUDa-1\IfDarkModeTF{a}{b}, smooth] { (gaussian(x, 11, 2) * 0.2) / (gaussian(x, 5, 2) * 0.8 + gaussian(x, 11, 2) * 0.2) }; \addlegendentry{\( p(b \given x) \)};
					\end{axis}
				\end{tikzpicture}
				\caption{Likelihood, \( \text{Likelihood} \times \text{Prior} \) und Posterior.}
				\label{fig:likelihoodPriorPosterior}
			\end{figure}
		% end

		\subsection{Entscheidungsregel}
			Durch die Minimierung des Erwartungswertes des Fehlers kann die Entscheidungsregel, wann \(x\) in eine Klasse einsortiert wird, hergeleitet werden. Dabei soll \(x\) genau dann in Klasse \(C_1\) sortiert werden, wenn
			\begin{equation*}
				P(C_1 \given x) > P(C_2 \given x)
			\end{equation*}
			Da die Posteriors im Allgemeinen nicht bekannt sind, werden die über Bayes Theorem berechnet:
			\begin{align*}
				               &  & P(C_1 \given x)                         & > P(C_2 \given x)                     & \\
				\quad\iff\quad &  & \frac{P(x \given C_1) P(C_1)}{P(x)}     & > \frac{P(c \given C_2) P(C_2)}{P(x)} & \\
				\quad\iff\quad &  & P(x \given C_1) P(C_1)                  & > P(c \given C_2) P(C_2)              & \\
				\quad\iff\quad &  & \frac{P(x \given C_1)}{P(x \given C_2)} & > \frac{P(C_1)}{P(C_2)}               &
			\end{align*}
			Dies wird auch \emph{Likelihood Ratio Test} genannt.

			Dieser Test kann sich für mehr als zwei Klassen verallgemeinern lassen: Wähle Klasse \( k \) genau dann, wenn
			\begin{equation*}
				P(C_k \given x) > P(C_j \given x) \quad\forall j \neq k
			\end{equation*}
			gilt. Äquivalent zu dem zwei-Klassen-Fall kann dies in einen Likelihood Ratio Test umgeformt werden:
			\begin{equation*}
				\frac{P(x \given C_k)}{P(x \given C_j)} > \frac{P(C_j)}{P(C_k)} \quad\forall j \neq k
			\end{equation*}
		% end

		\subsection{Naive Bayes Classifier}
			Bei mehr als zwei Merkmalen (\zB Höhe und Breite) werden \( P(x_1, x_2 \given C_k) \) und \( P(x_1, x_2) \) mehrdimensional und eine Schätzung der Dichte ist nicht immer möglich. Daher nimmt ein \emph{Naive Bayes Classifier} an, dass die Merkmale statistisch unabhängig sind. Damit gilt:
			\begin{align*}
				P(x_1, x_2 \given C_k) & = P(x_1 \given C_k) P(x_2 \given C_k) \\
				P(x_1, x_2)            & = P(x_1) P(x_2)
			\end{align*}
			In der Realität ist diese Annahme oft nicht korrekt, liefert aber häufig gute Ergebnisse und ist somit eine gute Basis zum Vergleich.
		% end
	% end

	\section{Probability Density Estimation}
		Bisher wurden die Wahrscheinlichkeiten \( P(x \given C_k) \) und \( P(C_k) \) als bekannt vorausgesetzt. In der Realität ist dies oft nicht der Fall, weshalb die Wahrscheinlichkeitsdichte geschätzt werden muss. Siehe hierzu auch Vorlesung \href{https://fabian.damken.net/summaries/cs/elective/vc/statml/}{Statistical Machine Learning}.
	% end

	\section{Gesichtsdetektion}
		Bei \emph{Appearance-Based Methods} wird ein Erscheinungsmodell aus (üblicherweise) großen Mengen von Bildern gelernt. Dabei wird am häufigsten der Sliding Window Ansatz genutzt (siehe~\ref{sec:slidingwindow}). Dabei sind vor allem drei Aspekte relevant:
		\begin{enumerate}
			\item Repräsentation des Objektes (lokale Merkmale, globale Anordnung)
			\item Trainingsdaten (positive und negative Beispiele)
			\item Klassifikator und Lernmethode
		\end{enumerate}

		\subsection{Sliding Window Ansatz}
			\label{sec:slidingwindow}

			Bei dem \emph{Sliding Window Ansatz} wird ein Bild in Ein-Pixel-Schritten horizontal und vertikal gescannt. Nach jedem Durchlauf wird das Bild immer wieder verkleinert, bis das Bild zu klein ist. So können auch mit einem Klassifikator, der nur Bilder einer Größe entgegen nehmen kann, große Bilder durchsucht werden.
		% end

		\subsection{Beispiel: Gesichtsdetektion}
			\begin{enumerate}
				\item Repräsentation des Objekts
					\begin{itemize}
						\item Die Bilder werden in Wavelets zerlegt, \dh die Gesichtsmerkmale werden mit Frequenzen und deren Ort und Orientierung dargestellt.
						\item Lokale Merkmale: Wavelet Koeffizienten (Frequenzen von \zB Auge und Mund).
						\item Globale Merkmale: Absolute Position der Frequenzen im Bild.
					\end{itemize}
				\item Trainingsdaten
					\begin{itemize}
						\item Positive Beispiele
							\begin{itemize}
								\item Möglichst vielfältig.
								\item Jedes Bild eines Gesichts wird manuell an den Rändern abgeschnitten und auf eine Größe normalisiert.
								\item Zusätzlich werden virtuelle Beispiele erstellt (\zB durch Spiegelung).
							\end{itemize}
						\item Negative Beispiele
							\begin{itemize}
								\item Beliebige Bilder, die keine Gesichter enthalten.
								\item Teilbilder von großen Bildern.
							\end{itemize}
					\end{itemize}
				\item Klassifikator und Lernmethode
					\begin{itemize}
						\item Naive Bayes Classifier
						\item Merkmale \(x_i\): Wavelet Koeffizienten an einer bestimmten Position.
						\item Zwei-Klasse-Problem:
							\begin{itemize}
								\item \(C_1\): Gesichter
								\item \(C_2\): Alles andere (keine Gesichter)
							\end{itemize}
						\item Das "Lernen" entspricht dem Schätzen der Wahrscheinlichkeiten der Wavelet-Koeffizienten.
						\item Durch Diskretisierung von Koeffizienten und Positionen gibt es eine diskrete und endliche Anzahl von \(x_i\).
						\item Schätzen: Zählen, wie häufig jedes \(x_i\) in Bilder mit und ohne Gesichtern vorkommt.
						\item Dann wird ein Likelihood Ration Test verwendet.
					\end{itemize}
			\end{enumerate}

			Um Bilder aus verschiedenen Perspektiven zu erkennen, wird für jede Ansicht ein eigener Detektor verwendet (jeder für eine Ansicht) und diese kombiniert.
		% end

		\subsection{Erkennungsarten}
			Eine Gesichtserkennung zählt zu den biometrischen Verfahren und werden \bspw in sicherheitstechnischen, kriminalistischen und forensischen Gebieten eingesetzt. Der Zweck ist dir Identifikation und Verifikation natürlicher Personen.
			\begin{itemize}
				\item Verifikation: Die Person muss dem System ihren Namen oder User-ID mitteilen und das System entscheidet, ob die Person dazu gehört.
				\item Identifikation: Die Person offenbart ausschließlich ihre biometrischen Merkmale und das System ermittelt daraus den Namen oder die User-ID.
			\end{itemize}
		% end
	% end
% end

\chapter{Fouriertheorie}
	Bei der Beugung an einem einfachen Spalt der breite \(a\) ergibt sich auf dem Schirm ein Beugungsmuster, welches im Zentrum ein Intensitätsmaximum und nach außen hin immer wieder Intensitätsminima und -maxima hat. Der Spalt kann durch eine Rechteckfunktion
	\begin{equation*}
		\text{Rect}(x) =
		\begin{cases}
			1 & -1 \leq x \leq 1 \\
			0 & \text{sonst}
		\end{cases}
	\end{equation*}
	beschrieben werden. Das sich ergebende Beugungsmuster, \bzw die zeitlich gemittelte Intensität \(I\), hat dann die Form
	\begin{equation*}
		I(\theta) = I_0 \, \Bigg( \frac{\sin(\theta)}{\theta} \Bigg)^2 = I_0 \cdot \sinc^2(\theta)
	\end{equation*}
	mit der \(\sinc\)-Funktion \( \sinc(\theta) = \sin(\theta) / \theta \). Dabei stellt \(\theta\) den Ausfallwinkel des Lichts aus dem Spalt hinaus dar.

	Dieser Zusammenhang zwischen der Gestalt des beugenden Objekts (hier der Spalt) und der Amplitudenfunktion \( I(\theta) \) ist durch eine \emph{Fourier-Transformation} gegeben.

	\section{Mathematische Grundlagen}
		\subsection{Vektorraum}
			Ein \emph{Vektorraum} ist eine algebraische Struktur über einen Zahlenbereich mit Operationen wie Addition und Multiplikationen mit einem Skalar. Alle Operationen müssen dabei Elemente des Vektorraums wieder auf selbigen abbilden. Die Elemente eines solchen Raums sind \emph{Vektoren}.

			\subparagraph{Beispiel}
				Ein Beispiel ist er euklidische Vektorraum über den reellen Zahlen. Dabei repräsentieren Vektoren Verschiebungen und es lassen sich Längen und Winkel messen (rechtwinkliges, kartesisches Koordinatensystem). Es ist außerdem ein Skalarprodukt definiert:
				\begin{gather*}
					\langle \vec{v}, \vec{w} \rangle = \sum_{i = 1}^{n} v_i w_i \in \R \\
					\langle \vec{v}, \vec{w} \rangle = v_1 w_2 + v_2 w_2 = \lVert \vec{v} \rVert \cdot \lVert \vec{w} \rVert \cos \big( \angle(\vec{v}, \vec{w}) \big)
				\end{gather*}
				Die letztere Eigenschaft gilt nur für \( n = 2 \) (\iA lassen sich solche Winkel aber auch mit beliebigem \(n\) definieren). In der euklidischen Ebene \( \R^2 \) lassen sich Vektoren durch Ortsvektoren (Pfeile) darstellen.
			% end
		% end

		\subsection{Basis eines Vektorraums}
			Jeder Satz (Menge) an linear unabhängigen Vektoren eines Vektorraums kann als Basis verwendet werden. Zwei Vektoren \(\vec{v}\), \(\vec{w}\) sind genau dann linear unabhängig, wenn \( \big\lvert \langle \vec{v}, \vec{w} \rangle \big\rvert < \lVert \vec{v} \rVert \cdot \lVert \vec{w} \rVert \) gilt.

			\subparagraph{Beispiel}
				In der euklidischen Ebene \( \R^2 \) ist eine Basis durch
				\begin{equation*}
					\vec{e}_1 = \begin{bmatrix} 1 \\ 0 \end{bmatrix} \quad\quad \vec{e}_2 = \begin{bmatrix} 0 & 1 \end{bmatrix}
				\end{equation*}
				gegeben, wobei \(\vec{e}_1\) und \(\vec{e}_2\) orthogonal aufeinander stehen (\( \langle \vec{v}, \vec{w} \rangle = 0 \)) und somit linear unabhängig sind. Alle \( \vec{v} \in \R^2 \) lassen sich dann als \emph{Linearkombination} der Basisvektoren darstellen (mit geeigneten \( a_1, a_2 \in \R \)):
				\begin{equation*}
					\vec{v} = a_1 \vec{e}_1 + a_2 \vec{e}_2
				\end{equation*}
			% end
		% end

		\subsection{Krummlinige Koordinatensysteme}
			Gerade in physikalischen Anwendungen kann es von Vorteil sein, keine kartesischen Koordinaten (mit \(x\)- und \(y\)-Wert) zu nutzen, sondern auf \emph{krummlinige Koordinaten} umzusteigen. Ein typisches krummliniges Koordinatensystem sind \zB Polarkoordinaten. Dabei wird ein Punkt in der Ebene durch den Abstand \(r\) vom Ursprung und durch den Winkel \(\varphi\) mit der \(x\)-Achse beschrieben. Die Koordinaten lassen sich durch
			\begin{align*}
				x(r, \varphi) & = r \cdot \cos(\varphi) \\
				y(r, \varphi) & = r \cdot \sin(\varphi)
			\end{align*}
			in kartesische Koordinaten umrechnen.

			Weitere krummlinige Koordinatensysteme sind \zB Kugel- oder Zylinderkoordinaten.
		% end

		\subsection{Andere Räume}
			Es ist auch möglich, dass die Elemente eines Vektorraums Funktionen sind (Funktionenräume). Auch kann ein Raum unendlich-dimensional sein.

			Die \emph{Fourier-Theorie} beschäftigt sich mit der Frage, ob es möglich ist, Basisfunktionen zu finden, mit denen sich beliebige Funktionen \bzgl dieser Basen darstellen lassen.
		% end

		\subsection{Komplexe Zahlen} % 4.20, 4.48
			Komplexe Zahlen haben zwei Komponenten: Einen Real- und einen Imaginärteil. Dabei können sie als kartesische Koordinaten in einer zwei-dimensionalen Ebene (der komplexen Ebene) aufgefasst werden und entsprechen dargestellt werden (mit der imaginären Zahl \( i \) mit der Eigenschaft \( i^2 = -1 \)):
			\begin{equation*}
				z = a + bi
			\end{equation*}
			Oder als Polarkoordinaten (in einer zwei-dimensionalen Ebene) mit der Darstellung
			\begin{equation*}
				z = r e^{\varphi i}
			\end{equation*}
			wobei sich kartesische und Polardarstellung wie bei Polarkoordinaten ineinander umrechnen lassen.

			Die Äquivalenz der beiden Darstellung geht auf die Euler-Identität
			\begin{equation*}
				e^{i \varphi} = \cos(\varphi) + i \sin(\varphi)
			\end{equation*}
			zurück, wobei hier \( r = 1 \) gilt. Aus aus dieser folgt (für \( \lvert z \rvert = 1 \)) ebenfalls:
			\begin{align}
				a = \cos(\varphi) & = \frac{1}{2} \big( e^{i \varphi} + e^{-i \varphi} \big)  \\
				b = \sin(\varphi) & = \frac{1}{2i} \big( e^{i \varphi} - e^{-i \varphi} \big)
			\end{align}
		% end

		\subsection{Gerade/Ungerade Funktionen}
			Für eine gerade Funktion gilt
			\begin{equation*}
				f(x) = f(-x)
			\end{equation*}
			für eine ungerade Funktion gilt
			\begin{equation*}
				f(x) = -f(-x)
			\end{equation*}
			für jeweils alle \(x\).
		% end
	% end

	\section{Fourier-Reihe}
		\subsection{Dirichlet-Bedingungen}
			Jede Funktion, die die \emph{Dirichlet-Bedingungen} erfüllt:
			\begin{enumerate}
				\item Die Anzahl Unstetigkeiten innerhalb einer Periode ist endlich.
				\item Die Anzahl Maxima und Minima innerhalb einer Periode ist endlich.
				\item Die Funktion ist in jeder Periode integrierbar (\dh die Fläche unter dem Betrag der Funktion ist endlich).
			\end{enumerate}
			Kann durch eine Summe von Kosinus- und Sinusfunktionen dargestellt werden.
		% end

		\subsection{\(2\pi\)-periodische Funktion}
			Ist \( f(x) \) eine periodische Funktion mit der Periodenlänge \( 2\pi \) (\dh die wiederholt sich alle \(2\pi\)), die die Dirichlet-Bedingungen erfüllt, so gilt
			\begin{equation*}
				f(x) = \sum_{n = 0}^{\infty} \big( a_n \cos(nx) + b_n \sin(nx) \big)
			\end{equation*}
			mit geeigneten \emph{Fourier-Koeffizienten} \( a_n \) und \( b_n \).
		% end

		\subsection{Skalarprodukt, Orthogonale Basis}
			Sei \(H\) der Raum aller \(2\pi\)-periodischen reellen Funktionen, die die Dirichlet-Bedingungen erfüllen. Dann wird durch
			\begin{equation*}
				\langle f, g \rangle \coloneqq \int_{-\pi}^{\pi} \! f(\tau) g(\tau) \dd{\tau}
			\end{equation*}
			ein Skalarprodukt definiert.

			Die Funktionen
			\begin{align*}
				u_n(x) & = \cos(nx) \\
				v_n(x) & = \sin(nx)
			\end{align*}
			bilden dann eine orthogonale Funktionenfolge in \(H\):
			\begin{align*}
				\langle u_n, u_m \rangle                            & =
				\begin{cases}
					0    & m \neq n  \\
					2\pi & m = n = 0 \\
					\pi  & m = n > 0
				\end{cases}                                          \\
				\langle v_n, v_m \rangle                            & =
				\begin{cases}
					0   & m \neq n  \\
					0   & m = n = 0 \\
					\pi & m = n > 0
				\end{cases}                                           \\
				\langle u_n, v_m \rangle = \langle v_m, u_n \rangle & = 0
			\end{align*}

			Durch diese Darstellung kann die allgemeine Fourier-Reihe mit \( u_n = u_n(x) \) und \( v_n = v_n(x) \) auch geschrieben werden als:
			\begin{equation*}
				f(x) = \sum_{n = 0}^{\infty} \big( a_n u_n + b_n v_n \big)
			\end{equation*}
		% end

		\subsection{Berechnung der Koeffizienten \(a_m\), \(b_m\)}
			Um die Koeffizienten \( a_m \), \( m = 1, 2, \cdots \) zu bestimmen, wird das Skalarprodukt zwischen \(f\) und \( u_m \) gebildet:
			\begin{equation*}
				\langle f, u_m \rangle
				= \Bigg\langle \sum_{n = 0}^{\infty} \big( a_n u_n + b_n v_n \big),\, u_m \Bigg\rangle
				= \Big\langle \big( a_m u_m + b_m v_m \big),\, u_m \Big\rangle
				= \langle a_m u_m,\, u_m \rangle
				= a_m \langle u_m, u_m \rangle
				= a_m \pi
			\end{equation*}
			Umstellen nach \( a_m \) liefert die Werte der Fourier-Koeffizienten:
			\begin{equation*}
				a_m = \frac{1}{\pi} \langle f, u_m \rangle = \frac{1}{\pi} \int_{-\pi}^{\pi} \! f(x) \cos(mx) \dd{x}
			\end{equation*}

			Analog für \( a_0 \) mit \( u_0 \):
			\begin{equation*}
				\langle f, u_0 \rangle
				= \Bigg\langle \sum_{n = 0}^{\infty} \big( a_n u_n + b_n v_n \big),\, u_0 \Bigg\rangle
				= \Big\langle \big( a_0 u_0 + b_0 v_0 \big),\, u_0 \Big\rangle
				= \langle a_0 u_0,\, u_0 \rangle
				= a_0 \langle u_0, u_0 \rangle
				= a_0 2\pi
			\end{equation*}
			Umstellen nach \( a_0 \):
			\begin{equation*}
				a_0 = \frac{1}{2\pi} \langle f, u_0 \rangle = \frac{1}{2\pi} \int_{-\pi}^{\pi} \! f(x) \cos(0x) \dd{x} = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(x) \dd{x}
			\end{equation*}

			Analog für \( b_m \), \( m = 1, 2, \cdots \):
			\begin{equation*}
				\langle f, v_m \rangle
				= \Bigg\langle \sum_{n = 0}^{\infty} \big( a_n u_n + b_n v_n \big),\, v_m \Bigg\rangle
				= \Big\langle \big( a_m u_m + b_m v_m \big),\, v_m \Big\rangle
				= \langle b_m v_m, v_m \rangle
				= b_m \langle v_m, v_m \rangle
				= b_m \pi
			\end{equation*}
			Umstellen nach \( b_m \):
			\begin{equation*}
				b_m = \frac{1}{\pi} \langle f, v_m \rangle = \frac{1}{\pi} \int_{-\pi}^{\pi} \! f(x) \sin(mx) \dd{x}
			\end{equation*}
			Da \( \sin(0x) = \sin(0) = 0 \) ist, muss \( b_0 \) nicht berechnet werden.
		% end

		\subsection{Beispiel: Rechteck-Schwingung}
			Sei eine Rechteck-Schwingung
			\begin{equation*}
				f(x) =
				\begin{cases}
					-k & -\pi < x < 0 \\
					k  & 0 < x < \pi
				\end{cases},\quad f(x) = f(x + 2\pi)
			\end{equation*}
			gegeben. Für diese lauten die Fourier-Koeffizienten:
			\begin{align*}
				a_0 & = 0                                       \\
				a_n & = 0                                       \\
				b_n & = \frac{4k}{n\pi} \text{ für ungerade } n
			\end{align*}
			Daraus ergibt sich die Fourier-Reihe:
			\begin{equation*}
				f(x) = \frac{4k}{\pi} \sum_{n = 0}^{\infty} \frac{1}{2k + 1} \sin\big((2k + 1) x\big)
			\end{equation*}

			Die Rechteck-Schwingung ist dabei eine ungerade Funktion. Allgemein gilt:
			\begin{itemize}
				\item Für gerade Funktionen sind alle \( b_n = 0 \).
				\item Für ungerade Funktionen sind all \( a_n = 0 \).
			\end{itemize}
		% end
	% end

	\section{Fourier-Transformation}
		Mit der Fourier-Transformation wird versucht, eine ähnliche Darstellung wie die Fourier-Reihe für Funktionen zu finden, die nicht \(2\pi\)-periodisch sind.

		Durch die Euler-Identität kann die allgemeine Fourier-Reihe umgeformt werden:
		\begin{align*}
			f(x)
			 & = a_0 + \sum_{n = 1}^{\infty} \big( a_n \cos(nx) + b_n \sin(nx) \big)                                                   \\
			 & = a_0 + \sum_{n = 1}^{\infty} \Bigg( a_n \frac{e^{inx} + e^{-inx}}{2} + b_n \frac{e^{inx} - e^{-inx}}{2i} \Bigg)        \\
			 & = a_0 + \sum_{n = 1}^{\infty} \Bigg( a_n \frac{e^{inx} + e^{-inx}}{2} - b_n i \frac{e^{inx} - e^{-inx}}{2} \Bigg)       \\
			 & = a_0 + \sum_{n = 1}^{\infty} \Bigg( \frac{a_n - ib_n}{2} e^{inx} + \frac{a_n + ib_n}{2} e^{-inx} \Bigg)                \\
			 & = a_0 + \sum_{n = 1}^{\infty} \frac{a_n - ib_n}{2} e^{inx} + \sum_{n = 1}^{\infty} \frac{a_n + ib_n}{2} e^{-inx}        \\
			 & = a_0 + \sum_{n = 1}^{\infty} \frac{a_n - ib_n}{2} e^{inx} + \sum_{n = -\infty}^{-1} \frac{a_{-n} + ib_{-n}}{2} e^{inx} \\
			 & = c_0 + \sum_{n = 1}^{\infty} c_n e^{inx} + \sum_{n = -\infty}^{-1} c_n e^{inx}                                         \\
			 & = \sum_{n = -\infty}^{\infty} c_n e^{inx}
		\end{align*}
		woraus sich eine äquivalente Formulierung der Fourier-Reihe mit den komplexen Koeffizienten
		\begin{equation*}
			c_n = \frac{a_n - ib_n}{2} e^{inx}, \quad n = 1, 2, \cdots \quad\quad\quad\quad c_n = \frac{a_{-n} + ib_{-n}}{2}, \quad n = -1, -2, \cdots \quad\quad\quad\quad c_0 = a_0
		\end{equation*}
		ergibt. Nun werden zunächst Funktionen \( f_L(x) \) mit einer beliebigen Periode \( 2L \) betrachtet:
		\begin{align*}
			f_L(x) & = \sum_{n = -\infty}^{\infty} c_n e^{in \frac{2\pi}{2L} x}                                                                                    \\
			       & = \sum_{n = -\infty}^{\infty} c_n e^{in \frac{\pi}{L} x}                                                                                      \\
			\intertext{Einsetzen der Koeffizienten \(c_n\):}
			       & = \sum_{n = -\infty}^{\infty} \Bigg( \frac{1}{2L} \int_{-L}^{L} \! f(\tau) e^{-in \frac{\pi}{L} \tau} \dd{\tau} \Bigg) e^{in \frac{\pi}{L} x}
		\end{align*}
		Nun wird der Übergang \( L \to \infty \), \dh zu nicht-periodischen Funktionen, betrachtet:
		\begin{align*}
			\lim\limits_{L \to \infty} f(x)
			 & = \lim\limits_{L \to \infty} \sum_{n = -\infty}^{\infty} \Bigg( \frac{1}{2L} \int_{-L}^{L} \! f(\tau) e^{-in \frac{\pi}{L} \tau} \dd{\tau} \Bigg) e^{in \frac{\pi}{L} x} \\
			 & = \lim\limits_{L \to \infty} \sum_{n = -\infty}^{\infty} \frac{1}{2L} \int_{-L}^{L} \! f(\tau) e^{-in \frac{\pi}{L} (\tau - x)} \dd{\tau}                                \\
			 & = \lim\limits_{L \to \infty} \sum_{n = -\infty}^{\infty} \int_{-L}^{L} \! \frac{1}{2L} f(\tau) e^{-in \frac{\pi}{L} (\tau - x)} \dd{\tau}                                \\
			 & = \lim\limits_{L \to \infty} \int_{-L}^{L} \sum_{n = -\infty}^{\infty} \frac{1}{2L} f(\tau) e^{-in \frac{2\pi}{2L} (\tau - x)} \dd{\tau}                                 \\
			 & = \lim\limits_{L \to \infty} \int_{-L}^{L} \! f(\tau) \int_{-\infty}^{\infty} \! e^{-2\pi i u (\tau - x)} \dd{u} \dd{\tau}                                               \\
			 & = \int_{-\infty}^{\infty} \! f(\tau) \int_{-\infty}^{\infty} \! e^{-2\pi i u (\tau - x)} \dd{u} \dd{\tau}
		\end{align*}
		Dieser Übergang lässt sich als "Superposition" auffassen mit:
		\begin{align*}
			f(x) & = \int_{-\infty}^{\infty} \! F(u) e^{ 2\pi i u x} \dd{u} \\
			F(u) & = \int_{-\infty}^{\infty} \! f(x) e^{-2\pi i u x} \dd{x}
		\end{align*}

		Dabei heißt der Übergang \( f(x) \to F(u) \) \emph{Fourier-Transformation} und der Übergang \( F(u) \to f(x) \) \emph{Inverse Fourier-Transformation}. Dabei ist \(F(u)\) oft komplex und \(f(x)\) ist reell.

		\subsection{Beispiel: Rechteckimpuls}
			Für einen Rechteckimpuls
			\begin{equation*}
				f(x) =
				\begin{cases}
					1 & -1 < x < 1   \\
					0 & \text{sonst}
				\end{cases}
			\end{equation*}
			ergibt sich die Fourier-Transformation
			\begin{equation*}
				F(u) = \int_{-1}^{1} \! e^{-2\pi i u \tau} \dd{\tau} = \frac{1}{2\pi i u} \big[ e^{-2\pi i u \tau} \big]_{-1}^{1} = \frac{1}{\pi u} \cdot \frac{e^{2\pi i u} - e^{-2\pi i u}}{2i} = 2 \frac{\sin(2\pi u)}{2\pi u} = 2 \sinc(2\pi u)
			\end{equation*}
			wie erwartet ein Vielfaches der \( \sinc \)-Funktion.
		% end

		\subsection{Transformationspaare}
			Die Fourier-Transformation zerlegt eine Funktion in ihre Frequenzbestandteile! Beispielhafte Fourier-Transformationspaare sind:
			\begin{itemize}
				\item \( \cos(0) = 1 \): \tabto{2.5cm} Delta-Funktion bei \( u = 0 \)
				\item \( \cos(kx) \):    \tabto{2.5cm} Delta-Funktion bei \( u = \pm k \)
				\item \( \sin(kx) \):    \tabto{2.5cm} Delta-Funktion bei \( u = \pm ik \)
			\end{itemize}
		% end

		\subsection{2D-Fourier-Transformation}
			Für eine zweidimensionale Funktion \( f(x, y) \) lautet die Fourier-Transformation:
			\begin{align*}
				f(u, v) & = \iint_{-\infty}^{\infty} \! f(x, y) e^{ 2\pi i (xu + vy)} \dd{u} \dd{v} \\
				F(u, v) & = \iint_{-\infty}^{\infty} \! f(x, y) e^{-2\pi i (ux + vy)} \dd{x} \dd{y}
			\end{align*}
			Die Fourier-Transformierte ist entspricht also zweidimensionalen Funktionen (Real- und Imaginärteil), die als Graustufenbilder visualisiert werden können. Meistens wird dabei aber nur das sogenannte Amplituden-Spektrum betrachtet, welches die Amplituden der Fourier-Transformation visualisiert. Dabei entspricht der Pixelwert an der Stelle \( (u, v) \) der Amplitude, \dh dem Betrag, der Frequenzen \( \big\lvert F(u, v) \big\rvert \).
		% end
	% end

	\section{Faltung}
		Werden zwei Funktionen \( F(u) \), \( G(u) \) im Frequenzraum multipliziert:
		\begin{align*}
			F(u) \cdot G(u)
			 & = \int_{-\infty}^{\infty} \! f(\tau) e^{-2\pi i u \tau} \dd{\tau} \cdot \int_{-\infty}^{\infty} \! g(t) e^{-2\pi i u t} \dd{t}             \\
			 & = \int_{-\infty}^{\infty} \! f(\tau) e^{-2\pi i u \tau} \int_{-\infty}^{\infty} \! g(t - \tau) e^{-2\pi i u (t - \tau)} \dd{\tau} \dd{t}   \\
			 & = \int_{-\infty}^{\infty} \! e^{-2\pi i u t} \underbrace{\int_{-\infty}^{\infty} \! f(\tau) g(t - \tau) \dd{\tau}}_{h(t) \coloneqq} \dd{t} \\
			 & = \int_{-\infty}^{\infty} \! h(t) e^{-2\pi i u t} \dd{t}                                                                                   \\
			 & = H(t)
		\end{align*}
		Das Integral \( h(t) = \int_{-\infty}^{\infty} \! h(t) e^{-2\pi i u t} \dd{t} \eqqcolon f(t) \ast g(t) \) ist das sogenannte \emph{Faltungsintegral} der Funktionen \(f\) und \(g\). Eine Faltung im Ortsraum entspricht somit einer Multiplikation im Frequenzraum!

		Eine Faltung \( f(t) \ast g(t) \) kann als Mittelwertbildung der Werte von \(f\) mit Gewichten \(g\) verstanden werden. So kann \bspw analytisch ein gleitender Durchschnitt (mit einer Kastenfunktion \(g\)) erstellt werden.

		\subsection{Anwendung: Filter} % 4.68
			\todo{Fourier: Anwendung Filter}
		% end
	% end

	\section{Abtastung}
		Ist eine kontinuierliche Funktion, \bzw ein analoges Signal, gegeben, so muss dieses für eine diskrete Repräsentation \emph{abgetastet} werden, \dh es müssen Messungen an einzelnen Stellen durchgeführt werden. Eine solche diskrete Abtastung kann durch die Funktion
		\begin{equation*}
			\hat{f}(x) = f(x) \cdot \sum_{n = -\infty}^{\infty} \delta(x - n \cdot \Delta x)
		\end{equation*}
		\dh als Produkt einer Funktion \( f(x) \) und einer Kamm-Funktion beschrieben werden. Die Fourier-Transformierte \( \hat{F}(u) \) der abgetasteten Funktion entspricht dann der Fourier-Transformierten \( F(u) \) der nicht abgetasteten Funktion, wird aber periodisch mit der Periode \( 1/\Delta x \) wiederholt und mit \( 1/\Delta x \) skaliert.

		\subsection{Abtasttheorie}
			Sei die Funktion \( f(x) \) bandbegrenzt durch eine Maximalfrequenz \( u_G \), \dh \( F(u) = 0 \) für \( \lvert u \rvert > u_G \).

			Gilt nun \( 2u_G < 1 / \Delta x \), so überlappen sich die Fouriertransformierten nicht, \dh die Spektren von \( F(u) \) und \( \hat{F}(u) \) stimmen auf dem Intervall \( [-u_G, u_G] \) (bis auf die Skalierung \( 1 / \Delta x \)) überein. Das Frequenzspektrum von \( F(u) \) kann somit vollständig aus dem Abtastsignal und den Abtastwerten berechnet werden.

			Gilt nun \( 2u_G > 1 / \Delta x \), so überlappen sich die Fouriertransformierten und in den Überschneidungsbereichen bilden sich Summen. Damit ist es unmöglich, das originale Frequenzspektrum von \( F(u) \) zu bestimmen (\emph{Aliasing}).
		% end

		\subsection{Abtasttheorem von Whittaker-Shannon}
			Aus den vorherigen Überlegungen ergibt sich das \emph{Abtasttheorem von Whittaker-Shannon}: Existiert für eine Funktion \( f(x) \) eine Grenzfrequenz \( u_G < \infty \), sodass \( F(u) = 0 \) für \( \lvert u \rvert > u_G \) gilt, dann ist \( f(x) \) fehlerfrei rekonstruierbar, sofern die Abtastfrequenz \( 1 / \Delta x \) mindestens doppelt so hoch wie \( u_G \) ist:
			\begin{equation*}
				\frac{1}{\Delta x} > 2 u_G
			\end{equation*}
		% end
	% end
% end

\chapter{Bilder}
\section{Bildverbesserung}
	Bei der Bildverbesserung wird versucht, die Bildinformationen so aufzubereiten, dass die für den Betrachter verbessert sind/wirken. Dafür gibt es (leider) keine allgemeine Theorie, sondern die möglichen Verbesserungen sind sehr Anwendungsspezifisch und abhängig von Bild und Betrachter. Typische Anwendungen sind dabei der Ausgleich von nicht-Linearitäten der Kamera, Anpassung von Helligkeit und Kontrast und Hervorhebung von Bildbereichen.

	Es wird unterschieden zwischen Methoden im Ortsraum (die direkt Pixelwerte manipulieren) und Methoden im Frequenzraum, bei denen das Bild zunächst durch eine Fourier-Transformation in seine Frequenzen zerlegt, manipuliert und rücktransformiert wird.

	\subsection{Histogramm}
		Das Histogramm eines Bildes ist die graphische Darstellung der Häufigkeitsverteilung von bestimmten Merkmalen (\zB von bestimmten Grauwerten). Histogramme von Bildern können viele Aussagen treffen, \zB über:
		\begin{itemize}
			\item Dynamik (Bereich reeller Lichtintensitäten, er auf der Grauwertskala abgebildet wird)
			\item Kontrast (Bereich der Grauwertskala, der zur Darstellung ausgenutzt wird)
			\item Helligkeit (Beleuchtungsstärke (der Grauwert))
		\end{itemize}
		Dabei entspricht die Helligkeit eines Grauwertbildes dem Mittelwert aller Grauwerte und der Bildkontrast der Varianz aller Grauwerte.
	% end

	\subsection{Pixeloperationen}
		Bei Pixeloperationen wird ein Pixel unabhängig von seiner Nachbarschaft modifiziert. Beispiele für solche Operationen sind:
		\begin{itemize}
			\item Negativ
			\item Binärisierung/Thresholding
			\item Fensterung
			\item Kontrastspreizung
			\item Dynamikkompression
			\item Gammakorrektur (Bildschirm)
			\item Helligkeit
			\item Histogrammausgleich
			\item Differenz
			\item Mittelung
		\end{itemize}

		\subsubsection{Bildnegativ}
			Bei einem Bildnegativ wird der Wert eines Pixels von der Maximal möglichen Intensität abgezogen und dieser Wert als neuer Pixelwert verwendet:
			\begin{equation*}
				g[m, n] = f_\text{max} - f[m, n]
			\end{equation*}
		% end

		\subsubsection{Binärisierung/Thresholding}
			Bei der Binärisierung wird ein Schwellwert \(\tau\) sowie zwei Werte \(f_\text{max}\) und \(f_\text{min}\) festgelegt und das Bild wie folgt manipuliert:
			\begin{equation*}
				g[m, n] =
				\begin{cases}
					f_\text{max} & f[m, n] > \tau    \\
					f_\text{min} & f[m, n] \leq \tau
				\end{cases}
			\end{equation*}
			Für den Spezialfall der Binärisierung gilt \( f_\text{max} = 1 \) und \( g_\text{min} = 0 \).
		% end

		\subsubsection{Grauwertfensterung}
			Bei der Grauwertfensterung wird ein bestimmtes Intensitätsintervall hervorgehoben (gespreizt) und alles andere wird auf einen fixen Wert gesetzt.
		% end

		\subsection{Kontrastspreizung}
			Bei der Kontrastspreizung wird der Grauwert auf eine neue Grauwertskala anhand einer einwertigen oder monotonen Funktion abgebildet.
		% end

		\subsection{Histogrammausgleich}
			Bei einem Histogrammausgleich wird die Grauwertskala anhand der Kurve der Summenwahrscheinlichkeiten, \dh anhand der kumulierten Wahrscheinlichkeiten bis zu einem bestimmten Wert, transformiert:
			\begin{equation*}
				p(g) = \max(\text{Intensität}) \cdot \sum_{i = 0}^{g} p(i)
			\end{equation*}
			Ein solcher Histogrammausgleich ist verlustbehaftet und nicht umkehrbar!
		% end

		\subsection{Mittelung}
			Ein unkorreliertes Rauschen im Bild kann durch Mittelung über \(k\) Aufnahmen des gleichen Motivs unterdrückt werden:
			\begin{equation*}
				g[m, n] = \frac{1}{k} \sum_{i = 0}^{k - 1} f_i[m, n]
			\end{equation*}
		% end
	% end
% end

\section{Bildfilterung}
Zur Bildfilterung gibt es zwei grundlegende Vorgehensweise:
\begin{itemize}
	\item Filterung im Ortsraum durch direkte Manipulation der Pixel und
	\item Filterung im Frequenzraum durch vorherige Fourier-Transformation und Rücktransformation.
\end{itemize}

\subsection{Ortsraum}
Filter im Ortsraum werden durch \emph{Filtermasken} beschrieben. Dabei wird ein Pixel in Abhängigkeit von seiner Nachbarschaft modifiziert. Eine Filtermaske wird durch eine \( k \times l \)-Matrix (mit \(k\), \(l\) ungerade) beschrieben, die die Gewichtung der umliegenden Pixel beschreibt. Dies entspricht einer linearen Filterung (Faltung) im Ortsraum:
\begin{equation*}
	(f \ast w)[m, n] = \sum_{i = -\lfloor k/2 \rfloor}^{\lfloor k/2 \rfloor} \sum_{j = -\lfloor l/2 \rfloor}^{\lfloor l/2 \rfloor} w_{ij} f[m + i, n + j]
\end{equation*}
Dabei entspricht \( w \in \R^{k \times l} \) der Filtermaske.

\subsubsection{Tiefpass-Filter}
	\begin{itemize}
		\item Die Koeffizienten (\dh die Einträge der Filtermaske) sind allesamt positiv und normalisiert, sodass die Summe \num{1} ergibt.
		\item Dadurch werden nur positive Werte produziert.
		\item Es kommt zu Randeffekten, da am Rand die Nachbarn eines Pixels nicht definiert sind.
		\item Typische Vertreter dieser Kategorie sind Mittelwert- und Gauß-Filter.
	\end{itemize}

	\paragraph{Mittelwert-Filter}
		\( 3 \times 3 \)-Mittelwertfilter ("Boxfilter"):
		\begin{equation*}
			\frac{1}{9}
			\begin{bmatrix}
				1 & 1 & 1 \\
				1 & 1 & 1 \\
				1 & 1 & 1
			\end{bmatrix}
		\end{equation*}
		\indent \( 5 \times 5 \)-Mittelwertfilter:
		\begin{equation*}
			\frac{1}{25}
			\begin{bmatrix}
				1 & 1 & 1 & 1 & 1 \\
				1 & 1 & 1 & 1 & 1 \\
				1 & 1 & 1 & 1 & 1 \\
				1 & 1 & 1 & 1 & 1 \\
				1 & 1 & 1 & 1 & 1
			\end{bmatrix}
		\end{equation*}

		Je größer der Filter gewählt wird, desto mehr wird das Bild "verwischt".
	% end

	\paragraph{Gauß-Filter}
		Bei einem Gauß-Filter werden die umliegenden Pixel durch eine diskrete Approximation der Funktion
		\begin{equation*}
			G(x, y) = \frac{1}{2\pi\sigma^2} \exp \Bigg\{ -\frac{x^2 + y^2}{2\sigma^2} \Bigg\}
		\end{equation*}
		gewichtet. Für \( 3 \times 3 \) ("Binomialfilter") und \( 5 \times 5 \) (\( \sigma = 1 \)) ergeben such folgende Approximationen:
		\begin{equation*}
			\frac{1}{16}
			\begin{bmatrix}
				1 & 2 & 1 \\
				2 & 4 & 2 \\
				1 & 2 & 1
			\end{bmatrix}
			\quad\quad\quad\quad
			\frac{1}{273}
			\begin{bmatrix}
				1 & 4  & 7  & 4  & 1 \\
				4 & 16 & 26 & 16 & 4 \\
				7 & 26 & 41 & 26 & 7 \\
				4 & 16 & 26 & 16 & 4 \\
				1 & 4  & 7  & 4  & 1
			\end{bmatrix}
		\end{equation*}
	% end

	\paragraph{Median-Filter}
		Der Median-Filter ist ein nichtlinearer Filter und kann daher nicht durch eine Faltung ausgedrückt werden. Bei ihm wird jeder Pixel durch den Medianwert seiner Nachbarschaft ersetzt. Dadurch werden keine Grautöne interpoliert, isolierte Punkte und Rauschen wird minimiert und dir Schärfe der Kanten bleibt erhalten. Jedoch ist dieser Filter aufgrund der Sortierung sehr rechenintensiv.
	% end
% end

\subsubsection{Hochpass-Filter}
\begin{itemize}
	\item Die Koeffizienten können sowohl negativ und positiv sein und sind normalisiert, sodass die Summe \num{0} ergibt.
	\item Dadurch werden positive und negative Werte produziert.
	\item Typischer Vertreter dieser Kategorie sind Ableitungen und Differenzfilter.
\end{itemize}

\paragraph{Diskretisierung von Ableitungen}
	Eine Ableitung \( \partial f / \partial x \) einer Funktion kann durch Rückwärtsdifferenzen approximiert werden:
			\begin{equation*}
				\frac{\partial f}{\partial x}(x) = \lim\limits_{h \to 0} \frac{f(x) - f(x - h)}{h} \overset{\,h \,=\, 1\,}{\approx} \frac{f(x) - f(x - 1)}{1} = f(x) - f(x - 1)
			\end{equation*}
			Dies gilt ebenfalls für die zweite Ableitung \( \partial^2 f / \partial x^2 \) (diesmal durch Vorwärtsdifferenzen):
					\begin{align*}
						\frac{\partial^2 f}{\partial x^2}(x)
						=       & \, \lim\limits_{h \to 0} \frac{\frac{\partial f}{\partial x}(x + h) - \frac{\partial f}{\partial x}(x)}{h} \\
						\approx & \, \lim\limits_{h \to 0} \frac{f(x + h) - f(x + h - 1) - f(x) + f(x - 1)}{h}                               \\
						\approx & \, \frac{f(x + 1) - f(x) - f(x) + f(x - 1)}{1}                                                             \\
						=       & \, f(x + 1) - 2f(x) + f(x - 1)
					\end{align*}
				% end

				\paragraph{Laplacian-Filter}
					Durch den Laplacian-Filter wird der Laplace-Operator
					\begin{equation*}
						\Delta f(x, y) = \nabla^2 f(x, y) = \frac{\partial^2 f(x, y)}{\partial x^2} + \frac{\partial^2 f(x, y)}{\partial y^2}
					\end{equation*}
					approximiert. Für \( 3 \times 3 \) ergibt sich \bspw die folgende diskrete Approximation:
					\begin{equation*}
						\begin{bmatrix}
							0 & 1  & 0 \\
							1 & -4 & 1 \\
							0 & 1  & 0
						\end{bmatrix}
					\end{equation*}

					Eine alternative Approximation mit Parametern stellt
					\begin{equation*}
						\frac{1}{\beta}
						\begin{bmatrix}
							\alpha     & 1 - \alpha & \alpha     \\
							1 - \alpha & -4         & 1 - \alpha \\
							\alpha     & 1 - \alpha & \alpha
						\end{bmatrix}
					\end{equation*}
					da, wobei
					\begin{equation*}
						\beta =
						\begin{cases}
							4             & 0 \leq \alpha \leq 1 \\
							4(q - \alpha) & -1 \leq \alpha < 0
						\end{cases}
					\end{equation*}
					gilt (der Parameter \( \alpha \) ist frei wählbar).
				% end

				\paragraph{Laplacian of Gaussian Filter}
					Oftmals wird zunächst ein Gauß- und danach ein Laplacian-Filter angewandt. Dieser Filter wird "Laplacian of Gaussian Filter", "Marr-Hildreth-Operator", "Mexican Hat Filter" oder "Sombrerofilter" genannt. Dabei wird die Funktion
					\begin{equation*}
						\Delta G(x, y) = \frac{\partial^2 G(x, y)}{\partial x^2} + \frac{\partial^2 G(x, y)}{\partial y^2},\quad G(x, y) = \frac{1}{2\pi\sigma^2} \exp \Bigg\{ -\frac{x^2 + y^2}{2\sigma^2} \Bigg\}
					\end{equation*}
					approximiert.
				% end
			% end

			\subsubsection{Bilaterale Filter}
				Bei der bilateralen Filterung wird versucht, die Bilder weichzuzeichnen, aber scharfe Kanten zu erhalten. Dabei fließen Pixelfarben aus der Nachbarschaft nicht nur in Abhängigkeit von der Entfernung, sondern auch vom Farbabstand in die Berechnung ein.
			% end
		% end

		\subsection{Frequenzraum}
			Durch eine Multiplikation jeder Frequenz-Komponente \( F(u, v) \) anhand einer Gewichtungsfunktion (Filter), können bestimmte Komponenten erhöht oder verringert werden. Durch eine inverse Fouriertransformation werden die Veränderungen sichtbar. Eine solche selektive Beseitigung von Frequenz-Komponenten heißt \emph{Fourier-Filterung}. Filter werden \bspw eingesetzt, um den Einfluss von Datenfehlern oder Störsignalen zu verringern, hoch-/niederfrequente Signale zu trennen oder bestimmte Frequenzen hervorzuheben.

			Es werden dabei drei grundlegende Filtertypen unterschieden:
			\begin{itemize}
				\item Hochpass-Filter \\ Tiefe Frequenzen \( \lvert \omega \rvert < D_0 \) werden abgeschnitten und es können nur hohe Frequenzen passieren. Dadurch werden scharfe Übergange deutlicher.
				\item Tiefpass-Filter \\ Hohe Frequenzen \( \lvert \omega \rvert > D_0 \) werden abgeschnitten und es können nur niedrige Frequenzen passieren. Dadurch wird Rauschen eliminiert, das Bild aber etwas unschärfer.
				\item Bandpass-Filter \\ Es können nur Frequenzen aus dem Band \( D_0 < \omega < D_1 \) passieren.
			\end{itemize}

			\subsubsection{Idealer Tiefpass-Filter}
				Bei einem idealen Tiefpass-Filter werden alle Frequenzen jenseits einer Grenzfrequenz \( D_0 \) abgeschnitten und der "Kegel" ist radialsymmetrisch zum Ursprung. Der Filter hat dann die Form
				\begin{align*}
					H(u, v) & =
					\begin{cases}
						1 & D \leq D_0 \\
						0 & D > D_0
					\end{cases}               \\
					D(u, v) & = \sqrt{u^2 + v^2}
				\end{align*}
				Dieser Filter ist aber so physikalisch nicht realisierbar (dies liegt an der unendlich langen Impulsantwort, \zB bei einer Rechteck-Funktion)!
			% end

			\subsubsection{Gaußscher Tiefpass-Filter}
				Stattdessen wird ein Gaußscher Tiefpass-Filter eingesetzt. Da die Fourier-Transformation einer Gauß-Glocke wieder eine Gauß-Glocke ist, ist dieser Filter realisierbar.
			% end

			\subsubsection{Idealer Hochpass-Filter}
				Ein idealer Hochpass-Filter schneidet alle Frequenzen unter einer Grenzfrequenz \( D_0 \) ab:
				\begin{align*}
					H(u, v) & =
					\begin{cases}
						0 & D \leq D_0 \\
						1 & D > D_0
					\end{cases}               \\
					D(u, v) & = \sqrt{u^2 + v^2}
				\end{align*}
				Ebenso wie der ideale Tiefpass-Filter ist auch dieser Filter physikalisch nicht realisierbar.
			% end
		% end

		\subsection{Vergleich: Orts- und Frequenzraum-Filter}
			\begin{itemize}
				\item Frequenzraum-Filter können schnell berechnet werden (Fast Fourier-Transform), Ortsraumfilter sind meistens aber noch schneller.
				\item Einfache Handhabung (Das Filterdesign im Frequenzraum ist intuitiv).
				\item Ortsraumfilter sind nur eine Approximation der Frequenzraum-Filter (es sind keine unendlich breiten Filter möglich) und Abschneiden führt zu Artefakten.
			\end{itemize}
		% end
	% end

	\section{Bildkompression}
		Die Rasterung und Abtastung einer Intensitätsfunktion von Licht erzeugt eine große Menge an Daten, was unpraktisch zur Speicherung und Übertragung ist. Es ist somit eine kompaktere Darstellung gewünscht (ohne oder mit zumutbarem Qualitätsverlust).

		Bildkompression versucht dabei die Menge an Daten zur Repräsentation zu reduzieren:
		\begin{itemize}
			\item Eliminierung redundanter Daten
			\item Kodierungen
			\item Nachbarschaftsbeziehungen (räumlich, zeitlich)
			\item Psychovisuelle Effekte (Wahrnehmung des Menschen, Farbauflösung des Auges)
		\end{itemize}
		Die Kompressionsverfahren werden dabei in zwei Klassen eingeteilt:
		\begin{itemize}
			\item Verlustfreie Kompression, z.\,B.:
				\begin{itemize}
					\item Variable-Length-Coding (Huffman Code, Arithmesischer Code)
					\item Bit-Plane Coding (Bit-PLane Slicing, Run-Length Coding)
					\item Predictive Coding
					\item Lempel-Ziv-Welch-Algorithmus (LZW; GIF, TIFF, Kombination von Variable-Length und Run-Length Coding)
				\end{itemize}
			\item Verlustbehaftete Kompression
				\begin{itemize}
					\item Die Bildinformationen werden so komprimiert, dass nicht alle Eigenschaften berücksichtigt werden und eine exakte Rekonstruktion \ggf nicht mehr möglich ist.
					\item Viele Verfahren erlauben dem Anwender das Qualitäts-Kompressions-Verhältnis einzustellen (\zB JPEG oder PNG).
					\item Häufig werden Modelle der menschlichen Wahrnehmung verwenden (zur Identifizierung von für den Betrachter irrelevanten Bildeigenschaften, die nicht kodiert werden müssen).
				\end{itemize}
		\end{itemize}

		Beispiele für Kompressionsverfahren:
		\begin{itemize}
			\item Audio
				\begin{itemize}
					\item Unkomprimiert: AIFF, WAV, \dots
					\item Verlustlos: MPEG-4-ALC, Apple Lossless (ALAC), WMA Lossless, \dots
					\item Verlustbehaftet: MP3, Ogg Vorbis, MPEG-Audio, AAC (iTunes), WMA, \dots
				\end{itemize}
			\item Bilder
				\begin{itemize}
					\item Unkomprimiert: BMP, RAW, \dots
					\item Verlustlos: TIFF, GIF, PNG, \dots
					\item Verlustbehaftet: JPEG, JPEG2000, \dots
				\end{itemize}
			\item Video
				\begin{itemize}
					\item Unkomprimiert: Nicht praktikabel.
					\item Verlustlos: Nicht praktikabel.
					\item Verlustbehaftet: H.264 (DivX, QuickTime), MPEG-4 part 2 (Xvid, DivX), WMV, \dots
				\end{itemize}
		\end{itemize}

		\subsection{Harmonische Transformation}
			Bei einer Kompression durch harmonische Transformation werden die Daten in verschiedene Frequenzanteile zerlegt, \zB durch Fourier-Transformation oder Wavelet-Transformation. Ein typischer Vertreter ist die das JPEG-Kompressionsverfahren.

			\subsubsection{JPEG}
				JPEG ist eine Familie von Algorithmen zur Kompression in Echtfarbqualität (dabei gibt es verlustfreie und verlustbehaftete Verfahren). Die verlustbehafteten Prozesse sind für fotografische Aufnahmen mit fließenden Farbübergängen optimiert und daher nicht so gut für Text oder ähnlichen Bilddaten mit harten Kontrasten geeignet.

				Durch JPEG sind Kompressionsraten bis zu \( 1:20 \) bis \( 1:35 \) erreichbar, wobei diese in den Hauptanwendungsgebieten verlustbehaftet sind. Dabei basiert JPEG auf einer diskreten Kosinustransformation.

				\paragraph{Schritt 1: Umwandlung in den YCbCr-Farbraum}
					Im ersten Schritt werden die Farben als
					\begin{itemize}
						\item \(Y\) Helligkeitswert
						\item \(C_B\) Abweichung von Grau in Richtung Blau
						\item \(C_R\) Abweichung von Grau in Richtung Rot
					\end{itemize}
					kodiert:
					\begin{equation*}
						\begin{bmatrix}
							Y   \\
							C_B \\
							C_R
						\end{bmatrix}
						\approx
						\begin{bmatrix}
							0   \\
							128 \\
							128
						\end{bmatrix}
						+
						\begin{bmatrix}
							0.299     & 0.587     & 0.114     \\
							-0.168736 & -0.331264 & 0.5       \\
							0.5       & -0.418688 & -0.081312
						\end{bmatrix}
						\cdot
						\begin{bmatrix}
							R \\
							G \\
							B
						\end{bmatrix}
					\end{equation*}
				% end

				\paragraph{Schritt 2: Farb-Subsampling}
					Die Farben werden verlustbehaftet Komprimiert (dies ist aufgrund der höheren Genauigkeit des Auges im grünen Bereich möglich). Dabei wird für ein kleines Gebiet (üblicherweise \(2 \times 2\) Pixel) die Farbdifferenzwerte \(C_R\) und \(C_B\) gemittelt und für das gesamte Gebiet zusammen angegeben.
				% end

				\paragraph{Schritt 3: Diskrete Kosinustransformation}
					In diesem Schritt werden die Bildinformationen in den Frequenzbereich zerlegt. Dazu wird zunächst jede Komponente \( (Y, C_B, C_R) \) in \( 8 \cdot 8 = 64 \) Bildblöcke gerastert und diese anschließend einer diskreten Fourier-Transformation unterzogen. Dabei wird nur der Kosinus-Teil berechnet, da dadurch die Berechnung einfacher wird.

					Das Ziel ist, die Informationen in eine Darstellung zu überführen, die besser für die folgenden Schritte geeignet ist.

					Vorteil: Wenn sich benachbarte Bildpunkte kaum unterscheiden, \dh das Bild keine scharfen Kanten hat, sind die meisten Koeffizienten gleich Null.
				% end

				\paragraph{Schritt 4: Quantisierung}
					Bei der Quantisierung werden die Informationsanteile beseitigt, die das Auge nicht oder nur schlecht wahrnimmt.
				% end

				\paragraph{Schritt 5: Kodierung der Koeffizienten}
					Aus den entstehenden Blöcken wird ein sequentieller Bitstrom erzeugt und die Koeffizienten werden als Differenzen zum vorhergehenden Koeffizienten kodiert (durch die Kohärenz ergeben sich hier kleine Werte). Die Koeffizienten werden dabei entlang einer Zick-Zack-Kurve kodiert (ähnliche wie bei Cantors Diagonalargument). Da hohe Frequenzen oft sehr klein sind, entsteht so eine für die Kompression günstige Reihenfolge.

					Bisher wurde noch nichts wirklich komprimiert, sondern nur grob Transformiert. Der entstehende Bitstrom kann nun aber durch typische Kompressionstechniken (Huffman-Algorithmus, Arithmetisches Kodieren) komprimiert werden.
				% end
			% end
		% end
	% end
% end

\chapter{Bildverarbeitung, Deblurring}
	Beim Deblurring wird versucht, eine vorhandene Verwischung (Blurring) eines Bildes zu entfernen.

	Sei bekannt, dass das Bild \(g\) die mit einer Faltung \(a\) verwischte Version (Blurring) eines Bildes \(f\) ist, \dh \( g = a(f) \) (oftmals ist \(a\) eine Gauß-Glocke). Im Fourier-Raum ergibt sich dann \( G = A \cdot F \) und die Rekonstruktion des Bildes \(f\), \bzw der Frequenzen \(F\), scheint mit \( F = A^{-1} \cdot G \) sehr einfach. Bei dieser Rekonstruktion treten jedoch mehrere Probleme auf:
	\begin{enumerate}
		\item Der Blurring-Kernel \(A\) kann unendlich klein werden, sodass es beinah zu einer Division durch Null kommt. Dadurch werden Rauschen und numerische Fehler verstärkt.
		\item Es gibt immer Rauschen \(n\): \( g = a(f) + n \)
	\end{enumerate}
	Für das erste Problem kann verwendet werden, dass \(A\) \iA komplex ist. Es gilt dann mit der komplex konjugierten Matrix \( A^\ast \):
	\begin{equation*}
		G = A \cdot F \quad\implies\quad A^\ast \cdot A \cdot F = \lvert A \rvert^2 \cdot F
	\end{equation*}
	Und mit \( \lvert A \rvert^2 > 0 \) kann die Rekonstruktion umgeformt werden:
	\begin{equation*}
		F = \frac{1}{A} G = \frac{A^\ast}{A^\ast A} G = \frac{A^\ast}{\lvert A \rvert^2} G
	\end{equation*}
	Dadurch hat das rekonstruierte Bild nun keine reellen Zahlen mehr.

	\section{Korrekt gestellte Probleme}
		Nach Jacques Hadamard ist ein mathematisches Modell \emph{korrekt gestellt}, wenn:
		\begin{itemize}
			\item Eine Lösung existiert,
			\item diese eindeutig ist und
			\item die Lösung in einer vernünftigen Topologie kontinuierlich von den Daten abhängt.
		\end{itemize}
		Ansonsten ist das Problem nicht korrekt gestellt.

		Als Konsequenz daraus folgt, dass Blurring korrekt gestellt, Deblurring aber nicht korrekt gestellt ist. Daher ist eine Regularisierung notwendig, \dh es werden zusätzliche Annahmen (Glätte, Informationen zum Rauschen) hinzugenommen.
	% end

	\section{Einschrittverfahren}
		\subsection{Wiener Filter}
			Das zweite Problem von Deblurring (Rauschen) kann durch eine Regularisierung des Filter im Fourierraum:
			\begin{equation*}
				F = \frac{A^\ast}{\lvert A \rvert^2 + R^2} G
			\end{equation*}
			reduziert werden (dies wird als \emph{Wiener Filter} bezeichnet). Dabei ist \(R\) das Verhältnis von Rauschen zum Signal. Der Parameter entscheidet dabei, das verstärkt wird.
			\begin{itemize}
				\item Ist \(R\) zu groß, so verhält sich der Filter wie ein Tiefpass-Filter, d.\,h.:
					\begin{itemize}
						\item Grobe Struktur bleibt erhalten.
						\item Kanten werden verwischt.
						\item Rauschen wird entfernt.
					\end{itemize}
				\item Ist \(R\) zu klein, so verhält sich der Filter wie ein Hochpass-Filter, d.\,h.:
					\begin{itemize}
						\item Grobe Strukturen werden entfernt.
						\item Kanten werden entfernt.
						\item Rauschen wird verstärkt.
					\end{itemize}
				\item Ist \(R\) optimal, so verhält sich der Filter wie ein Bandpass-Filter, d.\,h.:
					\begin{itemize}
						\item Grobe Struktur bleibt erhalten.
						\item Kanten werden verstärkt.
						\item Rauschen wird entfernt.
					\end{itemize}
			\end{itemize}

			\textbf{Vorteile:}
			\begin{itemize}
				\item Schnell
				\item Häufig verwendet
				\item Beliebt (dadurch viel Know-How vorhanden)
				\item Leicht zu implementieren
			\end{itemize}
			\textbf{Nachteile:}
			\begin{itemize}
				\item Nur ein Filter für das gesamte Bild
				\item Keine lokalen, spezifischen Verbesserungen
				\item Nur ein Wert für \(R\)
			\end{itemize}
			Der Wiener-Filter kann \zB durch lokale Verfeinerungen (Mehrkomponentenverfahren) oder iterative Verfeinerungen (Mehrschrittverfahren) verbessert werden.
		% end

		\subsection{Mehrkomponentenverfahren}
			\subsubsection{Scale-Space-Ansatz}
				Zum Schärfen wird der Laplace-Operator, multipliziert mit einer unabhängigen Konstante \(t\), vom Bild abgezogen:
				\begin{equation*}
					L_\text{schärfer} = L_0 - t \underbrace{\big( L_{xx} + L_{yy} \big)}_{= \Delta L} = L_0 - t \cdot \Delta L
				\end{equation*}
				Durch Hinzufügen von zusätzlichen Termen (mit Ableitungen höherer Ordnung) kann das Ergebnis weiter verfeinert werden:
				\begin{equation*}
					L_\text{schärfer} = L_0 - t \big( L_{xx} + L_{yy} \big) + \frac{1}{2} t^2 \big( L_{xxxx} + 2L_{xxyy} + L_{yyyy} \big) - \frac{1}{6} t^3 \big( L_{xxxxxx} + 3L_{xxxxyy} + 3L_{xxyyyy} + L_{yyyyyy} \big)
				\end{equation*}
				Diese Sequenz ist eine Taylorreihe der partiellen Differentialgleichung
				\begin{equation*}
					\frac{\partial L}{\partial t} = \frac{\partial^2 L}{\partial x^2} + \frac{\partial^2 L}{\partial y^2}
				\end{equation*}
				in \(-t\). Die Veränderung in einem Bild über eine gewisse Zeit ist also durch eine partielle Differentialgleichung zweiter Ordnung definiert.
			% end
		% end
	% end

	\section{Mehrschrittverfahren (Iterative Methoden)}
		\subsection{Energie und Variationsableitung}
			Sei \(E\) die \emph{Energie} eines Bildes \(L\):
			\begin{equation*}
				E(L) = \frac{1}{2} \iint_{x, y} \! L^2 \dd{x} \dd{y}
			\end{equation*}
			Diese "Energie" sagt aus, wie viel Intensität in den Pixeln vorhanden ist. Eine Minimierung der Intensität führt dann zum optimalen Bild \bzgl der definierten Energie. Dieses Minimum kann durch Variationsrechnung oder iterative Prozesse gefunden werden.

			Die Variationsableitung \( \delta E(L) \) eine eine Verallgemeinerung der normalen Ableitung, wobei dieses Prinzip hier nicht näher beleuchtet werden soll. Für die Energie
			\begin{equation*}
				E(L) = \frac{1}{2} \iint_{x, y} \! L^2 \dd{x} \dd{y}
			\end{equation*}
			gilt \( \delta E(L) = L \) und das Minimum liegt bei \( \delta E(L) \overset{!}{=} 0 \). In diesem Fall liegt das Minimum also bei \( L = 0 \), \dh \( E(L) = 0 \).

			Für die Energie
			\begin{equation*}
				E(L) = \frac{1}{2} \iint_{x, y} \! L_x^2 + L_y^2 \dd{x} \dd{y}
			\end{equation*}
			folgt \( \delta E(L) = -\big( L_{xx} + L_{yy} \big) = -\Delta L \). Hier ist es weniger trivial das Minimum zu finden, weshalb das System in eine partielle Differentialgleichung
			\begin{equation*}
				L_t = -\delta E(L)
			\end{equation*}
			überführt wird und das Minimum iterativ gesucht wird.
		% end

		\subsection{Alternativen}
			Da mit den bisherigen Energien keine guten interessanten Bilder generiert werden können, gibt es noch andere Energien, z.\,B.:
			\begin{itemize}
				\item Perona-Malik
					\begin{itemize}
						\item Rauschen wird verwischt
						\item Kanten werden verstärkt
						\item Smart Energy Term und Stoppzeit
					\end{itemize}
				\item Totale Variation
					\begin{itemize}
						\item Rauschen wird verwischt
						\item Kanten werden verstärkt
						\item Smart Energy Term und Distance Penalty
					\end{itemize}
				\item uvm.
			\end{itemize}
		% end

		\subsection{Perona-Malik}
			Die Heat-Equation \( L_t = L_{xx} + L_{yy} = \Delta L \) wird modifiziert zu
			\begin{equation*}
				\partial_t L = \nabla \circ \big( c \cdot \nabla L \big)
			\end{equation*}
			wobei \(c\) der \emph{Conductivity Coefficient} ist, durch den die Diffusion an lokale Bildstrukturen anpassbar ist (\dh \( c = c(L, L_x, L_xx, \cdots) \)). Mit \( c = 1 \) fällt die Methode zurück zum gaußschen Scale-Space.

			\subsubsection{Die Perona-Malik-Gleichung}
				Nach Perona und Malik ist \( c(\cdot) \) eine Funktion der Gradientenstärke, welche die Diffusion dort reduziert, wo Kanten sind (\(c\) nahe Null) und in flachen Bereichen erhöht. Dies eingesetzt ergibt:
				\begin{equation*}
					\partial_t L = \nabla \circ \Big( c\big( \lvert \nabla L \rvert^2 \big) \cdot \nabla L \Big)
				\end{equation*}
				Für \(c\) gibt es im Grunde zwei Möglichkeiten, die beide mehr oder weniger das gleiche Verhalten erzeugen:
				\begin{equation*}
					c_1 = \exp \Bigg\{ -\frac{\lvert \nabla L \rvert^2}{k^2} \Bigg\} \quad\quad\quad\quad c_2 = \Bigg( 1 + \frac{\lvert \nabla L \rvert^2}{k^2} \Bigg)^{-1}
				\end{equation*}
				Dabei bestimmt \(k\) den Einfluss der Kantenstärke. Bei einem großen \(k\) bleiben nur die größten Gradienten (starke Kanten) übrig, bei einem kleinen \(k\) bleiben (fast) alle Gradienten (Kanten, Rauschen) übrig.
			% end

			\subsubsection{Implementierung}
				Für die Lösung der Perona-Malik-Gleichung gibt es keine analytischen Methoden, weshalb eine iterative Methode eingesetzt wird:
				\begin{equation*}
					L^{(t + 1)} = L^{(t)} + \Delta t \cdot \bigg( \nabla \circ \Big( c\big(\lvert \nabla L \rvert^2\big) \cdot \nabla L \Big) \bigg)
				\end{equation*}
				dabei ist:
				\begin{itemize}
					\item \( L^{(0)} \) das originale Bild und
					\item \( \Delta t \) ein kleiner Zeitschritt.
				\end{itemize}
				"Irgendwann" muss die Iteration beendet werden, \zB nach \(n\) Schritten oder wenn \( L^{(t + 1)} \) "gut aussieht".
			% end

			\subsubsection{Stoppzeit}
				Während der Iteration steige das Signal-Rausch-Verhältnis \iA an und fällt danach wieder ab (das Bild konvergiert gegen eine gleichmäßig graue Fläche). Das heißt die Iteration stoppt nicht bei der optimalen Lösung und es wird eine \emph{Stoppzeit} benötigt.
			% end
		% end

		\subsection{Eingeschränkte Evolution: Totale Variation}
			Wird sichergestellt, dass die Iterationen gegen die optimale Lösung konvergieren, so ist keine benutzerdefinierte Stoppzeit nötigt. Daher wird versucht eine klug gewählte Energie zu minimieren und eine \emph{Distance Penalty} hinzuzufügen.

			\subsubsection{Distance Penalty}
				Bei einer Distance Penalty wird zusätzlich zum Modell des Bildes ein Rauschmodell angenommen (\zB gaußsches Zufallsrauschen), sodass es zusätzliche Bedingungen an die Lösung \(L\) gibt:
				\begin{align*}
					\iint_{x, y} \! \big( g - a(L) \big) \dd{x} \dd{y}   & = 0        \\
					\iint_{x, y} \! \big( g - a(L) \big)^2 \dd{x} \dd{y} & = \sigma^2
				\end{align*}
				Diese Beschränkungen können zur Energie hinzugefügt werden, um das Konvergenzverhalten zu verbessern.
			% end

			\subsubsection{Totale Variation}
				Bei der Methode der totalen Variation wird die Energie
				\begin{equation*}
					E(L) = \iint_{x, y} \! \Big( \lvert \nabla L \rvert + \lambda\big( g - a(L) \big)^2 \Big) \dd{x} \dd{y}
				\end{equation*}
				minimiert, wobei \(\lambda\) ein vom Rauschen abhängiger Parameter ist. Da die totale Variation gegen die optimale Lösung konvergiert, wird keine Stoppzeit benötigt.
			% end

			\subsubsection{Erweiterungen}
				\begin{itemize}
					\item Schwierigere Funktionen/Energien (Statistik, Niveaumengen für Segmentierung, \dots)
					\item Andere Rauschstatistiken (\zB Multiplikatives Rauschen)
					\item Andere Steuerungsmechanismen für den Conductive Coefficient \(c\) (\zB kantenverstärkende Diffusion, kohärenzverstärkende Diffusion, \dots)
				\end{itemize}
			% end
		% end
	% end
% end

\chapter{Grafikpipeline}
	\section{Hardware}
		Abbildung~\ref{fig:hardwareandparadigms} zeigt einen Überblick über Computer-Paradigmen. Die Überschneidungen sind dabei Bereiche, in denen mehrere Paradigmen zusammenlaufen.

		\begin{figure}
			\centering
			\begin{tikzpicture}[->, every node/.style = { align = center }]
				\node [draw, circle, minimum width = 5cm] (p1) at (0, 1) {P1: Large Scale};
				\node [draw, circle, minimum width = 5cm] (p4) at (0, 4) {P4: Network};
				\node [draw, circle, minimum width = 5cm] (p2) at (-2, 7) {P2: Personal};
				\node [draw, circle, minimum width = 5cm] (p3) at (2, 7) {P3: Mobile};

				\path (p1) -- coordinate(a) (p4);
				\path (p4) -- coordinate(b) (p2);
				\path (p4) -- coordinate(c) (p3);
				\path (p2) -- coordinate(d) (p3);
				\coordinate (e) at (intersection of b--p3 and c--p2);

				\node [left  = 3 of b, yshift = -1cm] (B) {Public-Personal};
				\node [right = 3 of c, yshift = -1cm] (C) {Virtual Reality, \\ Ambient, Invisible};
				\node [above = 5 of e]                (E) {Collaborative, \\ Wearable, Uniquitous};

				\draw (B) -- (b);
				\draw (C) -- (c);
				\draw (E) -- (e);
			\end{tikzpicture}
			\caption{Hardware und Paradigmen}
			\label{fig:hardwareandparadigms}
		\end{figure}

		\subsection{P1: Large-Scale-Computing}
			Die ursprünglichen Mainframe-Computer waren sehr große Rechenmaschinen, die als Hosts angesteuert wurden. Der Zugriff auf diese fand über externe, mit Tastaturen ausgerüstete, alphanumerische, Terminals statt (Host- und Terminal-System).
		% end

		\subsection{P2: Personal/Desktop Computing}
			Computer stehen nun Zuhause und sind sehr viel kleiner.
		% end

		\subsection{P3: Networked Computing}
			Computer kommunizieren untereinander.
		% end

		\subsection{P4: Mobile Computing}
			Es gibt viele verschiedene Arten von mobilen Geräten (Laptops, Tablets, Smartphones, \dots).
		% end

		\subsection{ZP1: Collaborative Computing}
			Zum Beispiel Multi-Touch-Tables.
		% end

		\subsection{ZP2: Virtual Reality}
			Virtuelle Realität lässt den Menschen in virtuelle Welten eintauchen. Dabei gibt es zwei Arten:
			\begin{itemize}
				\item Nicht-immersive Umgebungen (Bildschirm- und Zeigerbasiert, 3D-Anzeige, \mglw haptisches Feedback).
				\item Immersive Umgebungen (es wird tatsächlich der Eindruck erweckt, in einer Welt aus virtuellen Objekten zu sein).
			\end{itemize}
		% end

		\subsection{Augmented Reality}
			Augmented Reality beschreibt die Integration von virtuellen und realen Objekten, wobei die Wahrnehmung des Benutzers erweitert und verbessert werden soll. Die Ein- und Ausgabegeräte sind \zB Heads-Up-Displays (HUDs) oder Head-Mounted-Displays (HMDs).
		% end

		\subsection{Ambient/Invisible}
			Zum Beispiel Freiraum-Gestenerkennung.
		% end

		\subsection{Wearable/Ubiquitous}
			Die Geräte müssen nicht mehr explizit gehalten werden, sondern sie sind "anziehbar", \zB eine Smartwatch.
		% end
	% end

	\section{Computergrafik}
		Abbildung~\ref{fig:computergraphics} zeigt eine stark vereinfachte Pipeline der Computergrafik.

		\begin{figure}
			\centering
			\begin{tikzpicture}[->, every node/.style = { draw, rectangle, align = center }]
				\node (a) {3D-Objekte};
				\node [right = 2 of a] (b) {3D-Modelle \\ (Szene, Geometrie, \\ Material, Beleuchtung) \\ Notwendig zur \\ Rasterisierung};
				\node [right = 2 of b] (c) {Bilder \\ (Interaktion, \\ Animation)};

				\draw (a) -- (b);
				\draw (b) -- (c);
			\end{tikzpicture}
			\caption{Einfache Pipeline der Computergrafik.}
			\label{fig:computergraphics}
		\end{figure}

		\subsection{Uncanny Valley}
			Das \emph{Uncanny Valley} beschreibt eine Beziehung zwischen der Ähnlichkeit eines Objekts zum Menschen und der emotionalen Reaktion darauf. Dabei tritt kurz vor einer absoluten Ähnlichkeit ein Tief auf, welches als Uncanny Valleys bezeichnet wird. In diesem Bereich werden Objekte besonders gruselig wahrgenommen (siehe \zB \href{https://youtu.be/CNdAIPoh8a4}{hier} für eine Demonstration dieses Effekts).
		% end
	% end

	\section{Grafikpipeline}
		Abbildung~\ref{fig:graphicspipeline} zeigt die typische Grafikpipeline von der Anwendung zur Ausgabe. Dabei geschehen folgende Schritte:
		\begin{itemize}
			\item Anwendung \\ Dieser Schritt produziert eine Modellierung der Daten.
			\item Geometrieverarbeitung \\ Dieser Schritt produziert 2D-Koordinaten und zusätzliche Daten (z-Buffer Werte, Farbwerte pro Knoten/Primitiv).
			\item Rasterisierung \\ Dieser Schritt produziert ein Rasterbild (Bildspeicher).
			\item Ausgabe \\ Speichern/Anzeigen des Bilds.
		\end{itemize}

		\begin{figure}
			\centering
			\begin{tikzpicture}[block/.style = { draw, rectangle, minimum height = 1cm, minimum width = 3cm, align = center }]
				\node [block, label = above:CPU] (a) {Anwendung};
				\node [block, right = 1.5 of a] (b) {Geometrie- \\ verarbeitung};
				\node [block, right = 1 of b] (c) {Rasterisierung};
				\node [block, right = 1.5 of c, label = above:{\zB Monitor}] (d) {Ausgabe};

				\coordinate [above left  = 0.5 of b.north west] (A);
				\coordinate [below left  = 0.5 of b.south west] (B);
				\coordinate [below right = 0.5 of c.south east] (C);
				\coordinate [above right = 0.5 of c.north east] (D);
				\path (D) -- coordinate(X) (C);
				\path (A) -- coordinate(Y) (B);

				\path (A) -- node[above]{GPU} (D);

				\draw (A) -| (C) -| cycle;

				\draw [->] (a) -- (Y);
				\draw [->] (b) -- (c);
				\draw [->] (X) -- (d);
			\end{tikzpicture}
			\caption{Typische Grafikpipeline}
			\label{fig:graphicspipeline}
		\end{figure}
	% end

	\section{Anwendung}
		Die Anwendung beschäftigt sich mit der Eingabe von grafischen Daten, sowie deren Repräsentation.

		\subsection{Eingabe grafischer Daten}
			Grafische Daten können \bspw durch die Generierung von 3D-Modellen oder der Abtastung realer Objekte eingegeben werden.
		% end

		\subsection{Repräsentation von 3D-Daten}
			\subsubsection{Grafische Primitive}
				Grafische Primitive, aus denen ein größeres Modell zusammengesetzt werden kann, sind z.\,B.:
				\begin{itemize}
					\item Punkte
					\item Linien
					\item Dreiecke
				\end{itemize}
			% end

			\subsubsection{Transformationen}
				Siehe Kapitel~\ref{c:transformations}.
			% end
		% end

		\subsection{Räumliche Datenstrukturen}
			Räumliche Datenstrukturen werden \bspw zum View-Frustum-Culling (feststellen der Sichtbarkeit eines Objekts im Sichtvolumen), Occlusion Culling/z-Buffer (Verdeckung) oder Kollisionserkennung genutzt. Sie werden in folgende Klassen eingeteilt:
			\begin{itemize}
				\item Hüllkörperhierarchie (\bspw Bounding Sphere Hierarchy)
				\item Raumunterteilung (Gitter, Hierarchisch (k-d Tree, Quadtree, Octree, Binary Space Partition))
			\end{itemize}

			\subsubsection{Hüllkörper/-hierarchien}
				Ein \emph{Hüllkörper} ist eine einfache Form um den eigentlichen Körper herum, sodass sich Schnitttests mit anderen Primitiven einfach durchführen lassen. Typische Primitive sind \zB Kugeln, Rechtecke oder rotierte Rechtecke. Eine \emph{Hüllkörperhierarchie} wird dann aus mehreren solchen Hüllkörpern zusammengesetzt.
			% end

			\subsubsection{Raumunterteilung}
				\paragraph{Achsenparallele Gitter (Grids)}
					Der Raum wird in ein Gitter eingeteilt, wobei Objekte in mehreren Zellen enthalten sein können (Redundanz). Die Aufteilung kann sich demnach nicht der Geometrie anpassen und ist sehr speicherintensiv. Dennoch ist sie sehr effizient traversierbar und es ist ein schneller Zugriff auf Nachbarn möglich.
				% end

				\paragraph{Quadtree/Octree}
					Bei einem \emph{Quadtree} wird der Raum in ein Gitter aufgeteilt und aus diesem ein Baum berechnet (siehe \href{https://fabian.damken.net/summaries/cs/elective/ce/gdr/}{GdR Zusammenfassung}, Abschnitt "Approximative Zellzerlegung").

					Ein \emph{Octree} entspricht dann einem Quadtree im dreidimensionalen.
				% end

				\paragraph{Binary Space Partition}
					Bei einer \emph{Binary Space Partition} (BSP) wird der Raum binär unterteilt, wobei immer an den durch Polygone induzierten Ebenen geteilt wird. Dann entspricht jeder Knoten einer Unterteilungsebene, die den Raum in zwei Halbräume unterteilt.
				% end
			% end
		% end
	% end

	\section{Geometrieverarbeitung}
		\subsection{Simulation der Beleuchtung}
			Um die Beleuchtung eines Objekts zu simulieren, müssen zunächst die Leuchtdichten eines Primitivs bestimmt werden (in der Praxis findet die Bestimmung der Leuchtdichte pro Pixel erst während der Rasterisierung statt). Zur Bestimmung der Beleuchtung gibt es unterschiedliche Ansätze:
			\begin{itemize}
				\item Flat Shading
					\begin{itemize}
						\item Die Normale des Primitivs ergibt eine einheitliche Helligkeit.
					\end{itemize}
				\item Gouraud Shading
					\begin{itemize}
						\item Die Normalen in den Eckpunkten ergeben die Helligkeitswerte für die Eckpunkte.
						\item Die Helligkeitswerte der Eckpunkte werden linear interpoliert.
					\end{itemize}
				\item Phong Shading
					\begin{itemize}
						\item Die Normalen in den Eckpunkten werden für jeden Punkt linear interpoliert und normiert.
						\item Der Helligkeitswert ergibt sich aus der interpolierten Normalen.
					\end{itemize}
			\end{itemize}

			\subsubsection{Phong-Beleuchtungsmodell}
				Im Phong-Beleuchtungsmodell setzt sich die Gesamtbeleuchtung
				\begin{equation*}
					I_\textit{total} = I_\textit{amb} + I_\textit{diff} + I_\textit{spec}
				\end{equation*}
				aus
				\begin{itemize}
					\item Ambienter Reflexion \( I_\textit{amb} \) (global modelliert),
					\item Diffuser Reflexion \( I_\textit{diff} \) (lokal modelliert) und
					\item Spiegelnder Reflexion \( I_\textit{spec} \) (lokal modelliert).
				\end{itemize}
				zusammen. Die ambiente Komponente ist richtungsunabhängig und abhängig von dem Umgebungslicht \(C\) sowie der Materialkonstanten \(k\):
				\begin{equation*}
					I_\textit{amb} = k_\textit{amb} C_\textit{amb}
				\end{equation*}
				Die diffuse Reflexion ist abhängig von der Richtung des Lichts, der Oberflächennormalen \( \vec{N} \) und der Richtung zum Licht \( \vec{L} \):
				\begin{equation*}
					I_\textit{diff} = k_\textit{diff} C_\textit{light} \big( \vec{N} \circ \vec{L} \big)
				\end{equation*}
				Die spiegelnde Reflexion ist abhängig von der Richtung der Reflexion \( \vec{R} \), der Betrachtungsrichtung \( \mat{V} \) und der "Rauheit" \(m\) des Materials:
				\begin{equation*}
					I_\textit{spec} = k_\textit{spec} C_\textit{light} \big( \vec{R} \circ \vec{V} \big)^m
				\end{equation*}
			% end
		% end

		\subsection{Perspektivische Transformation und Clipping (Abschneiden)}
			Liegen mehrere Objekte voreinander, so ist der dem Auge nächste Punkt sichtbar (es sei denn, das Objekt ist transparent, dann wird auch der dahinterliegende Punkt sichtbar, usw.). \emph{Clipping} bezeichnet nun das Abschneiden von Objekten am Rand des gewünschten Bildausschnitts.

			\subsubsection{Painters Algorithmus}
				Bei diesem Algorithmus werden die Primitive "wie von einem Maler" gezeichnet: Es wird mit dem tiefsten z-Wert begonnen und die Objekte mit aufsteigendem z-Wert darüber gezeichnet. So verdecken sich hintereinander liegende Polygone "automatisch". Jedoch sind transparente Objekte sowie "im Kreis überdeckende" Objekte nicht korrekt darstellbar.
			% end
		% end

		\subsection{Culling (Verdeckungsrechnung im Objektraum)}
			Üblicherweise machen die Rückseiten von Objekten \ca die Hälfte der vorkommenden Flächen aus, können aber nicht gesehen werden. Durch \emph{Culling} werden die Rückseiten berechnet und beim Rendering explizit ausgeschlossen, um Rechenleistung zu sparen.

			Eine Fläche ist immer dann eine Rückseite, wenn das Skalarprodukt von Sehstrahl \( \vec{s} \) und Normale \( \vec{n} \) positiv ist: \( \vec{n} \circ \vec{s} > 0 \).
		% end

		\subsection{Projektion}
			Siehe~\ref{sec:projection}.
		% end
	% end

	\section{Rasterisierung}
		Bei der \emph{Rasterisierung} werden die Primitive (Linien, Polygone) in Pixel zerlegt und zusätzlich pro Pixel eine Verdeckungsrechnung und Shading durchgeführt.

		\subsection{Scan-Konvertierung}
			\subsubsection{Rasterisierung von Linien (Bresenham-Algorithmus)}
				Der Bresenham-Algorithmus~\ref{alg:bresenham} ist ein Algorithmus zum Zeichnen von Linien von Anfangspunkt \( (x_1, y_1) \) mit Endpunkt \( (x_2, y_2) \). Mit \( \Delta x \coloneqq x_2 - x_1 \geq 0 \) und \( \Delta y \coloneqq y_2 - y_1 \geq 0 \) (die Bedingung \( \geq 0 \) kann durch geschicktes Vertauschen immer eingehalten werden), muss der Algorithmus genau \( \max \big\{ \Delta x, \Delta y \big\} + 1 \) Pixel zeichnen.

				\begin{algorithm}
					\DontPrintSemicolon

					\KwIn{Startpunkt \( (x_1, y_1) \), Endpunkt \( (x_2, y_2) \)}

					\( \delta x \gets x_2 - x_1 \)\;
					\( \delta y \gets y_2 - y_1 \) \;
					\( x \gets x_1 \) \;
					\( y \gets y_1 \) \;


					Setze Pixel \( (x, y) \) \;
					\( \xi \gets \delta x / 2 \) \;

					\While{\( x < x_2 \)}{
						\( x \gets x + 1 \) \;
						\( \xi \gets \xi - \delta y \) \;

						\If{\( \xi < 0 \)}{
							\( y \gets y + 1 \) \;
							\( \xi \gets \xi + \delta x \)
						}

						Setze Pixel \( (x, y) \);
					}

					\caption{Bresenham-Algorithmus zum Rastern einer Linie.}
					\label{alg:bresenham}
				\end{algorithm}
			% end

			\subsubsection{Rasterisierung von Polygonen (Scanline Algorithmus)}
				Polygone können \bspw mit dem \emph{Scanline Algorithmus} gerastert werden. Dieser scannt die Pixelebene von oben nach unten mit einer "Scan Line" durch und findet alle Schnittpunkte mit den Kanten des Polygons. Anschließend werden die Schnittpunkte nach \(x\)-Koordinaten sortiert und die Pixel zwischen Paaren aufeinanderfolgender Schnittpunkte gefüllt. Dabei wird eine Parität mitgeführt, die in jedem Schritt um Eins erhöht wird (beginnend von Null). Ist die Parität ungerade, wird der Pixel gesetzt, sonst nicht.
			% end
		% end

		\subsection{Verdeckungsrechnung}
			\subsubsection{z-Buffer-Algorithmus}
				\begin{itemize}
					\item Zu jedem Bildpunkt wird noch ein z-Wert gespeichert.
					\item Initialisierung: Der Bildspeicher wird auf die Hintergrundfarbe gesetzt, der z-Speicher auf den maximalen Wert.
					\item Anschließend werden alle Objekte der Szene nacheinander gerastert, wobei keine besondere Reihenfolge notwendig ist: \\
						Für jeden Punkt \( (x, y) \) eines Polygons wird \( z(x, y) \) berechnet. Aufgrund der perspektivischen Transformation ist eine lineare Interpolation nicht mehr einfach möglich! Ist \( z(x, y) \) kleiner als der bereits gespeicherte Wert, so wird \( z(x, y) \) gespeichert und der zugehörige Farbwert in den Bildspeicher geschrieben.
					\item Nach der Behandlung aller Objekte steht im Objektspeicher das Bild der gewünschten (Teil-)~Flächen.
				\end{itemize}

				\textbf{Vorteile:}
				\begin{itemize}
					\item Jede Szene mit jeder Art von Objekten kann behandelt werden.
					\item Die Komplexität ist unabhängig von der Tiefenkomplexität.
					\item In eine fertige Szene können nachträglich Objekte eingefügt werden.
					\item Spezielle Objekte (\zB ein 3D-Cursor) können in der Szene mit korrekter Verdeckung dargestellt werden.
					\item Leicht in Hardware zu realisieren.
				\end{itemize}
				\textbf{Nachteile:}
				\begin{itemize}
					\item Für jeden Bildpunkt wird nur ein Objekt gespeichert (dies führt zu Abtastfehlern).
					\item Transparenz ist prinzipiell nicht realisierbar.
					\item Die Genauigkeit des z-Buffers ist beschränkt (getrennte Objekte erhalten den selben z-Wert, die Farbe wird dann von der Objektreihenfolge bei der Rasterung bestimmt).
				\end{itemize}
			% end
		% end
	% end
% end

\chapter{Transformationen}
\label{c:transformations}

Aus Sicht der Grafikpipeline gibt es viele unterschiedliche Koordinaten:
\begin{itemize}
	\item Objektkoordinaten (Festlegung der lokalen Lage von 3D-Objekten)
	\item Weltkoordinaten (Beschreibung der gesamten Szene in 3D)
	\item Projektionskoordinaten (nach der Anwendung der Projektionstransformation)
	\item Bildschirmkoordinaten (Darstellung der Szene in einem Fenster mit gewählter Position und Größe)
\end{itemize}
Abbildung~\ref{fig:pipelinecoordinates} zeigt die Umwandlung der Koordinaten innerhalb der Grafikpipeline.

\begin{figure}
	\centering
	\begin{tikzpicture}[ every node/.style = { align = center }, block/.style = { draw, rectangle }]
		\coordinate (a);
		\node [block, right = 1.5 of a] (b) {Modell};
		\node [block, right = 1 of b] (c) {Projektions- \\ Transformation};
		\node [block, right = 1 of c] (d) {Normierung};
		\node [block, right = 1 of d] (e) {Viewport \\ Transformation};
		\coordinate [right = 1.5 of e] (f);

		\draw [->] (a) -- coordinate[label = above:Vertex](A) (b);
		\draw [->] (b) -- coordinate(B) (c);
		\draw [->] (c) -- coordinate(C) (d);
		\draw [->] (d) -- coordinate(D) (e);
		\draw [->] (e) -- coordinate(E) (f);

		\node [below = 2 of A] (aA) {Objekt- \\ koordinaten};
		\node [below = 2 of B] (bB) {Welt- \\ koordinaten};
		\node [below = 2 of C] (cC) {Projektions- \\ koordinaten};
		\node [below = 2 of D] (dD) {Normierte \\ Koordinaten};
		\node [below = 2 of E] (eE) {Bildschirm- \\ koordinaten};

		\draw [<-] (aA) -- (A);
		\draw [<-] (bB) -- (B);
		\draw [<-] (cC) -- (C);
		\draw [<-] (dD) -- (D);
		\draw [<-] (eE) -- (E);
	\end{tikzpicture}
	\caption{Änderung der Koordinaten innerhalb der Grafikpipeline.}
	\label{fig:pipelinecoordinates}
\end{figure}

\section{Affine Abbildungen}
	\subsection{Eigenschaften}
		\emph{Affine Abbildungen} (Translation, Rotation, Skalierung, Scherung) haben folgende Eigenschaften:
		\begin{itemize}
			\item Geraden werden auf Gerade abgebildet.
			\item Beschränkte Objekte bleiben beschränkt.
			\item Verhältnisse von Längen, Flächen, Volumen bleiben erhalten.
			\item Parallele Objekte bleiben parallel.
		\end{itemize}

		Mathematischer formuliert: Eine Abbildung \( \Phi : \R^n \to \R^n \) heißt \emph{affin}, \gdw \( \Phi \) in der Form
		\begin{equation*}
			\Phi(\vec{v}) = A(\vec{v}) + I(\vec{b})
		\end{equation*}
		mit einer linearen Abbildung \( A : \R^n \to \R^n \), der Identitätsabbildung \( I : \R^n \to \R^n \) und \( \vec{v}, \vec{b} \in \R^n \) darstellbar ist. Eine affine Abbildung setzt sich also aus einer linearen Abbildung und einer Translation zusammen.

		Eine Abbildung \( A : \R^n \to \R^n \) heißt \emph{linear}, \gdw
		\begin{equation*}
			A(\lambda \vec{u} + \mu \vec{v}) = \lambda \, A(\vec{u}) + \mu \, A(\vec{v})
		\end{equation*}
		für alle \( \vec{u}, \vec{v} \in \R^n \) und \( \lambda, \mu \in \R \) gilt.
	% end

	\subsection{Homogene Koordinaten}
		Anstelle der aufwendigen Notation \( A\vec{v} + \vec{b} \) für affine Abbildungen können homogene Koordinaten verwendet werden (sei dazu im folgenden \( n = 3 \)). Dann wird eine Äquivalenzklasse nach \( \R^4 \) definiert durch:
		\begin{equation*}
			\begin{bmatrix}
				x \\
				y \\
				z
			\end{bmatrix}
			\to
			\begin{bmatrix}
				x \\
				y \\
				z \\
				1
			\end{bmatrix}
			\quad\quad\quad\quad
			\begin{bmatrix}
				x \\
				y \\
				z \\
				w
			\end{bmatrix}
			\to
			\begin{bmatrix}
				x/w \\
				y/w \\
				z/w
			\end{bmatrix}
		\end{equation*}
		Wobei \( w \neq 0 \) einen Skalierungsfaktor darstellt (dieser ist meistens \num{1}).

		Eine reine Translation kann dann in homogenen Koordinaten ausgedrückt werden als:
		\begin{equation*}
			\begin{bmatrix}
				x' \\
				y' \\
				z' \\
				1
			\end{bmatrix}
			=
			\begin{bmatrix}
				1 & 0 & 0 & x_0 \\
				0 & 1 & 0 & y_0 \\
				0 & 0 & 1 & z_0 \\
				0 & 0 & 0 & 1
			\end{bmatrix}
			\cdot
			\begin{bmatrix}
				x \\
				y \\
				z \\
				1
			\end{bmatrix}
			=
			\begin{bmatrix}
				x + x_0 \\
				y + y_0 \\
				z + z_0 \\
				1
			\end{bmatrix}
		\end{equation*}

		Allgemein ergibt sich für homogene Koordinaten die Transformationsmatrix
		\begin{equation*}
			\begin{bmatrix}
				a_{11} & a_{12} & a_{13} & x_0 \\
				a_{21} & a_{22} & a_{23} & y_0 \\
				a_{31} & a_{32} & a_{33} & z_0 \\
				0      & 0      & 0      & 1
			\end{bmatrix}
		\end{equation*}
		was einer Anwendung der Matrix \( \mat{A} \) und eine Translation um den Vektor \( \begin{bmatrix} x_0 & y_0 & z_0 \end{bmatrix}^T \) entspricht (dies ist leicht nachzurechnen).

		Diese einheitliche und kompakte Darstellung von Rotationen/Skalierungen/\dots und Translationen als eine Matrixmultiplikation erlaubt eine einfache Implementierung und eine Hintereinanderausführung mehrerer Operationen entspricht einer reinen Multiplikation von Matrizen.
	% end
% end

\section{Skalierung, Scherung, Rotation}
	Die affinen Abbildungen der Skalierung, Scherung und Rotation lassen den Ursprung invariant, \dh der Vektor \( \begin{bmatrix} 0 & 0 & 0 \end{bmatrix}^T \) wird nicht verschoben. Hierfür wären theoretisch normale \( (3 \times 3) \)-Matrizen ausreichend.

	\subsection{Skalierung}
		Eine Skalierung wird durch eine Diagonalmatrix
		\begin{equation*}
			\begin{bmatrix}
				s_1 & 0   & 0   \\
				0   & s_2 & 0   \\
				0   & 0   & s_3
			\end{bmatrix}
			\to
			\underbrace{
				\begin{bmatrix}
					s_1 & 0   & 0   & 0 \\
					0   & s_2 & 0   & 0 \\
					0   & 0   & s_3 & 0 \\
					0   & 0   & 0   & 1
				\end{bmatrix}
			}_\text{Homogene Koordinaten}
		\end{equation*}
		beschrieben. Dabei wird die \(x\)-Achse um \(s_1\) skaliert, die \(y\)-Achse um \(s_2\) und die \(z\)-Achse um \(s_3\). Gilt \( s_1 = s_2 = s_3 \), so werden alle Koordinaten gleichermaßen skaliert.
	% end

	\subsection{Scherung}
		Eine Scherung wird durch eine Matrix
		\begin{equation*}
			\begin{bmatrix}
				1   & s_2 & s_5 \\
				s_1 & 1   & s_6 \\
				s_3 & s_4 & 1
			\end{bmatrix}
			\to
			\underbrace{
				\begin{bmatrix}
					1   & s_2 & s_5 & 0 \\
					s_1 & 1   & s_6 & 0 \\
					s_3 & s_4 & 1   & 0 \\
					0   & 0   & 0   & 1
				\end{bmatrix}
			}_\text{Homogene Koordinaten}
		\end{equation*}
		beschrieben.
	% end

	\subsection{Rotation}
		Eine Basisrotation (um eine der Koordinatenachsen) wird durch folgende drei Matrizen beschrieben (von oben nach unten jeweils um die \(x\)-, \(y\)- und \(z\)-Achse):
		\begin{align*}
			\mat{R}(x; \alpha) & =
			\begin{bmatrix}
				1 & 0            & 0             \\
				0 & \cos(\alpha) & -\sin(\alpha) \\
				0 & \sin(\alpha) & \cos(\alpha)
			\end{bmatrix} \\
			\mat{R}(y; \alpha) & =
			\begin{bmatrix}
				\cos(\alpha)  & 0 & \sin(\alpha) \\
				0             & 1 & 0            \\
				-\sin(\alpha) & 0 & \cos(\alpha)
			\end{bmatrix} \\
			\mat{R}(z; \alpha) & =
			\begin{bmatrix}
				\cos(\alpha) & -\sin(\alpha) & 0 \\
				\sin(\alpha) & \cos(\alpha)  & 0 \\
				0            & 0             & 1
			\end{bmatrix}
		\end{align*}
		Der Kompaktheit halber wurden hier die homogenen Formulierungen weg gelassen, sie werden aber analog zu denen der Skalierung und Scherung gebildet.

		\subsubsection{Rotation um beliebige Achse}
			Zur Rotation um einen beliebigen, normierten Vektor \( \vec{r} \) um den Winkel \(\alpha\) muss zunächst das körperfeste Koordinatensystem des zu rotierenden Körpers koinzident zu den Ursprungsachsen gedreht werden (durch eine Rotationsmatrix \( \mat{R} \)) und anschließend um eine der Achsen (\zB der \(x\)-Achse) rotiert werden und anschließend wieder zurück in das körperfeste Koordinatensystem gedreht werden:
			\begin{equation*}
				\mat{R}_{\vec{r}}(\alpha) = \mat{R}^{-1} \cdot \mat{R}(x; \alpha) \cdot \mat{R}
			\end{equation*}
			Dazu muss zunächst eine orthonormale Basis \( (\vec{r}, \vec{s}, \vec{t}) \) bestimmt werden. Der erste Basisvektor ist die Drehachse \(\vec{r}\), die anderen beiden Vektoren werden wie folgt berechnet:
			\begin{align*}
				\vec{s} & =
				\begin{cases}
					\frac{\vec{r} \times \vec{e}_y}{\lVert \vec{r} \times \vec{e}_y \rVert} & \text{falls } r \parallel \vec{e}_x \\
					\frac{\vec{r} \times \vec{e}_x}{\lvert \vec{r} \times \vec{e}_x \rVert} & \text{sonst}
				\end{cases} \\
				\vec{t} = \vec{r} \times \vec{s}
			\end{align*}
			Daraus ergibt sich mit \( \mat{R} = \begin{bmatrix} \vec{r} & \vec{s} & \vec{t} \end{bmatrix} \) die benötigte Transformationsmatrix.
		% end

		\subsubsection{Rotation um beliebigen Punkt}
			Soll ein Objekt um einen anderen Punkt als den Ursprung gedreht werden, so muss das Rotationszentrum zunächst in den Ursprung verschoben, dann die Rotation durchgeführt und anschließend das Rotationszentrum wieder zurückgeschoben werden.

			\subparagraph{Beispiel}
				Es soll eine Transformationsmatrix erstellt werden, die eine Rotation um die \(i\)-Achse (\( i \in \{\, x, y, z \,\} \)) im Punkt beschrieben durch \( \vec{r} \) durchführt. Die Transformationsmatrix lautet dann allgemein:
				\begin{equation*}
					\mat{T} = \text{Trans}(\vec{r}) \cdot \text{Rot}(i; \alpha) \cdot \text{Trans}(-\vec{r})
				\end{equation*}
			% end
		% end

		\subsection{Nicht-Kommutativität von Transformationen}
			Die Reihenfolge von Transformationen darf \iA nicht vertauscht werden (insbesondere ist die Matrixmultiplikation nicht kommutativ).
		% end

		\subsection{Rechenaufwand}
			Bei vielen nacheinander ausgeführten Transformationen ist es sinnvoller, einmal die gesamte Transformationsmatrix zu berechnen, statt oftmals Matrix-Vektor-Multiplikationen durchzuführen.
		% end
	% end

	\section{Projektion}
		\label{sec:projection}

		Zur Projektion von 3D-Elementen gibt es viele unterschiedliche Möglichkeiten:
		\begin{itemize}
			\item Aufriss (Frontansicht)
			\item Kabinett-/Kavallierperspektive
			\item Allgemeine Parallelprojektion
			\item Isometrische Perspektive
			\item Zentralperspektive
			\item Vogelperspektive
		\end{itemize}

		Solche projektiven Abbildungen können durch homogene Transformationen beschrieben werden, wobei jedoch die Winkel verändert werden! Außerdem geht die Parallelität von Linien oft verloren (\dh Parallelen schneiden sich in Fluchtpunkten). Dies ist der allgemeine Unterschied zwischen perspektivischer und paralleler Projektion:
		\begin{itemize}
			\item Bei einer perspektivischen Projektion treffen sich die Strahlen im Augenpunkt (Projektionszentrum) und Winkel werden verändert.
			\item Bei der parallelen Projektion sind die Projektionsstrahlen parallel und die Winkel bleiben erhalten.
		\end{itemize}

		\subsection{Perspektivische Projektion}
			\begin{itemize}
				\item Vergleichbar mit einem fotografischen System.
				\item Entspricht der natürlichen Wahrnehmung.
				\item Der Abstand zwischen Objekten und Projektionsebene geht ein.
				\item Längenverhältnisse ändern sich.
				\item Winkel ändern sich.
				\item Parallele Geraden bleiben nicht parallel.
			\end{itemize}

			Das heißt perspektivische Projektionen sind im Allgemeinen keine affinen Abbildungen! Insbesondere werden vom Blickpunkt weiter entfernte Objekte kleiner dargestellt.

			Ist der zu projizierende Punkt \( P = (x, y) \) und der Augpunkt \( A = (-x_0, 0) \) gegeben, so gilt für den Bildpunkt \( B = (0, y_0) \):
			\begin{equation*}
				\frac{y_0}{y} = \frac{x_0}{x + x_0}
			\end{equation*}
			Allgemein lautet mit \( y_0 = y x_0 / (x + x_0) \) die Abbildung wie folgt:
			\begin{equation*}
				\begin{bmatrix}
					x \\
					y
				\end{bmatrix}
				\mapsto
				\begin{bmatrix}
					0 \\
					\frac{y x_0}{x_0 + x}
				\end{bmatrix}
			\end{equation*}
			Daraus ergibt sich die homogene \( (3 \times 3) \)-Matrix:
			\begin{equation*}
				\begin{bmatrix}
					0                     \\
					\frac{y x_0}{x_0 + x} \\
					1
				\end{bmatrix}
				\simeq
				\begin{bmatrix}
					0     \\
					y x_0 \\
					x_0 + x
				\end{bmatrix}
			\end{equation*}
			Und somit die perspektivische Transformation:
			\begin{equation*}
				\begin{bmatrix}
					x' \\
					y' \\
					1
				\end{bmatrix}
				=
				\begin{bmatrix}
					0 & 0   & 0   \\
					0 & x_0 & 0   \\
					1 & 0   & x_0
				\end{bmatrix}
				\cdot
				\begin{bmatrix}
					x \\
					y \\
					1
				\end{bmatrix}
				\quad\simeq\quad
				\frac{1}{x_0}
				\begin{bmatrix}
					x' \\
					y' \\
					1
				\end{bmatrix}
				=
				\begin{bmatrix}
					0     & 0 & 0 \\
					0     & 1 & 0 \\
					1/x_0 & 0 & 1
				\end{bmatrix}
				\cdot
				\begin{bmatrix}
					x \\
					y \\
					1
				\end{bmatrix}
			\end{equation*}

			\subsubsection{Allgemeine perspektivische Transformation}
				Mit dem Fluchtpunkt in \( (x_0, y_0, z_0) \) wird eine allgemeine perspektivische Transformation beschrieben durch:
				\begin{equation*}
					\begin{bmatrix}
						x' \\
						y' \\
						z' \\
						w'
					\end{bmatrix}
					=
					\begin{bmatrix}
						1     & 0     & 0     & 0 \\
						0     & 1     & 0     & 0 \\
						0     & 0     & 1     & 0 \\
						1/x_0 & 1/y_0 & 1/z_0 & 1
					\end{bmatrix}
					\cdot
					\begin{bmatrix}
						x \\
						y \\
						z \\
						w
					\end{bmatrix}
				\end{equation*}
			% end
		% end

		\subsection{Parallele Projektion}
			\begin{itemize}
				\item Ist weniger realistisch.
				\item Winkel ändern sich \iA nicht.
				\item Parallele Geraden bleiben parallel.
			\end{itemize}
		% end

		\subsection{Kanonisches Sichtvolumen}
			Die perspektivische Projektion wird in zwei Abbildungen zerlegt:
			\begin{itemize}
				\item Die perspektivische Transformation und
				\item eine anschließende Parallelprojektion.
			\end{itemize}
			Nach der perspektivischen Transformation ist das Sichtvolumen ein Würfel und durch eine Rotation in den Augpunkt wird erreicht, dass der Würfel achsenparallel wird:
			\begin{equation*}
				\underbrace{
					\begin{bmatrix}
						0     & 0 & 0 \\
						0     & 1 & 0 \\
						1/x_0 & 0 & 1
					\end{bmatrix}
				}_\text{Perspektivische Projektion}
				=
				\underbrace{
					\begin{bmatrix}
						0 & 0 & 0 \\
						0 & 1 & 0 \\
						0 & 0 & 1
					\end{bmatrix}
				}_\text{Parallelprojektion}
				\cdot
				\underbrace{
					\begin{bmatrix}
						1     & 0 & 0 \\
						0     & 1 & 0 \\
						1/x_0 & 0 & 1
					\end{bmatrix}
				}_\text{Perspektivische Transformation}
			\end{equation*}

			Ist die Kamera im Unendlichen (Parallelprojektion), wird das Sichtvolumen ein Einheitswürfel. Bei einer perspektivischen Projektion ist das Sichtvolumen eine Pyramide, nach der perspektivischen Transformation hingegen wieder ein Würfel!
		% end
	% end

	\section{3D-Interaktion}
		\label{sec:3dinteraction}

		Bei der 3D-Interaktion ist nicht immer klar, welche Bewegung der Nutzer ausgeführt haben möchte. Ansätze hierzu sind
		\begin{itemize}
			\item Desktop,
			\item Multi-Window (Mehrfachauswahl),
			\item Direktes 2D-Maus-Mapping oder
			\item Manipulatoren.
		\end{itemize}

		\subsection{Manipulatoren}
			Im Zweidimensionalen werden Manipulatoren häufig eingesetzt (Manipulatoren sind dabei zum Beispiel kleine "Rädchen" um ein Objekt, mit welchen dieses gedreht werden kann). Auch werden \zB Kästchen eingesetzt, mit denen skaliert und verschoben werden kann (Drag-and-Drop). Dabei sind sie im zweidimensionalen sehr einfach zu implementieren, da eine Eins-zu-Eins Abbildung der Mauszeigerposition zum Manipulator erstellt werden kann. Außerdem gibt es keine Probleme bei der perspektivischen Abbildung.

			Immer häufiger werden Manipulatoren auch im Dreidimensionalen, \zB zur Navigation der Kamera eingesetzt. Hier ist die Implementierung jedoch sehr viel schwieriger, da es unendlich viele Möglichkeiten gibt, eine Cursorposition auf einer geraden Linie im 3D-Raum abzubilden. Dennoch stellen Manipulatoren eines der besten momentan verfügbaren Werkzeuge zur 3D-Interaktion dar.
		% end
	% end
% end

\chapter{3D-Visualisierung}
	\section{(Gewinnung) 3D-Daten}
		Bei 3D-Daten werden die Messwerte dreidimensional im Raum verteilt und jeder Wert hat drei Koordinaten \( (x, y, z) \). Dabei können die Werte gleichmäßig oder unterschiedlich verteilt sein und ein Wert kann skalar oder höherdimensional sein (\zB ein Vektor in einem Strömungsfeld).

		\paragraph{Terrain}
			Zum Scannen eines Terrains wird an beliebigen Positionen \( (x, y) \) die Höhe \(z\) gemessen, wodurch sich eine 3D-Position ergibt. Oberflächeninformationen (\zB Vegetation) können durch Satellitenbilder gewonnen werden.
		% end

		\paragraph{Laser Scanning}
			Beim Laser Scanning wird ein Laserstrahl auf eine Oberfläche projiziert und das rückstrahlende Licht gemessen. Aus dem Abstand zwischen Laser und Kamera kann dann die Distanz durch Triangulation berechnet werden. Dies ergibt eine unstrukturierte Punktwolke.
		% end

		\paragraph{Range Images}
			Aus einem Range Image \( r(u, v) \) ergibt sich die Pixelinformation als 3D-Punkt \( \big(u, v, r(u, v)\big) \).
		% end

		\paragraph{Medizinische Bilddaten}
			In der Medizin werden viele bildgebende Geräte verwendet, um physikalische Eigenschaften zu messen (MRI, CT, Ultraschall, \dots). Diese produzieren einen "Stapel" von parallelen Scheiben (Slices), die jeweils einem regulären 2D-Gitter entsprechen. Ein solcher Scan produziert jedoch riesige Datenmengen!
		% end

		\paragraph{Wetter}
			Die Parameter des Wetters (\zB Temperatur, Druck, Niederschlag, Windrichtung, \dots) sind für bestimmte Regionen auf verschiedenen Höhen unterschiedlich und sowohl vektoriell als auch skalar. Zur Messung dieser Daten wird die Erde in viele Zellen einer bestimmten Größe (meist einige Kilometer) aufgeteilt.
		% end
	% end

	\section{Triangulation von Punktwolken}
		Eine unstrukturierte Punktmenge \( s_i = (x_i, y_i, z_i) \) auf einer Oberflächen \(S\) wird als \emph{Punktwolke} bezeichnet. Für einfache Oberflächen (ohne Falten) können Punkte auf eine Ebene projiziert und in 2D trianguliert werden (planare Triangulation). Dieses 2D-Netz wird dann entsprechend der \(z_i\)-Werte deformiert.

		\subsection{Ideal Triangulation}
			Bei einer idealen Triangulation haben alle Dreiecke die Innenwinkel \( (\ang{60}, \ang{60}, \ang{60}) \), \dh die Dreiecke sind gleichschenklig. Dies führt zu einer numerischen Stabilität und vereinfacht das Post-Processing.
		% end

		\subsection{Voronoi-Diagramm}
			Statt einer idealen Triangulation, die schwer zu berechnen ist, kann ein Voronoi-Diagramm eingesetzt werden. Dabei wird für jeden (in 2D projizierten) Punkt \( \bar{s}_i \) eine \emph{Voronoi-Zelle} definiert, die alle Punkte enthält, die näher an \( \bar{s}_i \) als an allen anderen Orten (andere \( \bar{s}_j \)) liegen. Die Kante einer Voronoi-Zelle liegt dann auf den Punkten mit dem gleichen Abstand zu den zwei nächsten Orten und der Knoten einer Voronoi-Zelle auf einem Punkt, der den gleichen Abstand zu drei anderen Orten hat. Dadurch wird die 2D-Fläche "parkettiert".
		% end

		\subsection{Delaunay-Triangulation}
			Durch Betrachtung des dualen Graph zu einem Voronoi-Diagramm wird eine \emph{Delaunay-Triangulation} beschrieben (es sind jedoch \mglw Korrekturen nötig). Dabei ist ein Dreiecksnetz nur dann eine Delaunay-Triangulation, wenn alle Umkreise von allen Dreiecken leer sind, \dh es liegt kein Ort in ihnen. Dies kann durch das "Umdrehen" einer Kante korrigiert werden.
		% end
	% end

	\section{Indirekte Volumenvisualisierung}
		Eine Menge von Volumendaten enthält viele Informationen, was das Rendering verlangsamt, obwohl vieles (\zB verdeckte) Elemente gar nicht angezeigt wird. Deshalb wird \iA nicht das gesamte Volumen, sondern nur ein Teil angezeigt.

		\subsection{3D-Volumen und Nachbarschaft}
			In einem normalen 3D-Raster wird ein Volumenelement (Würfel) als \emph{Voxel} bezeichnet. Die Dicke eines Slices (\dh die Breite eines Würfels) ist dabei oftmals größer als die Pixelabstände (anisotropische Volumen). Die eine Rasterposition wird dabei durch einen Index \( (i, j, k) \) beschrieben.

			Ein Voxel ist dabei adjazent zu einem Referenzvoxel:
			\begin{itemize}
				\item Im Zweidimensionalen sind dies über \num{4} Kanten und \num{4} Ecken insgesamt \num{8} Nachbarvoxel.
				\item Im Dreidimensionalen sind dies über \num{6} Flächen, \num{12} Kanten und \num{8} Ecken insgesamt \num{26} Nachbarvoxel.
			\end{itemize}
		% end

		\subsection{2D: Konturlinien}
			Eine \emph{Konturlinie} ist eine Linie entlang derselben Höhe, \dh der Wert ist entlang einer Konturlinie konstant (\dh ein Kontur-Diagramm entspricht einer Höhenkarte). Dabei ist die Ausrichtung des Gefälles orthogonal zu den Konturlinien.
		% end

		\subsection{3D: Isoflächen}
			Eine Trennung zwischen verschiedenen Strukturen führt zu einer Eingrenzung von Strukturen, wodurch ebendiese Strukturen erkannt werden können. Die Voxel an einer solchen Eingrenzung bilden, wenn sie die gleiche Intensität haben, sogenannte \emph{Isoflächen}. Eine Isofläche ist dabei eine implizite Fläche
			\begin{equation*}
				i(x) = V(x) - \tau = 0
			\end{equation*}
			wobei \( V(x) \) der Voxelwert und \( \tau = \const \) ein festgesetzter Isowert ist. Die Datenmenge wird dann aufgeteilt in innen (\( i(x) > 0 \)) und außen (\( i(x) < 0 \)) liegende Flächen. Die Definition eines solchen Isowerts entspricht also einem Thresholding der Daten.
		% end

		\subsection{2D: Marching Squares}
			Der \emph{Marching Squares Algorithmus} versucht, gegeben einen fixen Isowert \( \tau \) die Isolinie \( s(x) = 0 \) zu finden, um so die Fläche in innen und außen zu unterteilen. Dabei sei jede Bildzelle durch ihre vier umgebenden Pixel definiert. Danach wird jede Bildzelle abgelaufen, um sie einem der in Abbildung~\ref{fig:marchingsquares} aufgezeigten Fälle zuzuordnen und die Isolinie entsprechend zu ziehen. Ein Pixel hat dabei den "Set Pixel"-Zustand, wenn dieser größer oder gleich \( \tau \) ist.

			Werden Rotationen und Symmetrien beachtet, so reduzieren sich die Fälle auf vier Fälle.

			\begin{figure}
				\centering
				\begin{tikzpicture}[xscale = 1/2, yscale = 1/2, set/.style = { draw, circle, fill = fgcolor }, reset/.style = { draw, circle }, alt/.style = { dashed }]
					\node [reset] (ref) {};
					\node [reset, right = 0.5 of ref] (b) {};
					\node [reset, below = 0.5 of b] (c) {};
					\node [reset, left = 0.5 of c] (d) {};
					\coordinate [right = 0.25 of ref] (A);
					\coordinate [below = 0.25 of b] (B);
					\coordinate [left = 0.25 of c] (C);
					\coordinate [above = 0.25 of d] (D);
					\path (c) -- node[below, yshift = -0.2cm]{Fall 0} (d);
					\draw (ref) -- (b) -- (c) -- (d) -- (ref);

					\node [reset, right = 1 of b] (a) {};
					\node [reset, right = 0.5 of a] (b) {};
					\node [reset, below = 0.5 of b] (c) {};
					\node [  set, left = 0.5 of c] (d) {};
					\coordinate [right = 0.25 of a] (A);
					\coordinate [below = 0.25 of b] (B);
					\coordinate [left = 0.25 of c] (C);
					\coordinate [above = 0.25 of d] (D);
					\path (c) -- node[below, yshift = -0.2cm]{Fall 1} (d);
					\draw (a) -- (b) -- (c) -- (d) -- (a);
					\draw (C) -- (D);

					\node [reset, right = 1 of b] (a) {};
					\node [reset, right = 0.5 of a] (b) {};
					\node [  set, below = 0.5 of b] (c) {};
					\node [reset, left = 0.5 of c] (d) {};
					\coordinate [right = 0.25 of a] (A);
					\coordinate [below = 0.25 of b] (B);
					\coordinate [left = 0.25 of c] (C);
					\coordinate [above = 0.25 of d] (D);
					\path (c) -- node[below, yshift = -0.2cm]{Fall 2} (d);
					\draw (a) -- (b) -- (c) -- (d) -- (a);
					\draw (B) -- (C);

					\node [reset, right = 1 of b] (a) {};
					\node [reset, right = 0.5 of a] (b) {};
					\node [  set, below = 0.5 of b] (c) {};
					\node [  set, left = 0.5 of c] (d) {};
					\coordinate [right = 0.25 of a] (A);
					\coordinate [below = 0.25 of b] (B);
					\coordinate [left = 0.25 of c] (C);
					\coordinate [above = 0.25 of d] (D);
					\path (c) -- node[below, yshift = -0.2cm]{Fall 3} (d);
					\draw (a) -- (b) -- (c) -- (d) -- (a);
					\draw (B) -- (D);

					\node [reset, right = 1 of b] (a) {};
					\node [  set, right = 0.5 of a] (b) {};
					\node [reset, below = 0.5 of b] (c) {};
					\node [reset, left = 0.5 of c] (d) {};
					\coordinate [right = 0.25 of a] (A);
					\coordinate [below = 0.25 of b] (B);
					\coordinate [left = 0.25 of c] (C);
					\coordinate [above = 0.25 of d] (D);
					\path (c) -- node[below, yshift = -0.2cm]{Fall 4} (d);
					\draw (a) -- (b) -- (c) -- (d) -- (a);
					\draw (A) -- (B);

					\node [reset, right = 1 of b] (a) {};
					\node [  set, right = 0.5 of a] (b) {};
					\node [reset, below = 0.5 of b] (c) {};
					\node [  set, left = 0.5 of c] (d) {};
					\coordinate [right = 0.25 of a] (A);
					\coordinate [below = 0.25 of b] (B);
					\coordinate [left = 0.25 of c] (C);
					\coordinate [above = 0.25 of d] (D);
					\path (c) -- node[below, yshift = -0.2cm]{Fall 5} (d);
					\draw (a) -- (b) -- (c) -- (d) -- (a);
					\draw [alt] (A) -- (B);
					\draw [alt] (C) -- (D);
					\draw (A) -- (D);
					\draw (B) -- (C);

					\node [reset, right = 1 of b] (a) {};
					\node [  set, right = 0.5 of a] (b) {};
					\node [  set, below = 0.5 of b] (c) {};
					\node [reset, left = 0.5 of c] (d) {};
					\coordinate [right = 0.25 of a] (A);
					\coordinate [below = 0.25 of b] (B);
					\coordinate [left = 0.25 of c] (C);
					\coordinate [above = 0.25 of d] (D);
					\path (c) -- node[below, yshift = -0.2cm]{Fall 6} (d);
					\draw (a) -- (b) -- (c) -- (d) -- (a);
					\draw (A) -- (C);

					\node [reset, right = 1 of b] (a) {};
					\node [  set, right = 0.5 of a] (b) {};
					\node [  set, below = 0.5 of b] (c) {};
					\node [  set, left = 0.5 of c] (d) {};
					\coordinate [right = 0.25 of a] (A);
					\coordinate [below = 0.25 of b] (B);
					\coordinate [left = 0.25 of c] (C);
					\coordinate [above = 0.25 of d] (D);
					\path (c) -- node[below, yshift = -0.2cm]{Fall 7} (d);
					\draw (a) -- (b) -- (c) -- (d) -- (a);
					\draw (A) -- (D);


					\node [  set, below = 2 of ref] (a) {};
					\node [reset, right = 0.5 of a] (b) {};
					\node [reset, below = 0.5 of b] (c) {};
					\node [reset, left = 0.5 of c] (d) {};
					\coordinate [right = 0.25 of a] (A);
					\coordinate [below = 0.25 of b] (B);
					\coordinate [left = 0.25 of c] (C);
					\coordinate [above = 0.25 of d] (D);
					\path (c) -- node[below, yshift = -0.2cm]{Fall 8} (d);
					\draw (a) -- (b) -- (c) -- (d) -- (a);
					\draw (A) -- (D);

					\node [  set, right = 1 of b] (a) {};
					\node [reset, right = 0.5 of a] (b) {};
					\node [reset, below = 0.5 of b] (c) {};
					\node [  set, left = 0.5 of c] (d) {};
					\coordinate [right = 0.25 of a] (A);
					\coordinate [below = 0.25 of b] (B);
					\coordinate [left = 0.25 of c] (C);
					\coordinate [above = 0.25 of d] (D);
					\path (c) -- node[below, yshift = -0.2cm]{Fall 9} (d);
					\draw (a) -- (b) -- (c) -- (d) -- (a);
					\draw (A) -- (C);

					\node [  set, right = 1 of b] (a) {};
					\node [reset, right = 0.5 of a] (b) {};
					\node [  set, below = 0.5 of b] (c) {};
					\node [reset, left = 0.5 of c] (d) {};
					\coordinate [right = 0.25 of a] (A);
					\coordinate [below = 0.25 of b] (B);
					\coordinate [left = 0.25 of c] (C);
					\coordinate [above = 0.25 of d] (D);
					\path (c) -- node[below, yshift = -0.2cm]{Fall 10} (d);
					\draw (a) -- (b) -- (c) -- (d) -- (a);
					\draw [alt] (A) -- (D);
					\draw [alt] (B) -- (C);
					\draw (A) -- (B);
					\draw (C) -- (D);

					\node [  set, right = 1 of b] (a) {};
					\node [reset, right = 0.5 of a] (b) {};
					\node [  set, below = 0.5 of b] (c) {};
					\node [  set, left = 0.5 of c] (d) {};
					\coordinate [right = 0.25 of a] (A);
					\coordinate [below = 0.25 of b] (B);
					\coordinate [left = 0.25 of c] (C);
					\coordinate [above = 0.25 of d] (D);
					\path (c) -- node[below, yshift = -0.2cm]{Fall 11} (d);
					\draw (a) -- (b) -- (c) -- (d) -- (a);
					\draw (A) -- (B);

					\node [  set, right = 1 of b] (a) {};
					\node [  set, right = 0.5 of a] (b) {};
					\node [reset, below = 0.5 of b] (c) {};
					\node [reset, left = 0.5 of c] (d) {};
					\coordinate [right = 0.25 of a] (A);
					\coordinate [below = 0.25 of b] (B);
					\coordinate [left = 0.25 of c] (C);
					\coordinate [above = 0.25 of d] (D);
					\path (c) -- node[below, yshift = -0.2cm]{Fall 12} (d);
					\draw (a) -- (b) -- (c) -- (d) -- (a);
					\draw (B) -- (D);

					\node [  set, right = 1 of b] (a) {};
					\node [  set, right = 0.5 of a] (b) {};
					\node [reset, below = 0.5 of b] (c) {};
					\node [  set, left = 0.5 of c] (d) {};
					\coordinate [right = 0.25 of a] (A);
					\coordinate [below = 0.25 of b] (B);
					\coordinate [left = 0.25 of c] (C);
					\coordinate [above = 0.25 of d] (D);
					\path (c) -- node[below, yshift = -0.2cm]{Fall 13} (d);
					\draw (a) -- (b) -- (c) -- (d) -- (a);
					\draw (B) -- (C);

					\node [  set, right = 1 of b] (a) {};
					\node [  set, right = 0.5 of a] (b) {};
					\node [  set, below = 0.5 of b] (c) {};
					\node [reset, left = 0.5 of c] (d) {};
					\coordinate [right = 0.25 of a] (A);
					\coordinate [below = 0.25 of b] (B);
					\coordinate [left = 0.25 of c] (C);
					\coordinate [above = 0.25 of d] (D);
					\path (c) -- node[below, yshift = -0.2cm]{Fall 14} (d);
					\draw (a) -- (b) -- (c) -- (d) -- (a);
					\draw (C) -- (D);

					\node [  set, right = 1 of b] (a) {};
					\node [  set, right = 0.5 of a] (b) {};
					\node [  set, below = 0.5 of b] (c) {};
					\node [  set, left = 0.5 of c] (d) {};
					\coordinate [right = 0.25 of a] (A);
					\coordinate [below = 0.25 of b] (B);
					\coordinate [left = 0.25 of c] (C);
					\coordinate [above = 0.25 of d] (D);
					\path (c) -- node[below, yshift = -0.2cm]{Fall 15} (d);
					\draw (a) -- (b) -- (c) -- (d) -- (a);
				\end{tikzpicture}
				\caption{Möglichkeiten zur Isolinien-Legung in den Bildzellen beim Marching Squares Algorithmus. Dabei bedeutet ein schwarzer Pixel, dass der Pixel im Zustand "Set Pixel" ist, weiß bedeutet dementsprechend "Reset Pixel". In den Fällen \num{5} und \num{10} ist die Legung der Linie nicht eindeutig und es kann wahlweise die gestrichelte Version oder die durchgezogene Version gewählt werden.}
				\label{fig:marchingsquares}
			\end{figure}
		% end

		\subsection{3D: Marching Cubes}
			Der \emph{Marching Cubes Algorithmus} erweitert die Idee von Marching Squares auf drei Dimensionen, wobei eine Volumenzelle durch ihre acht umgebenden Voxel definiert ist. Es gibt, ohne Beachtung von Symmetrien, \num{256} verschiedene Kombination von "Set Pixel" und "Reset Pixel" Zuständen. Unter Einbeziehung von Symmetrien werden diese auf \num{15} Klassen reduziert.
		% end

		\subsection{Große Polygonmodelle und Performanz}
			Der Marching Cubes Algorithmus erzeugt sehr viele Millionen Dreiecke, was zu einem hohen Berechnungsaufwand führt. Daher muss das entstehende Mesh vor dem Rendern noch reduziert werden.

			\subsubsection{Culling von Geometrie}
				Eine Möglichkeit stellt das Cullung von Geometrie dar, bei dem unsichtbare Polygone aus der Rendering-Pipeline entfernt werden:
				\begin{itemize}
					\item Backface-Culling: Rückseiten werden nicht gezeichnet.
					\item View-Frustum-Culling: Polygone, die sich ganz oder teilweise außerhalb des View-Frustums befinden, werden nicht (oder nur teilweise) gezeichnet.
					\item Occlusion-Culling: Polygone werden nach der Tiefe sortiert und nur gerendert, wenn sie nicht vollständig verdeckt sind (Transparenz muss beachtet werden!).
					\item uvm.
				\end{itemize}
			% end

			\subsubsection{Meshreduktion}
				Bei der Meshreduktion wird die Anzahl der Polygone verringert, wobei die "Größe" der Vereinfachung stark vom Szenario abhängt (Genauigkeit \vs Zeitbegrenzung).
			% end

			\subsubsection{Mesh-Glättung}
				Das Ziel der Mesh-Glättung ist die Bereitstellung einer guten Visualisierung sowie der Artefakt-Reduzierung und Entfernung von "Löchern". Die Herausforderung hierbei ist das Volumen zu erhalten.

				Laplacian Glättung:
				\begin{itemize}
					\item Es wird eine "Regenschirm"-Region betrachtet (\dh ausgehend von einem Vertex alle durch eine Kante verbundenen Vertexe sowie deren Verbindungen).
					\item Anschließend werden hochfrequente Oberflächeninformationen reduziert.
					\item Dies sorgt für eine Reduktion von Krümmungen.
				\end{itemize}
			% end
		% end
	% end

	\section{Direkte Volumenvisualisierung}
		Im Gegensatz zur indirekten Volumenvisualisierung, bei der zunächst eine Zwischendarstellung generiert wird und die Komplexität von der Anzahl an Polygonen abhängig ist, werden bei der direkten Volumenvisualisierung die Voxel direkt ohne Zwischendarstellung visualisiert. Dabei ist die Komplexität von der Anzahl der Voxel und der Auflösung der Anzeigefläche abhängig.

		\subsection{Density Emitter Model}
			\begin{itemize}
				\item Es werden nur Emission und Absorption betrachtet.
				\item Jeder Voxel in der Datenmenge ist eine kleine Lichtquelle.
				\item Das Licht wird schwächer, wenn es durch die Volumendatenmenge wandert.
				\item Das Medium ist eine homogene Dichtewolke.
			\end{itemize}

			Dadurch wird die Bestrahlungsstärke \( I(s) \) eines Voxels \(s\) beschrieben durch:
			\begin{equation}
				I(s) = I_{s_0} \cdot \exp \Bigg\{ -\int_{s_0}^{s} \! \tau(t) \dd{t} \Bigg\} + \int_{s_0}^{s} \! Q(\tilde{s}) \cdot \exp \Bigg\{ -\int_{\tilde{s}}^{s} \tau(t) \dd{t} \Bigg\} \dd{\tilde{s}}  \label{eq:volumerendering}
			\end{equation}
			wobei \( I_{s_0} \) die Beleuchtungsstärke des Hintergrunds und \( Q(\tilde{s}) \) die aktive Emission des Voxels \( \tilde{s} \) ist. Mit \( t_i \coloneqq \exp\Big\{ -\tau(i \cdot \Delta t) \cdot \Delta t \Big\} \) kann die Volumen-Rendering-Gleichung~\eqref{eq:volumerendering} diskretisiert werden:
			\begin{equation}
				I(s) = I_0 \prod_{k = 0}^{n - 1} t_k + \sum_{k = 0}^{n - 1} \Bigg( Q(k \cdot \Delta s) \cdot \Delta s \prod_{j = k + 1}^{n - 1} t_j \Bigg)  \label{eq:volumerenderingdiscrete}
			\end{equation}
		% end

		\subsection{Volumen-Rendering-Pipeline}
			In der Volumen-Rendering-Pipeline gibt es drei grundlegende Schritte:
			\begin{enumerate}
				\item Abtastung (Sampling)
				\item Klassifizierung und Beleuchtung
				\item Komposition
			\end{enumerate}
			welche nacheinander durchlaufen werden.

			\paragraph{Abtastung}
				Es werden Voxelwerte an bestimmten Orten angesammelt, wobei die Position dieser Orte durch die Abtastdistanz \( \Delta s \) festgelegt ist (diese sollte kleiner als die Hälfte der Rasterauflösung sein, Shannons Abtasttheorem). Dabei befinden sich die Abtastpositionen meistens zwischen den Rasterpostionen.

				Anschließend werden die Werte Interpoliert, \bspw mit Nearest Neighbor, Trilinear oder B-Spline Modellen.
			% end

			\paragraph{Klassifikation und Beleuchtung}
				Für jeden Abtastpunkt wird nun der Anteil (Farbwert) \( Q_k = Q(k \cdot \Delta s) \) sowie der Abschwächungsfaktor (Transparenz) \( t_i \) bei jedem Abtastpunkt berechnet. Dadurch wird der beleuchtete Anteil berechnet, wobei Volumenabtastungen als gerichtete Lichtquellen betrachtet werden (Shading).
			% end

			\paragraph{Komposition}
				Die abgetasteten, klassifizierten und beleuchteten Objekte werden nun zusammengeführt (akkumuliert), wobei die Volumen-Rendering-Gleichung numerisch approximiert wird (Gleichung~\eqref{eq:volumerenderingdiscrete}). Diese Akkumulation wird in zwei Unterschritte geteilt:
				\begin{enumerate}
					\item Back-to-Front-Komposition
					\item Front-to-Back-Komposition
				\end{enumerate}

				\subparagraph{Back-to-Front-Komposition}
					Bei der Back-to-Front-Komposition wird mit der Abtastposition am Ende des Volumens mit der Zusammensetzung begonnen und in die Richtung des Sichtpunktes fortgefahren. Dabei wird iterativ die Farbe \( C_k \) sowie die Intensität \( T_k \) an der aktuellen Abtastposition \( k \) berechnet:
					\begin{align*}
						C_k & = Q(k \cdot \Delta s) \cdot \Delta s \\
						I_k & = I_{k - 1} \cdot t_k + C_k
					\end{align*}
				% end

				\subparagraph{Front-to-Back-Komposition}
					Es wird mit der Abtastposition am Anfang des Volumens begonnen und in Richtung des Endes fortgefahren. Dabei werden iterativ Intensität \( I_{k - 1} \) und Transparenz \( \tau_{k - 1} \) berechnet:
					\begin{align*}
						I_{n - 1}    & = C_{n - 1}              \\
						\tau_{n - 1} & = t_{n - 1}              \\
						I_{k - 1}    & = T_k + C_k \cdot \tau_k \\
						\tau_{k - 1} & = t_k \cdot \tau_k
					\end{align*}
					Es müssen immer zwei akkumulierte Werte (Intensität und Transparenz) berechnet werden, um \zB Nebel darzustellen (bei diesem sind Lichtquellen ab einem bestimmten Punkt irrelevant).

					Die Zusammensetzung wird gestoppt, sobald die akkumulierte Transparenz zu klein wird (Early Ray Termination). Dadurch müssen nicht alle Positionen entlang des Strahls betrachtet werden und die Rendering-Geschwindigkeit wird erhöht. Üblicherweise ist dies durch ein Transparenz-Threshold definiert.
				% end
			% end

			\subsubsection{Transferfunktion}
				Die \emph{Transferfunktion} bildet gemessene und abgetastete Werte auf optische Eigenschaften ab (der Farbwert \(Q\) durch eine Farbtransferfunktion und die Transparenz \(t\) durch eine Opazitätstransferfunktion). Dabei bilden die Voxelwerte den Definitionsbereich und die optischen Eigenschaften den Bildbereich einer eindimensionalen Transferfunktion:
				\begin{equation*}
					\textit{tf}_i : V \to O_i
				\end{equation*}

				Bei Grauwertabbildungen werden nur Grauwerte betrachtet und die Intensität wird gemäß den Abtastwerten zugewiesen (niedrigster Wert schwarz, höchster Wert weiß, die Werte dazwischen werden linear interpoliert). Dunklen Werten wird dabei eine geringe Opazität (Transparenz) zugewiesen.

				Spezifikation:
				\begin{itemize}
					\item Es kann \zB ein Histogramm verwendet werden.
					\item Die Referenzeinstellungen für Opazität und Farbe werden für eine begrenzte Anzahl von Werten im Definitionsbereich spezifiziert.
					\item Zwischen den Werten wird dann linear interpoliert.
					\item Oftmals werden Höchstwerte gewählt, um die Opazität zu erhöhen und die Farbe zu ändern.
					\item Meistens sind fotorealistische Renderings das Ziel.
					\item Den verschiedenen Strukturen werden demnach "echte" Farben zugewiesen.
					\item Problem: Zwei Strukturen mit dem gleichen Grauwert haben \mglw verschiedene echte Farben.
				\end{itemize}
			% end
		% end
	% end
% end

\chapter{Szenengraphen am Beispiel X3DOM}
	Zur Beschreibung einer 3D-Szene werden viele Informationen benötigt:
	\begin{itemize}
		\item Objekt-Geometrie (Säule, Ball, \dots)
		\item Transformationen (Positionierung einer Säule, Rotation, \dots)
		\item Materialien (Farbe, Textur, \dots)
		\item Kameras (Ansichten, Kontrolle der Kamera, \dots)
		\item Lichter (Art der Lichtquelle, Farben, \dots)
		\item Spezialeffekte (Nebel, Schatten, Skyboxes, \dots)
		\item uvm.
	\end{itemize}
	Dabei haben die Daten untereinander komplexe Beziehungen, \zB verwenden verschiedene Objekte das gleiche Material, das gleiche Objekt ist an mehreren Orten instantiiert, Objekte werden gruppiert, uvm.. \emph{Szenengraphen} strukturieren diese Informationen.

	\section{Szenengraph}
		Ein Szenengraph ist ein gerichteter, azyklischer Graph mit einem Wurzelknoten, der die gesamte Szene "zusammenhält". Zum Rendering wird der Graph durchlaufen (traversiert):
		\begin{itemize}
			\item Das Durchlaufen startet an der Wurzel und jeder Kindknoten wird rekursiv abgearbeitet.
			\item Wurde der gesamte Graph traversiert, ist das Bild fertig (da keine Zyklen erlaubt sind, geschieht dies immer).
			\item Die konkreten Operationen hängen vom Typ des jeweiligen Knotens ab:
				\begin{itemize}
					\item Gruppierungen: Ist die aktuelle Gruppe eingeschaltet, traversiere die Kindknoten. Wenn nicht, gehe zum nächsten Knoten.
					\item Transformationen: Anwendung der hinterlegten Transformationsmatrix auf alle Kindknoten.
					\item Objektdaten: Ein tatsächliches Objekt, welches gezeichnet wird (unter Anwendung aller vorherigen Transformationen).
				\end{itemize}
			\item Während der Traversierung werden die verschiedenen Zustände jeweils aktualisiert.
		\end{itemize}
		Dieses Konzept hat viele Vorteile:
		\begin{itemize}
			\item Bereits definierte Objekte können wiederverwendet werden.
			\item Objektdaten sind semantisch gruppierbar.
			\item Die Transformationshierarchie ermöglicht die Transformation kompletter Gruppen, ohne diese explizit ändern zu müssen.
		\end{itemize}
	% end

	\section{X3DOM}
		X3DOM ist eine deklarative Szenengraph-API auf Basis von X3D im DOM:
		\begin{itemize}
			\item Deklarativ: Der Szenengraph wird durch ein strukturiertes Textformat (\zB XML) beschrieben.
			\item X3D: Szenengraph-Standard, XML-basiert, Nachfolger von VRML; Benötigt traditionell sogenannte X3D-Player.
			\item DOM: HTML Document Object Modell; Die Dokumenten-Baumstruktur und API in HTML/JavaScript.
		\end{itemize}
		X3DOM erlaubt dabei die Verwendung von X3D im DOM durch eine reine Implementierung in JavaScript (dadurch ist kein gesonderter X3D-Player nötig), wobei als Rendering-Backend \zB WebGL verwendet wird. Bisher gibt es jedoch noch keine native Implementierung in den Browsern.
	% end
% end

\chapter{Informationsvisualisierung}
	Bei der Informationsvisualisierung werden textuelle (oder andere) Daten visuell dargestellt, da das Gehirn im Allgemeinen visuelle Informationen besser als textuelle Daten erfassen, darin Muster erkennen und diese bewerten kann.

	\section{Informationsdesign}
		Abbildung~\ref{fig:referencemodelcard} zeigt das Referenzmodell von Card zum Prozess der Visualisierung von Daten bis hin zur eigentlichen Visualisierung. Gute Visualisierungen sollten dabei:
		\begin{itemize}
			\item Die Daten nicht verzerren und Informationen beibehalten.
			\item Typische Fehler vermeiden:
				\begin{itemize}
					\item Falsche/irreführende Skalierung
					\item Verzerrung, Größenverhältnisse, Farben
					\item Zu volle Darstellung
					\item Keine Legende
				\end{itemize}
		\end{itemize}

		\begin{figure}
			\centering
			\begin{tikzpicture}[->, every node/.style = { align = center }, block/.style = { draw, rectangle, minimum width = 2.2cm, minimum height = 1.3cm }]
				\node [block] (a) {Raw \\ Data};
				\node [block, right = 1.5 of a] (b) {Data \\ Tables};
				\node [block, right = 1.5 of b] (c) {Visual \\ Structures};
				\node [block, right = 1.5 of c] (d) {View};
				\node [right = 0.5 of d] (e) {Human};

				\draw (a) -- node[below, minimum height = 1.2cm, yshift = -0.7cm](A){Data \\ Transformations} (b);
				\draw (b) -- node[below, minimum height = 1.2cm, yshift = -0.7cm](B){Visual \\ Mappings} (c);
				\draw (c) -- node[below, minimum height = 1.2cm, yshift = -0.7cm](C){View \\ Transformations} (d);

				\coordinate [below = 0.7 of A] (X);
				\coordinate [below = 0.7 of B] (Y);
				\coordinate [below = 0.7 of C] (Z);

				\draw (e) |- (X) -- (A);
				\draw (e) |- (Y) -- (B);
				\draw (e) |- (Z) -- (C);

				\draw (b) to[out = 160, in = 140, looseness = 4] (b);
				\draw (c) to[out = 160, in = 140, looseness = 4] (c);
				\draw (d) to[out = 160, in = 140, looseness = 4] (d);
			\end{tikzpicture}
			\caption{Referenzmodell von Card}
			\label{fig:referencemodelcard}
		\end{figure}
	% end

	\section{Datentypen}
		\subsection{1D-Daten, Zeitreihen}
			\begin{itemize}
				\item 1D-Daten haben nur eine Spalte mit Datenwerten (\zB Datenwerte über Objekte oder Verteilungen).
				\item Zeitreihen sind dabei spezielle 1D-Daten, da jeweils ein Wert einer Zeit zugeordnet wird.
			\end{itemize}
		% end

		\subsection{2D-Daten}
			\begin{itemize}
				\item Zwei Spalten mit Datenwerten, \zB mehrere Datenwerte über Objekte.
			\end{itemize}
		% end

		\subsection{mD-Daten (multidimensional)}
			\begin{itemize}
				\item Daten mit drei oder mehr Spalten.
				\item Auch "multivariate Daten" genannt.
				\item Zum Beispiel Befragungsdaten, Objektbeschreibungen, \dots
			\end{itemize}
		% end

		\subsection{Hierarchien}
			\begin{itemize}
				\item Hierarchien sind ein natürlicher Weg, Daten zu strukturieren, \bspw Dateisystem, Unternehmensstruktur, \dots.
			\end{itemize}
		% end

		\subsection{Graphen/Netzwerke}
			\begin{itemize}
				\item Graphen sind (nicht zwangsweise gerichtete) Beziehungen zwischen einzelnen Knoten.
			\end{itemize}
		% end
	% end

	\section{Kuchendiagramm (1D)}
		\begin{itemize}
			\item Ein Kuchendiagramm ist beliebt für Anteildaten.
			\item Werte werden als Größe abgebildet.
			\item Objekte als Farben.
			\item Problem: Kleine Wertunterschiede sind schlecht sichtbar.
		\end{itemize}
	% end

	\section{Balkendiagramm (1D)}
		\begin{itemize}
			\item Werte werden als Größe abgebildet.
			\item Gut geeignet, um Werte zu vergleichen und abzubilden.
		\end{itemize}
	% end

	\section{Liniendiagramm (Zeitreihe)}
		\begin{itemize}
			\item Gut zur Visualisierung zeitbezogener Daten.
			\item Werte werden auf die \(y\)-Positionen abgebildet und mit Linien verbunden.
			\item Problem: Viele Zeitreihen
				\begin{itemize}
					\item Bei vielen Daten ist nicht mehr erkennbar, wo welche Linie entlang geht.
					\item Lösung: Wertfilter (nur bestimmte Linien anzeigen).
				\end{itemize}
			\item Problem: Länge bei langen Zeitreihen
		\end{itemize}
	% end

	\section{Scatterplot (2D, 3D)}
		\begin{itemize}
			\item Daten werden abgebildet auf \(x/y\)-Positionen, Größe, Farbe, \dots
			\item Es können also viele Dimensionen abgebildet werden.
			\item Ein Scatterplot ist intuitiv und leicht lesbar.
			\item Problem: Overplotting; Es überschneiden sich zu viele Punkte.
		\end{itemize}
	% end

	\section{Scatterplotmatrix (nD)}
		\begin{itemize}
			\item Alle Paare der Datenspalten werden jeweils einem Scatterplot zugeordnet (was eine Matrix an Scatterplots bildet).
			\item Dies ist gut für paarweise Korrelationen und Abhängigkeiten.
			\item Sehr beliebt.
			\item Probleme:
				\begin{itemize}
					\item Viele Dimensionen sind unübersichtlich.
					\item Limitierter Platz für jeden einzelnen Scatterplot.
					\item Nur paarweise Abhängigkeiten sichtbar.
				\end{itemize}
		\end{itemize}
	% end

	\section{Parallele Koordinaten (3D, nD)}
		\begin{itemize}
			\item Die Koordinatenachsen der jeweiligen Eigenschaften werden parallel zueinander angeordnet.
			\item Aus einem Punkt im Scatterplot wird dann eine Linie in parallelen Koordinaten.
			\item Dadurch sind Abhängigkeiten über mehrere Dimensionen erkennbar.
			\item Die Achsenanordnung ist wichtig! Schneiden sich an einer Achse zwei oder mehr Linien, so können diese danach nicht mehr unterschieden werden. Dies kann durch Umsortierung gelöst werden.
			\item Problem: Overplotting
				\begin{itemize}
					\item Zu viele Linien sind unübersichtlich.
					\item Lösung: Filter
				\end{itemize}
			\item Problem: Viele Dimensionen; Bei vielen Dimensionen wird der Plot zu lang.
		\end{itemize}
	% end

	\section{Node-Link-Diagramm (Hierarchien, Graphen)}
		\begin{itemize}
			\item Jeder Datenpunkt wird ein Knoten.
			\item Jede Eltern-Kind-Beziehung wird durch eine Kante dargestellt.
			\item Dabei ist die Position der Knoten wichtig.
			\item Problem: Layout
				\begin{itemize}
					\item Kanten sollten sich möglichst wenig überschneiden.
					\item Bei Hierarchien sollten Elemente einer Ebene auf einer Ebene dargestellt werden.
					\item Begrenzter Platz und Lesbarkeit.
				\end{itemize}
			\item Problem: Viele Knoten; Große Hierarchien/Graphen machen den Graph unübersichtlich.
		\end{itemize}
	% end

	\section{Treemap (Hierarchien)}
		\begin{itemize}
			\item Ein Kasten mit einer fixen Größe wird rekursiv aufgeteilt:
				\begin{itemize}
					\item Beginnend mit der Wurzel.
					\item Der Kasten wird anhand der Teilbaumgröße geteilt.
					\item Es wird zwischen horizontaler und vertikaler Aufteilung iteriert.
				\end{itemize}
			\item Problem: Überlappung; Eine Überlappung der Elemente führt zu schlechter Lesbarkeit.
			\item Problem: Größendarstellung; Der Vergleich der Größen von Rechtecken mit unterschiedlichen Dimensionen ist schwierig.
		\end{itemize}
	% end

	\section{Zusammenfassung}
		\begin{table}[b]
			\centering
			\begin{tabular}{l|l}
				\textbf{Datentyp}     & \textbf{Visualisierungstechniken}        \\ \hline
				1D                    & Balkendiagramm, Kuchendiagramm           \\
				Zeitreihe             & Liniendiagramm                           \\
				2D                    & Scatterplot                              \\
				3D                    & Scatterplot+, Parallele Koordinaten      \\
				nD, wenig Dimensionen & Scatterplotmatrix, Parallele Koordinaten \\
				Hierarchien           & Node-Link-Diagramm, Treemap              \\
				Graphen/Netzwerke     & Node-Link-Diagramm
			\end{tabular}
		\end{table}
	% end
% end

\chapter{Farbe}
	Farbe ist ein wichtiger Bestandteil bei der Aufnahme, Verarbeitung, Kommunikation und Reproduktion von Bildern. Um Empfindungen aufzunehmen, zu verarbeiten und zu reproduzieren wird ein gutes Modell des menschlichen visuellen Systems, von Aufnahmegeräten und Wiedergabegeräten benötigt. Außerdem werden Algorithmen zur Transformation von Wahrnehmungskorrelaten in Ansteuerungswerte von Wiedergabegeräten, und zur Transformation der Geräteantworten in Wahrnehmungskorrelate, benötigt.

	\section{Dimensionalität und Farbattribute}
		Die Farbwahrnehmung hat dabei fünf Dimensionen:
		\begin{itemize}
			\item Helligkeit (Brightness) \\ Attribut der Farbwahrnehmung, nach dem eine Fläche mehr oder weniger Licht auszustrahlen scheint.
			\item Relative Helligkeit (Lightness) \\ Die Helligkeit einer Fläche relativ zur Helligkeit einer gleich beleuchteten Fläche, die weiß erscheint (nur für bezogene Farben).
			\item Farbton (Hue) \\ Attribut, nach dem eine Fläche gleich Rot, Gelb, Grün oder Blau oder einer Kombination von zwei von ihnen erscheint. Eine \emph{achromatische Farbe} ist eine wahrgenommene Farbe ohne Farbton (\zB Schwarz oder Weiß).
			\item Farbigkeit (Colorfulness) \\ Attribut, nach dem eine Fläche mehr oder weniger farbig empfunden wird.
			\item Buntheit (Chroma) \\ Farbigkeit einer Fläche relativ zur Helligkeit einer gleich beleuchteten Fläche, die weiß erscheint (nur für bezogene Farben).
		\end{itemize}
		Die Sättigung kann durch Helligkeit und Farbigkeit beschrieben werden:
		\begin{equation*}
			\text{Sättigung} = \frac{\text{Farbigkeit}}{\text{Helligkeit}} = \frac{\text{Buntheit}}{\text{Relative Helligkeit}}
		\end{equation*}

		Eine \emph{bezogene Farbe} wird dabei in Bezug zu anderen Farben wahrgenommen, \emph{unbezogene Farben} werden isoliert wahrgenommen.

		Buntheit und relative Helligkeit können wie folgt berechnet werden:
		\begin{equation*}
			\text{Buntheit} = \frac{\text{Farbigkeit}}{\text{Helligkeit von Weiß}} \quad\quad\quad\quad \text{Relative Helligkeit} = \frac{\text{Helligkeit}}{\text{Helligkeit von Weiß}}
		\end{equation*}
	% end

	\section{Berechnung von Farbattributen}
		\subsection{Das Auge} % 12.15, 12.16, 12.17
			\todo{Farbe: Auge}
		% end

		\subsection{Spektrale Charakterisierung des Auges}
			Das Auge lässt sich durch
			\begin{equation*}
				y_i = \int_{\Lambda} \! x(\lambda) s_i(\lambda) \dd{\lambda},\quad i = 1, 2, \cdots
			\end{equation*}
			mit der \(i\)-ten Farbfrequenz \(y_i\), dem Farbreiz \(x(\lambda)\), der \(i\)-ten Spektralempfindlichkeit \(s_i(\lambda)\) und dem Sensitivitätsbereich \( \Lambda = [\SI{380}{\nano\meter}, \SI{730}{\nano\meter}] \) modellieren.

			Wird angenommen, dass es nur drei Fotorezeptoren gibt und das Auge ein lineares System ist, so kann das Modell mit
			\begin{equation*}
				\vec{y} = \mat{S} \vec{x}
			\end{equation*}
			diskretisiert werden (mit dem 3-dimensionalen Farbvalenzvektor \(\vec{y}\), dem \(N\)-dimensionalen Farbreizvektor \(\vec{x}\) und der \( (3 \times N) \)-dimensionalen Systemmatrix \( \mat{S} \)).

			Die Systemmatrix \(S\) kann dabei \zB durch ein Farbabgleichsexperiment gewonnen werden. Dabei sollen Probanden das Licht dreier linear unabhängiger Lichtquellen \( \vec{p}_1, \vec{p}_2, \vec{p}_3 \in \R^N \) durch Verstellung der Intensität \( \vec{a}_1, \vec{a}_2, \vec{a}_3 \in \R \) mit einer Testlichtquelle \( \vec{f} \in \R^N = \const \) abgleichen. Dieser Zusammenhang wird durch
			\begin{equation*}
				\mat{S} \vec{f} = \mat{S} \mat{P} \vec{a}
			\end{equation*}
			beschrieben, wobei \( \mat{P} = \begin{bmatrix} \vec{p}_1 & \vec{p}_2 & \vec{p}_3 \end{bmatrix} \) und \( a = \begin{bmatrix} a_1 & a_2 & a_3 \end{bmatrix}^T \) gelte. Ein Abgleich mit \(N\) monochromatischen Lichtquellen \( \vec{f} = \vec{e}_i \), \( i = 1, \cdots, N \) und abspeichern aller Einstellungen \( \vec{a}^i = \begin{bmatrix} a_1^i & a_2^i & a_3^i \end{bmatrix}^T \) resultiert mit einer Matrix \( \mat{A} = \begin{bmatrix} \vec{a}^1 & \cdots & \vec{a}^N \end{bmatrix} \) in der Gleichung
			\begin{equation*}
				\mat{S} \mat{I}_N = \mat{S} = \mat{S} \mat{P} \mat{A} \quad\iff\quad A = ( \mat{S} \mat{P} )^{-1} \mat{S}
			\end{equation*}
			wobei \( \mat{A} \) die \emph{Spektralwertmatrix} für dir Primärlichtarten \( \mat{P} \) ist. Dadurch kann die Matrix \( \mat{S} \) bestimmt werden.
		% end

		\subsection{Spektralwertfunktion}
			Erscheinen zwei Farbreize \( g, f \) gleich, so gilt:
			\begin{equation*}
				\mat{S} \vec{g} = \mat{S} \vec{f} \quad\iff\quad (\mat{S}\mat{P})^{-1} \mat{S} \vec{g} = (\mat{S} \mat{P})^{-1} \mat{S} \vec{f} \quad\iff\quad \mat{A} \vec{g} = \mat{A} \vec{f}
			\end{equation*}
			Die Zeilen von \( \mat{A} \) heißen \emph{Spektralwertfunktionen} (Color Matching Functions, CMF). Durch die Multiplikation mit einer regulären \( (3 \times 3) \)-Matrix werden neue CMFs erzeugt: \( \mat{B} = \mat{M} \mat{A} \).

			Hilfreiche Eigenschaften von CMFs sind:
			\begin{itemize}
				\item Die Funktionen sind positiv.
				\item Eine CMF ist die Helligkeitsempfindlichkeitsfunktion \( V(\lambda) \).
				\item Viele Nullen (dadurch einfache Berechnungen).
			\end{itemize}
		% end

		\subsection{Cone Fundamentals}
			\emph{Cone Fundamentals} sind die spektralen Empfindlichkeiten der Zapfen multipliziert mit der Transmission der Augenoptik.
		% end
	% end

	\section{Objektfarben, Lichtmatrix und CIEXYZ-Farbraum}
		Für eine Spektralwertmatrix \( \mat{A} \) wird die reflektionsbasierte Farbvalenz \( \vec{y} \) wie folgt berechnet:
		\begin{equation*}
			\vec{y} = \underbrace{\mat{A} \text{diag}(\vec{l})}_{L \coloneqq} \vec{r} = \mat{L} \vec{r}
		\end{equation*}
		Dabei ist
		\begin{itemize}
			\item \( \vec{l} \in \R^N \): Spektrale Dichteverteilung der Lichtart.
			\item \( \vec{r} \in \R^N \): Reflektionsspektrum
		\end{itemize}
		Die Matrix \( \mat{L} \) wird \emph{Lichtmatrix} genannt.

		Die \emph{CIEXYZ-Farbkoordinaten} (Farbvalenzen) sind definiert als
		\begin{equation*}
			\begin{bmatrix}
				X \\
				Y \\
				Z
			\end{bmatrix}
			=
			\frac{100}{\bar{\vec{y}}^T \vec{l}} \mat{L} \vec{r}
		\end{equation*}
		wobei \( \mat{L} = \begin{bmatrix} \bar{\vec{x}} & \bar{\vec{y}} & \bar{\vec{z}} \end{bmatrix}^T \) die Lichtmatrix und \( \vec{r} \) das Reflektionszentrum ist. Diese Farbwerte heißen \emph{Normfarbwerte}.

		\subsection{Das CIE Chromaticity Diagramm}
			Chromaticity-Koordinaten sind gegeben durch:
			\begin{equation*}
				x = \frac{X}{X + Y + Z} \quad\quad y = \frac{Y}{X + Y + Z} \quad\quad z = \frac{Z}{X + Y + Z}
			\end{equation*}
			Die \(x\)-\(y\)-Koordinaten der monochromatischen Farbreize erzeugen dabei die Spektralfarblinie. Die restlichen \(x\)-\(y\)-Chromaticity-Koordinaten mit einer additiven Mischung aus Farbreizen liegen alle innerhalb der konvexen Hülle der Chromaticity-Koordinaten der monochromatischen Farbreize (\dh der Spektralfarblinie).

			Der CIEXYZ-Farbraum ist dabei nicht wahrnehmungsgleichabständig, \dh der wahrgenommene Farbabstand \( \Delta V (\vec{q}, \vec{p}) \) ist nicht immer gleich dem Abstand im CIEXYZ-Farbraum:
			\begin{equation*}
				\exists \vec{q}, \vec{p} \in \text{CIEXYZ} : \big( \Delta V(\vec{q}, \vec{p}) \neq \lVert \vec{q} - \vec{p} \rVert_2 \big)
			\end{equation*}
			Außerdem modelliert das CIEXYZ-Modell nur die erste Phase des Farbsehens und erlaubt keinen einfachen Zugriff auf Farbattribute.
		% end
	% end

	\section{Metamerie}
		\begin{itemize}
			\item Metamerie: \\ Zwei Farbreize \( \vec{g} \neq \vec{f} \) heißen \emph{Metamere}, wenn \( \mat{A} \vec{g} = \mat{A} \vec{f} \) gilt.
			\item Beleuchtungsmetamerie: \\ Zwei Reflektionsspektren \( \vec{r}_1 \neq \vec{r}_2 \) heißen \emph{metamere Reflektionsspektren} unter der Lichtart \( \vec{l} \), wenn \( \mat{L} \vec{r}_1 = \mat{L} \vec{r}_2 \) gilt.
			\item Beobachtermetamerie: \\ Zwei Farbreize \( \vec{g} \neq \vec{f} \) erzeugen bei gleichen Betrachtungsbedingungen für eine Person die gleichen, für eine andere Person unterschiedliche Farbvalenzen, \dh \( \mat{A}_1 \vec{g} = \mat{A}_2 \vec{f} \), aber \( \mat{A}_2 \vec{g} = \mat{A}_2 \vec{f} \), wobei \( \mat{A}_i \) die Spektralwertmatrix für Person \(i\) ist.
				\begin{itemize}
					\item Einflussfaktoren sind \zB die Dichte der Linse, verursacht durch Alter, Ernährung, Ethnie, Rauchen, \dots
					\item Außerdem variiert die maximale spektrale Empfindlichkeit zwischen Personen (um bis zu \SI{9}{\nano\meter}).
				\end{itemize}
		\end{itemize}
	% end

	\section{Gegenfarbtheorie}
		Nach der Gegenfarbtheorie sind die Gegenfarben gegeben durch
		\begin{equation*}
			\begin{bmatrix}
				L \\
				M \\
				S
			\end{bmatrix}
			=
			\begin{bmatrix}
				\vec{l}^T \\
				\vec{m}^T \\
				\vec{s}^T
			\end{bmatrix}
			\vec{f}
		\end{equation*}
		wobei \( \vec{l}, \vec{m}, \vec{s} \) die Cone Fundamentals darstellen und \(\vec{f}\) ein Farbreiz ist.
	% end

	\section{Stevenssche Potenzfunktion}
		Die \emph{Stevenssche Potenzfunktion}
		\begin{equation*}
			\psi = k(I - I_0)^n
		\end{equation*}
		beschreibt die Beziehung der Reizstärke zur Empfindungsstärke, wobei:
		\begin{itemize}
			\item \(\psi\): Wahrnehmungskorrelation
			\item \(I\): Reiz
			\item \(I_0\): Letzter gerade noch wahrnehmbarer Reiz
			\item \(k\): Anpassbare Verstärkung
			\item \(n\): Anpassbarer Exponent
		\end{itemize}
	% end

	\section{CIELAB Farbraum}
		Zur Umwandlung vom CIEXYZ-Farbraum in den CIELAB-Farbraum sind folgende Eingabedaten nötig:
		\begin{itemize}
			\item XYZ-Farbvalenz: \( (X, Y, Z) \)
			\item XYZ-Farbvalenz des Weißpunkts: \( (X_n, Y_n, Z_n) \)
		\end{itemize}
		Für die Umrechnung gilt dann:
		\begin{align*}
			L^\ast    & = 116 f(Y / Y_n) - 16                     \\
			a^\ast    & = 500 \big( f(X / X_n) - f(Y / Y_n) \big) \\
			b^\ast    & = 200 \big( f(Y / Y_n) - f(Z / Z_n) \big) \\
			f(\omega) & =
			\begin{cases}
				\sqrt[3]{\omega}     & \omega > 0.008856    \\
				7.787\omega + 16/116 & \omega \leq 0.008856
			\end{cases}
		\end{align*}
		wobei \( L^\ast \) die relative Helligkeit, \( a^\ast \) die Rot-Grün Komponente und \( b^\ast \) die Blau-Gelb Komponente darstellt.

		Eigenschaften des CIELAB-Farbraums:
		\begin{itemize}
			\item Gegenfarbraum
			\item Es werden Nichtlinearitäten des visuellen Systems modelliert.
			\item Nahezu wahrnehmungsgleichabständig, \dh \( \forall \vec{q}, \vec{p} \in \text{CIELAB} : \big( \Delta V (\vec{q}, \vec{p}) \approx \rVert \vec{q} - \vec{p} \lVert_2 \big) \).
		\end{itemize}

		Durch die zylindrische Darstellung des CIELAB-Farbraums lässt sich der LCh-Farbraum ableiten mit der Buntheit \( C_{ab}^\ast = \sqrt{\big(a^\ast\big)^2 + \big(b^\ast\big)^2} \) und dem Farbton \( h_{ab} = \arctan\big( b^\ast / a^\ast \big) \).
	% end

	\section{Technische Farbräume}
		\subsection{Geräte RGB}
			Bestimmte Gräte verwenden meist eigene RGB-Standards.
		% end

		\subsection{Geräteunabhängige RGB}
			Zum Beispiel sRGB und Adobe RGB.
		% end

		\subsection{YCbCr}
			Der YCbCr-Farbraum ist ein Gegenfarbraum mit
			\begin{itemize}
				\item \(Y\): Luminanz
				\item \(C_b\): Blau-Gelb
				\item \(C_r\): Rot-Grün
			\end{itemize}
			und kann als lineare Transformation über den RGB-Farbraum definiert werden. Wird \bspw in JPEGs eingesetzt.
		% end

		\subsection{HSI/HSV/HSL}
			Der HSI/HSV/HSL-Farbraum wird definiert durch
			\begin{itemize}
				\item \(H\): Hue (Farbton)
				\item \(S\): Saturation (Sättigung)
				\item \(I\)/\(V\)/\(L\): Intensity/Value/Lightness (Helligkeit)
			\end{itemize}
			und wird \bspw zur intuitiven Farbauswahl in Grafikprogrammen oder zum Thresholding eingesetzt.
		% end

		\subsection{CMY/CMYK}
			Der CMY/CMYK-Farbraum wird definiert durch
			\begin{itemize}
				\item \(C\): Cyan
				\item \(M\): Magenta
				\item \(Y\): Yellow (Gelb)
				\item \(K\): Black (Schwarz)
			\end{itemize}
			und wird \bspw für die Ansteuerungswerte zur Tintenauswahl bei Druckern verwendet.
		% end
	% end

	\section{Komplexität von Farbe}
		\subsection{Chromatische Adaptation}
			\emph{Chromatische Adaption} beschreibt die weitgehend unabhängige Regulierung der Mechanismen beim Farbsehen. Häufig wird \zB die unabhängig Anpassung der Zapfenempfindlichkeit an den dominanten Farbreiz der Umgebung betrachtet.

			\subsubsection{Von-Kries-Modell}
				Unter den Hypothesen
				\begin{enumerate}
					\item Jeder Zapfen wird individuell adaptiert.
					\item Lineare Transformation.
				\end{enumerate}
				wird das \emph{Von-Kries-Modell} der Adaption formuliert:
				\begin{equation*}
					\begin{bmatrix}
						X_2 \\
						Y_2 \\
						Z_2
					\end{bmatrix}
					=
					K^{-1}
					\begin{bmatrix}
						L_{w, 2} / L_{w, 1} & 0                   & 0                   \\
						0                   & M_{w, 2} / M_{w, 1} & 0                   \\
						0                   & 0                   & S_{w, 2} / S_{w, 2}
					\end{bmatrix}
					K
					\begin{bmatrix}
						X_1 \\
						Y_2 \\
						Z_1
					\end{bmatrix}
				\end{equation*}
				wobei \( L_{w, 1}, M_{w, 1}, S_{w, 1} \) die Zapfenantwort bei Lichtart \( \vec{l}_1 \) und \( L_{w, 2}, M_{w, 2}, S_{w, 2} \) die Zapfenantwort bei Lichtart \(l_2\) ist sowie:
				\begin{equation*}
					\begin{bmatrix}
						L \\
						M \\
						S
					\end{bmatrix}
					=
					\underbrace{
						\begin{bmatrix}
							0.3897  & 0.6890 & -0.0787 \\
							-0.2298 & 1.1834 & 0.0464  \\
							0.0000  & 0.0000 & 1.0000
						\end{bmatrix}
					}_{K \coloneqq}
					\begin{bmatrix}
						X \\
						Y \\
						Z
					\end{bmatrix}
					= K
					\begin{bmatrix}
						X \\
						Y \\
						Z
					\end{bmatrix}
				\end{equation*}
				Dieses Modell weist jedoch kleine Fehler von der tatsächlichen Adaption ab, primär da die Zapfenregulierung nicht der einzige Mechanismus der Adaption ist. Eine Optimierung der Matrix \(K\) kann führt aber zu spitzeren Zapfenantworten und besserer Vorhersagegenauigkeit.
			% end
		% end

		\subsection{Farbwahrnehmungsphänomene}
			\subsubsection{Simultankontrast}
				Der Hintergrund, auf dem eine Farbe wahrgenommen wird, beeinflusst die wahrgenommene Farbe. Dabei folgt die Farbverschiebung der Gegenfarbtheorie:
				\begin{itemize}
					\item Ein heller Hintergrund induziert dunklere Farbe und umgekehrt.
					\item Rot induziert Grün und Grün induziert Rot.
					\item Blau induziert Gelb und Gelb induziert Blau.
				\end{itemize}
			% end

			\subsubsection{Crispening Effekt}
				Der wahrgenommene Farbunterschied zweier Farbreize wird durch einen ähnlichen Hintergrund vergrößert.
			% end

			\subsubsection{Stevens Effekt}
				Bei steigender Leuchtdichte sehen dunkle Farben dunkler und helle Farben heller aus.
			% end

			\subsubsection{Hunt Effekt}
				Die Farbigkeit steigt mit der Leuchtdichte.
			% end
		% end

		\subsection{Farbwahrnehmungsmodelle}
			Da gleiche Farbreize unterschiedlichen Farben entsprechen können, werden akkurate Farbwahrnehmungsmodelle benötigt, um die Farbreize für den Farbabgleich bei unterschiedlichen Betrachtungsbedingungen anzupassen.

			\subsubsection{CIECAM02}
				Das \emph{CIECAM02-Farbwahrnehmungsmodell} ist ein invertierbares Modell, welches eine vorhersage über den empfundenen Farbabstand zulässt. Dabei gibt es von den Betrachtungsbedingungen abhängige Nichtlinearitäten zur Relation der Leuchtdichte mit der (relativen) Helligkeit.

				Vorgehen zur Farbanpassung:
				\begin{enumerate}
					\item Berechnung der Farbattribute mit dem CIECAM02-Modell für die erste Betrachtungsbedingung.
					\item Berechnung der CIEXYZ-Werte mit dem inversen CIECAM02-Modell für die zweite Betrachtungsbedingung.
				\end{enumerate}
			% end
		% end

		\newpage
		\subsection{Kontrastsensitivität}
			Die achromatische Kontrastsensitivität ist größer bei höheren Ortsfrequenzen.

			Aufgrund der Kontrastsensitivität sind zusätzlich zu Farbwahrnehmungsmodellen auch Bildwahrnehmungsmodelle nötigt. Beispiele sind:
			\begin{itemize}
				\item S-CIELAB (Spacial-CIELAB)
				\item iCAM (Image Color Appearance Model)
			\end{itemize}
		% end
	% end
% end

\chapter{User Interfaces}
	\section{Interaktion}
		\subsection{Möglichkeiten}
			Es gibt viele verschiedene Möglichkeiten zur Interaktion:
			\begin{itemize}
				\item Kommandozeile
				\item Menüs
				\item Formulare
				\item Fragen und Antworten
				\item Direkte Manipulation
				\item 3D-Umgebung
				\item Natürliche Sprache
				\item Gesten
			\end{itemize}

			\subsubsection{Kommandozeile}
				\begin{itemize}
					\item Schnell und mächtig.
					\item Viele Befehle sind Abkürzungen und meistens schnell und sehr effizient.
					\item Befehle können gleichzeitig auf mehrere Objekte angewandt werden.
					\item Einige Befehle haben viele Parameter, die präzise und flexibel eingesetzt werden können.
				\end{itemize}
			% end

			\subsubsection{Menüs}
				\begin{itemize}
					\item Menübasierte Interfaces konfrontieren den Nutzer mit sequentiellen, hierarchischen Menüs, die Listen von Funktionen enthalten.
					\item Die Bedienung erfolgt per Maus oder Pfeiltasten zur Funktionswahl (alternativ Nummerntasten und weitere Tastenkürzel).
					\item Vorteile:
						\begin{itemize}
							\item Es müssen keine Befehle auswendig gelernt werden.
							\item Es werden alle Optionen angezeigt.
							\item Die Liste ist beschränkt.
							\item Geeignet für kleine Bildschirme.
						\end{itemize}
				\end{itemize}
			% end

			\subsubsection{Formulare}
				\begin{itemize}
					\item Ähnlich zu Menüs werden Informationen Bildschirmweise angezeigt.
					\item Im Gegensatz zu Menüs werden die Informationen linear verarbeitet, \dh es gibt keine Hierarchie.
					\item Die Nutzer müssen verstehen, welche Daten in welchem Format benötigt werden.
				\end{itemize}
			% end

			\subsubsection{Fragen und Antworten}
				\begin{itemize}
					\item Fragen und Antwort Verfahren werden auch "Wizards" genannt.
					\item Sie schränken Experten ein, sind aber leicht verständlich für Anfänger.
					\item Eventuell bieten sie jedoch nicht die benötigten Informationen.
				\end{itemize}
			% end

			\subsubsection{Direkte Manipulation}
				\begin{itemize}
					\item Objekte und Aktionen sollten mit passenden visuellen Metaphern (Diskette zum Speichern, Sanduhr zum Warten, \dots) dargestellt werden.
					\item Die Ausführung von Aktionen geschieht durch "physische" Aktionen wie das Drücken von Knöpfen.
				\end{itemize}
			% end

			\subsubsection{3D-Umgebungen}
				\begin{itemize}
					\item 3D-Interaktionen sind in der Realität natürlich und auch in Spielen oft anzutreffen.
					\item Jedoch benötigen komplexe 3D-Umgebungen viel Rechenleistung.
					\item Heutige GUIs sind größtenteils 2D.
					\item 3D-Umgebungen sind schwer zu bedienen (3D-Interaktion).
				\end{itemize}
			% end

			\subsubsection{Natürliche Sprache}
				Natürliche (Alltags-)~Sprache zur Interaktion wird häufig in der Science Fiction verwendet. Aktuelle Problematiken dabei sind:
				\begin{itemize}
					\item Mehrdeutigkeit
					\item Kontextabhängigkeit
					\item Abhängigkeit von visuellen Informationen
				\end{itemize}
				Dabei gibt es zwei zentrale Forschungsgebiete: Spracherkennung und Semantik.
			% end

			\subsubsection{Gesten}
				Zur Gestenerkennung und -interaktion werden Bewegungen (Zeigen, Wischen, \dots) aufgezeichnet, detektiert und mit Funktionen assoziiert.
			% end
		% end

		\subsection{Designprozess}
			Der Prozess des Interaktionsdesigns ist ein iterativer Prozess, wobei nach einem Schritt vorwärts auch mal zurück gegangen werden muss, der "Pfad des Wissens" sich jedoch konstant vorwärts bewegt.

			Das Ziel von \emph{User Centered Design} (UCD) ist es, ein Framework zu entwickeln, das Interaktionsdesignern ermöglicht, besser nutzbare Systeme zu entwickeln. Das Fundament des UCD liegt dabei auf dem frühen Fokus auf Nutzer und deren Aufgaben, eine andauernde Bewertung bezogen auf Erlernbarkeit und Benutzbarkeit und ein iteratives Design.

			Siehe hierzu auch \href{https://fabian.damken.net/summaries/cs/mandatory/fs3/se/}{Software Engineering Zusammenfassung}.

			\subsubsection{Wasserfallmodell}
				Bei dem Wasserfallmodell wird zunächst alles entwickelt und anschließend bewertet. Dies ist nicht benutzerzentrisch und nur für kleine Projekte sinnvoll.
			% end

			\subsubsection{Spiralmodell}
				Das Spiralmodell ist
				\begin{itemize}
					\item Flexibler
					\item Fokussiert auf Risikominimierung
					\item Ermutigt zu Iterationen
					\item Beginnt mit Vorschlagen von Werten
					\item Das Projekt wird in Unterprojekte zerlegt, konkrete Risiken werden identifiziert.
				\end{itemize}
			% end

			\subsubsection{V-Modell}
				Bei dem Spiralmodell wird der Entwicklungsprozess aus technischer Sicht beschrieben und Prozesse werden als Folge von Aktivitäten modelliert, die "Produkte" erstellen oder "Produkte" verarbeiten. Produkte sind dabei sämtliche Ergebnisse einer Aktivität.
			% end

			\subsubsection{Dynamic Systems Development Method (DSDM)}
				\begin{itemize}
					\item Zeitfenster und Ressourcen sind fest.
					\item Funktionale Anforderungen sind flexibel.
					\item Drei Schritte:
						\begin{itemize}
							\item Pro-Projekt, Machbarkeitsstudie und BWL-Phasen
							\item Iterationen zwischen der funktionalen Modelliteration, Design- und Builditeration und Implementierungsphase
							\item Post-Projekt-Phase
						\end{itemize}
				\end{itemize}
			% end

			\subsubsection{Design Process Model}
				\begin{itemize}
					\item Die Designphase besteht aus zwei Teilen:
						\begin{itemize}
							\item Konzeptuelles Design: Evaluierung der Möglichkeiten, mit denen das Design den Problemen begegnen kann.
							\item Physisches Design: Evaluierung der Möglichkeiten, das konzeptuelle Design umzusetzen.
						\end{itemize}
					\item Evalutionsfragen:
						\begin{itemize}
							\item Wie können die relativen Vorzüge eines Designs gegenüber einem anderen bestimmt werden?
							\item Wie kann der Erfolg eines Designs gemessen werden?
							\item Wie können echte Nutzer dazu gebracht werden, Feedback zum Design zu geben?
							\item Wie können Usability-Tests im frühen Designprozess untergebracht werden?
						\end{itemize}
					\item Dies wird durch die Ergebnisse von formellem und informellem Usability-Testing dokumentiert.
				\end{itemize}
			% end
		% end
	% end

	\section{GUI: Benutzeroberflächen}
		\subsection{Das WIMP-Interface}
			Das \emph{WIMP-Interface} bestehen aus
			\begin{itemize}
				\item Windows (Fenstern)
				\item Icons
				\item Menus (Menüs)
				\item Pointers (Mauszeiger)
			\end{itemize}

			\subsubsection{Fenster-Komponenten}
				Die meisten Fenster-Systeme verwenden Standardfenster, die ähnlich aussehen und sich ähnlich verhalten.

				\paragraph{Multiple Document Interface (MDI)}
					Alle geöffneten Dokumente werden in einem Hauptfenster angezeigt.
				% end

				\paragraph{Single Document Interface (SDI)}
					Für jedes Dokument wird ein neues Hauptfenster geöffnet.
				% end

				\paragraph{Tabbed Document Interface (TDI)}
					Mehrere Dokumente werden in einem Hauptfenster geöffnet und als Tabs dargestellt (diese Tabs können entweder in Vollbild, also nicht überlappend, dargestellt werden, oder verschiebbar sein).
				% end
			% end

			\subsubsection{Dialogboxen}
				Dialogboxen stellen Platz für untergeordnete Funktionalitäten bereit, bspw.:
				\begin{itemize}
					\item Setzen und Verändern von Objekteigenschaften
					\item Funktionen ausführen (\zB Speichern mit Zusatzfunktionen)
					\item Prozesse ausführen (\zB Kopieren mit Fortschrittsanzeige)
					\item Aktionen bestätigen
					\item Den Benutzer informieren (meistens über Fehler\dots)
				\end{itemize}

				\paragraph{Checkboxen}
					Checkboxen repräsentieren binäre Sachverhalte ("Ja"/"Nein", "An"/"Aus", \dots).
				% end

				\paragraph{Radio Buttons}
					Radio Buttons verhalten sich ähnlich wie Checkboxen, innerhalb einer \emph{Gruppe} kann aber jeweils nur eine Radio Box ausgewählt werden.
				% end

				\paragraph{Listboxen}
					Listboxen repräsentieren eindimensionale Datenmengen, \dh die Einträge verzweigen nicht weiter. In der Regel kann festgelegt werden, ob nur ein oder mehrere Elemente selektierbar sind.
				% end

				\paragraph{Comboboxen}
					Comboboxen verhalten sich wie Listboxen, die Liste ist aber nur bei Bedarf sichtbar (Drop-Down).
				% end

				\paragraph{Spinner}
					Spinner enthalten eine beschränkte Liste an Werten, \zB Zahlen. Hier ist die Inkrementierung/Dekrementierung über Pfeile möglich.
				% end

				\paragraph{Slider}
					Slider sind Kalibrierungswerkzeuge, bei denen die Wertemenge links und rechts beschränkt ist.
				% end

				\paragraph{Weiteres}
					\begin{itemize}
						\item Toolbars
						\item Scrollbars
						\item Splitter
						\item Textboxen
						\item Buttons
					\end{itemize}
				% end
			% end
		% end

		\subsection{Menübasierte Programme}
			Menübasierte Programme sind \zB Fahrkartenautomaten.

			\subsubsection{Untermenüs}
				Ein Menü kann weitere Untermenüs enthalten, um zusammengehörige Optionen zu gruppieren.
			% end

			\subsubsection{Auswahl}
				Durch einen geschickten Menüaufbau sind Verzweigungen (if-then-else-, case-Strukturen) umsetzbar, die sich auch verschachteln lassen
			% end

			\subsubsection{Modularisierung}
				Da ein menübasiertes Programm viele Funktionen ausführen kann, sollte es in Module aufgeteilt werden, wobei für jeden ausführbaren Fall ein Modul existieren sollte.
			% end
		% end

		\subsection{GUI-Anwendungen und Event-basiertes Programmieren}
			\subsubsection{Graphical User Interfaces (GUIs)}
				Bei der Erstellung eines GUI-Programmes bleiben die meisten Schnittstellen gleich, es müssen jedoch zusätzlich GUI-Elemente gestaltet werden. Dies beinhaltet den Verlauf von Fenster zu Fenster. Mit neueren Sprachen ist die Erstellung von GUI-Programmen sehr einfach geworden.
			% end

			\subsubsection{Event-Handler}
				Event-Handler reagieren auf Ereignisse, die durch Interaktion mit der GUI ausgelöst wurden (\zB das Klicken auf einen Button). Im Gegensatz zu prozeduraler Programmierung wird der Code dann nicht mehr in prozeduraler Reihenfolge ausgeführt, sondern genau dann, wenn der Nutzer mit der GUI interagiert.
			% end
		% end
	% end

	\section{3D-Interaktion}
		3D-Interaktionen sind sehr viel komplizierter als 2D-Interaktionen, da die gewünschte Aktionen mehrdeutig sind (siehe auch Abschnitt~\ref{sec:3dinteraction}).
	% end
% end

\chapter{Multimedia Information Retrieval}
	Musik, Bilder, 3D-Modellen, Videos, \dots haben einen riesigen Informationsgehalt, der klassischerweise über textuelle Annotationen erschlossen wird. Jedoch beinhalten automatisch erzeugte Annotationen meistens nur syntaktische Informationen (Dateigröße, Pixelanzahl, Dauer, Bildrate, \dots) und keine semantischen Annotationen (Aussehen, Funktion, Stil, Genre, \dots), welche manuell annotiert werden müssen. Dazu kommt das Problem, dass Sprache \iA ungenau ist.

	\section{Inhaltsbasierte Suche}
		Bei der inhaltsbasierten Suche wird versucht, Eigenschaften abzuleiten, die die Dokumente sinnvoll beschreiben. Dabei werden mathematische Deskriptoren aus dem Inhalt des Dokuments berechnet. Ein Distanzmaß über diese Deskriptoren erlaubt einen inhaltlichen Vergleich.

		Die Wahl eines Deskriptors sollte dabei abhängen von Problemstellung, Semantik der Objekte und den verfügbaren Daten sein.

		\subsection{Distanzmaße}
			Ein Distanzmaß soll die Ähnlichkeit der Featurevektoren von Dokumenten messen. Typische Distanzmaße (euklidische Distanz, City-Block) sind dabei Metriken.

			Sei \(S\) die Menge an Features. Dann ist eine Funktion \( d : S \times S \to \R \) genau dann, wenn gilt:
			\begin{enumerate}
				\item \( d(x, y) \geq 0 \) (Nicht-Negativität)
				\item \( d(x, y) = 0 \iff x = y \) (Definitheit)
				\item \( d(x, y) = d(y, x) \) (Symmetrie)
				\item \( d(x, y) \leq d(x, z) + d(z, y) \) (Dreiecksungleichung)
			\end{enumerate}
			Eine Funktion ist eine Semi-Metrik, wenn nur 1, 2 und 3 erfüllt sind und eine Pseudo-Metrik, wenn nur 1, 4 und 4 erfüllt sind. Dabei entsprechen Symmetrie und die Dreiecksungleichung nicht (immer) der menschlichen Wahrnehmung.
		% end

		\subsection{Query-Modalitäten}
			\subsubsection{Text}
				Query-by-Text arbeitet (meistens) auf manuell annotierten Metadaten, die als Zeichenfolge dargestellt sind. Der "Text" kann auch sprachlich eingegeben werden, wobei hier zunächst die einzelnen Wörter erkannt werden müssen (aber nur syntaktisch, nicht semantisch!).
			% end

			\subsubsection{Example}
				Bei Query-by-Example wird ein Beispielobjekt vorgegeben und es werden ähnliche Objekte gesucht (\zB über k-Nearest-Neighbors)).
			% end

			\subsubsection{Sketch}
				Bei Query-by-Sketch ist kein Beispielbild erforderlich, sondern der Nutzer hat nur eine Idee, was er sucht. Diese Idee stellt er durch eine Skizze (Sketch) dar.

				Problem: Die zeichnerischen Fähigkeiten haben großen Einfluss auf den (Miss-)~Erfolg.
			% end
		% end
	% end

	\section{Explorative Suche}
		Im Vergleich zum Querying (Retrieval) werden bei der explorativen Suche keine konkreten Suchanforderungen gestellt, \dh es sollten "interessante" Objekte gefunden werden.
	% end
% end

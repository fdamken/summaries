% !TeX spellcheck = en_US

\chapter{EinfÃ¼hrung} % 1.5
	\todo{Content}

	\section{Beispiele} % 1.6, 1.7, 1.8, 1.10, 1.11
		\todo{Content}
	% end

	\section{Fragestellungen} % 1.12
		\todo{Content}
	% end

	\section{Allgemeine Formulierung eines Optimierungsproblems} % 1.13, 1.14
		\todo{Content}
	% end

	\section{Statische vs. Dynamische Optimierung} % 1.15, 1.16, 1.17, 1.18
		\todo{Content}
	% end

	\section{Klassifizierung von Optimierungsverfahren} % 1.23
		\todo{Content}
	% end

	\section{Typische Struktur} % 1.24
		\todo{Content}
	% end
% end

\chapter{Gradient-Based Optimization without Constraints}
	\label{c:gradientOptimization}

	\section{Solution Characterization}
		This section covers the theoretical results for solving a nonlinear optimization problem using calculus.

		\subsection{One-Dimensional Optimization}
			For a one-dimensional function \( \varphi(p) : \R \to \R \) the first-order necessary condition for a minimum is that the derivative of \( \varphi(p) \) w.r.t. the parameter \(p\) vanishes:
			\begin{align*}
				\dv{\varphi(p^\ast)}{p} \! = 0
			\end{align*}
			Where \( p^\ast \) denotes the optimal solution, i.e. the minimum.

			All solutions that fulfill this condition are \emph{candidates} for a minimum. If \( \varphi \) is twice continuous differentiable, the sufficient condition for a minimum is that the second-order derivative is positive:
			\begin{align*}
				\dv{\varphi(p^\ast)}{p} > 0
			\end{align*}
			Then \(p^\ast\) is called a \emph{strict minimum}. This condition is sufficient, but not necessary! The second-order necessary condition for a minimum is that the second-order derivative is non-negative, i.e. \( \dv{\varphi(p^\ast)}{p} \geq 0 \).

			\subsubsection{Possibilities for a Minimum}
				There are three cases for a minimum:
				\begin{itemize}
					\item \(\varphi(p)\) is twice continuously differentiable everywhere
					\item \(\varphi'(p)\) is not continuous everywhere but at \(p^\ast\)
					\item \(\varphi'(p)\) is not continuous everywhere, not even at \(p^\ast\)
				\end{itemize}
				While the latter case is common, it is problematic as the solution can typically not be determined analytically (if a function is not continuous at one point, it is rarely invertible).
			% end
		% end

		\subsection{Multi-Dimensional Optimization}
			\label{subsec:multiDimensionalOptimalityConditions}

			For multi-dimensional objective functions \( \varphi : \R^{n_p} \to \R \), where \(n_p\) is the dimensionality of the parameters, the first-order necessary condition is that the gradient vanishes:
			\begin{align*}
				\grad{\varphi}(\vec{p}^\ast) =
				\begin{bmatrix}
					\pdv{\varphi}{p_1} \\
					\vdots             \\
					\pdv{\varphi}{p_{n_p}}
				\end{bmatrix}
				=
				\begin{bmatrix}
					0      \\
					\vdots \\
					0
				\end{bmatrix}
			\end{align*}

			If \(\varphi(\vec{p})\) is twice continuously differentiable, the second-order sufficient condition is that the Hessian of \(\varphi(\vec{p})\) is positive definite. Analogous to the one-dimensional case, the second-order necessary condition is that the Hessian is positive semi-definite, i.e.:
			\begin{align*}
				\mat{H}_\varphi(\vec{p}^\ast) =
				\begin{bmatrix}
					\pdv[2]{\varphi}{p_1}        & \cdots & \pdv{\varphi^2}{p_{n_p} p_1} \\
					\vdots                       & \ddots & \vdots                       \\
					\pdv{\varphi^2}{p_1 p_{n_p}} & \cdots & \pdv[2]{\varphi}{p_{n_p}}
				\end{bmatrix}
				> 0
				\quad\text{or respectively}\quad
				\mat{H}_\varphi(\vec{p}^\ast) \geq 0
			\end{align*}

			\paragraph{Example} % 2.7, 2.8, 2.9
				\todo{Content}
			% end
		% end
	% end

	\section{Numerical Gradient-Based Methods}
		\subsection{Starting Point}
			\subsubsection{Structure of Gradient-Based Methods}
				Given a initial approximation \( \vec{p}^{(0)} \), an approximation of the minimum \( \vec{p}^\ast \) is wanted. Gradient-based methods are iteration methods based on the iteration rule
				\begin{align*}
					\vec{p}^{(k + 1)} = \vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)},\quad k = 0, 1, 2, \cdots
				\end{align*}
				where
				\begin{itemize}
					\item \(\vec{d}^{(k)}\) is the search direction found as the solution of a linear sub problem and
					\item \(\alpha^{(k)}\) is the step size found by a one-dimensional \emph{line search}.
				\end{itemize}
				The iteration terminates once \( \vec{p}^{(k + 1)} \) is "close to" \(\vec{p}^\ast\), e.g. when the gradient nearly vanishes.
			% end

			\subsubsection{Descent Direction}
				Gradient-based methods have to ensure the the local search direction \(\vec{d}^{(k)}\) really is a descent direction (the algorithm shall not "run up the hill"). This property is ensured iff the angle \( \delta \) between the search direction and the gradient \( \grad{\varphi}\big(\vec{}^{(k)}\big) \) greater than \SI{90}{\degree}, i.e.
				\begin{align}
					\cos \delta = \frac{\big( \vec{d}^{(k)} \big)^T \Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big)}{\big\lVert \vec{d}^{(k)} \big\rVert \cdot \big\lVert \grad{\varphi}\big(\vec{p}^{(k)}\big) \big\rVert} < 0 \quad\iff\quad \big( \vec{d}^{(k)} \big)^T \Big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big) < 0  \label{eq:descentDirection}
				\end{align}
				This is called the "necessary descent condition".
			% end

			\subsubsection{Algorithmic Structure}
				The \autoref{alg:gradientBasedAlgorithmStructure} shows the basic structure of any gradient-based optimization algorithm.

				\begin{algorithm}  \DontPrintSemicolon
					\textbf{Initialization:} Choose an initial approximation \(\vec{p}^{(0)}\), set \( k \gets 0 \) \;
					\While{not converged}{
						Determine new search direction: \tabto{6cm} \( \vec{d}^{(k)} \in \R^{n_p} \) \;
						Determine new step size: \tabto{6cm} \( \alpha^{(k)} \in \R^+ \) \;
						Update the approximation: \tabto{6cm} \( \vec{p}^{(k + 1)} \gets \vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)} \) \;
						\( k \gets k + 1 \) \;
					}

					\caption{Algorithmic structure of a gradient-based optimization algorithms.}
					\label{alg:gradientBasedAlgorithmStructure}
				\end{algorithm}
			% end
		% end

		\subsection{Steepest Descent}
			Steepest descent is the straightforward way for getting a search direction. The search direction is just set to the negative of the gradient:
			\begin{align*}
				\vec{d}^{(k)} = -\grad{\varphi}\big(\vec{p}^{(k)}\big)
			\end{align*}

			\begin{itemize}
				\item Advantages:
					\begin{itemize}
						\item Often quickly reaches areas around the local minimum.
						\item No second derivatives needed.
					\end{itemize}
				\item Disadvantages:
					\begin{itemize}
						\item Very slow in areas around the local minimum compared to (Quasi-) Newton Methods.
					\end{itemize}
			\end{itemize}
		% end

		\subsection{Conjugate Gradient}
			Basic approach for conjugate gradient:
			\begin{align*}
				\vec{d}^{(0)} & = -\grad{\varphi}\big(\vec{p}^{(0)}\big)                                                                                                            \\
				\vec{d}^{(k)} & = \text{Component of } -\grad{\varphi}\big(\vec{p}^{(k)}\big) \text{ that is conjugate to } \vec{d}^{(0)}, \vec{d}^{(1)}, \cdots, \vec{d}^{(k - 1)}
			\end{align*}

			For a quadratic objective function
			\begin{align*}
				\varphi(\vec{p}) = \frac{1}{2} \vec{p}^T \mat{A} \vec{p} - \vec{b}^T \vec{p}
			\end{align*}
			with a positive semi-definite matrix \(\mat{A}\) and constant \(\mat{A}\), \(\vec{b}\), the search direction is given as the solution of:
			\begin{align*}
				\big(\vec{d}^{(k)}\big)^T \mat{A} \vec{d}^{(j)} = 0,\quad j = 1, \cdots, k - 1
			\end{align*}

			With an optimal step size \( \alpha^{(k)} \), i.e.
			\begin{align*}
				\alpha^{(k)} = \arg\min_{\alpha} \varphi\big( \vec{p}^{(k)} + \alpha \vec{d}^{(k)} \big)
				\quad\implies\quad \alpha^{(k)} = -\frac{1}{\big(\vec{d}^{(k)}\big)^T \mat{A} \vec{d}^{(k)}} \Big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big)^T \big(\vec{d}^{(k)}\big)
			\end{align*}
			the minimum of \(\varphi\) is reached in \(n_p\) steps.

			The extension for nonlinear objective functions is given in~\autoref{alg:conjugateGradient}.

			\begin{algorithm}  \DontPrintSemicolon
				\textbf{Initialization:} Choose an initial approximation \(\vec{p}^{(0)}\), set \( \vec{d}^{(0)} \gets -\grad{\varphi}\big(\vec{p}^{(0)}\big) \) and \( k \gets 0 \) \;
				\While{not converged}{
					\( \alpha^{(k)} \gets \arg\min_{\alpha} \varphi\big( \vec{p}^{(k)} + \alpha \vec{d}^{(k)} \big) \) \;
					\( \vec{p}^{(k + 1)} \gets \vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)} \) \;
					\( \beta^{(k + 1)} \gets \frac{\big(\! \grad{\varphi}\big(\vec{p}^{(k + 1)}\big) \!\big)^T \big(\! \grad{\varphi}\big(\vec{p}^{(k + 1)}\big) \!\big)}{\big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\big)^T \big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\big)} \) \;
					\( \vec{d}^{(k + 1)} \gets -\grad{\varphi}\big(\vec{p}^{(k + 1)}\big) + \beta^{(k + 1)} \vec{d}^{(k)} \) \;
					\( k \gets k + 1 \) \;
				}

				\caption{Conjugate Gradient for nonlinear Objective Function.}
				\label{alg:conjugateGradient}
			\end{algorithm}

			\begin{itemize}
				\item Exact line search necessary.
				\item Different variants of GC-algorithms mainly distinguish in the choice of of \( \beta^{(k)} \).
				\item Advantages:
					\begin{itemize}
						\item Faster then steepest descent.
						\item No explicit storing of the Hessian \( \mat{H}_\varphi\big(\vec{p}^{(k)}\big) \) necessary.
						\item No explicit matrix-vector multiplication.
						\item Useful even for extreme high dimensions \( n_p \).
						\item Exact for quadratic objectives \( \varphi(\vec{p}) \).
					\end{itemize}
				\item Disadvantages:
					\begin{itemize}
						\item A lot slower then (Quasi-) Newton Methods.
						\item In general not useful for optimizing simulation models.
					\end{itemize}
			\end{itemize}
		% end

		\subsection{Newton Method}
			Assuming the approximation of each iteration, \( \vec{p}^{(k)} \), is close to the minimum \( \vec{p}^\ast \), the gradient \( \grad{\varphi}(\vec{p}^\ast) \) can be taylor-expanded around \( \vec{p}^{(k)} \):
			\begin{align*}
				\grad{\varphi}(\vec{p}^\ast) \overset{T\big(\!\vec{p}^{(k)}\!\big)}{=} \grad{\varphi}\big(\vec{p}^{(k)}\big) + \mat{H}_\varphi\big(\vec{p}^{(k)}\big) \big( \vec{p}^\ast - \vec{p}^{(k)} \big) + \cdots \overset{!}{=} \vec{0}
			\end{align*}
			By leaving our the higher order terms the search direction \( \vec{d}^{(k)} \coloneqq \vec{p}^\ast - \vec{p}^{(k)} \) is given by the solution of the system of linear equations
			\begin{align*}
				\mat{H}_\varphi\big(\vec{p}^{(k)}\big) \vec{d}^{(k)} = -\grad{\varphi}\big(\vec{p}^{(k)}\big)
			\end{align*}
			The realization is shown in~\autoref{alg:newtonMethod}.

			\begin{algorithm}  \DontPrintSemicolon
				\textbf{Initialization:} Choose an initial approximation \(\vec{p}^{(0)}\), set \( k \gets 0 \) \;
				\While{not converged}{
					Solve \( \mat{H}_\varphi\big(\vec{p}^{(k)}\big) \vec{d}^{(k)} = -\grad{\varphi}\big(\vec{p}^{(k)}\big) \) for \( \vec{d}^{(k)} \) \;
					\( \alpha^{(k)} \gets \arg\min_{\alpha} \varphi\big( \vec{p}^{(k)} + \alpha \vec{d}^{(k)} \big) \) \;
					\( \vec{p}^{(k + 1)} \gets \vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)} \) \;
					\( k \gets k + 1 \) \;
				}

				\caption{Newton Method}
				\label{alg:newtonMethod}
			\end{algorithm}

			When plugging the search direction into the necessary descent condition~\eqref{eq:descentDirection}
			\begin{align*}
				\big(\vec{d}^{(k)}\big)^T \Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big)\!\Big)
				= -\Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big)^T\Big) \Big(\!\mat{H}_\varphi\big(\vec{p}^{(k)}\big)\!\Big)^{-1} \Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big)\!\Big)
				< 0
			\end{align*}
			it is clear that this is only fulfilled iff the Hessian is positive definite. But this is only the case in a region around the minimum! If the approximation is far away from the minimum, the search direction might also be an ascent direction causing the Newton method to diverge. There are two main solutions to this problem:
			\begin{enumerate}
				\item If the Hessian is not positive definite, replace it by an identity matrix. That is, set the search direction to the steepest descent.
				\item Regularize the equation system with a weight \( \nu > 0 \) such that the new matrix is positive definite (this "rotates" the matrix in the direction of the steepest descent such that the new search direction always fulfills the descent condition):
			\end{enumerate}
			\begin{align*}
				\Big( \mat{H}_\varphi\big(\vec{p}^{(k)}\big) + \nu \mat{I} \Big) \vec{d}^{(k)} = -\grad{\varphi}\big(\vec{p}^{(k)}\big)
			\end{align*}

			\begin{itemize}
				\item Advantages:
					\begin{itemize}
						\item Near to strong local minima of twice continuous differentiable objective, the Newton method is quadratic convergent.
					\end{itemize}
				\item Disadvantages:
					\begin{itemize}
						\item Computationally expensive as a linear system has to be solved in every iteration.
						\item Not only first, but also second-order derivatives have to be available. This is a big disadvantage:
							\begin{itemize}
								\item In practice, the first derivative is rarely and the second derivative is never available.
								\item Even a single wrong component in the gradient or the Hessian destroys the quadratic convergence.
							\end{itemize}
					\end{itemize}
			\end{itemize}

			\subsubsection{Availability of Second-Order Derivatives}
				The obvious idea is to approximate the Hessian using finite differences. The approximated Hessian is then given as
				\begin{align*}
					\mat{H}_\varphi\big(\vec{p}^{(k)}\big) = \frac{1}{2} \big( \tilde{\mat{H}} + \tilde{\mat{H}}^T )  \label{eq:approxHessianSymm}
				\end{align*}
				where \(\tilde{\mat{H}}\) is given by
				\begin{align*}
					\tilde{\mat{H}}_i = \pdv{p_i} \Big( \grad{\varphi}\big(\vec{p}^{(k)}\big) \Big) \approx \frac{1}{h_i} \Big( \grad{\varphi}\big(\vec{p}^{(k)} + h_i \vec{e}_i\big) - \grad{\varphi}\big(\vec{p}^{(k)}\big) \Big)
				\end{align*}
				where \( \tilde{\mat{H}}_i \) is the \(i\)-th column of \(\tilde{\mat{H}}\). The equation~\eqref{eq:approxHessianSymm} is used to force the Hessian to be symmetric.

				Problems:
				\begin{itemize}
					\item The Hessian \( \mat{H}_\varphi\big(\vec{p}^{(k)}\big) = \frac{1}{2} \big( \tilde{\mat{H}} + \tilde{\mat{H}}^T ) \) is not necessarily positive definite.
					\item In every iteration the Gradient has to be evaluated \(n_p\) times more.
					\item The linear system still needs to be solved.
					\item Only useful for high-dimensional problems with sparse gradients!
				\end{itemize}
				Another possibility are \emph{Quasi-Newton Methods}.
			% end
		% end

		\subsection{Quasi-Newton Methods}
			Quasi-Newton methods are equivalent to the Newton method, however, the Hessian (or its inverse) is approximated by a positive definite matrix
			\begin{align*}
				\hat{\mat{H}}^{(k)} \approx \mat{H}_\varphi\big(\vec{p}^{(k)}\big)
			\end{align*}
			that is updated in every iteration. This yields a lot of advantages over the classic Newton method:
			\begin{itemize}
				\item Only first-order derivatives needed.
				\item As \( \hat{\mat{H}} \) constructed positive definite, the descent condition is fulfilled anytime.
				\item If the inverse Hessian is directly approximated, only \( \mathcal{O}(n_p^2) \) multiplications instead of \( \mathcal{O}(n_p^3) \) for solving the linear system.
			\end{itemize}

			But how to do the Hessian update? By Taylor-expanding the gradient \( \grad{\varphi}\big(\vec{p}^{(k)}\big) \) around \( \vec{p}^{(k + 1)} \)
			\begin{align*}
				\grad{\varphi}\big(\vec{p}^{(k)}\big) \overset{T\big(\!\vec{p}^{(k + 1)}\!\big)}{=} \grad{\varphi}\big(\vec{p}^{(k + 1)}\big) + \mat{H}_\varphi\big(\vec{p}^{(k + 1)}\big) \big( \vec{p}^{(k)} - \vec{p}^{(k + 1)} \big) + \cdots \overset{!}{=} \vec{0}
			\end{align*}
			and cutting off the higher-order terms, the following approximation holds:
			\begin{align*}
				\mat{H}_\varphi\big(\vec{p}^{(k + 1)}\big) \vec{d}^{(k)} \approx \grad{\varphi}\big(\vec{p}^{(k + 1)}\big) - \grad{\varphi}\big(\vec{p}^{(k)}\big)
			\end{align*}
			The approximation of the Hessian must therefore fulfill the \emph{secant condition}
			\begin{align*}
				\tilde{\mat{H}}^{(k + 1)} \vec{d}^{(k)} = \grad{\varphi}\big(\vec{p}^{(k + 1)}\big) - \grad{\varphi}\big(\vec{p}^{(k)}\big)
			\end{align*}

			There exist a lot of different approaches for doing the Hessian updates \( \tilde{\mat{H}}^{(k + 1)} = \tilde{\mat{H}}^{(k)} + \mat{U}{(k)} \) for rank-1 or rank-2 matrices \(\mat{U}^{(k)}\):
			\begin{itemize}
				\item Approach for rank-1 corrections: \tabto{6cm} \( \tilde{\mat{H}}^{(k + 1)} = \tilde{\mat{H}}^{(k)} + \beta_1 \vec{u} \vec{u}^T \)
				\item Approach for rank-2 corrections: \tabto{6cm} \( \tilde{\mat{H}}^{(k + 1)} = \tilde{\mat{H}}^{(k)} + \beta_1 \vec{u} \vec{u}^T + \beta_2 \vec{v} \vec{v}^T \)
			\end{itemize}
			The vectors \( \vec{u}, \vec{v} \in \R^{n_p} \) and scalars \( \beta_1, \beta_2 \in \R \) must have to be chosen such that \( \tilde{\mat{H}}^{(k + 1)} \) is
			\begin{itemize}
				\item positive definite,
				\item symmetric,
				\item fulfills the secant condition and
				\item adding up the matrices is efficient and robust.
			\end{itemize}

			\subsubsection{BFGS-Update}
				The most known rank-2 update for the Hessian is the \emph{BFGS-Update}\footnote{"BFGS" stands for the authors Broyden, Fletcher, Goldfarb and Shanno.}
				\begin{align*}
					\vec{u} & = \tilde{\mat{H}}^{(k)} \vec{d}^{(k)} & \beta_1 & = -\frac{1}{\big(\vec{d}^{(k)}\big)^T \tilde{\mat{H}}^{(k)} \vec{d}^{(k)}} \\
					\vec{v} & = \vec{g}^{(k)}                       & \beta_2 & = \frac{1}{\big(\vec{g}^{(k)}\big)^T \vec{d}^{(k)}}
				\end{align*}
				where \( \vec{g}^{(k)} = \grad{\varphi}\big(\vec{p}^{(k + 1)}\big) - \grad{\varphi}\big(\vec{p}^{(k)}\big) \). Plugging that into the general approach for rank-2 updates yields the update rule for BFGS-approximations:
				\begin{align*}
					\tilde{\mat{H}}^{(k + 1)} = \tilde{\mat{H}}^{(k)}
					- \frac{1}{\big(\vec{d}^{(k)}\big)^T \tilde{\mat{H}}^{(k)} \vec{d}^{(k)}} \tilde{\mat{H}}^{(k)} \vec{d}^{(k)} \big(\tilde{\mat{H}}^{(k)} \vec{d}^{(k)}\big)^T
					+ \frac{1}{\big(\vec{g}^{(k)}\big)^T \vec{d}^{(k)}} \vec{g}^{(k)} \big(\vec{g}^{(k)}\big)^T
				\end{align*}
				\begin{itemize}
					\item The direct approximation of the Hessian inverse is not really robust (e.g. for a non-optimal step size rule).
					\item A better alternative is to directly approximate a useful factorization, e.g. the Cholesky decomposition. This is more robust and equally efficient (\( \mathcal{O}(n_p^2) \)).
				\end{itemize}

				The pseudo code for the BFGS update is shown in~\autoref{alg:bfgs}.

				\begin{algorithm}  \DontPrintSemicolon
					\textbf{Initialization:} Choose an initial approximation \(\vec{p}^{(0)}\), set \( \tilde{\mat{H}}^{(0)} = \mat{I} \) and \( k \gets 0 \) \;
					\While{not converged}{
						Solve \( \tilde{\mat{H}}^{(k)} \vec{d}^{(k)} = -\grad{\varphi}\big(\vec{p}^{(k)}\big) \) for \( \vec{d}^{(k)} \) \;
						\( \alpha^{(k)} \gets \arg\min_{\alpha} \varphi\big( \vec{p}^{(k)} + \alpha \vec{d}^{(k)} \big) \) \;
						\( \vec{p}^{(k + 1)} \gets \vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)} \) \;
						\( \vec{g}^{(k)} \gets \grad{\varphi}\big(\vec{p}^{(k + 1)}\big) - \grad{\varphi}\big(\vec{p}^{(k)}\big) \) \;
						\( \tilde{\mat{H}}^{(k + 1)} \gets \tilde{\mat{H}}^{(k)} - \frac{1}{(\vec{d}^{(k)})^T \tilde{\mat{H}}^{(k)} \vec{d}^{(k)}} \tilde{\mat{H}}^{(k)} \vec{d}^{(k)} \big(\tilde{\mat{H}}^{(k)} \vec{d}^{(k)}\big)^T + \frac{1}{(\vec{g}^{(k)})^T \vec{d}^{(k)}} \vec{g}^{(k)} \big(\vec{g}^{(k)}\big)^T \) \;
						\( k \gets k + 1 \) \;
					}

					\caption{Quasi-Newton Method with BFGS Update.}
					\label{alg:bfgs}
				\end{algorithm}
			% end
		% end

		\subsection{Comparison} % 2.29, 2.30, 2.31, 2.32, 2.33, 2.34, 2.35, 2.36
			\todo{Content}
		% end

		\subsection{Notes and Discussion}
			\begin{itemize}
				\item The convergence of gradient-based methods can be shown under weak preconditions.
				\item As the search direction is only a local descent direction, gradient-based algorithms only yields local minima.
				\item There is no algorithm that can guarantee to find the global minimum!
				\item Some approaches for determining a global minimum:
					\begin{itemize}
						\item Choose the initialization well, i.e. close to the global minimum.
						\item Execute the algorithm multiple times with different starting points.
						\item Validate the solution against properties of the original problem.
						\item Execute direct search methods beforehand to find promising regions for the local minimum search.
					\end{itemize}
			\end{itemize}

			\begin{itemize}
				\item Advantages:
					\begin{itemize}
						\item If gradient-based algorithms converge, they converge utterly fast.
						\item Efficient also for high-dimensional problems, i.e. a large \( n_p \).
					\end{itemize}
				\item Disadvantages:
					\begin{itemize}
						\item Only applicable for functions that are differentiable almost everywhere.
						\item Require gradient information exact up to four to eight decimal points.
						\item Convergence to a local minimum near the initialization \( \vec{p}^{(0)} \).
						\item Require some expert knowledge.
					\end{itemize}
			\end{itemize}
		% end
	% end

	\section{Step Size Rules, Line Search}
		\label{sec:gradientUnconsStepSize}

		In every iteration of gradient-based algorithms, the step size has to be determined by minimizing the one-dimensional function:
		\begin{align*}
			\psi(\alpha) = \varphi\big(\vec{p}^{(k)} + \alpha \vec{d}^{(k)}\big)
		\end{align*}
		As the necessary first-order condition for a minimum, the derivative w.r.t. \(\alpha\) has to vanish:
		\begin{align*}
			\dv{\psi\big(\alpha^{(k)}\big)}{\alpha} = \dv{\alpha} \varphi\big(\vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)}\big) = \Big(\!\grad{\varphi}\big(\vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)}\big)\!\Big)^T \vec{d}^{(k)} \overset{!}{=} 0
		\end{align*}
		Thus the gradient of \(\varphi\) at the minimum \(\alpha^{(k)}\) as to be orthogonal to the search direction \(\vec{d}^{(k)}\). Intuitively, the optimal step size has to be chosen such that the iteration step cannot go any further without ascending again ("hitting an ascending contour line").

		Goal of the line search is to reach the minimum of \(\psi\) with least invocations of \(\psi\) as possible. Most of the existing search methods can be classified into
		\begin{itemize}
			\item \emph{Polynomial approximation}, e.g. quadratic or cubic interpolation
			\item \emph{Direct search methods}, e.g. Fibonacci-search, golden ratio search
			\item \emph{Optimal vs. non-optimal search methods}, e.g. by finding an improvement but not the minimum
			\item Usage of the gradient information \( \psi' \) or not.
		\end{itemize}

		Requirements:
		\begin{itemize}
			\item Finding the \(\alpha^{(k)}\) with a minimal value of \(\psi\).
			\item Do not waste too much computation time on the line search.
		\end{itemize}
		In general, an exact line search requires lots of \(\psi\)-evaluations. But how far does \(\psi\) need to be reduced in order to guarantee convergence? In general, the condition \( \varphi\big(\vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)}\big) < \varphi\big(\vec{p}^{(k)}\big) \) is not enough!

		\subsection{Inexact Line Search}
			Procedure: Generate and inspect a series of candidates for \(\alpha^{(k)}\) and terminate once one of the candidates fulfills specific criteria, e.g. the Armijo rule or Wolfe conditions.

			\subsubsection{Armijo Rule}
				The \emph{Armijo rule} guarantees a sufficient reduction in \(\varphi\):
				\begin{align*}
					\varphi\big(\vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)}\big) \leq \varphi\big(\vec{p}^{(k)}\big) + c_1 \alpha^{(k)} \Big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big)^T \vec{d}^{(k)} = \varphi\big(\vec{p}^{(k)}\big) + c_1 \alpha^{(k)} \psi'(0)
				\end{align*}
				Where \( 0 < c_1 < 1 \) is any constant, e.g. \( c_1 = 10^{-4} \).

				Hence, the minimal reduction has to be proportional to \(\alpha^{(k)}\) and the derivative \( \psi'(0) \).
			% end

			\subsubsection{Curvature Condition}
				But a sufficient descent condition is not enough as the step sizes must not be too small (otherwise progress would stop). Thus a second condition has to be employed, the \emph{curvature condition} that requires a minimum curvature on \(\psi\):
				\begin{align*}
					\Big(\!\grad{\varphi}\big(\vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)}\big)\!\Big)^T \vec{d}^{(k)} \geq c_2 \cdot \Big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big)^T \vec{d}^{(k)} = c_2 \psi'(0) \quad\iff\quad \psi'\big(\vec{p}^{(k)}\big) \geq c_2 \psi'(0)
				\end{align*}
				Where \( c_1 < c_2 < 1 \) is any constant, e.g. \( c_2 = 0.9 \).
			% end

			\subsubsection{Wolfe and Goldstein Conditions}
				Combining the Armijo rule and the curvature condition yields the Wolfe conditions that guarantee both a minimal reduction and a minimal curvature. They are especially useful for Quasi-Newton methods as the Wolfe conditions are scale invariant, i.e. independent of
				\begin{itemize}
					\item multiplying \(\varphi\) with any constant and
					\item affine transformations of \(\vec{p}\).
				\end{itemize}

				There are other possibilities are, e.g. the \emph{Goldstein conditions}
				\begin{align*}
					\varphi\big(\vec{p}^{(k)}\big) + (1 - c) \alpha^{(k)} \Big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big)^T \vec{d}^{(k)}
					\leq \varphi\big( \vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)} \big)
					\leq \varphi\big( \vec{p}^{(k)} \big) + c \alpha^{(k)} \Big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big)^T \vec{d}^{(k)}
				\end{align*}
				with any \( 0 < c < 1/2 \), that are useful for Newton, but not for Quasi-Newton methods.
			% end
		% end

		\subsection{Notes}
			\begin{itemize}
				\item For gradient-based methods a step size \( \alpha^{(k)} > 1 \) is in general not useful because:
					\begin{itemize}
						\item The search direction \( \vec{d}^{(k)} \) is determined using a linear or quadratic Taylor approximation.
						\item The Taylor approximation is only valid in a small region around the current approximation \( \vec{p}^{(k)} \), i.e. for \( 0 < \alpha^{(k)} \leq 1 \).
					\end{itemize}
				\item The local quadratic or super-linear convergence of Newton-type methods is visible in practice as the last step can be executed with full step size \( \alpha^{(k)} = 1 \).
			\end{itemize}
		% end
	% end

	\section{Trust Region Methods}
		Gradient-based methods with line search determine a fixed search direction and adjust the step size \( \alpha^{(k)} \) according to that search direction to reach global convergence.

		Another approach is to determine both the length and direction of \( \vec{d}^{(k)} \). The iteration step then becomes
		\begin{align*}
			\vec{p}^{(k + 1)} = \vec{p}^{(k)} + \vec{d}^{(k)}
		\end{align*}
		without any explicit step size. The length and direction of \( \vec{d}^{(k)} \) are then determined as a solution of the quadratic sub-problem
		\begin{align*}
			\min_{\vec{d} \in \R^{n_p}} & \, \Big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big)^T \vec{d} + \frac{1}{2} \vec{d}^T \mat{H}_\varphi\big(\vec{p}^{(k)}\big) \vec{d} \\
			\mathrm{subject~to}\quad    & \,
			\begin{alignedat}[t]{2}
				\lVert \vec{d} \rVert_2 & \leq \delta
			\end{alignedat}
		\end{align*}
		where \(\delta\) describes the area around the current approximation \( \vec{p}^{(k)} \) where the quadratic approximation of \( \varphi\big(\vec{p^{(k)}} + \vec{d}\big) \) makes sense, i.e. the \emph{trust region}.

		The value of \(\delta\) is extremely important for the efficiency of the method.
		\begin{itemize}
			\item If \(\delta\) is too small, opportunities for large steps are missed.
			\item If \(\delta\) is too large, the minimum of the quadratic approximation might be far off the minimum of the objective if the Hessian is indefinite or negative definite.
		\end{itemize}
		It is possible to add a regularization parameter \(\beta \geq 0\) to the quadratic approximation
		\begin{align*}
			\Big( \mat{H}_\varphi\big(\vec{p}^{(k)}\big) + \beta \mat{I} \Big) \vec{d} = -\grad{\varphi}\big(\vec{p}^{(k)}\big)
		\end{align*}
		such that the matrix is positive semidefinite. The solution of this regularized problem also solves the trust region problem if either \( \beta = 0 \), \( \lVert \vec{d} \rVert \leq \delta \) or \( \beta \geq 0 \), \( \lVert \vec{d} \rVert = \delta \).
	% end

	\section{Rate of Convergence}
		A common criteria to measure the performance of a gradient method are \emph{rates of convergence}. These provide information on how fast an algorithm converges, i.e. how fast \( \vec{p}^{(k)} \to \vec{p}^\ast \) or \( \big\lVert \vec{p}^{(k)} - \vec{p}^\ast \big\rVert \to 0 \).

		\textbf{Definition:}
		Let \( \big(\{\, \vec{p}^{(k)} \,\big\} \) be the series of approximations produced by an optimization method. Then this series has rate of convergence \(r\) if \(r\) is the greatest positive number such that the limit
		\begin{align*}
			0 \leq \liminfty[k] \frac{\big\lVert \vec{p}^{(k + 1)} - \vec{p}^\ast \big\rVert}{\big\lVert \vec{p}^{(k)} - \vec{p}^\ast \big\rVert^r} = \gamma < \infty
		\end{align*}
		converges (where \( \vec{p}^\ast = \liminfty[k] \vec{p}^{(k)} \)). If \( r = 1 \), then \( \gamma < 1 \) has to hold for the method to converge.

		A sequence is said to converge superlinearly if
		\begin{align*}
			\liminfty[k] \frac{\big\lVert \vec{p}^{(k + 1)} - \vec{p}^\ast \big\rVert}{\big\lVert \vec{p}^{(k)} - \vec{p}^\ast \big\rVert} = 0
		\end{align*}
		holds. Even though technically this condition holds for \( r > 1 \), in practice only methods with \( 1 < r < 2 \) are said to converge superlinearly (e.g. for \(r = 2\), the sequence is called to converge quadratically).

		\paragraph{Examples} % 2.53
			\todo{Content}
		% end

		\subsection{Gradient-Based Methods}
			Under ideal conditions (i.e. \(\varphi\) is twice continuously differentiable and \( \mat{H}_\varphi(\vec{p}^\ast) \) is positive definite), all of the following hold:
			\begin{itemize}
				\item Steepest descent is (locally) linearly convergent (with exact line search).
				\item The Newton method is (locally) quadratically convergent.
				\item Quasi-Newton methods with BFGS-update are (locally) superlinearly convergent (for inexact line search using the Wolfe conditions).
			\end{itemize}
			But: Even a single wrong component in the Hessian reduced the quadratic convergence of the Newton method to linear convergence!
		% end
	% end
% end

\chapter{Gradient-Free Optimization without Constraints}
	This chapter covers different types of sampling methods (direct search methods):
	\begin{enumerate}
		\item Metaheuristics (random search),
		\item Deterministic Sampling Methods (pattern search) and
		\item Surrogate optimization.
	\end{enumerate}
	These optimization methods only use evaluations of the objective \(\varphi\) and do not user gradient information (neither analytically nor using numerical differentiation). That is, \(\varphi\) is used as a "Black Box" for function evaluations.

	Even for \(\varphi\) that are not differentiable and have lots of local minima, gradient-free algorithms are remarkably robust, but can also fail fast.

	\section{Introduction}
		The objective \(\varphi\) that is to be optimized often has suboptimal properties, e.g.:
		\begin{itemize}
			\item the evaluation is noisy
			\item not differentiable
			\item high computation time for evaluation (e.g. when a simulation has to be run for each evaluation)
		\end{itemize}

		But gradient-free techniques have a wide range of applications, e.g. in automobile, aerospace industry, robotics, financial, etc. The goal is to reduce the objective function as much as possible and find regions in which the objective is differently sensitive w.r.t. changes in the optimization variables.

		\subsection{Simulation-Based Optimization}
			In a simulation-based setting, the objective is evaluated by running a simulation (e.g. by solving a set of differential equations or running a real experiment). This yields a suboptimal setting for optimization, yet a common one:
			\begin{itemize}
				\item Only function values are computable and not gradient information is available.
				\item Source code of the optimization is often not available. Hence, no information about how the simulation is computed.
				\item Discontinuous/non-differentiable systems and model properties, e.g. due to collisions.
				\item Non-differentiable structure inside the simulation (e.g. if-else).
				\item Discontinuities caused by subprograms, heuristics, table data, \dots
				\item Function evaluations are computationally expensive.
				\item Numerical "noise" overlay the actual system properties.
				\item Non-deterministic simulations (e.g. due to complex friction).
			\end{itemize}
			Hence, simulation-based optimization is a black box problem and can be solved (or approximated) using gradient-free methods!
		% end

		\subsection{Black-Box Optimization}
			Naturally, gradient-based optimization methods are not well-suited for problems with "low" differentiability and computationally expensive function evaluations. These problems may be solved using gradient-free optimization methods. But\dots
			\begin{itemize}
				\item Gradient-based techniques are really slow in comparison to gradient-based ones for differentiable optimization problems.
				\item They need a lot of function evaluations for high-dimensional problems and are thus practically only applicable for problems with dimensions \( n_p < 100 \), better \( n_p < 20 \).
				\item Have lots of problems with nonlinear equality constraints!
				\item The theory of gradient-free methods is not as mature as the theory for gradient-based methods.
			\end{itemize}
		% end
	% end

	\section{Metaheuristics}
		\subsection{Evolutionary Algorithm (EA)}
			A \emph{evolutionary algorithm} mimics the biological evolutionary strategy with random search methods.

			\begin{enumerate}
				\item Initialization: \tabto{2.5cm} Choose one "Parent" \( \vec{p}^{(0)} \) and a number of descendants \(\ell\).
				\item Iteration: \tabto{2.5cm} Create \(\ell\) descendants via "mutation"
			\end{enumerate}
			\begin{align*}
				\vec{p}^{(k, i)} = \vec{p}^{(k)} + \alpha_i^{(k)} \vec{d}_i,\quad i = 1, \cdots, \ell
			\end{align*}
			\begin{itemize}
				\item[] where \( \vec{d}_i \in \R^{n_p} \) are vectors of Gaussian distributed variables and \( \alpha_i \in \R \) art suitable "mutation step sizes" Then select the descent with the lower \( \varphi \) value and repeat.
			\end{itemize}
		% end

		\subsection{Genetic Algorithms (GA)}
			\emph{Genetic algorithms} are inspired by biological evolutionary strategies the positive properties caused by mutation are kept through natural selection. GAs are applicable for both real and discrete optimization variables \(\vec{p}\).
			\begin{enumerate}
				\item Initialization: \tabto{3cm} Choose a suitable set of different "individuals" (first generation).
				\item Evaluation: \tabto{3cm} Determine the "fitness" of each candidate using the objective/fitness function.
				\item Selection: \tabto{3cm} Randomly select candidates of the current generation (the higher the fitness, the higher the probability to be chosen).
				\item Recombination: \tabto{3cm} Combine values (genomes) of the selected individuals and create new individuals.
				\item Mutation: \tabto{3cm} Randomly change the genomes.
				\item New Generation: \tabto{3cm} Select new individuals as the new generation and continue with step 2.
			\end{enumerate}

			\paragraph{Example} % 3.13, 3.14
				\todo{Content}
			% end
		% end

		\subsection{Further Metaheuristics}
			Further metaheuristics based on real representations of \(\vec{p}\) like in evolutionary algorithms:
			\begin{itemize}
				\item Particle Swam: Population method, uses idea of combining local and swarm knowledge, direction and speed for particles are adjusted
				\item \dots
			\end{itemize}

			Further metaheuristics based on binary representations of \(\vec{p}\) like in genetic algorithms:
			\begin{itemize}
				\item Tabu Search: A list of possible manipulations is given, another lists dynamically the inverses of them, these cannot be applied any more
				\item \dots
			\end{itemize}
		% end
	% end

	\section{Deterministic Sampling Methods (Pattern Search Methods)}
		Deterministic sampling methods can be further categorized into
		\begin{itemize}
			\item Qualitative methods: Only ranking w.r.t. to the function value.
				\begin{itemize}
					\item Simplex Methods
					\item Coordinate- or compass-search
					\item Multidirectional search
					\item Pattern search methods
					\item \dots
				\end{itemize}
			\item Quantitative methods: Consideration of the real function values.
				\begin{itemize}
					\item Implicit filtering (based on the simplex method)
					\item DIRECT (dividing rectangles)
					\item \dots
				\end{itemize}
		\end{itemize}

		\subsection{Nelder-Mead Simplex Method}
			A \emph{simplex} is a simple object that consists of \( n_p + 1 \) points \( \vec{p}^{(i)} \) in the parameter space (which is \(n_p\)-dimensional). In a 2D space, the three points form a triangle. In the iteration phase the values of \(\varphi\) at the corners are compared and the simplex is transformed according to specific rules (see~\autoref{subsec:nelderMeadIteration}). The algorithm terminates once the simplex contracts onto a single point.

			\subsubsection{Iteration Phase}
				\label{subsec:nelderMeadIteration}

				The iteration phase starts by sorting the edges according to its \(\varphi\)-values:
				\begin{align*}
					\varphi\big(\vec{p}^{(1)}\big) \leq \varphi\big(\vec{p^{(2)}}\big) \leq \cdots \leq \varphi\big(\vec{p}^{(n_p + 1)}\big)
				\end{align*}
				where \( \vec{p}^{(1)} \) is called the \emph{best} point and \( \vec{p}^{(n_p + 1)} \) is called the \emph{worst}. The algorithm now tried to replace the worst point \( \vec{p}^{(n_p + 1)} \) with another point of the form
				\begin{align*}
					\vec{p}(\mu) = (1 + \mu) \bar{\vec{p}} - \mu \vec{p}^{(n_p + 1)}
				\end{align*}
				Where \(\bar{\vec{p}}\) is the centroid of the of all points \emph{except the worst}, i.e.:
				\begin{align*}
					\bar{\vec{p}} = \frac{1}{n_p} \sum_{i = 1}^{n_p} \vec{p}^{(i)}
				\end{align*}
				This corresponds to a reflection of the worst point over the centroid with a weight \(\mu\) that specifies "how far the point point gets pushed out", i.e. the ratio of the original distance of the worst point to the centroid that is preserved while reflecting. If \(\mu = 1\), the point is mirrored.

				In every iteration, the value \(\mu\) is chosen of a set of four values
				\begin{align*}
					-1 < \mu_{ic} < 0 < \mu_{oc} < \mu_{r} < \mu_{e}
				\end{align*}
				for example \( (\mu_{ic}, \mu_{oc}, \mu_r, \mu_\mathit{e}) = (-0.5, 0.5, 1, 2) \).
			% end

			\subsubsection{Algorithm}
				Some termination criteria are for example:
				\begin{itemize}
					\item Exactness in the objective: \( \varphi\big( \vec{p}^{(n_p + 1)} \big) - \varphi\big(\vec{p}^{(1)}\big) \leq \varepsilon \)
					\item Maximum number of function evaluations: \( k = k_\mathrm{max} \)
					\item Sufficient small distance on the simplex corners.
				\end{itemize}

				\begin{itemize}
					\item Initialization: Choose a start simplex, evaluate the objective at the corners, sort them and set \( k = n_p + 1 \).
					\item Iteration: While \( \varphi\big( \vec{p}^{(n_p + 1)} \big) - \varphi\big(\vec{p}^{(1)}\big) > \varepsilon \) and \( k < k_\mathrm{max} \), do:
						\begin{enumerate}[label = (\alph*)]
							\item Calculate the centroid \( \bar{\vec{p}} = \frac{1}{n_p} \sum_{i = 1}^{n_p} \vec{p}^{(i)} \).
							\item \emph{Reflection:} If \( \varphi\big(\vec{p}^{(1)}\big) \leq \varphi\big(\vec{p}(\mu_r)\big) < \varphi\big(\vec{p}^{(n_p)}\big) \), replace \( \vec{p}^{(n_p + 1)} \) with \( \vec{p}(\mu_r) \); go to (g). \\
								"Use the reflected point if it is better than the second worst, but not better than the best."
							\item \emph{Expansion:} If \( \varphi\big(\vec{p}(\mu_r)\big) < \varphi\big(\vec{p}^{(1)}\big) \), then: \\
								"If the reflected point is better than the best, \dots"
								\begin{itemize}
									\item If \( \varphi\big( \vec{p}(\mu_e) \big) < \varphi\big( \vec{p}(\mu_r) \big) \), replace \( \vec{p}^{(n_p + 1)} \) with \( \vec{p}(\mu_e) \); go to (g). \\
										"\dots and the expanded point is better than the reflected point, use the expanded point."
									\item If \( \varphi\big( \vec{p}(\mu_r) \big) < \varphi\big( \vec{p}(\mu_e) \big) \), replace \( \vec{p}^{(n_p + 1)} \) with \( \vec{p}(\mu_r) \); go to (g). \\
										"\dots and the expanded point is worst than the reflected point, use the reflected point."
								\end{itemize}
							\item \emph{Outer Contraction:} If \( \varphi\big(\vec{p}^{(n_p)}\big) \leq \varphi\big(\vec{p}(\mu_r)\big) < \varphi\big(\vec{p}^{(n_p + 1)}\big) \), then: \\
								"If the reflected point is better than the worst, but worst than second worst, \dots"
								\begin{itemize}
									\item If \( \varphi\big(\vec{p}(\mu_{oc})\big) < \varphi\big(\vec{p}(\mu_r)\big) \), replace \( \vec{p}^{(n_p + 1)} \) with \( \vec{p}(\mu_{oc}) \); go to (g). \\
										"\dots and the outer contraction point is better than the reflected point, use the outer contraction point."
									\item Else, go to (f).
								\end{itemize}
							\item \emph{Inner Contraction:} If \( \varphi\big(\vec{p}^{(n_p + 1)}\big) \leq \varphi\big(\vec{p}(\mu_r)\big) \), then: \\
								"If the reflected point is worst than the worst point, \dots"
								\begin{itemize}
									\item If \( \varphi\big(\vec{p}(\mu_{ic})\big) < \varphi\big(\vec{p}^{(n_p + 1)}\big) \), replace \( \vec{p}^{(n_p + 1)} \) with \( \vec{p}(\mu_{ic}) \); go to (g). \\
										"\dots and the inner contraction point is better than the worst, use the inner contraction point."
									\item Else, go to (f).
								\end{itemize}
							\item \emph{Shrink:} For all \( 2 \leq i \leq n_p + 1 \), set \( \vec{p}^{(i)} = \vec{p}^{(1)} - \frac{1}{2} \big( \vec{p}^{(i)} - \vec{p}^{(1)} \big) \).
							\item \emph{Sort:} Sorting the current simplex corners such that \( \varphi\big(\vec{p}^{(1)}\big) \leq \varphi\big(\vec{p^{(2)}}\big) \leq \cdots \leq \varphi\big(\vec{p}^{(n_p + 1)}\big) \) holds again. Repeat.
						\end{enumerate}
				\end{itemize}

				The Nelder-Mead method therefore always tries to create a big simplex and only shrink if every other action would yield a worst corner/simplex.
			% end

			\subsubsection{Notes}
				\begin{itemize}
					\item The method is not guaranteed to converge. But in practice, it yields good results.
					\item Can get stuck on a suboptimal point such that the algorithm has to be restarted with other initial simplex corners.
				\end{itemize}

				\paragraph{Simplex-Gradient}
					It is possible to detect stagnation using a \emph{Simplex-Gradient} \( \vec{D}^{(k)} \in \R^{n_p} \), \( \vec{D}^{(k)} = \big( \mat{V}^{(k)} \big)^{-T} \vec{\delta}^{(k)} \) where \( \mat{V}^{(k)} \) is the matrix of the simplex directions
					\begin{align*}
						\mat{V}^{(k)} =
						\begin{bmatrix}
							\vec{p}^{(2)} - \vec{p}^{(1)} & \vec{p}^{(3)} - \vec{p}^{(1)} & \cdots & \vec{p}^{(n_p + 1)} - \vec{p}^{(1)}
						\end{bmatrix}
						\eqqcolon
						\begin{bmatrix}
							\vec{v}^{(1)} & \vec{v}^{(2)} & \cdots & \vec{v}^{(n_p)}
						\end{bmatrix}
						\in \R^{n_p \times n_p}
					\end{align*}
					and \( \vec{\delta}^{(k)} \) is the vector of the objective differences:
					\begin{align*}
						\vec{\delta}^{(k)} =
						\begin{bmatrix}
							\varphi\big(\vec{p}^{(2)}\big) - \varphi\big(\vec{p}^{(1)}\big) \\
							\varphi\big(\vec{p}^{(3)}\big) - \varphi\big(\vec{p}^{(1)}\big) \\
							\vdots                                                          \\
							\varphi\big(\vec{p}^{(n_p + 1)}\big) - \varphi\big(\vec{p}^{(1)}\big)
						\end{bmatrix}
						\in \R^{n_p}
					\end{align*}
					Analogous to a gradient-based method, this yields a condition for \emph{minimum progress}
					\begin{align*}
						\hat{\varphi}^{(k + 1)} - \hat{\varphi}^{(k)} < -\alpha \big\lVert \vec{D}^{(k)} \big\rVert^2,\qquad
						\hat{\varphi} = \frac{1}{n_p + 1} \sum_{i = 1}^{n_p + 1} \varphi\big(\vec{p}^{(i)}\big)
					\end{align*}
					with a small \( \alpha > 0 \). One approach for a condition on when to restart is to restart if both
					\begin{align*}
						\hat{\varphi}^{(k + 1)} - \hat{\varphi}^{(k)} > -\alpha \big\lVert \vec{D}^{(k)} \big\rVert^2
						\quad\text{and}\quad
						\hat{\varphi}^{(k + 1)} - \hat{\varphi}^{(k)} < 0
					\end{align*}
					hold.
				% end
			% end
		% end

		\subsection{Multidirectional Search Methods}
			\begin{itemize}
				\item In the Nelder-Mead method a bad conditioning of the simplex, i.e. the matrix \( \mat{V}^{(k)} \), leads to problems that cannot be avoided.
				\item In multidirectional search methods this problem is avoided by making every simplex congruent to its predecessors.
				\item The algorithm uses similar steps for reflection, expansion and contraction, but possibly needs a lot more function evaluations.
			\end{itemize}
		% end

		\subsection{Asynchronous Parallel Pattern Search (APPS)}
			\begin{itemize}
				\item \emph{Asynchronous Parallel Pattern Search} is a pattern-based search method on a grid.
				\item The direction of the pattern determines the descent direction.
				\item Patterns can be varied while maintaining their mathematical properties.
				\item It is "naturally" parallelizable.
			\end{itemize}
		% end

		\subsection{Implicit Filtering}
			\emph{Implicit filtering} is a descent method using "smooth" approximations of the gradients. It uses a central approximation of the simplex gradient
			\begin{align*}
				\vec{D}_C^{(k)} = \frac{1}{2} \big( \vec{D}^{(k)} + \vec{D}_R^{(k)} \big)
			\end{align*}
			where \( \vec{D}_R^{(k)} \) is the gradient of the simplex that is reflected around \( \vec{p}^{(k)} \).

			The structure of implicit filtering is sketched in\autoref{alg:implicitFiltering}.

			\begin{algorithm}  \DontPrintSemicolon
				\textbf{Initialization:} Choose \( \alpha, \beta \in (0, 1) \) and set \( \mat{H} = \mat{I} \) \;
				\While{not converged}{
				Calculate \( \varphi\big(\vec{p}^{(k)}\big) \), \( \vec{D}_C^{(k)} \) and the search direction \( \vec{d}^{(k)} = -\mat{H}^{-1} \vec{D}_C^{(k)} \) \;
				Inexact line search for \( j = 1, \cdots, j_\mathrm{max} \), \( \lambda \coloneqq \beta^j \) until the following holds:
				\begin{align*}
					\varphi(\vec{p} + \lambda \vec{d}) - \varphi(\vec{p}) \leq \alpha \lambda \grad_{\Delta \vec{p}_k} \varphi^T(\vec{p}) \vec{d}^{(k)}
				\end{align*} \;
				\( \vec{p}^{(k + 1)} \gets \vec{p}^{(k)} + \lambda \vec{d}^{(k)} \) \;
				\If{line search successful}{
				Quasi-Newton update of the Hessian \(\mat{H}\) with \( \vec{p}^{(k + 1)} - \vec{p}^{(k)} \) and \( \vec{D}_C^{(k + 1)} - \vec{D}_C^{(k)} \) \;
				} \Else{
					\( \mat{H} \gets \mat{I} \) \;
					Shrink the simplex \;
				}
				}

				\caption{Implicit Filtering.}
				\label{alg:implicitFiltering}
			\end{algorithm}
		% end
	% end

	\section{Surrogate Optimization}
		In \emph{surrogate optimization methods}, the (complex) objective is replaced with a simpler approximation that maintains the key properties of the objective (e.g. a noisy measurement might be replaced by a simpler regression model). This surrogate function is then minimized and adjusted in order to find a good approximation of the solution of the original problem (this can also be applied for constraint functions).

		Requirements for the surrogate function \( \hat{\varphi} : \R^{n_p} \to \R \): For all function evaluation (or "sampling") points \( \Big( \vec{p}^{(i)}, \varphi\big(\vec{p}^{(i)}\big) \Big) \), \( i = 1, \cdots, m \) it must hold that
		\begin{align*}
			\varphi\big( \vec{p}^{(i)} \big) = \hat{\varphi}\big( \vec{p}^{(i)} \big) + \epsilon
		\end{align*}
		where \( \epsilon \in \R \) is some "slack" constant that determines how exact the surrogate function shall be.
		\begin{itemize}
			\item For \(\epsilon = 0\), the problem is the same as interpolation.
			\item For \(\epsilon > 0\), the surrogate function does not perfectly reproduce the objective, but might be smoother.
		\end{itemize}
		Further requirements are that \( \hat{\varphi} \) should be fast to compute and the gradients of \(\hat{\varphi}\) should be available in closed form.

		This raises some questions:
		\begin{enumerate}
			\item \(\varphi\) might be too complex for a simple approximation \(\implies\) which approximation method should be used?
			\item How to generate the data basis of the function evaluations? Generating all in one point will not yields good results\dots
			\item Which method is feasible to minimize \(\hat{\varphi}\)?
		\end{enumerate}

		\subsection{Approximation Methods}
			\subsubsection{Response Surface Methods (RSMs)}
				\emph{Response surface methods} use simple polynomials of a low degree as the model function \(\hat{\varphi}\), e.g.:
				\begin{itemize}
					\item Degree one (linear): \tabto{5.5cm} \( \hat{\varphi}(\vec{p}) = \beta_0 + \beta_1^T \vec{p} \)
					\item Degree one with mixed terms: \tabto{5.5cm} \( \hat{\varphi}(\vec{p}) = \beta_0 + \vec{\beta}_1^T \vec{p} + \sum_{i} \sum_{\substack{j \\ j \neq i}} \beta_2^{i, j} p_i p_j \)
					\item Degree two (quadratic): \tabto{5.5cm} \( \hat{\varphi}(\vec{p}) = \beta_0 + \vec{\beta}_1 \vec{p} + \vec{p}^T \mat{\beta}_2 \vec{p} \)
					\item Higher degree: \dots
				\end{itemize}
				The unknown parameters \( \beta_1 \in \R \), \( \vec{\beta}_2 \in \R^{n_p} \) and \( \mat{\beta}_2 \in \R^{n_p \times n_p} \) can be approximated using least squares:
				\begin{align*}
					\min_{\beta_1, \vec{\beta}_2, \mat{\beta}_2} \sum_i \Big( \varphi\big(\vec{p}^{(i)}\big) - \hat{\varphi}\big(\vec{p}^{(i)}\big) \Big)
				\end{align*}

				\begin{itemize}
					\item Advantage: Simple and the approximations are easy to compute.
					\item Disadvantage: The RSMs cover only the global behavior and not local accuracy.
				\end{itemize}
			% end

			\subsubsection{Radial Basis Functions (RBFs)}
				Now the surrogate function \( \hat{\varphi} \) uses a linear combination of \emph{radial basis functions}:
				\begin{align*}
					\hat{\varphi}(\vec{p}) = \sum_{i = 1}^{m} \gamma_i h\Big( \big\lVert \vec{p} - \vec{p}^{(i)} \big\rVert \Big)
				\end{align*}
				with basis function \( h(\cdot) \) based only on the euclidean distance from the interpolation point, e.g.
				\begin{itemize}
					\item Linear: \tabto{2cm} \( h(r_i) = r_i \)
					\item Cubic: \tabto{2cm} \( h(r_i) = r_i^3 \)
					\item Thin-Plate: \tabto{2cm} \( h(r_i) = r_i^2 \log r \)
				\end{itemize}
				where \( r_i = \big\lVert \vec{p} - \vec{p}^{(i)} \big\rVert \).

				A suitable combination of RSM and RBF yield cubic spline-approximation:
				\begin{align*}
					\hat{\varphi}(\vec{p}) = \beta_0 + \vec{\beta}_1 \vec{p} + \sum_i \gamma_i h(\vec{p}),\quad \beta_1 \in \R, \beta_2 \in \R^{n_p}, \gamma_i \in \R
				\end{align*}
				\begin{itemize}
					\item Univariate: \tabto{2cm} \( h(\vec{p}) = \frac{1}{12} \sum_i r_i^3 \)
					\item Bivariate: \tabto{2cm} \( h(\vec{p}) = \frac{1}{16 \pi} \sum_i r_i^2 \log r_i \)
				\end{itemize}
			% end

			\subsubsection{Design and Analysis of Computer Experiments (DACE)}
				Assuming the model function is a realization of a stochastic process
				\begin{align*}
					\hat{\varphi}(\vec{p}) = \vec{v}^T(\vec{p}) \vec{\beta} + Z(\vec{p})
				\end{align*}
				where \( \vec{v}(\vec{p}) \) is a vector of basis functions (e.g. RSM, RBF) and \( Z(\vec{p}) \) is a stationary random variable that is Gaussian distributed with zero mean. The covariance between two points \( \vec{p}^{(l)} \) and \( \vec{p}^{(k)} \) is given as
				\begin{align*}
					\Cov\Big[ Z\big(\vec{p}^{(l)}\big), Z\big(\vec{p}^{(k)}\big) \Big] = \sigma^2 R\big(\vec{p}^{(l)}, \vec{p}^{(k)}\big)
					\quad\text{with}\quad
					R\big(\vec{p}^{(l)}, \vec{p}^{(k)}\big) = \prod_{i = 1}^{n_p} e^{-\theta_i d_i^2},\quad d_i = \big\lVert \vec{p}_i^{(l)} - \vec{p}_i^{(k)} \big\rVert_2
				\end{align*}
				Die unknown parameters \(\vec{\beta}\), \(\vec{\theta}\) and \(\sigma^2\) are then estimated using statistical estimators, e.g. maximum likelihood.
			% end
		% end

		\subsection{Select of the Sampling Points}
			\subsubsection{Design of Experiments (DoE)}
				The classical strategy for selection sampling points, \emph{design of experiments} is mainly designed for physical experiments, not for deterministic ones! Typical approach:
				\begin{itemize}
					\item Classical selection
						\begin{itemize}
							\item orthogonal arrays
							\item latin hypercubes
							\item combinations
						\end{itemize}
					\item Metric-bases methods
						\begin{itemize}
							\item MiniMax: minimizing the maximal distance between the sampling points
							\item MaxiMin: maximizing the minimal distance
						\end{itemize}
					\item Stochastic selection for Gaussian processes
						\begin{itemize}
							\item Entropy Design or D-Opt: maximizing the determinant of the covariance matrix
							\item A-Opt: depends on the trace of the covariance matrix
							\item G-Opt: minimize the maximum mean squared error
						\end{itemize}
				\end{itemize}
			% end
		% end

		\subsection{Minimizing the Surrogate Function}
			To successfully minimize the original objective function, the data basis of the surrogate function has to be expanded sequentially. The following sections describe two methods for this, the Strawman and the Shoemaker method.

			\subsubsection{Strawman}
				\begin{enumerate}
					\item Calculate the current minimum of the surrogate function (e.g. using gradient descent).
					\item Add the minimum of the surrogate function as a sampling point.
					\item Calculate the new surrogate function. Repeat.
				\end{enumerate}
			% end

			\subsubsection{Shoemaker}
				\begin{enumerate}
					\item Calculate the minimum with a minimal distance to all sampling points.
					\item Extend the sampling points by this point.
					\item Determine the new surrogate function. Repeat.
				\end{enumerate}
			% end

			\subsubsection{DACE-Based, Sequential Update Strategy}
				\begin{itemize}
					\item The the expected mean error as a criteria for the quality of the surrogate function.
					\item Weigh small function values and uncertainties in the approximations.
					\item There exist different strategies following this basic idea.
					\item Termination criteria:
						\begin{itemize}
							\item Number of function evaluations
							\item No more improvements in the objective function
							\item \(\varphi(\vec{p})\) close to the minimal possible function value
						\end{itemize}
					\item Sequential methods are better on normal computers, for parallel computers a special scheme should be used.
				\end{itemize}
			% end
		% end

		\subsection{Discussion}
			\begin{itemize}
				\item Independent of the approximation method, the surrogate function has to be minimized one or more times per iteration (depending on the actual method).
				\item But the effort for the minimization is negligible as one simulation run for the evaluation of \(\varphi\) often takes a lot longer.
				\item Every method of~\autoref{c:gradientOptimization} can be used for minimizing \(\hat{\varphi}\) as the gradients are available by design.
				\item Advantages:
					\begin{itemize}
						\item \(\hat{\varphi}\) is given in closed form as well as the gradientd.
						\item Easy to compute, Newton-type methods applicable!
						\item "Smooth" surrogate function.
					\end{itemize}
				\item Disadvantages:
					\begin{itemize}
						\item Approximation accuracy is limited.
						\item The number of sample points rises a lot for high-dimensional problems (curse of dimensionality).
					\end{itemize}
			\end{itemize}
		% end
	% end

	\section{Comparison}
		\subsection{Magnetic Bearing Design} % 3.56, 3.57, 3.58, 3.59, 3.60, 3.61
			\todo{Content}
		% end

		\subsection{Walking Optimization of a Humanoid Robot} % 3.62, 3.63
			\todo{Content}
		% end
	% end

	\section{Discussion}
		\begin{itemize}
			\item Advantages:
				\begin{itemize}
					\item Application is easy, no or little prior knowledge required.
					\item Robust toward discontinuities of \(\varphi\) or \(\grad{\varphi}\).
					\item No calculation of gradients necessary.
					\item No need to start "close to" a solution.
					\item Some methods (e.g. evolutionary algorithms) are highly parallelizable, some are not (e.g. Nelder-Mead).
				\end{itemize}
			\item Disadvantages:
				\begin{itemize}
					\item Slow convergence, high number of steps needed and high computation time due to many \(\varphi\)-evaluations.
					\item Inefficient for large \( n_p \).
					\item Major difficulties for nonlinear constraints in \(\vec{p}\).
				\end{itemize}
		\end{itemize}
	% end
% end

\chapter{Gradient-Based Optimization with Constraints}
	This chapter covers the optimization problems with nonlinear equality- and inequality-constraints:
	\begin{align*}
		\min_{\vec{p} \in \R^{n_p}} & \, \varphi(\vec{p}) \\
		\mathrm{subject~to}\quad    & \,
		\begin{alignedat}[t]{3}
			\vec{a}(\vec{p}) & = \vec{0},\quad    & \vec{a} : \R^{n_p} \to \R^{n_a} \\
			\vec{b}(\vec{p}) & \geq \vec{0},\quad & \vec{b} : \R^{n_p} \to \R^{n_b}
		\end{alignedat}
	\end{align*}
	Such an optimization problem is called a \emph{nonlinear programming problem} (NLP).

	\begin{itemize}
		\item A point \( \vec{p} \in \R^{n_p} \) that fulfills the constraints is called a \emph{feasible point}.
		\item The set of all feasibly points is called the \emph{feasible region}.
		\item For a local minimum \( \vec{p}^\ast \) it holds that \( \varphi(\vec{p}^\ast) \leq \varphi(\vec{p}) \) for all feasible points \(\vec{p}\) in a neighborhood of \(\vec{p}^\ast\).
		\item All constraints that are \emph{active} at a point \(\vec{p}\) make up the \emph{active set}. This set includes all equality constraints and the active inequality constraints \( A(\vec{p}) \coloneqq \{\, j \in \N : 1 \leq j \leq n_b,\, b_j(\vec{p}) = 0 \,\} \).
	\end{itemize}

	\section{Solution Characterization}
		By a geometric view it is clear that the gradient of the active constraints must be parallel to the gradient of the objective function, i.e. there has to be a constant \( \mu_i^\ast \) for each active constraint \( a_i \) such that
		\begin{align*}
			\grad{\varphi}(\vec{p}^\ast) = \mu_i^\ast \grad{a_i}(\vec{p}^\ast)
			\quad\iff\quad
			\grad{\varphi}(\vec{p}^\ast) - \mu_i^\ast \grad{a_i}(\vec{p}^\ast) = \vec{0}
		\end{align*}

		This leads directly leads to the definition of the Lagrangian
		\begin{align*}
			L(\vec{p}, \vec{\mu}, \vec{\sigma}) = \varphi(\vec{p}) - \vec{\mu}^T \vec{a}(\vec{p}) - \vec{\sigma}^T \vec{b}(\vec{p})
		\end{align*}
		with the Lagrange multiplier \( \vec{\mu} \), \( \vec{\sigma} \) which encodes the insight above.

		For formulating the \emph{first-order optimality conditions} analogous to the ones for unconstrained optimization (see~\autoref{subsec:multiDimensionalOptimalityConditions}), a \emph{constraint qualification} must hold: The gradients
		\begin{align*}
			\grad{a_1}(\vec{p}^\ast), \cdots, \grad{a_{n_p}}(\vec{p}^\ast) \quad\text{and}\quad \grad{b_j}(\vec{p}^\ast), j \in A(\vec{p}^\ast)
		\end{align*}
		of the active constraints have to linearly independent.

		\subsection{First-Order Necessary Optimality Conditions (Karush-Kuhn-Tucker Conditions, KKT)}
			Let \( \varphi : \R^{n_p} \to \R \), \( \vec{a} : \R^{n_p} \to \R^{n_a} \) and \( \vec{b} : \R^{n_p} \to \R^{n_b} \) be continuously differentiable and let the constraint qualification be fulfilled. If \( \vec{p} \) is a feasible local minimum of the NLP, then there exist Lagrange multiplier \( \vec{\mu} \in \R^{n_a} \), \( \vec{\sigma} \in \R^{n_b} \) such that the \emph{Karush-Kuhn-Tucker} conditions hold:
			\begin{align}
				\grad_{\vec{p}}L(\vec{p}, \vec{\mu}, \vec{\sigma}) & = \vec{0}    \tag{KKT.i}                      \\
				\vec{\mu}^T \vec{a}(\vec{p})                       & = \vec{0}    \tag{KKT.iia}                    \\
				\vec{\sigma}^T \vec{b}(\vec{p})                    & = \vec{0}    \tag{KKT.iib}                    \\
				\vec{\sigma}                                       & \geq \vec{0} \tag{KKT.iii} \label{eq:kkt-iii}
			\end{align}
			Here, \( L(\vec{p}, \vec{\mu}, \vec{\sigma}) \) is the Lagrangian
			\begin{align*}
				L(\vec{p}, \vec{\mu}, \vec{\sigma}) \coloneqq \varphi(\vec{p}) - \vec{\mu}^T \vec{a}(\vec{p}) - \vec{\sigma}^T \vec{b}(\vec{p})
			\end{align*}
			and the inequality in~\eqref{eq:kkt-iii} is element-wise.
		% end

		\subsection{Second-Order Necessary Optimality Conditions}
			The second-order necessary optimality condition is fulfilled if the first-order condition is fulfilled and the Hessian of the Lagrangian has a positive curvature \emph{along the feasible directions}:
			\begin{gather*}
				%\forall \vec{z} \in \R^{n_p} \setminus \{\vec{0}\},\, \vec{z}^T \cdot \grad{a_i}(\vec{p}) = \vec{0},\, \vec{z}^T \cdot \grad{b_j}(\vec{p}) :
				\vec{z}^T \mat{H}_L^{\vec{p}}(\vec{p}, \vec{\mu}, \vec{\sigma}) \vec{z} \geq 0 \\
				\quad\text{for all }
				\vec{z} \in \R^{n_p} \setminus \{\vec{0}\}
				\text{ with }
				\big( \vec{z}^T \cdot \grad{a_i}(\vec{p}) = \vec{0} \big)_{i = 1, \cdots, n_a},
				\text{ and }
				\big( \vec{z}^T \cdot \grad{b_j}(\vec{p}) = \vec{0} \big)_{j \in A(\vec{p})}
			\end{gather*}
		% end

		\subsection{Example} % 4.11, 4.13
			\todo{Content}
		% end
	% end

	\section{Simple Bounds, Box Constraints}
		The most common type of inequality-constraints are simple lower and upper bounds on the optimization variables:
		\begin{align*}
			\min_{\vec{p} \in \R^{n_p}} & \, \varphi(\vec{p}) \\
			\mathrm{subject~to}\quad    & \,
			\vec{p}_{i, \mathrm{min}} \leq \vec{p}_i \leq \vec{p}_{i, \mathrm{max}}
		\end{align*}
		In practice, these are not only used in nearly all constrained, but also in unconstrained optimization problems as they might increase the efficiency by constraining the search space. They also increase robustness of optimization techniques that ensure the fulfilling of the constrains in every iteration.

		Box constraints can also be used to prohibit "insecure values", e.g. negative variables when the objective function takes square roots.

		For gradient-free methods, the constraints can be ensured by clipping the new iteration value to the bounds:
		\begin{align*}
			p_i^{(k + 1)} = \max\big\{\, p_{i, \mathrm{min}},\, \min\big\{\, p_{i, \mathrm{max}},\, p_i^{(k + 1)} \,\big\} \big\}
		\end{align*}
		For gradient-based methods, box constraints can be considered like every other linear and nonlinear constraint (more on that later).
	% end

	\section{Penalty Function}
		The approach of \emph{penalty functions} is to transform the constrained problem to an unconstrained problem by punishing violations of the constraints. As this arises big problems with lots of and highly nonlinear constraints, penalty functions are rarely used in practice anymore in favor of sequential quadratic programming (see~\autoref{sec:sqp}).

		As of today, penalty functions are mainly used for step size determination for nonlinear programs and in the application of robust, gradient-free methods in the unconstrained optimization for solving NLPs.

		\subsection{Exterior Penalty Functions}
			The original nonlinear problem is replaced by an unconstrained problem with a penalty function \(\Phi\)
			\begin{align*}
				\min_{\vec{p} \in \R^{n_p}} \Phi(\vec{p}, \rho),\quad \Phi(\vec{p}, \rho) = \varphi(\vec{p}) + \rho \sum_j \pi_j(\vec{p}),\quad \rho \in \R^+
			\end{align*}
			with a function \( \pi_j \) per constraint that is positive if the constraint is violated and zero otherwise.

			Often a series of unconstrained optimization problems \( \Phi(\vec{p}, \rho) \) is solved with an increasing \(\rho\), such that the solution \( \vec{p}^\ast(\rho) \) gets pushed into the feasible region of the NLP step by step in in the hope of
			\begin{align*}
				\liminfty[\rho] \vec{p}^\ast(\rho) = \vec{p}^\ast
			\end{align*}
			where \( \vec{p}^\ast \) is the real minimum.

			This approach is called \emph{exterior penalty functions} as the penalty term is only relevant if \( \vec{p} \) violates the according constraints.

			\subsubsection{Quadratic Penalty Function}
				The \emph{quadratic penalty function} is the most common exterior penalty function:
				\begin{align*}
					\Phi_Q(\vec{p}, \rho) = \varphi(\vec{p}) + \rho \cdot \frac{1}{2} \Bigg( \sum_{k = 1}^{n_a} \big(a_k(\vec{p})\big)^2 + \sum_{jk = 1}^{n_b} \big(\hat{b}_k(\vec{p})\big)^2 \Bigg)
				\end{align*}
				Here, \( \hat{b}_j(\vec{p}) \) denotes the violation of the inequality constraints:
				\begin{align*}
					\hat{b}_j(\vec{p}) =
					\begin{cases*}
						0             & iff \( b_j(\vec{p}) \geq 0 \) \\
						-b_j(\vec{p}) & iff \( b_j(\vec{p}) < 0 \)
					\end{cases*}
					= \big\lvert\! \min\{ 0,\, b_j(\vec{p}) \} \big\rvert
				\end{align*}

				But in the case of \( \rho \to \infty \), the Hessian of \( \Phi_C \) gets more and more ill-condition and may even be singular in the limit.
			% end

			\paragraph{Example} % 4.22, 4.23
				\todo{Content}
			% end
		% end

		\subsection{Interior Penalty Functions}
			When using exterior penalty functions, it is not guaranteed that every solution of the iterating unconstrained problems fulfills the constraints of the original NLP. Therefore, exterior penalty functions are not applicable if fulfilling the constraints in every iteration is required. \emph{Interior penalty functions} guarantee exactly that: Every subproblem yields a feasible solution.

			Given a NLP with only inequality constraints, interior penalty functions use \emph{barrier functions} that have the following properties:
			\begin{itemize}
				\item Value of infinity everywhere except inside the feasible region.
				\item Continuously differentiable inside the feasible region.
				\item The values go to \( +\infty \) as \(\vec{p}\) gets closer to the edge of the feasible region.
			\end{itemize}

			\subsubsection{Logarithmic Barrier Function}
				The \emph{logarithmic barrier function} is the most used interior penalty function:
				\begin{align*}
					\Phi_B(\vec{p}, r) = \varphi(\vec{p}) - r \sum_{k = 1}^{n_b} \ln b_i(\vec{p})
				\end{align*}
				The barrier parameter \( r > 0 \) will be decreased step by step and the solution should converge to the minimum as \( r \to 0 \).

				But in the case of \( r \to 0 \), the Hessian of \( \Phi_B \) gets more and more ill-condition and may even be singular in the limit.
			% end

			\paragraph{Example} % 4.26
				\todo{Content}
			% end
		% end

		\subsection{Exact Penalty Functions}
			Neither the quadratic penalty function nor the logarithmic barrier function are "exact" penalty function so that, with an appropriate \(\rho\) or respectively \(r\), the solution of the unconstrained NLP yields the exact solution.

			One important penalty function of this class is the \emph{exact \(\ell_1\)-penalty function}:
			\begin{align*}
				\Phi_{\ell_1}(\vec{p}, \rho) = \varphi(\vec{p}) + \rho \sum_{k = 1}^{n_a} \big\lvert a_k(\vec{p}) \big\rvert + \rho \sum_{k = 1}^{n_b} \big\lvert\! \min\{ 0,\, b_j(\vec{p}) \} \big\rvert
			\end{align*}
			However, the \(\ell_1\)-penalty function is not differentiable! This make the numerical solution difficult. But the minimization yields, for adequate big \(\rho\), the minimum of the NLP!

			\paragraph{Example 1} % 4.28
				\todo{Content}
			% end

			\paragraph{Example 2} % 4.29
				\todo{Content}
			% end
		% end

		\subsection{Augmented Lagrangian}
			The downsides of exterior, interior and the \(\ell_1\)-penalty function can be avoided by not using the objective directly, but by using the Lagrangian:
			\begin{align*}
				L_Q(\vec{p}, \vec{\mu}, \vec{\sigma}, \rho) = \varphi(\vec{p}) - \vec{\mu}^T \vec{a}(\vec{p}) - \vec{\sigma}^T \vec{b}(\vec{p}) + \rho \cdot \frac{1}{2} \Bigg( \sum_{k = 1}^{n_a} \big(a_k(\vec{p})\big)^2 + \sum_{jk = 1}^{n_b} \big( \min\{ 0,\, b_j(\vec{p}) \} \big)^2 \Bigg)
			\end{align*}
			But this requires a good approximation of the Lagrange multiplier \( \vec{\mu} \) and \( \vec{\sigma} \).

			\paragraph{Example 1} % 4.31, 4.32
				\todo{Content}
			% end

			\paragraph{Example 2} % 4.33
				\todo{Content}
			% end

			\subsubsection{Notes}
				\begin{itemize}
					\item In practice, the augmented Lagrangian \( L_Q \) is used in the following fashion:
						\begin{enumerate}
							\item Choose Lagrange multipliers \( \vec{\mu} \), \( \vec{\sigma} \) and the parameter \( \rho > 0 \).
							\item Calculate a local minimum \( \vec{p}^\ast(\rho) \) of \( L_Q \) using methods of the unconstrained optimization.
							\item Update the Lagrange multipliers and \(\rho\). Repeat with 2.
						\end{enumerate}
					\item The update of the Lagrange multipliers according to \( \vec{\mu}(\rho) = -\rho \vec{a}\big(\vec{p}^\ast(\rho)\big) \) is based on the convergence properties of the quadratic penalty function.
					\item If
						\begin{itemize}
							\item \(\vec{p}^\ast\) is a minimum of the NLP,
							\item the constraint qualification hold and
							\item the KKT-conditions and the second-order sufficient optimality condition is fulfilled for \( \vec{\mu}^\ast \), \( \vec{\sigma}^\ast \),
						\end{itemize}
						then
						\begin{itemize}
							\item exists a threshold \(\hat{\rho}\) for which it holds that for all \( \rho \geq \hat{\rho} \) it holds that
							\item \(\vec{p}^\ast\) is a strict local minimum of the (quadratic) augmented Lagrangian \( L_Q(\vec{p}, \vec{\mu}^\ast, \vec{\sigma}^\ast, \rho) \).
						\end{itemize}
				\end{itemize}
			% end
		% end
	% end

	\section{Constraint Elimination}
		For NLPs with active constraints it is often tempting to transform the NLP to remove some of the constraints and to reduce the number of optimization variables (the degrees of freedom). But this might yields wrong results! It has to be assured that:
		\begin{itemize}
			\item The original minimum is not eliminated.
			\item The nonlinearity of the problem is not increased too much (causing problems in finite difference approximations of derivatives).
			\item No singularities arise in the transformed objective.
			\item No new discontinuities and non-differentiabilities are added to the objective.
			\item The Hessian of the surrogate problem is not singular or ill-conditioned near the minimum.
			\item The transformed problem does not have additional local minima or stationary points.
			\item And a lot more.
		\end{itemize}
		In general: Keep more constraints and keep the function as linear as possible instead of applying transformations that behave badly.

		\paragraph{Example 1} % 4.38
			\todo{Content}
		% end

		\paragraph{Example 2} % 4.39
			\todo{Content}
		% end

		\paragraph{Example 3} % 4.40
			\todo{Content}
		% end
	% end

	\section{Sequential Quadratic Programming (SQP)}
		\label{sec:sqp}

		To take care of highly nonlinear equality and inequality constraints, information about the trajectory of these have to be considered, i.e. gradient and Hessian information. Assuming that the constraints that are active on the solution are known, the optimization problem is given as:
		\begin{align*}
			\min_{\vec{p} \in \R^{n_p}} & \, \varphi(\vec{p})                                                 \\
			\mathrm{subject~to}\quad    & \, \vec{a}(\vec{p}) = \vec{0},\quad \vec{a} : \R^{n_p} \to \R^{n_a}
		\end{align*}
		The KKT-conditions are then equivalent to the simpler formulation
		\begin{align*}
			\grad{L}(\vec{p}, \vec{\mu}) \coloneqq
			\begin{bmatrix}
				\grad_{\vec{p}}L(\vec{p}, \vec{\mu}) \\
				\grad_{\vec{\mu}}L(\vec{p}, \vec{\mu})
			\end{bmatrix}
			=
			\begin{bmatrix}
				\grad{\varphi}(\vec{p}) - \sum_{k = 1}^{n_a} \mu_k \cdot \grad{a_k}(\vec{p}) \\
				-\vec{a}(\vec{p})
			\end{bmatrix}
			= \vec{0}
		\end{align*}
		with the Lagrangian
		\begin{align*}
			L(\vec{p}, \vec{\mu}) = \varphi(\vec{p}) - \vec{\mu}^T \vec{a}(\vec{p})
		\end{align*}
		This yields a system of \( n_p + n_a \) nonlinear equations for \( n_p + n_a \) unknowns \( \vec{p} \), \( \vec{\mu} \).

		Taylor-expanding the gradient of the Lagrangian around \( \big( \vec{p}^{(k)}, \vec{\mu}^{(k)} \big) \) yields
		\begin{align*}
			\grad{L}(\vec{p}^\ast, \vec{\mu}^\ast) \overset{T\big(\vec{p}^{(k)}, \vec{\mu}^{(k)}\big)}{=} \grad{L}\big(\vec{p}^{(k)}, \vec{\mu}^{(k)}\big) + \mat{H}_L\big(\vec{p}^{(k)}, \vec{\mu}^{(k)}\big) \begin{bmatrix} \vec{d}_p^{(k)} \\ \vec{d}_\mu^{(k)} \end{bmatrix} + \cdots \overset{!}{=} 0
		\end{align*}
		with \( \vec{d}_p^{(k)} \coloneqq \vec{p}^\ast - \vec{p}^{(k)} \) and \( \vec{d}_\mu^{(k)} \coloneqq \vec{\mu}^\ast - \vec{\mu}^{(k)} \). Cutting off the higher-order terms yields the \emph{Lagrange-Newton method} where the search direction \( \vec{d}_p^{(k)} \), \( \vec{d}_\mu^{(k)} \) is given as the solution of
		\begin{align}
			\mat{H}_L\big(\vec{p}^{(k)}, \vec{\mu}^{(k)}\big) \begin{bmatrix} \vec{d}_p^{(k)} \\ \vec{d}_\mu^{(k)} \end{bmatrix} = -\grad{L}\big(\vec{p}^{(k)}, \vec{\mu}^{(k)}\big) \label{eq:sqpEqSystem}
		\end{align}
		The Iteration equation \( k \to k + 1 \) is analogous to the Newton method:
		\begin{align*}
			\begin{bmatrix}
				\vec{p}^{(k + 1)} \\
				\vec{\mu}^{(k + 1)}
			\end{bmatrix}
			=
			\begin{bmatrix}
				\vec{p}^{(k)} \\
				\vec{\mu}^{(k)}
			\end{bmatrix}
			+
			\begin{bmatrix}
				\vec{d}_p^{(k)} \\
				\vec{d}_\mu^{(k)}
			\end{bmatrix}
		\end{align*}
		But there is one catch: The set of active constraints at the minimum is generally not known and can change in every iteration.

		\subsection{Finding the Search Direction}
			Further analysis of the linear system~\eqref{eq:sqpEqSystem} with the Lagrangian
			\begin{align*}
				L(\vec{p}, \vec{\mu}) = \varphi(\vec{p}) - \vec{\mu}^T \vec{a}(\vec{p})
			\end{align*}
			yields that the linear system has the following structure:
			\begin{align*}
				\begin{bmatrix}
					\mat{H}_L^{\vec{p}}\big(\vec{p}^{(k)}, \vec{\mu}^{(k)}\big) & -\mat{J}_{\vec{a}}\big(\vec{p}^{(k)}\big) \\
					-\mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big)                 & \mat{0}
				\end{bmatrix}
				\begin{bmatrix}
					\vec{d}_p^{(k)} \\
					\vec{d}_\mu^{(k)}
				\end{bmatrix}
				=
				\begin{bmatrix}
					-\grad{\varphi}\big(\vec{p}^{(k)}\big) + \mat{J}_{\vec{a}}\big(\vec{p}^{(k)}\big) \vec{\mu}^{(k)} \\
					\vec{a}\big(\vec{p}^{(k)}\big)
				\end{bmatrix}
			\end{align*}
			Where \( \mat{H}_L^p\big(\vec{p}^{(k)}\big) \) is the Hessian of the Lagrangian w.r.t. \(\vec{p}\) and \( \mat{J}_{\vec{a}}\big(\vec{p}^{(k)}\big) \) is the Jacobian
			\begin{align*}
				\mat{J}_{\vec{a}}\big(\vec{p}^{(k)}\big) =
				\begin{bmatrix}
					\pdv{a_1}{p_1}     & \cdots & \pdv{a_{n_a}}{p_1}     \\
					\vdots             & \ddots & \vdots                 \\
					\pdv{a_1}{p_{n_p}} & \cdots & \pdv{a_{n_a}}{p_{n_p}}
				\end{bmatrix}
			\end{align*}
			\textbf{Note that this definition of the Jacobian differs from the usual definition! This one has the gradients of the function has columns!}

			This linear system can be transformed to
			\begin{align*}
				\begin{bmatrix}
					\mat{H}_L^{\vec{p}}\big(\vec{p}^{(k)}, \vec{\mu}^{(k)}\big) & -\mat{J}_{\vec{a}}\big(\vec{p}^{(k)}\big) \\
					-\mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big)                 & \mat{0}
				\end{bmatrix}
				\begin{bmatrix}
					\vec{d}_p^{(k)} \\
					\underbrace{\vec{d}_\mu^{(k)} + \vec{\mu}^{(k)}}_{\vec{\mu}^{(k + 1)}}
				\end{bmatrix}
				=
				\begin{bmatrix}
					-\grad{\varphi}\big(\vec{p}^{(k)}\big) \\
					\vec{a}\big(\vec{p}^{(k)}\big)
				\end{bmatrix}
			\end{align*}
			where \( \vec{d}_p^{(k)} \) can be viewed as the solution of a quadratic minimization problem!

			\subsubsection{Quadratic Problem (QP)}
				The said quadratic problem is given as:
				\begin{align*}
					\min_{\vec{d}_p \in \R^{n_p}} & \, \varphi\big(\vec{p}^{(k)}\big) + \Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big)\!\Big)^T \vec{d}_p + \frac{1}{2} \vec{d}_p^T \mat{H}_L^{\vec{p}}\big(\vec{p}^{(k)}, \vec{\mu}^{(k)}\big) \vec{d}_p \\
					\mathrm{subject~to}\quad      & \, \vec{a}\big(\vec{p}^{(k)}\big) + \mat{J}_a^T\big(\vec{p}^{(k)}\big) \vec{d}_p = \vec{0}
				\end{align*}
				Where the quadratic objective function of the QP consists of a quadratic Taylor-approximation of the NLP objective plus a weighted curvature condition via the Hessian of the activate conditions:
				\begin{align*}
					\mat{H}_L^{\vec{p}}\big(\vec{p}^{(k)}, \vec{\mu}^{(k)}\big) = \mat{H}_\varphi\big(\vec{p}^{(k)}\big) - \sum_{i = 1}^{n_a} \mu_i^T \mat{H}_{a_i}\big(\vec{p}^{(k)}\big)
				\end{align*}
				The linear constraints of the QP also consist of Taylor-approximations of the active NLP constraints.

				Solving this quadratic optimization problem is more robust and possibly faster than solving the linear system of equations. There exist special algorithms for solving QPs. But even though they are not active, the inequality constraints must be fulfilled. Therefore, the QP is extended to also determine the Lagrange multipliers \( \vec{\mu}^{(k)} \), \( \vec{\sigma}^{(k)} \) along with the search direction \( \vec{d}_p^{(k)} \):
				\begin{align}
					\min_{\vec{d}_p \in \R^{n_p}} & \, \varphi\big(\vec{p}^{(k)}\big) + \Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big)\!\Big)^T \vec{d}_p + \frac{1}{2} \vec{d}_p^T \mat{H}_L^{\vec{p}}\big(\vec{p}^{(k)}, \vec{\mu}^{(k)}, \vec{\sigma}^{(k)}\big) \vec{d}_p \\
					\mathrm{subject~to}\quad      & \,
					\begin{alignedat}[t]{2}
						\vec{a}\big(\vec{p}^{(k)}\big) + \mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) \vec{d}_p & = \vec{0}    \\
						\vec{b}\big(\vec{p}^{(k)}\big) + \mat{J}_{\vec{b}}^T\big(\vec{p}^{(k)}\big) \vec{d}_p & \geq \vec{0}
					\end{alignedat}  \label{eq:qp}
				\end{align}
				Iteration step \( k \to k + 1 \): \( \vec{p}^{(k + 1)} = \vec{p}^{(k)} + \vec{d}_p^{(k)} \)
			% end

			\subsubsection{Notes}
				\begin{itemize}
					\item Similar to the Newton method, it can be shown under some assumptions that the SQP method converges to a local minimum.
					\item Algorithms for solving general QPs can be used to determine the active constraints in each iteration by solving the QP. \\ Even though it is not needed, it is useful for efficiency to use as much information as possible from the last iteration ("hot start").
					\item Other approaches determine the active constraints (working set) outside of the QP solution to only solve QPs with equality constraints which is especially efficient.
				\end{itemize}
			% end
		% end

		\subsection{Step Size Rules}
			If the initialization \(\vec{p}^{(0)}\) is "far" away from the minimum (or the QP is a bad, local approximation of the NLP), the convergence can be improved by determining the optimal step size for
			\begin{align*}
				\vec{p}^{(k + 1)} = \vec{p}^{(k)} + \alpha^{(k)} \vec{d}_p^{(k)}
				\quad\text{or respectively}\quad
				\begin{bmatrix}
					\vec{p}^{(k + 1)} \\
					\vec{\mu}^{(k + 1)}
				\end{bmatrix}
				=
				\begin{bmatrix}
					\vec{p}^{(k)} \\
					\vec{\mu}^{(k)}
				\end{bmatrix}
				+
				\alpha^{(k)}
				\begin{bmatrix}
					\vec{d}_p^{(k)} \\
					\vec{\mu}_{QP}^{(k + 1)} - \vec{\mu}^{(k)}
				\end{bmatrix}
			\end{align*}
			Common methods for determining the step size are
			\begin{itemize}
				\item the (quadratic) augmented Lagrangian \( L_Q \) or
				\item the exact \(\ell_1\)-penalty function \( \Phi_{\ell_1} \)
			\end{itemize}
			with step size rules like the Armijo rule similar as in the unconstrained optimization (see~\autoref{sec:gradientUnconsStepSize}).
		% end

		\subsection{Approximation of the Lagrange Multipliers}
			If the approximation \( \vec{p}^{(k)} \) is far away from the NLP solution, it is not useful to use the Lagrange multipliers of the QP for the NLP (as the linearization is only valid locally). Another method is to approximate the Lagrange multipliers after calculating \( \vec{p}^{(k + 1)} \) (i.e. solving the QP) using minimum least squares:
			\begin{align*}
				\min_{\mu \in \R^{n_a}} & \,
				\bigg\lVert
				\grad{\varphi}\big(\vec{p}^{(k + 1)}\big)
				- \sum_{i = 1}^{n_a} \mu_i^{(k + 1)} \cdot \grad{a_i}\big(\vec{p}^{(k + 1)}\big)
				\bigg\rVert_2^2              \\
				\intertext{or equivalently model the active inequality constraints explicitly:}
				\min_{\mu \in \R^{n_a}} & \,
				\bigg\lVert
				\grad{\varphi}\big(\vec{p}^{(k + 1)}\big)
				- \sum_{i = 1}^{n_a} \mu_i^{(k + 1)} \cdot \grad{a_i}\big(\vec{p}^{(k + 1)}\big)
				- \sum_{i \in A\big(\vec{p}^{(k + 1)}\big)} \sigma_i^{(k + 1)} \cdot \grad{a_i}\big(\vec{p}^{(k + 1)}\big)
				\bigg\rVert_2^2
			\end{align*}
			Solving least squares problem will be discussed in detail in~\autoref{c:leastSquares}.
		% end

		\subsection{Termination Criteria}
			An obvious termination criteria would be to check whether the necessary KKT-conditions are sufficiently fulfilled, i.a. \( \grad{L}(\vec{p}, \vec{\mu}) \approx \vec{0} \).

			The commonly used SQP method \emph{NPSOL} terminates if all of the following criteria are fulfilled:
			\begin{enumerate}
				\item Old and new approximation do not change any more:
			\end{enumerate}
			\begin{align*}
				\big\lVert \vec{p}^{(k + 1)} - \vec{p}^{(k)} \big\rVert = \alpha^{(k)} \cdot \big\lVert \vec{d}_p^{(k)} \big\rVert_2 \leq \sqrt{\varepsilon_\mathrm{opt}} \Big( 1 + \big\lVert \vec{p}^{(k + 1)} \big\rVert_2 \Big)
			\end{align*}
			\begin{enumerate}
				\setcounter{enumi}{1}
				\item Gradient of the objective function that is projected onto the active constraints vanishes:
			\end{enumerate}
			\begin{align*}
				\Big\lVert \vec{Z}^T \cdot \grad{\varphi}\big(\vec{p}^{(k + 1)}\big) \Big\rVert_2 \leq \sqrt{\varepsilon_\mathrm{opt}} \Big( 1 + \max\Big\{ 1 + \big\lvert \varphi\big(\vec{p}^{(k + 1)}\big) \big\rvert,\, \big\lVert \grad{\varphi}\big(\vec{p}^{(k + 1)}\big) \big\rVert_2 \} \Big)
			\end{align*}
			\begin{enumerate}
				\setcounter{enumi}{2}
				\item Constraints are sufficiently fulfilled:
			\end{enumerate}
			\begin{align*}
				\big\lvert a_i\big(\vec{p}^{(k + 1)}\big) \big\rvert & \leq \varepsilon_\mathrm{ft},\quad i = 1, \cdots, n_a  \\
				b_j\big(\vec{p}^{(k + 1)}\big)                       & \geq -\varepsilon_\mathrm{ft},\quad i = 1, \cdots, n_b
			\end{align*}
			Where the constraints \( \varepsilon_\mathrm{opt} \), \( \varepsilon_\mathrm{ft} \) have to be chosen by the user.

			In the more modern method \emph{SNOPT}, all of the following two criteria have to be fulfilled:
			\begin{gather*}
				\frac{
					\max_{i = 1, \cdots, n_p} \bigg\{ \bigg\lvert \pdv{\varphi\big(\vec{p}^{(k)}\big)}{p_i} - \sum_{j = 1}^{n_a} \mu_j^{(k)} \cdot \pdv{a_j\big(\vec{p}^{(k)}\big)}{p_i} - \sum_{l \in A\big(\vec{p}^{(k)}\big)} \sigma_j^{(k)} \cdot \pdv{b_l\big(\vec{p}^{(k)}\big)}{p_i} \bigg\rvert \bigg\}
				}{
					\sqrt{\sum_{j = 1}^{n_a} \big( \mu_j^{(k)} \big)^2 + \sum_{l \in A\big(\vec{p}^{(k)}\big)} \big(\sigma_j^{(k)}\big)^2}
				} \leq \varepsilon_\mathrm{opt} \\
				\frac{\max\Big\{ \big\lvert a_j\big(\vec{p}^{(k)}\big) \big\rvert : i = 1, \cdots, n_p \Big\} \cup \Big\{ \big\lvert \min\big\{ 0,\, b_l\big(\vec{p}^{(k)}\big) \big\} \big\rvert : l \in A\big(\vec{p}^{(k)}\big) \Big\}}{\big\lVert \vec{p}^{(k)} \big\rVert_2} \leq \varepsilon_\mathrm{ft}
			\end{gather*}

			A common criteria to detect failures is a maximum number of iterations \( k_\mathrm{max} \).
		% end

		\subsection{Hessian Approximation}
			The quadratic problem\eqref{eq:qp} needs the \( (n_p \times n_p) \)-dimensional Hessian of the Lagrangian
			\begin{align*}
				\mat{H}_L^{\vec{p}}\big(\vec{p}^{(k)}, \vec{\mu}^{(k)}, \vec{\sigma}^{(k)}\big) = \mat{H}_\varphi\big(\vec{p}^{(k)}\big) - \sum_{i = 1}^{n_a} \mu_i^{(k)} \mat{H}_{a_i}\big(\vec{p}^{(k)}\big) - \sum_{i \in A\big(\vec{p}^{(k)}\big)} \sigma_i^{(k)} \mat{H}_{b_i}\big(\vec{p}^{(k)}\big)
			\end{align*}
			for (theoretical) quadratic convergence of the SQP method. But in practice, the second-order derivatives (the Hessian) is often not available! Additionally it is assumed that the Hessian has a positive curvature (i.e. is positive definite) along all feasible directions, which is fulfilled near a strict minimum. However, this cannot be assumed in every iteration.

			Hence, approximations/modifications of the Hessian are required.

			\subsubsection{Na{\"i}ve Approach: BFGS Approximation}
				It is tempting to use the BFGS update for the Hessian that is really successful in the unconstrained optimization. The update rule is given as:
				\begin{align*}
					\tilde{\mat{H}}^{(k + 1)} & = \tilde{\mat{H}}^{(k)}
					- \frac{1}{\big(\vec{d}^{(k)}\big)^T \tilde{\mat{H}}^{(k)} \vec{d}^{(k)}} \tilde{\mat{H}}^{(k)} \vec{d}^{(k)} \big(\tilde{\mat{H}}^{(k)} \vec{d}^{(k)}\big)^T
					+ \frac{1}{\big(\vec{g}^{(k)}\big)^T \vec{d}^{(k)}} \vec{g}^{(k)} \big(\vec{g}^{(k)}\big)^T                                                                                                                   \\
					\vec{g}^{(k)}             & = \grad_{\vec{p}}L\big(\vec{p}^{(k + 1)}, \vec{\mu}^{(k + 1)}, \vec{\sigma}^{(k + 1)}\big) - \grad_{\vec{p}}L\big(\vec{p}^{(k)}, \vec{\mu}^{(k + 1)}, \vec{\sigma}^{(k + 1)}\big)
				\end{align*}
				But it is not necessary that the full Hessian of the Lagrangian is positive definite! Additionally, it is very inefficient to calculate the full Hessian if the NLP has lots of active constraints. Hence, the approximation has to be modified in order to be useful for SQP methods.
			% end

			\subsubsection{Reduced Hessian} % 4.61, 4.63, 4.64, 4.65, 4.66, 4.67, 4.68
				Every active constraints reduces the degrees of freedom by one Hence the degrees of freedom are the number of optimization variables \(n_p\) minus the number of active and linearly independent constraints. In all of the following it is assumed that it is known which constraints are active at the solution, yielding the following, simpler, view of the NLP:
				\begin{align}
					\min_{\vec{d}_p \in \R^{n_p}} & \, \varphi\big(\vec{p}^{(k)}\big) + \Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big)\!\Big)^T \vec{d}_p + \frac{1}{2} \vec{d}_p^T \mat{H}_L^{\vec{p}}\big(\vec{p}^{(k)}, \vec{\mu}^{(k)}, \vec{\sigma}^{(k)}\big) \vec{d}_p \\
					\mathrm{subject~to}\quad      & \,
					\begin{alignedat}[t]{2}
						\vec{a}\big(\vec{p}^{(k)}\big) + \mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) \vec{d}_p & = \vec{0}
					\end{alignedat}  \label{eq:activeQp}
				\end{align}
				where the last Jacobian \( \mat{J}_{\vec{a}}^T \) might also contain active inequality constraints which are left out here for brevity. But they are handled the same as regular equality constraints and can be considered to be part of it knowing which are active (as assumed). The degrees of freedom of this NLP are therefore \( n_p - n_a \).

				Assuming the current approximation \( \vec{p}^{(k)} \) fulfills the constraints, \( \vec{a}\big(\vec{p}^{(k)}\big) = \vec{0} \), the next approximation also has to fulfill the constraints:
				\begin{align*}
					\vec{a}\big(\vec{p}^{(k)} + \vec{d}_p\big) = \vec{0}
				\end{align*}
				By Taylor-expanding this equation around \( \vec{p}^{(k)} \)
				\begin{align*}
					\vec{a}\big(\vec{p}^{(k)} + \vec{d}_p\big) \overset{T\big(\vec{p}^{(k)}\big)}{=} \vec{a}\big(\vec{p}^{(k)}\big) + \mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) \vec{d}_p + \cdots = 0
				\end{align*}
				a linear system for the search direction \( \vec{d}_p \) can be found such that the new iteration is also feasible:
				\begin{align*}
					\mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) \vec{d}_p = \vec{0}
				\end{align*}
				Thus, \( \vec{d}_p \) has to lie in the kernel of \( \mat{J}_{\vec{a}}^T \) and the dimensionality of the kernel is \( n_p - n_a \).

				Hence, the kernel is spanned by \( n_p - n_a \) basis vectors (which are not uniquely determined). Let \( \mat{Z}^{(k)} \) be the matrix that contains these basis vectors as columns, then
				\begin{align*}
					\mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) \mat{Z}^{(k)} = \mat{0}
				\end{align*}
				holds and the search direction \( \vec{d}_p \in \R^{n_p} \) can be represented as
				\begin{align*}
					\vec{d}_p = \mat{Y}^{(k)} \vec{d}_p^y + \mat{Z}^{(k)} \vec{d}_p^z
				\end{align*}
				where \( \mat{Z}^{(k)} \in \R^{n_p \times (n_p - n_a)} \) are the basis vectors of the kernel and \( \mat{Y}^{(k)} \in \R^{n_p \times n_a} \) are the basis vectors of the image space of \( \mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) \). The vectors \( \vec{d}_p^y \in \R^{n_a} \) and \( \vec{d}_p^z \in \R^{n_p - n_a} \) are unknown and have to be computed to solve the QP. Plugging this formulation of \( \vec{d}_p \) into the constraints of~\eqref{eq:activeQp}:
				\begin{align}
					     &  & \vec{0}                         & = \vec{a}\big(\vec{p}^{(k)}\big) + \mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) \vec{d}_p                                                                                  & \nonumber             \\
					\iff &  & -\vec{a}\big(\vec{p}^{(k)}\big) & = \mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) \vec{d}_p                                                                                                                   & \nonumber             \\
					     &  &                                 & = \mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) \big( \mat{Y}^{(k)} \vec{d}_p^y + \mat{Z}^{(k)} \vec{d}_p^z \big)                                                           & \nonumber             \\
					     &  &                                 & = \mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) \mat{Y}^{(k)} \vec{d}_p^y + \underbrace{\mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) \mat{Z}^{(k)}}_{=\, \mat{0}} \vec{d}_p^z & \nonumber             \\
					     &  &                                 & = \mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) \mat{Y}^{(k)} \vec{d}_p^y                                                                                                   & \label{eq:qpDYlinear}
				\end{align}
				The vector \( \vec{d}_p^y \) is therefore determined by the active constraints via the linear system~\eqref{eq:qpDYlinear}. Now there remain \( n_p - n_a \) degrees of freedom for the actual optimization.

				Plugging the formulation of \( \vec{d}_p \) into the objective of the QP\footnote{Note that the parameters of the Hessian are kept implicitly for brevity.}
				\begin{align*}
					  & \, \varphi\big(\vec{p}^{(k)}\big) + \Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big)\!\Big)^T \vec{d}_p + \frac{1}{2} \vec{d}_p^T \mat{H}_L^{\vec{p}} \vec{d}_p                                                                                                                                                                         \\
					= & \, \varphi\big(\vec{p}^{(k)}\big) + \Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big)\!\Big)^T \big( \mat{Y}^{(k)} \vec{d}_p^y + \mat{Z}^{(k)} \vec{d}_p^z \big) + \frac{1}{2} \big( \mat{Y}^{(k)} \vec{d}_p^y + \mat{Z}^{(k)} \vec{d}_p^z \big)^T \mat{H}_L^{\vec{p}} \big( \mat{Y}^{(k)} \vec{d}_p^y + \mat{Z}^{(k)} \vec{d}_p^z \big)
				\end{align*}
				yields the following objective when leaving out all constant parts w.r.t. \( \vec{d}_p^z \), as that is the optimization variable:
				\begin{align*}
					\varphi\big(\vec{p}^{(k)}\big) + \Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big)\!\Big)^T \mat{Z}^{(k)} \vec{d}_p^z + \big(\vec{d}_p^y\big)^T \big(\mat{Z}^{(k)})^T \mat{H}_L^{\vec{p}} \big( \mat{Z}^{(k)} \vec{d}_p^z \big) + \frac{1}{2} \big(\vec{d}_p^z\big)^T \big(\mat{Z}^{(k)})^T \mat{H}_L^{\vec{p}} \mat{Z}^{(k)} \vec{d}_p^z
				\end{align*}
				The solution of this optimization problem can be computed by solving the following linear system (if the reduced Hessian is positive definite, which it is close to a strict local minimum):
				\begin{align*}
					\Big( \big(\mat{Z}^{(k)}\big)^T \mat{H}_L^{(k)} \mat{Z}^{(k)} \Big) \vec{d}_p^z = -\big(\mat{Z}^{(k)}\big)^T \mat{H}_L^{(k)} \mat{Y}^{(k)} \vec{d}_p^y - \big(\mat{Z}^{(k)}\big)^T \cdot \grad{\varphi}\big(\vec{p}^{(k)}\big)
				\end{align*}

				\paragraph{Summary}
					Using the representation \( \vec{d}_p = \mat{Y}^{(k)} \vec{d}_p^y + \mat{Z}^{(k)} \vec{d}_p^z \), the search direction as the solution of the QP~\eqref{eq:activeQp} can be computed by solving two staggered linear systems:
					\begin{align*}
						\mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) \mat{Y}^{(k)} \vec{d}_p^y            & = -\vec{a}\big(\vec{p}^{(k)}\big)                                                                                                              \\
						\Big( \big(\mat{Z}^{(k)}\big)^T \mat{H}_L^{(k)} \mat{Z}^{(k)} \Big) \vec{d}_p^z & = -\big(\mat{Z}^{(k)}\big)^T \mat{H}_L^{(k)} \mat{Y}^{(k)} \vec{d}_p^y - \big(\mat{Z}^{(k)}\big)^T \cdot \grad{\varphi}\big(\vec{p}^{(k)}\big)
					\end{align*}

					\begin{itemize}
						\item If \( \vec{a}\big(\vec{p}^{(k)}\big) = \vec{0} \), i.e. the constraints are linear, \( \vec{d}_p^y = \vec{0} \).
						\item If additionally \( \big(\mat{Z}^{(k)}\big)^T \cdot \grad{\varphi}\big(\vec{p}^{(k)}\big) = \vec{0} \), then \( \vec{d}_p^z = \vec{0} \).
					\end{itemize}
				% end

				\paragraph{First- and Second-Order Conditions}
					Using the matrix \( \mat{Z} \) some of the necessary and sufficient conditions can be formulated equivalent:
					\begin{itemize}
						\item First-order necessary condition:
					\end{itemize}
					\begin{align*}
						\grad{\varphi}(\vec{p}^\ast) - \sum_{i = 1}^{n_a} \mu_i \cdot \grad{a_i}(\vec{p}^\ast) - \sum_{i \in A(\vec{p}^\ast)} = \vec{0}
						\quad\iff\quad
						\mat{Z}^T(\vec{p}^\ast) \cdot \grad{\varphi}(\vec{p}^\ast) = \vec{0}
					\end{align*}
					\begin{itemize}
						\item Second-order necessary condition: \\ The reduced Hessian of the Lagrangian \( \mat{Z}^T(\vec{p}^\ast) \cdot \mat{H}_L^{\vec{p}}(\vec{p}^\ast, \vec{\mu}^\ast, \vec{\sigma}^\ast) \cdot \mat{Z}(\vec{p}^\ast) \) is positive semidefinite.
						\item Second-order sufficient condition: \\ The reduced Hessian of the Lagrangian is positive definite.
					\end{itemize}
				% end

				\paragraph{Example} % 4.62
					\todo{Content}
				% end
			% end

			\subsubsection{Approximation of the Reduced Hessian}
				The reduced Hessian contains all information that is needed to compute the QP solution! Hence, SQP methods can be built on this reduced Hessian. A Quasi-Newton approximation, e.g. BFGS, can be used to update the reduced Hessian:
				\begin{align*}
					\tilde{\mat{H}}^{(k + 1)} & = \tilde{\mat{H}}^{(k)}
					- \frac{1}{\big(\vec{d}^{(k)}\big)^T \tilde{\mat{H}}^{(k)} \vec{d}^{(k)}} \tilde{\mat{H}}^{(k)} \vec{d}^{(k)} \big(\tilde{\mat{H}}^{(k)} \vec{d}^{(k)}\big)^T
					+ \frac{1}{\big(\vec{g}^{(k)}\big)^T \vec{d}^{(k)}} \vec{g}^{(k)} \big(\vec{g}^{(k)}\big)^T                                                                                                                                                                                       \\
					\vec{d}^{(k)}             & = \vec{d}_p^z                                                                                                                                                                                                                                         \\
					\vec{g}^{(k)}             & = \big(\mat{Z}^{(k + 1)}\big)^T \cdot \grad_{\vec{p}}L\big(\vec{p}^{(k + 1)}, \vec{\mu}^{(k + 1)}, \vec{\sigma}^{(k + 1)}\big) - \big(\mat{Z}^{(k)}\big)^T \cdot \grad_{\vec{p}}L\big(\vec{p}^{(k)}, \vec{\mu}^{(k + 1)}, \vec{\sigma}^{(k + 1)}\big)
				\end{align*}

				Analogous to unconstrained optimization, it is helpful to replace the rank-2 update with a rank-1 update and directly approximate a Cholesky decomposition.
			% end
		% end

		\subsection{SQP Method (Algorithm)}
			A sketch of the implementation of an SQP method is given in~\autoref{alg:sqp}.

			\begin{algorithm}  \DontPrintSemicolon
				\textbf{Initialization:} Choose an initial approximation \(\vec{p}^{(0)}\), set \( k \gets 0 \) \;
				\While{not converged}{
				Calculate the Lagrange multipliers \(\vec{\mu}^{(k)}\), \(\vec{\sigma}^{(k)}\) using least squares:
				\begin{align*}
					\min_{\mu \in \R^{n_a}} & \,
					\bigg\lVert
					\grad{\varphi}\big(\vec{p}^{(k + 1)}\big)
					- \sum_{i = 1}^{n_a} \mu_i^{(k + 1)} \cdot \grad{a_i}\big(\vec{p}^{(k + 1)}\big)
					- \sum_{i \in A\big(\vec{p}^{(k + 1)}\big)} \sigma_i^{(k + 1)} \cdot \grad{a_i}\big(\vec{p}^{(k + 1)}\big)
					\bigg\rVert_2^2
				\end{align*} \;
				\If{termination criteria fulfilled}{
					\Return \;
				}
				Calculate new search direction \( \vec{d}_p^{(k)} \) by solving the quadratic problem:
				\begin{align*}
					\min_{\vec{d}_p \in \R^{n_p}} & \, \varphi\big(\vec{p}^{(k)}\big) + \Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big)\!\Big)^T \vec{d}_p + \frac{1}{2} \vec{d}_p^T \mat{H}_L^{\vec{p}}\big(\vec{p}^{(k)}, \vec{\mu}^{(k)}, \vec{\sigma}^{(k)}\big) \vec{d}_p \\
					\mathrm{subject~to}\quad      & \,
					\begin{alignedat}[t]{2}
						\vec{a}\big(\vec{p}^{(k)}\big) + \mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) \vec{d}_p & = \vec{0}    \\
						\vec{b}\big(\vec{p}^{(k)}\big) + \mat{J}_{\vec{b}}^T\big(\vec{p}^{(k)}\big) \vec{d}_p & \geq \vec{0}
					\end{alignedat}
				\end{align*} \;
				Calculate the step size \( \alpha^{(k)} \) using by minimizing a merit function, e.g. the augmented Lagrangian
				\begin{align*}
					\min_{\alpha \in \R^+} \, L_Q\big( \vec{p}^{(k)} + \alpha \vec{d}_p^{(k)},\, \vec{\mu}^{(k)} + \alpha \vec{d}_\mu^{(k)},\, \vec{\sigma}^{(k)} + \alpha \vec{d}_\sigma^{(k)},\, \rho^{(k)} \big)
				\end{align*}
				or the exact \(\ell_1\)-penalty function \( \min_{\alpha \in \R^+} \, \Phi_{\ell_1} \big( \vec{p}^{(k)} + \alpha \vec{d}_p^{(k)},\, \rho^{(k)} \big) \) \;
				Calculate the new solution approximation:
				\begin{align*}
					\vec{p}^{(k + 1)} \gets \vec{p}^{(k)} + \alpha^{(k)} \vec{d}_p^{(k)}
				\end{align*} \;
				}

				\caption{Sequential Quadratic Programming}
				\label{alg:sqp}
			\end{algorithm}
		% end

		\subsection{Notes}
			\begin{itemize}
				\item When using the reduced Hessian, the matrix \( \mat{Z}^{(k)} \) has to be updated in every iteration.
				\item Especially for high-dimensional problems, the gradients and Jacobians are sparse, i.e. only a few entries are nonzero. This behavior can be exploited in order to optimize implementations a lot.
				\item To rise efficiency and robustness further, sophisticated implementations of SQP methods (e.g. NPSOL, SNOPT) differentiate between constraints like
					\begin{itemize}
						\item upper and lower bounds
						\item linear constraints
						\item nonlinear constraints
					\end{itemize}
			\end{itemize}
		% end

		\subsection{Examples} % N/A
			\todo{Content}

			\subsubsection{Optimal Control of a 6-DoF Industry Robot} % 4.71, 4.72
				\todo{Content}
			% end

			\subsubsection{Car Drive} % 4.73, 4.74
				\todo{Content}
			% end
		% end

		\subsection{Wrap-Up}
			\begin{itemize}
				\item Motivation:
					\begin{itemize}
						\item The Newton method is applied to determine a root ("zero point") of the gradient of the Lagrangian.
						\item This yields a linear equation system to determine a search direction.
						\item Requires first- and second-order derivatives of the objective and the constraints.
						\item Preliminaries for the derivative where differentiability and knowing which constraints are active.
					\end{itemize}
				\item Basic structure:
					\begin{itemize}
						\item The linear system derived as a first step is equivalent to the solution of a quadratic problem (QP).
						\item Solving the QP is faster and more robust than solving the linear system directly.
						\item This yields a sequence of quadratic problems, thus \emph{sequential quadratic programming} (SQP).
					\end{itemize}
				\item Improvement of the basic structure:
					\begin{itemize}
						\item For globalizing the method, a step size determination was introduced using line search on an appropriate test function (penalty function).
						\item As the second-order derivatives are commonly not available, Quasi-Newton approximations of the Lagrangian are used.
					\end{itemize}
				\item SQP for high-dimensional NLPs:
					\begin{itemize}
						\item If the NLP is high-dimensional, the reduced Hessian shall be used.
						\item The QP can be solved faster and the Hessian of the Lagrangian can be approximation using Quasi-Newton approaches!
						\item High-dimensional NLPs often have sparse gradients as Jacobians which can be used for further performance improvements.
					\end{itemize}
				\item Outlook:
					\begin{itemize}
						\item There are lots of SQP methods, e.g. trust-region SQP that can work with indefinite or negative definite Hessians or methods that allow only feasible approximations in every iteration (feasible SQP methods), while the "classic" SQP only fulfills the constraints at the end (infeasible SQP method).
						\item Other numerical methods for solving nonlinear, constrained optimization problems exist, e.g. inner point methods.
					\end{itemize}
			\end{itemize}
		% end
	% end
% end

\chapter{Calculation of Derivatives}
	To use efficient gradient-based algorithms, the first-order derivatives
	\begin{align*}
		\pdv{\varphi}{p_i} &  & \pdv{a_k}{p_i} &  & \pdv{b_l}{p_i}
	\end{align*}
	of the objective and the constraints are needed. But these are typically not available directly! And even one wrong derivative could destroy the fast convergence properties\dots

	\section{Finite Difference Approximation (numerical Differentiation)}
		\subsection{Forward Difference Approximation}
			The most known approximation of the first derivative  is the \emph{forward difference approximation}
			\begin{align*}
				\pdv{\varphi(\vec{p})}{p_i} \approx D_{V, i} \varphi(\vec{p}) = \frac{1}{\delta_i} \big( \varphi(\vec{p} + \vec{e}_i \delta_i) - \varphi(\vec{p}) \big)
			\end{align*}
			where \( \vec{e}_i \in \R^{n_p} \) is the \(i\)-th unit vector and \(\delta_i\) is an appropriate step size. The complete error is:

			\subsubsection{Error}
				The complete error of the approximation is composed of the following:
				\begin{itemize}
					\item Approximation error (theoretical error)
					\item Function precision
					\item Rounding error
				\end{itemize}

				\paragraph{Approximation Error}
					The (theoretical) approximation error is given as the neglected terms of the Taylor approximation (i.e. the Lagrangian remainder):
					\begin{align*}
						     &  & \varphi(\vec{p} + \vec{e}_i \delta_i)                                                                                                                             & \overset{\mathclap{\,T(\vec{p})\,}}{=} \varphi(\vec{p}) + \pdv{\varphi(\vec{p})}{p_i} \delta_i + \frac{1}{2} \pdv[2]{\varphi(\tilde{\vec{p}})}{p_i} \delta_i^2,\quad \tilde{\vec{p}} \in [\vec{p}, \vec{p} + \vec{e}_i \delta_i] & \\
						\iff &  & \varphi(\vec{p} + \vec{e}_i \delta_i) - \varphi(\vec{p}) - \pdv{\varphi(\vec{p})}{p_i} \delta_i                                                                   & = \frac{1}{2} \pdv[2]{\varphi(\tilde{\vec{p}})}{p_i} \delta_i^2                                                                                                                                                                  & \\
						\iff &  & \underbrace{\frac{1}{\delta_i} \big( \varphi(\vec{p} + \vec{e}_i \delta_i) - \varphi(\vec{p}) \big)}_{=\, D_{V, i}\varphi(\vec{p})} - \pdv{\varphi(\vec{p})}{p_i} & = \frac{1}{2} \pdv[2]{\varphi(\tilde{\vec{p}})}{p_i} \delta_i                                                                                                                                                                    & \\
						\iff &  & D_{V, i}\varphi(\vec{p}) - \pdv{\varphi(\vec{p})}{p_i}                                                                                                            & = \frac{1}{2} \pdv[2]{\varphi(\tilde{\vec{p}})}{p_i} \delta_i \eqqcolon T_{V, i}(\varphi; \delta_i)                                                                                                                              &
					\end{align*}
					In theory, the error should decrease with the step size. But today's computers have finite arithmetic! Other error factors have a serious role demolishing this theoretical result.
				% end

				\paragraph{Function Precision}
					The function precision takes into account that the target function \( \varphi \) cannot be calculated with machine precision, e.g. because the evaluation depends on other methods or because rounding errors have summed up due to cancellation or ill-conditioning.

					This can be taken into account with the absolute errors \( \varepsilon \), \( \varepsilon_{\delta_i} \) of the function evaluations:
					\begin{align*}
						\hat{\varphi}(\vec{p}) = \varphi(\vec{p}) + \varepsilon &  & \hat{\varphi}(\vec{p} + \vec{e}_i \delta_i) = \varphi(\vec{p} + \vec{e}_i \delta_i) + \varepsilon_{\delta_i}
					\end{align*}
					Where the absolute error can be expressed in terms of a relative error \( \varepsilon_R = 10^{-n_d} \) as \( \varepsilon = \varepsilon_R \varphi(\vec{p}) \) where \(n_d\) are the number of decimal places that are correct. Plugging \( \hat{\varphi} \) into the forward approximation yields the function precision error \( C(D_{V, i}\varphi; \delta_i) \):
					\begin{align*}
						D_{V, i}\hat{\varphi}(\vec{p})
						= \frac{1}{\delta_i} \big( \hat{\varphi}(\vec{p} + \vec{e}_i \delta_i) - \hat{\varphi}(\vec{p}) \big)
						= \frac{1}{\delta_i} \big( \varphi(\vec{p} + \vec{e}_i \delta_i) - \varphi(\vec{p}) \big) + \frac{\varepsilon_{\delta_i} - \varepsilon}{\delta_i}
						\eqqcolon D_{V, i} \varphi(\vec{p}) + C(D_{V, i} \varphi; \delta_i)
					\end{align*}
				% end

				\paragraph{Rounding Error}
					Additionally to the function precision and the theoretical error, rounding errors are produced by the subtraction and the division. But if \(\delta_i\) does not get "too small", these are negligible compared to the approximation error and the function precision.
				% end

				\paragraph{Total Error}
					Hence, the total error is given as:
					\begin{align}
						T_{V, i}(\varphi; \delta_i) + C(D_{V, i} \varphi; \delta_i) = \frac{1}{2} \pdv[2]{\varphi(\tilde{\vec{p}})}{p_i} \delta_i + \frac{\varepsilon_{\delta_i} - \varepsilon}{\delta_i}  \label{eq:forwardError}
					\end{align}
				% end
			% end

			\subsubsection{Choosing the Step Size}
				Ideally, the step size should be chooses such that the error is minimal. As the error term~\eqref{eq:forwardError} contains second-order derivatives that are not available, an upper bound has to be drawn on the error that more or less independent of the derivatives:
				\begin{align*}
					\Bigg\lvert \frac{1}{2} \pdv[2]{\varphi(\tilde{\vec{p}})}{p_i} \delta_i + \frac{\varepsilon_{\delta_i} - \varepsilon}{\delta_i} \Bigg\rvert
					                           & \leq \frac{1}{2} \delta_i \Bigg\lvert \pdv[2]{\varphi(\tilde{\vec{p}})}{p_i} \Bigg\rvert + \frac{1}{\delta_i} \big\lvert \varepsilon_{\delta_i} - \varepsilon \big\rvert
					\leq \frac{1}{2} \delta_i L_{\varphi\prime\prime, i} + \frac{2}{\delta_i} \varepsilon_R L_\varphi                                                                                                     \\
					L_{\varphi\prime\prime, i} & \coloneqq \max\Bigg\{\, \Bigg\lvert \pdv[2]{\varphi(\tilde{\vec{p}})}{p_i} \Bigg\rvert \,:\, \tilde{\vec{p}} \in [\vec{p}, \vec{p} + \vec{e}_i \delta_i] \,\Bigg\}       \\
					L_\varphi                  & \coloneqq \max\Big\{\, \big\lvert \varphi(\vec{p}) \big\rvert,\, \big\lvert \varphi(\vec{p} + \vec{e}_i \delta_i) \big\rvert \,\Big\}
				\end{align*}
				Minimizing this w.r.r. to the step size yields:
				\begin{gather*}
					\min_{\delta_i \in \R^+} \, \Phi(\delta_i),\quad \Phi(\delta_i) = \frac{1}{2} \delta_i L_{\varphi'', i} + \frac{2}{\delta_i} \varepsilon_R L_\varphi \\
					\implies\quad \Phi'(\delta_i) = \frac{1}{2} L_{\varphi\prime\prime, i} - \frac{2}{\delta_i^2} \varepsilon L_\varphi \overset{!}{=} 0
					\quad\overset{L_{\varphi\prime\prime, i} \,\neq\, 0}{\iff}\quad \delta_i = \sqrt{\frac{4 \varepsilon_R L_\varphi}{L_{\varphi'', i}}}
				\end{gather*}
				If \( L_\varphi / L_{\varphi\prime\prime, i} \approx \), then \( \delta_i \approx 2 \sqrt{\varepsilon_R} \). If additionally it is possible to evaluate the function with machine precision, i.e. \( \varepsilon_R = \varepsilon_\mathrm{mach} \), the optimal step size is simply
				\begin{align*}
					\delta_i \approx 2 \sqrt{\varepsilon_\mathrm{mach}}
				\end{align*}
				This step size often is a good choice and thus set as the default in most implementations. It also explains the rule of thumb that forward-differences can approximately evaluate half of decimal places correctly.
			% end

			\subsubsection{Notes}
				\begin{itemize}
					\item For evaluating the gradient \( \grad{\varphi}(\vec{p}) \) it is necessary to
						\begin{itemize}
							\item Determine \(n_p\) step sized \(\delta_i\) and to evaluate \(\varphi\) at least \( 2n_p \) times to approximate \( L_{\varphi\prime\prime, i} \).
							\item Every iteration of a gradient-based method needs the gradients causing high computation times.
							\item It is better to use a one-time approximation of \emph{relative step sizes} \( \varepsilon_i \) with \( \delta_i = \varepsilon_i \big(1 + \lvert p_i \rvert\big) \) at the initialization \(\vec{p}^{(0)}\).
							\item The value \( \varepsilon_i = 5 \sqrt{\varepsilon_\mathrm{mach}} \) can be used as an initial relative step size.
							\item If the optimization fails, restart with a new initialization and re-calculate the step sizes.
						\end{itemize}
				\end{itemize}
			% end
		% end

		\subsection{Central-Difference Approximation}
			For forward difference approximation often yields results that are good enough, except the gradients are too small. Additionally, the forward approximation is not sufficient if the optimization step size \( \alpha^{(k)} \) such that the changes in \( \vec{p} \) are less than the step size \(\delta\) or the differences in the function values are "too small" relative to \(\delta\).

			A potentially more exact approximation are \emph{central differences}:
			\begin{align*}
				\pdv{\varphi(\vec{p})}{p_i} \approx D_{Z, i}\varphi(\vec{p}) = \frac{1}{2 \delta_i} \big( \varphi(\vec{p} + \vec{e}_i \delta_i) - \varphi(\vec{p} - \vec{e}_i \delta_i) \big)
			\end{align*}
			Analogous to the forward differences, Taylor-expand this formula yields the insight that the order of the approximation error is \( \mathcal{O}(\delta_i^2) \) while the order of the forward differences is \( \mathcal{O}(\delta_i) \). Analogous to the forward differences, the optimal step size \( \delta_i^Z \) can be calculated by minimizing an upper bound on the total error, yielding the optimal step size
			\begin{align*}
				\delta_i^Z = \sqrt[3]{\frac{3 \varepsilon_R L_\varphi}{L_{\varphi\prime\prime\prime, i}}},\quad L_{\varphi\prime\prime\prime, i} = \max\Bigg\{\, \Bigg\lvert \pdv[3]{\varphi(\tilde{\vec{p}})}{p_i} \Bigg\rvert \,:\, \tilde{\vec{p}} \in \big[ \vec{p} - \vec{e}_i \delta_i^Z, \vec{p} + \vec{e}_i \delta_i^Z \big] \,\Bigg\}
			\end{align*}
			For functions with \( L_\varphi / L_{\varphi\prime\prime\prime, i} \approx 1 \) it follows \( \delta_i^Z \approx \sqrt[3]{3\varepsilon_R} \approx \delta_i^{2/3} \). If additionally \( L_\varphi \approx 1 \) and \( L_{\varphi\prime\prime\prime, i} \approx 1 \), it follows:
			\begin{align*}
				\Bigg\lvert D_{Z, i}\hat{\varphi}(\vec{p}) - \pdv{\varphi(\vec{p})}{p_i} \Bigg\rvert \leq \varepsilon_R^{2/3}
			\end{align*}
			Hence, the rule of thumb that central difference can approximately evaluate two third of decimal places correctly.

			But central differences produce much more computational overhead compared to forward/backward differences! Thus they should only be used if needed (by switching from forward to central differences).

			To approximate the second derivative, the following schema can be used
			\begin{align*}
				\pdv[2]{\varphi(\vec{p})}{p_i} \approx \frac{1}{\delta_i^2} \big( \varphi(\vec{p} + \vec{e}_i \delta_i) - 2\varphi(\vec{p}) + \varphi(\vec{p} - \vec{e}_i \delta_i) \big)
			\end{align*}
			that is a combination of forward and backward differences to approximate the second-order derivative. This directly gives the \(i\)-th diagonal entry of the Hessian of \(\varphi\), which can reduce the number of iterations needed.
		% end
	% end

	\section{Numerical Differentiation of Simulation Models}
		An important class in optimization is simulation-based optimization where the system state \( \vec{x}(t) \) is given as the numerical solution of ODEs or PDEs. In this setting, the objective \(\varphi\) and the constraints \(\vec{a}\), \(\vec{b}\) are dependent on the state variables \(\vec{x}\) of an ODE/PDE system. Hence, the calculation of \( \varphi\big(\vec{x}(\vec{p})\big) \) requires solving the ODE/PDE numerically:
		\begin{itemize}
			\item Every calculation of \( \varphi\big(\vec{x}(\vec{p})\big) \), \(\vec{a}\) and \(\vec{b}\) is computationally expensive.
			\item The calculation is only possible with simulation errors (i.e. approximation error in the ODE/PDE solver and accumulated rounding errors).
			\item The gradients (see below) are generally not available and have to me approximated.
		\end{itemize}
		\begin{align*}
			\grad{\varphi}\big(\vec{x}(\vec{p})\big) = \pdv{\varphi\big(\vec{x}(\vec{p})\big)}{\vec{p}} = \pdv{\varphi}{\vec{x}} \pdv{\vec{x}}{\vec{p}}
		\end{align*}

		\subsection{Derivative of ODE-Simulation Models}
			\label{subsec:derivativeOde}

			Given an IVP (initial value problem) \( \dot{\vec{x}} = \vec{f}(t, \vec{x}; \vec{p}) \), \( \vec{x}(0) = \vec{x}_0 \), \( 0 \leq t \leq t_f \), the derivatives of the (numerical) solution \( \vec{x}(t; \vec{p}) \) w.r.t. to the parameters \( \vec{p} \) are required. Formally, the parameters \(\vec{p}\) can be transformed to initial values \(\vec{x}_0\) and thus the derivatives w.r.t. the parameters to derivatives w.r.t. the initial values. The derivative w.r.t. the initial values is called the \emph{sensitivity matrix}:
			\begin{align*}
				\pdv{\vec{x}(t; \vec{x}_0)}{\vec{x}_0}
			\end{align*}

			The IVP with the parameters \(\vec{p}\) transformed to initial values is given as
			\begin{align*}
				\begin{bmatrix}
					\dot{x}_1 \\
					\vdots    \\
					\dot{x}_{n_x}
				\end{bmatrix}
				= \dot{\vec{x}} = \vec{f}(t, \vec{x}, x_{n_x + 1}, \cdots, x_{n_x + n_p}) \\
				\begin{bmatrix}
					\dot{x}_{n_x + 1} \\
					\vdots            \\
					\dot{x}_{n_x + n_p}
				\end{bmatrix}
				=
				\begin{bmatrix}
					0      \\
					\vdots \\
					0
				\end{bmatrix}
			\end{align*}
			with \( x_{n_x + 1} \coloneqq p_1 \), \( \cdots \), \( x_{n_x + n_p} \coloneqq p_{n_p} \) and the initial values
			\begin{align*}
				\vec{x}(0) = \vec{x}_0 \qquad
				\begin{bmatrix}
					x_{n_x + 1}(0) \\
					\vdots         \\
					x_{n_x + n_p}(0)
				\end{bmatrix}
				=
				\begin{bmatrix}
					p_1    \\
					\vdots \\
					p_{n_p}
				\end{bmatrix}
			\end{align*}
			That is, the "parameter states" are added as time invariant states always equaling the parameters.
		% end

		\subsection{External Numerical Differentiation}
			\subsubsection{Na{\"i}ve Approach} % 5.29, 5.30, 5.31
				Just use forward differences:
				\begin{align*}
					\pdv{\vec{x}(t; \vec{p})}{p_i} \approx \frac{1}{\delta_i} \big( \vec{x}(t; \vec{p} + \vec{e}_i \delta_i) - \vec{x}(t; \vec{p}) \big)
				\end{align*}
				\begin{itemize}
					\item This approach requires solving \(n_p\) additional IVP solutions for calculating the tweaked evaluations.
					\item To solve the IVP as best as possible, it may use variable step sizes.
					\item This causes gradient-based algorithms to have extremely big problems in finding the minimum!
					\item But this is not only caused by the optimization method\dots
				\end{itemize}

				\paragraph{Runge-Kutta Methods}
					When using Runge-Kutta methods of order \(m\), the numerical solutions depends on the integration tolerance and the internal step width of the integration. By just looking at the initial values, the error of RK methods of order \(m\) is
					\begin{align*}
						\tilde{\vec{x}}(t; \vec{x}_0, h_1)                      & = \vec{x}(t; \vec{x}_0) + \sum_{j = m}^{N} \tilde{c}_j(t, \vec{x}_0) h_1^j + \mathcal{O}\big(h_1^{N + 1}\big)                                           \\
						\tilde{\vec{x}}(t; \vec{x}_0 + \vec{e}_i \delta_i, h_2) & = \vec{x}(t; \vec{x}_0 + \vec{e}_i \delta_i) + \sum_{j = m}^{N} \tilde{c}_j(t, \vec{x}_0 + \vec{e}_i \delta_i) h_2^j + \mathcal{O}\big(h_2^{N + 1}\big)
					\end{align*}
					with differentiable functions \( \tilde{c}_j(t, \vec{x}_0) \). Plugging this into the forward difference scheme yields:
					\begin{align*}
						  & \, \frac{1}{\delta_i} \big( \tilde{\vec{x}}(t; \vec{x}_0 + \vec{e}_i \delta_i, h_2) - \tilde{\vec{x}}(t; \vec{x}_0, h_1) \big)                                                                                                                                                                                                                                                                                                \\
						= & \, \pdv{\vec{x}(t; \vec{x}_0)}{x_{0, i}} + \mathcal{O}(\delta_i) + \sum_{j = m}^{N} \tilde{c}_j(t, \vec{x}_0 + \vec{e}_i \delta_i) \cdot \underbrace{\frac{h_2^j - h_1^j}{\delta_i}}_{\to\,\infty} + \sum_{j = m}^{N} \Bigg( \pdv{\tilde{c}_j(t, \vec{x}_0)}{x_{0, i}} + \mathcal{O}(\delta_i) \!\Bigg) h_1^j + \mathcal{O}\Bigg( \frac{h_1^{N + 1}}{\delta_i} \Bigg) + \mathcal{O}\Bigg( \frac{h_2^{N + 1}}{\delta_i} \Bigg)
					\end{align*}
					This is the problem! If the integration steps \( h_1 \neq h_2 \) are not equal, the error term becomes dominant as \( \delta_i \) usually is "small".

					A "solution" would be to set \( h_1 = h_2 \), causing bad integration results.
				% end
			% end

			\subsubsection{Coupled Forward Differences Approximation}
				It is possible to simultaneously integrate an \( (n_p + 1) \)-times big IVP for each tweaked value guaranteeing \( h_1 = h_2 \):
				\begin{align*}
					\dot{\vec{x}} & = \vec{f}(t, \vec{x}),\quad \vec{x}(0) = \vec{x}_0 \\
					\dot{\vec{x}} & = \vec{f}(t, \vec{x})                              \\
					              & \,\,\,\vdots                                       \\
					\dot{\vec{x}} & = \vec{f}(t, \vec{x})
				\end{align*}
			% end
		% end

		\subsection{Internal Numerical Differentiation}
			The external numerical differentiation costs a lot of time! One insight: The sensitivity matrix can be expresses as the solution of a matrix-ODE:
			\begin{align*}
				\dv{t} \pdv{\vec{x}(t; \vec{x}_0)}{\vec{x}_0} = \pdv{\vec{f}\big(t, \vec{x}(t; \vec{x}_0)\big)}{\vec{x}} \pdv{\vec{x}(t; \vec{x}_0)}{\vec{x}_0},\quad \pdv{\vec{x}(0; \vec{x}_0)}{\vec{x}_0} = \mat{I}
			\end{align*}

			\begin{itemize}
				\item Variant 1: Simultaneously integrate the ODE and the matrix-ODE with a standard integrator.
				\item Variant 2 (better): Differentiated integrate method calculates \( \vec{x}(t; \vec{p}) \), \( \pdv{\vec{x}(t; \vec{p})}{\vec{p}} \), \( \pdv{\dot{\vec{x}}(t; \vec{p})}{\vec{p}} \)
					\begin{itemize}
						\item Advantage: Really efficient.
						\item Disadvantage: Implementation complexity; especially complication for switching points.
					\end{itemize}
			\end{itemize}
		% end
	% end

	\section{Symbol Differentiation}
		If the functions \( \varphi \), \( \vec{a} \), \( \vec{b} \) are given as explicit formulas, the derivatives \( \grad{\varphi}(\vec{p}) \), \( \mat{J}_{\vec{a}}(\vec{p}) \), \( \mat{J}_{\vec{b}}(\vec{p}) \) could be evaluated using a computer algebra system (e.g. Maple, Mathematica, SymPy, \dots) in closed form. This is based on a systematic application of chain, product, \dots rules and often requires a special input format.
		\begin{itemize}
			\item Advantage: No approximation error, only rounding errors.
			\item Disadvantages: Can lead to complex functions that are computationally expensive to evaluate.
		\end{itemize}
	% end

	\section{Automatic Differentiation}
		\emph{Automatic differentiation} refers to a technique to generate first and possible second-order derivatives of an existing program that calculates \(\varphi\), \(\vec{a}\), \(\vec{b}\). This is based on the insight that every every so complex function can be composed of as a sequence of elementary functions with one or two arguments:
		\begin{itemize}
			\item 1-argument functions: Sine, Cosine, Exponential, Logarithm, \dots
			\item 2-argument functions: Addition, Subtraction, Multiplication, Division, Exponentiation
		\end{itemize}
		AD-methods are based on an analysis of the evaluation sequence as a \emph{computation graph} of elementary functions.

		There are two main design decisions:
		\begin{itemize}
			\item Pre-compile a program to get the derivative function to be able to evaluation the function and the derivatives simultaneously (forward mode).
			\item Build the computation graph after the function has been evaluated and evaluate the derivative afterwards (backward mode).
		\end{itemize}
		The computational complexity of the gradient in forward mode on a scalar function with multiply variables \(\varphi(\vec{p})\) can be as high as for symbolic differentiation: \( \mathcal{O}(n_p \). But for the backward mode, a complexity of \( \mathcal{O}(5) \) can be reached independent of \( n_p \)! But the memory complexity can rise a lot\dots

		\begin{itemize}
			\item Advantages:
				\begin{itemize}
					\item No approximation error, only rounding errors.
					\item AD is continuously improving and even today's algorithms are capable of a lot of calculations.
				\end{itemize}
			\item Disadvantages:
				\begin{itemize}
					\item It is problematic to handle piecewise functions (if-then-else), approximation of tabular data, approximations of Sine, Cosine, \dots by rationale functions. \\ But this is problematic even for analytical derivatives\dots
				\end{itemize}
		\end{itemize}
		Even ODE- and PDE-simulations can be handled using AD!

		Popular implementations, commonly used in machine learning, are libraries like TensorFlow (static computation graph) and PyTorch (dynamic computation graph).
	% end
% end

\chapter{Parameter Estimation}
	\label{c:leastSquares}

	This chapter covers a special type of objective function: the sum of squares, called \emph{least squares} optimization problems:
	\begin{align*}
		\varphi(\vec{p}) = \frac{1}{2} \sum_{i = 1}^{n_r} r_i^2(\vec{p}) = \frac{1}{2} \lVert \vec{r}(\vec{p}) \rVert_2^2
	\end{align*}
	This objective function arises in a lot of optimization problems, e.g.: Given some detected corner points \( (x_i, y_i)_{i = 1, \cdots, n_r} \) of a ball, what is the radius \(R_K\) and the position \( (x_K, y_K) \) if the ball? The residual (error) function \(\vec{r}\) can be determined from the circle equation:
	\begin{align*}
		R_K^2 = (x_i - x_K)^2 + (y_i - y_K)^2 \quad\implies\quad r_i(x_K, y_K, R_K) = \sqrt{(x_i - x_K)^2 + (y_i - y_K)^2} - R_K
	\end{align*}

	If, in practice, some parametric model is used, this almost always leads to a parameter fitting problem to measure/minimize the differences between the model and measurements. By minimizing the differences, the model with that most correspond to the measurements can be found (e.g. the parameters of a friction model or inertia of a robot).

	\section{Objective Functions}
		There are various different objective functions that could be used:
		\begin{itemize}
			\item Absolute sum: \tabto{4cm} \( \varphi_1(\vec{p}) = \lVert \vec{r}(\vec{p}) \rVert_1 = \sum_{i = 1}^{n_r} \lvert r_i(\vec{p}) \rvert \)
			\item Sum of squares: \tabto{4cm} \( \varphi_2(\vec{p}) = \frac{1}{2} \lVert \vec{r}(\vec{p}) \rVert_2^2 = \frac{1}{2} \sum_{i = 1}^{n_r} r_i^2(\vec{p}) \)
			\item Maximum difference: \tabto{4cm} \( \varphi_\infty(\vec{p}) = \lVert \vec{r}(\vec{p}) \rVert_\infty = \max\big\{ \lvert r_i(\vec{p}) \rvert : i = \dotsrange{1}{n_r} \big\} \)
		\end{itemize}
		But even if \(\vec{r}\) is differentiable, \(\varphi_1\) and \(\varphi_\infty\) are generally not. But \(\varphi_\infty\) can be transformed into a NLP with differentiable functions:
		\begin{align*}
			\min_{\vec{p}, p_{n_r + 1}} & \, p_{n_r + 1} \\
			\mathrm{subject~to}         & \,
			-p_{n_r + 1} \leq r_i(\vec{p}) \leq p_{n_r + 1},\quad i = \dotsrange{1}{n_r}
		\end{align*}
		Anyway, \(\varphi_\infty\) is extremely sensitive towards outliers. On the other hand, \(\varphi_2\) is less sensitive towards outliers and is differentiable (if \(\vec{r}\) is differentiable)!

		Hence, in most cases the least squares objective \(\varphi_2\) is used. Besides the differentiability, it is also statistically appealing: If the measurement noise \(\varepsilon_{ij}\) are statistically independent and Gaussian distributed with mean \(0\) and variance \(\sigma^2\), the solution of \( \varphi_2 \to \mathrm{min} \) is a maximum likelihood estimator!

		Additionally, by exploiting the structure of \(\varphi_2\), the efficiency of gradient-based algorithms can be increased (e.g. due to sparse Jacobians).
	% end

	\section{Linear Least Squares}
		In the special case of a linear residual function \( \vec{r}(\vec{p}) = \mat{J}^T \vec{p} + \vec{f} \) and the least squares objective
		\begin{align*}
			\varphi(\vec{p}) = \varphi_2(\vec{p}) = \frac{1}{2} \lVert \vec{r}(\vec{p}) \rVert_2^2
		\end{align*}
		the optimization problem is quadratic in \(\vec{p}\):
		\begin{align*}
			\varphi(\vec{p}) = \frac{1}{2} \big\lVert \mat{J}^T \vec{p} + \vec{f}_r \big\rVert = \frac{1}{2} \big( \vec{p}^T \mat{J} + \vec{f}_r^T \big) \big( \mat{J}^T \vec{p} + \vec{f}_r \big)
		\end{align*}
		Zeroing the gradient \( \grad{\varphi}(\vec{p}) \overset{!}{=} \vec{0} \) yields the \emph{normal equations}
		\begin{align*}
			\mat{J} \mat{J}^T \vec{p}^\ast = -\mat{J}\vec{f}_r
		\end{align*}
		that a solution must fulfill (where \( \mat{J} \mat{J}^T \) is symmetric and positive definite). This linear system should not be solved as a normal linear system as it is ill-conditioned! Better use special methods like orthogonalization or single value decomposition.
	% end

	\section{Optimality Conditions and Special Methods}
		Let \( \varphi(\vec{p}) = \varphi_2(\vec{p}) \) be two times continuously differentiable. Then the following first- and second-order necessary conditions can be formulated:
		\begin{enumerate}
			\item The gradient has to vanish:
		\end{enumerate}
		\begin{align*}
			\grad{\varphi_2}(\vec{p}^\ast) = \mat{J}_{\vec{r}}(\vec{p}^\ast) \cdot \vec{r}(\vec{p}^\ast) = \vec{0}
		\end{align*}
		\begin{enumerate}
			\setcounter{enumi}{1}
			\item The Hessian has to be positive semidefinite:
		\end{enumerate}
		\begin{align}
			\mat{H}_{\varphi_2}(\vec{p}^\ast) = \mat{J}_{\vec{r}}(\vec{p}^\ast) \, \mat{J}_{\vec{r}}^T(\vec{p}^\ast) + \sum_{i = 1}^{n_r} \big( r_i(\vec{p}^\ast) \cdot \mat{H}_{r_i}(\vec{p}^\ast) \big)  \label{eq:leastSquaresHessian}
		\end{align}

		\subsection{Gauss-Quasi-Newton Method}
			In the Quasi-Newton method, the search direction is determined as a solution of the linear system
			\begin{align*}
				\mat{H}_\varphi\big(\vec{p}^{(k)}\big) \vec{d}^{(k)} = -\grad{\varphi}\big(\vec{p}^{(k)}\big)
			\end{align*}
			where the Hessian is approximated, e.g. using a BFGS update. But in the case of a least squares optimization problem, the only part of the Hessian~\eqref{eq:leastSquaresHessian} depending on second-order derivatives is:
			\begin{align*}
				\sum_{i = 1}^{n_r} \big( r_i(\vec{p}^\ast) \cdot \mat{H}_{r_i}(\vec{p}^\ast) \big)
			\end{align*}
			But as the objective gets smaller and smaller, this term also vanishes. Hence, the Hessian can be approximated using first-order derivatives only
			\begin{align*}
				\mat{H}_{\varphi_2}(\vec{p}^\ast) \approx \mat{J}_{\vec{r}}(\vec{p}^\ast) \, \mat{J}_{\vec{r}}^T(\vec{p}^\ast)
			\end{align*}
			if the residuals are "small". This leads to the \emph{Gauss-Newton Method} where the search direction is given as the solution of the normal equations
			\begin{align*}
				\mat{J}_r\big(\vec{p}^{(k)}\big) \mat{J}_r^T\big(\vec{p}^{(k)}\big) \vec{d}^{(k)} = -\mat{J}_r\big(\vec{p}^{(k)}\big) \cdot \vec{r}\big(\vec{p}^{(k)}\big)
			\end{align*}
			or as the solution of a (better conditioned) linear least squares problem:
			\begin{align*}
				\min_{\vec{d} \in \R^{n_p}} & \, \frac{1}{2} \Big\lVert \mat{J}_r^T\big(\vec{p}^{(k)}\big) \vec{d} + \vec{r}\big(\vec{p}^{(k)}\big) \Big\rVert_2^2
			\end{align*}

			If residuals are big or the problem is ill-conditioned, it can be modified with a suitable matrix \( \mat{B}^{(k)} \):
			\begin{align*}
				\Big( \mat{J}_{\vec{r}}\big(\vec{p}^{(k)}\big) \mat{J}_{\vec{r}}^T\big(\vec{p}^{(k)}\big) + \mat{B}^{(k)} \Big) \vec{d}^{(k)} = -\mat{J}_r\big(\vec{p}^{(k)}\big) \cdot \vec{r}\big(\vec{p}^{(k)}\big)
			\end{align*}

			Additionally a step size rule should be used. This method is implemented, e.g. in \emph{NLSCON} which also allows additional nonlinear inequality constraints.
		% end

		\subsection{Levenberg-Marquardt Methods}
			Instead of a Newton-approach, trust region methods can be used to determine the search direction. In the \emph{Levenberg-Marquardt Method}, the search direction \(\vec{d}^{(k)}\) is given as the solution of:
			\begin{align*}
				\Big( \mat{J}_{\vec{r}}\big(\vec{p}^{(k)}\big) \mat{J}_{\vec{r}}^T\big(\vec{p}^{(k)}\big) + \gamma^{(k)} \mat{I} \Big) \vec{d}^{(k)} = -\mat{J}_{\vec{r}}\big(\vec{p}^{(k)}\big) \cdot \vec{r}\big(\vec{p}^{(k)}\big),\quad \gamma^{(k)} \geq 0
			\end{align*}
			That is, the search direction is a mixture of the Gauss-Newton direction and steepest descent. The same search direction can be received by solving a constrained linear least squares problem:
			\begin{align*}
				\min_{\vec{d} \in \R^{n_p}} & \, \frac{1}{2} \Big\lVert \mat{J}_{\vec{r}}^T\big(\vec{p}^{(k)}\big) \vec{d} + \vec{r}\big(\vec{p}^{(k)}\big) \Big\rVert_2^2 \\
				\mathrm{subject~to}         & \,
				\lvert \vec{d} \rVert_2 \leq \delta^{(k)}
			\end{align*}
			Where \(\delta\) and \(\gamma\) have a connection.

			This is implemented, e.g. in \emph{LMDER}, \emph{LMJAC} of \emph{MINPACK}.
		% end

		\subsection{Notes}
			\begin{itemize}
				\item The Gauss-Newton method needs an appropriate test function that also exploits the structure of the objective function to determine the step size to ensure global convergence.
				\item In the Levenberg-Marquardt method, globalization happens by choosing an appropriate trust region.
				\item Levenberg-Marquardt and Gauss-Newton methods are in general a lot more efficient (faster and more precise) than solving least squares problems using general purpose optimization techniques (e.g. Quasi-Newton or SQP).
				\item When optimization by simulation, the sensitivity matrix \( \pdv{\vec{x}(t; \vec{p})}{\vec{p}} \) is needed (see\autoref{subsec:derivativeOde}).
			\end{itemize}
		% end
	% end

	\section{Conditioning of Normal Equations}
		Often, the Jacobian of the objective function of linear least squares \( \mat{J} \) is ill-conditioned, i.e. \( \cond \mat{J} \) is high, causing round error to be increased. In extreme cases, \( \mat{J} \) does not has full rank, i.e. \( \cond \mat{J} = \infty \). In this case, the solution \( \vec{d} \) of the normal equations
		\begin{align*}
			\mat{J} \mat{J}^T \vec{d} = -\mat{J}\vec{r}
		\end{align*}
		is not unique! Common reasons for ill-conditioning are:
		\begin{itemize}
			\item "too few" measurements \\ E.g. the measurements are not "dense enough".
			\item not the "correct" measurements \\ E.g. the measurements to not depend or weakly depend on the optimization variables.
			\item System model does not fit to the measurements (it is incompatible). Then either the measurements or the model is wrong.
		\end{itemize}
	% end

	\section{Result Interpretation}
		\subsection{Common Problems}
			Common problems if the residuals are still high after the optimization terminates:
			\begin{itemize}
				\item Derivatives are not precise enough.
				\item The optimization method is not suitable for the problem, e.g. if the method cannot handle inexact function evaluations or has problems with local minima of \(\varphi_2\).
				\item In parameter estimation settings,
					\begin{itemize}
						\item the system model and measurements might be incompatible (wrong measurements) or
						\item the system model and the physical might be incompatible (wrong model).
					\end{itemize}
			\end{itemize}

			Common problems for small residuals:
			\begin{itemize}
				\item Some optimization variables might not be uniquely determined.
				\item Too less measurements, variables are not unique in general (e.g. linearly dependent in the model).
			\end{itemize}

			Practical aspects:
			\begin{itemize}
				\item Scaling/balancing of the variables: By the transformation \( p_i \to \hat{p}_i = s_i p_i \), \( s_i = \const > 0 \), the derivative changes to \( \pdv{\varphi}{\hat{p}_i} = \pdv{\varphi}{p_i} \frac{1}{s_i} \).
				\item Scaling of the residuals by weights \( w_i > 0 \): \( \varphi(\vec{p}) = \frac{1}{2} \sum_{i = 1}^{n_r} w_i r_i^2(\vec{p}) \)
			\end{itemize}
		% end

		\subsection{The Covariance Matrix}
			Assuming the measurement errors \( \varepsilon_i \) (in the measurements \(r_i\)) are normally distributed with zero mean and constant variance \(\sigma^2\). Then the solution of the linear least squares problem is a maximum likelihood estimator for the parameters! The covariance matrix \(\mat{V}\) is then given by \( \mat{V} = \sigma^2 \big(\mat{J} \mat{J}^T\big)^{-1} \) where the variance can be approximated by
			\begin{align*}
				\sigma^2 \approx \frac{\lVert \vec{r}(\vec{p}^\ast) \rVert^2}{n_r - n_p}
			\end{align*}
			and the mean of the residual squares is given as
			\begin{align*}
				\E\big[ \lVert \vec{r}(\vec{p}^\ast) \rVert^2 \big] = (n_r - n_p) \sigma^2
			\end{align*}
			Hence, a bad conditioning of \(\mat{J}\) implies high variance!
		% end
	% end

	\section{Optimal Experimental Design}
		Goal of \emph{optimal experimental design} is a good conditioning of the parameter identification problem, i.e. the optimal experimental parameters \( \vec{s}^\ast \) is a solution of the optimization problem
		\begin{align*}
			\min_{\vec{s}} & \, \phi_\mathrm{exp}\big(\mat{V}(\vec{s})\big)
		\end{align*}
		where \(\mat{V}\) is the covariance matrix. Some objective \(\phi_\mathrm{exp}\) are:
		\begin{enumerate}
			\item Determinant of \(\mat{V}\).
			\item Trace mean, i.e. \( \tr \mat{V} / n_p \)
			\item Biggest eigenvalue of \(\mat{V}\)
			\item Absolute length of the biggest confidence interval.
			\item Conditional number.
		\end{enumerate}
	% end

	\section{Examples} % 6.39
		\todo{Content}

		\subsection{Parameter-Dependent Vehicle Dynamics} % 6.39, 6.40
			\todo{Content}

			\subsubsection{Simulated Measurements} % 6.41, 6.42, 6.43
				\todo{Content}
			% end

			\subsubsection{Real Measurements} % 6.44
				\todo{Content}
			% end

			\subsubsection{Comparison} % 6.45
				\todo{Content}
			% end
		% end

		\subsection{Parameter Estimation for "BioBiped"} % 6.47, 6.48, 6.49
			\todo{Content}
		% end
	% end
% end

\chapter{Minimization of Functionals}
	In the setting of \emph{variational problems}, the unknown \(\vec{x}\) is a function of \(t\) (where \(t\) is the independent variable) and the objective is a functional \( J[\vec{x}] \) of integral type:
	\begin{align*}
		J[\vec{x}] = \int\! L\big( \vec{x}(t), \dot{\vec{x}}(t), t \big) \dd{t}
	\end{align*}
	the solution \( \vec{x}^\ast \) must fulfill given constraints, e.g.:
	\begin{itemize}
		\item Initial and end conditions (boundary conditions)
		\item Inequality constraints
		\item Integral-type constraints
		\item Differential equations
	\end{itemize}
	If differential equations are given, the problem is called an \emph{optimal control problem}.

	\section{Euler-Lagrange Equation} % 7.5, 7.6, 7.9
		Given an optimization problem
		\begin{align*}
			\min_{\vec{x}}      & \, J[\vec{x}],\quad J[\vec{x}] = \int_a^b\! L\big(\vec{x}(t), \dot{\vec{x}}, t\big) \dd{t} \\
			\mathrm{subject~to} & \,
			\begin{alignedat}{2}
				\vec{x}(a) & = \vec{x}_a \\
				\vec{x}(b) & = \vec{x}_b
			\end{alignedat}
		\end{align*}
		where \( \vec{x} : \R \to \R^{n_x} \), \( L : \R^{n_x} \times \R^{n_x} \times R \to R \) are two times continuously differentiable.

		A \emph{stationary} solution \( \vec{x}^\ast \), i.e. a solution that is a minimum candidate, has to fulfill the \emph{Euler-Lagrange Equation}:
		\begin{align*}
			\pdv{L}{x_i} - \dv{t} \pdv{L}{\dot{x}_i} = 0,\quad i = \dotsrange{1}{n_x}
		\end{align*}
		Additionally, the boundary conditions \( \vec{x}(a) = \vec{x}_a \), \( \vec{x}(b) = \vec{x}_b \) must be fulfilled, yielding a second-order ordinary boundary value problem.

		\subsection{Example}
			\label{subsec:ballExample}

			As of the \emph{Hamilton's principle} \(\vec{x}(t)\) between \(t_1\) and \(t_2\) is a stationary point of the action functional
			\begin{align*}
				\int_{t_1}^{t_2} \! L\big(\vec{x}(t), \dot{\vec{x}}(t), t\big) \dd{t}
			\end{align*}
			where \(L\) is the Lagrangian of the system, i.e. the difference of kinetic and potential energy:
			\begin{align*}
				L = T - V
			\end{align*}

			For a ball with mass \(m\) and gravity acceleration \(g\) that is thrown into the air in a straight line (i.e. one-dimensional), the kinetic and potential energy are given as:
			\begin{align*}
				T = \frac{m}{2} \dot{x}^2 \qquad\qquad V = mgx
			\end{align*}
			The Lagrangian then is \( L = T - V = \frac{m}{2} \dot{x}^2 - mgx \). Plugging that in the Euler-Lagrange equations yields a second-order differential equation for the movement of the ball:
			\begin{align*}
				\pdv{L}{x} = -mg \qquad \pdv{L}{\dot{x}} = mx \qquad \dv{t} \pdv{L}{\dot{x}} = m\ddot{x} \quad\implies\quad -mg - m\ddot{x} = 0 \quad\iff\quad \ddot{x} = -g
			\end{align*}
			Solving this differential equation with the initial values \( x(0) = 0 \), \( \dot{x}(0) = \dot{x}_0 \) yields the equation of movement of the ball:
			\begin{align*}
				x(t) = -\frac{g}{2} t^2 + \dot{x}_0 t
			\end{align*}
			As expected, this equation is a perfect parabola w.r.t. time! The same can be done in two dimensions.
		% end

		\subsection{Notes}
			\begin{itemize}
				\item The Euler-Lagrange equations are in general not tractable. Hence, numerical methods have to be used.
				\item The equations can be extended to solutions with non-differentiabilities, (in-) equality constraints, integral-type constraints, \dots
				\item Also, second-order necessary conditions can be formulated.
			\end{itemize}
		% end

		\subsection{Derivation} % 7.6, 7.7, 7.8, 7.9
			\todo{Content}
		% end
	% end
% end

\chapter{Optimal Control}
	\label{c:optimalControl}

	A dynamical system is described by the ODE
	\begin{align*}
		\dot{\vec{x}} = \vec{f}(\vec{x}, \vec{u}, \vec{p}, \vec{z}),\quad \vec{x}(0) = \vec{x}_0
	\end{align*}
	where \(\vec{x}\) is the state, \(\vec{u}\) are the control variables, \(\vec{p}\) are (constant) system parameters and \(\vec{z}\) is noise. An optimal control problem has given \(\vec{x}_0\), \(\vec{p}\) and \(\vec{x}_i(t_f)\) and seeks for an optimal \(\vec{u}\) and \(\vec{x}\).

	A complete optimal control problem is given as:
	\begin{align*}
		\min                & \, J[\vec{u}],\quad J[\vec{u}] = \phi\big(\vec{x}(t_f), t_f\big) + \int_{0}^{t_f} \! L\big(\vec{x}(t), \vec{u}(t)\big) \dd{t} \\
		\mathrm{subject~to} & \,
		\begin{alignedat}[t]{2}
			\dot{\vec{x}}(t)                                             & = \vec{f}\big(\vec{x}(t), \vec{u}(t)\big)                                              \\
			x_i(0)                                                       & = x_{i, 0} = \const,\quad i \in \{\, \dotsrange{1}{n_x} \,\}                           \\
			\vec{r}\big(\vec{x}(t_f), t_f\big)                           & = \vec{0},\quad \text{e.g. } x_j(t_f) = x_{j, f},\, j \in \{\, \dotsrange{1}{n_x} \,\} \\
			\vec{g}\big(\vec{x}(t), \vec{u}(t)\big)                      & \geq \vec{0}                                                                           \\
			\vec{g}\big(\vec{x}(t)\big)                                  & \geq \vec{0}                                                                           \\
			\vec{r}^i\big( \vec{x}(t_s - 0), \vec{x}(t_s + 0), t_s \big) & = \vec{0}
		\end{alignedat}
	\end{align*}
	The constraints (from to bottom) are called:
	\begin{itemize}
		\item Equations of movement, these are the defining property of an optimal control problem over a regular variational problem.
		\item Initial conditions (optional).
		\item Final conditions (optional).
		\item Control constraints (optional, often box constraints).
		\item State constraints (optional).
		\item Interior point constraints (optional).
	\end{itemize}
	Additionally, the final time \(t_f\) might either fixed or free. The objective (called the \emph{Bolza functional}) is split into an endpoint cost (also called \emph{Mayer term}) and a Lagrangian:
	\begin{align*}
		J[\vec{u}] = \underbrace{\phi\big(\vec{x}(t_f), t_f\big)}_{\text{Endpoint Cost}}
		+ \int_{0}^{t_f} \! \underbrace{L\big(\vec{x}(t), \vec{u}(t)\big)}_{\text{Lagrangian}} \dd{t}
	\end{align*}
	Note that the endpoint cost is more general than the Lagrangian in terms that the Lagrangian term can be transformed to an endpoint cost by introducing a new state
	\begin{align*}
		\dot{x}_{n_x + 1} = L\big(\vec{x}(t), \vec{u}(t)\big),\quad x_{n_x + 1}(0) = 0
	\end{align*}
	and changing the endpoint cost to:
	\begin{align*}
		\tilde{\phi}\big(\tilde{\vec{x}}(t_f), t_f\big) \coloneqq \phi\big(\vec{x}(t_f), t_f\big) + x_{n_x + 1}(t_f)
	\end{align*}
	Additionally, non-autonomous problems can be transformed to autonomous problems by introducing a "clock state" \( \dot{x}_{n_x + 1} = 1 \), \( x_{n_x + 1}(0) = 0 \) and changing the ODE accordingly:
	\begin{align*}
		\dot{\vec{x}}(t) = \vec{f}\big(t, \vec{x}(t), \vec{u}(t)\big) \quad\to\quad \dot{\vec{x}}(t) = \vec{f}\big(x_{n_x + 1}, \vec{x}(t), \vec{u}(t)\big)
	\end{align*}

	\section{Necessary Optimality Conditions for the Basis Problem}
		The basis optimal control problem does not have control, state or interior constraints:
		\begin{align*}
			\min                & \, J[\vec{u}],\quad J[\vec{u}] = \phi\big(\vec{x}(t_f), t_f\big) + \int_{0}^{t_f} \! L\big(\vec{x}(t), \vec{u}(t)\big) \dd{t} \\
			\mathrm{subject~to} & \,
			\begin{alignedat}[t]{2}
				\dot{\vec{x}}(t)                   & = \vec{f}\big(\vec{x}(t), \vec{u}(t)\big)                                              \\
				x_i(0)                             & = x_{i, 0} = \const,\quad i \in \{\, \dotsrange{1}{n_x} \,\}                           \\
				\vec{r}\big(\vec{x}(t_f), t_f\big) & = \vec{0},\quad \text{e.g. } x_j(t_f) = x_{j, f},\, j \in \{\, \dotsrange{1}{n_x} \,\}
			\end{alignedat}
		\end{align*}
		For formulating the necessary optimality conditions, the following auxiliary functions are needed (the latter one is called \emph{Hamiltonian}):
		\begin{align*}
			\Phi(\vec{x}, t, \vec{\nu})        & \coloneqq \phi(\vec{x}, t) + \vec{\nu}^T \vec{r}(\vec{x}, t)              \\
			H(\vec{x}, \vec{u}, \vec{\lambda}) & \coloneqq L(\vec{x}, \vec{u}) + \vec{\lambda}^T \vec{f}(\vec{x}, \vec{u})
		\end{align*}
		Note that, while kept implicitly, the auxiliary variables \(\vec{\nu}\) and \(\vec{\lambda}\) are time-dependent! The \(\vec{\lambda}\) are also called the \emph{adjunct variables} of the optimal control problem.

		\subsection{Boundary Conditions}
			\label{subsec:boundary}

			To solve the optimal control problem, \(2n_x\) boundary conditions are needed (the optimality conditions will yield one first-order ODE for every state and adjunct variable). Additional to the given boundary conditions of the problem formulation, enough conditions have to be found to get \(2n_x\) conditions. Common cases:
			\begin{enumerate}[label = (\roman*)]
				\item Fixed initial conditions \( x_i(0) = x_{i, 0} = \const \): \\
					Either \( x_k(0) \) is given or, if not, set \( \lambda_i(0) = -\pdv{\phi(\vec{x}(0), \vec{x}(t_f), t_f)}{x_i(0)} \)
				\item Fixed final conditions \( x_i(t_f) = x_{i, f} = \const \): \\
					Either \( x_i(t_f) \) is given or, if not and \(x_i\) is not part of \( \vec{r}(\cdots) \), set \( \lambda_i(t_f) = \pdv{\phi(\vec{x}(t_f), t_f)}{x_i(t_f)} \)
				\item Mixed boundary conditions:
					\begin{enumerate}
						\item General boundary conditions \( \vec{r}\big(\vec{x}(0), \vec{x}(t_f), t_f\big) = \vec{0} \): \\
							If \( x_i(0) \) is free, set \( \lambda_i(0) + \pdv{\Phi}{x_i(0)} \Big\vert_{t = 0} = 0 \) \\
							If \( x_i(t_f) \) is free, set \( \lambda_i(t_f) - \pdv{\Phi}{x_i(t_f)} \Big\vert_{t = t_f} = 0 \)
						\item Periodic boundary conditions \( x_i(0) - x_j(t_f) = 0 \): \\
							Set \( \lambda_i(0) - \lambda_j(t_f) + \pdv{\phi}{x_i(0)} + \pdv{\phi}{x_j(t_f)} = 0 \)
					\end{enumerate}
			\end{enumerate}
			If the final time \(t_f\) is free, an additional condition has to be employed:
			\begin{align*}
				H\big(\vec{x}(t_f), \vec{u}(t_f), \vec{\lambda}(t_f)\big) = -\pdv{\Phi}{t_f}
			\end{align*}
		% end

		\subsection{First-Order Necessary Optimality Conditions (Maximum Principle)}
			Functions \( \vec{x}^\ast \), \( \vec{u}^\ast \) and \( \vec{\lambda}^\ast \not\equiv \vec{0} \) are the optimal solution of the basis problem iff the \emph{canonical differential equations}
			\begin{align*}
				\dot{\vec{x}} = \pdv{H}{\vec{\lambda}} \qquad \dot{\vec{\lambda}} = -\pdv{H}{\vec{x}}
			\end{align*}
			and the boundary conditions are fulfilled and the optimal control \(\vec{u}^\ast\) minimizes the Hamilton function:
			\begin{align}
				H\big(\vec{x}^\ast(t), \vec{u}^\ast(t), \vec{\lambda}^\ast(t)\big) = \min_{\tilde{\vec{u}} \in U} \, H\big(\vec{x}^\ast(t), \tilde{\vec{u}}, \vec{\lambda}^\ast(t)\big),\quad \forall t \in [0, t_f]  \label{eq:maximumPrinciple}
			\end{align}
			Where \( U \subseteq \R^{n_u} \) is the set of feasible controls. These conditions are called the \emph{maximum principle}.

			\textbf{Definition:} The Hamiltonian is called \emph{regular} if and only if it has an unique minimum.

			If the Hamiltonian is regular and the control \(\vec{u}\) appears nonlinear in the Hamiltonian, condition~\eqref{eq:maximumPrinciple} can be formulated as
			\begin{align*}
				\pdv{H}{\vec{u}} = \vec{0}
			\end{align*}

			\subsubsection{Derivation} % 8.8, 8.9, 8.10, 8.11
				\todo{Content}
			% end
		% end

		\subsection{Second-Order Necessary Optimality Condition (Legendre-Clebsch Condition)} % 8.28, 8.31
			For stationary \(\vec{x}^\ast\), \(\vec{u}^\ast\), \(\vec{\lambda}^\ast\), the second-order necessary optimality condition, also called the \emph{Legendre-Clebsch Condition} is that the Hessian of the Hamiltonian is positive semidefinite along the optimal state and control:
			\begin{align*}
				\mat{H}_H^{\vec{p}}\big(\vec{x}^\ast(t), \vec{u}^\ast(t), \vec{\lambda}^\ast(t)\big) \geq 0,\quad \forall t \in [0, t_f]
			\end{align*}
		% end

		\subsection{Example} % 8.12
			\todo{Content}
		% end

		\subsection{Application: Optimal Robot Control} % 8.13
			\todo{Content}

			\subsubsection{The Robot Dynamics} % 8.14, 8.15
				\todo{Content}
			% end

			\subsubsection{Optimal Control} % 8.16, 8.17
				\todo{Content}
			% end

			\subsubsection{Objective Functionals} % 8.18
				\todo{Content}
			% end

			\subsubsection{Necessary Conditions} % 8.24, 8.25, 8.26, 8.29, 8.30
				\todo{Content}
			% end
		% end
	% end

	\section{Bang-Bang Singular Control}
		This chapter covers optimal control functions where the control function appears only linear in the Lagrangian and the motion equations:
		\begin{align*}
			L\big(\vec{x}(t), \vec{u}(t)\big)       & = L_0\big(\vec{x}(t)\big) + \vec{L}_1^T\big(\vec{x}(t)\big) \vec{u}(t),\quad L_0 : \R^{n_x} \to R,\, \vec{L}_1 : \R^{n_x} \to \R^{n_u}                            \\
			\vec{f}\big(\vec{x}(t), \vec{u}(t)\big) & = \vec{f}_0\big(\vec{x}(t)\big) + \mat{f}_1^T\big(\vec{x}(t)\big) \vec{u},\quad \vec{f}_0 : \R^{n_x} \to \R^{n_x},\, \mat{f}_1 : \R^{n_x} \to \R^{n_u \times n_x}
		\end{align*}
		Also, it is assumed to have box constraints for the controls: \( \vec{u}_\mathrm{min} \leq \vec{u}(t) \leq \vec{u}_\mathrm{max} \).

		The according Hamiltonian is
		\begin{align*}
			H(\vec{x}, \vec{u}, \vec{\lambda})
			 & = L_0\big(\vec{x}(t)\big) + \vec{L}_1^T(\vec{x}(t)\big) \vec{u}(t) + \vec{\lambda}^T \big( \vec{f}_0\big(\vec{x}(t)\big) + \mat{f}_1^T\big(\vec{x}(t)\big) \vec{u} \big)                                                                    \\
			 & = \underbrace{\big( \vec{L}_1^T(\vec{x}(t)\big) + \vec{\lambda}^T \mat{f}_1^T\big(\vec{x}(t)\big) \big)}_{\vec{s}(\vec{x}, \vec{\lambda}) \,\coloneqq} \vec{u}(t) + L_0\big(\vec{x}(t)\big) + \vec{\lambda}^T \vec{f}_0\big(\vec{x}(t)\big)
		\end{align*}
		where \( \vec{s}(\vec{x}, \vec{\lambda}) \) is called the \emph{switching function}. As of the maximum principle, the control \(\vec{u}\) is optimal iff it minimizes the Hamiltonian. Hence, the optimal control function \(\vec{u}\) is given as a piecewise function depending on the switching function:
		\begin{align*}
			u_i(t) =
			\begin{cases*}
				u_{i, \mathrm{min}}  & iff \( s_i\big(\vec{x}(t), \vec{\lambda}(t)\big) > 0 \)      \\
				u_{i, \mathrm{max}}  & iff \( s_i\big(\vec{x}(t), \vec{\lambda}(t)\big) < 0 \)      \\
				u_{i, \mathrm{sing}} & iff \( s_i\big(\vec{x}(t), \vec{\lambda}(t)\big) \equiv 0 \) \\
			\end{cases*}
		\end{align*}
		Here, \( u_{i, \mathrm{sing}} \) is the \emph{singular control} that is only needed if the switching function is zero over an interval. The other two cases are called \emph{bang-bang control}.

		All of the following will only cover a single control \( u_i(t) \), but the methods can be applied to more than one control variable in a setting. Of course it is possible that some control variables are nonlinear in the Lagrangian, hence a bang-bang or singular control is not needed in that case.

		\subsection{Singular Control}
			Let \(t_1\) and \(t_2\), \( t_1 < t_2 \) denote the start and end point of a singular control interval, respectively, i.e. the switching function is zero in that interval:
			\begin{align*}
				s_i\big(\vec{x}, \vec{\lambda}\big) = 0,\quad \forall t \in [t_1, t_2]
			\end{align*}
			With the insight that a system has to keep itself in the singular control, also the time derivative \( s_i^{(1)} \coloneqq \dv{s_i}{t} \) of the switching function has to vanish. And also the second-order time derivative \( s_i^{(2)} \). In fact, every time derivative has to vanish, yielding a recursive process for calculating the \(m\)-th time derivative:
			\begin{align*}
				s_i^{(m)} = \dv{t} s_i^{(m - 1)}\big(\vec{x}(t), \vec{\lambda}(t)\big) \equiv 0
			\end{align*}
			Let \(m\) be the smallest number of which the control \(u_i\) appears explicitly in the time derivative, \( \pdv{s_i^{(m)}}{u_1} \neq 0 \). This yields a formula to determine \( u_{i, \mathrm{sing}} \):
			\begin{align*}
				s_i^{(m)}\big(\vec{x}(t), \vec{\lambda}(t), \vec{u}(t)\big) \equiv 0
			\end{align*}
			Where \( u_i \) is part of the vector \(\vec{u}\).

			\subsubsection{Second-Order Necessary Optimality Condition for Singular Control}
				\textbf{Theorem:} If \( m < \infty \), then \( m \) is even. Let \( m = 2p \), where \(p\) is called the \emph{order} of the singular control.

				The second-order necessary optimality condition for singular control, i.e. the generalized Legendre-Clebsch is:
				\begin{align*}
					(-1)^p \pdv{u_i} s_i^{(m)}\big( \vec{x}^\ast(t), \vec{\lambda}^\ast(t), \vec{u}^\ast(t) \big) \geq 0,\quad \forall t \in [t_1, t_2]
				\end{align*}
				The condition can be expanded to a sufficient condition by replacing the inequality with a strict inequality, i.e. the left side has to be positive.

				If the sufficient condition is fulfilled, two conclusions can be proven:
				\begin{itemize}
					\item If \(p\) is odd, \(u_i\) is either discontinuous or continuously differentiable in \( t_1 \).
					\item If \(p\) is even, \(u_i\) is continuous in \( t_1 \), but \emph{chattering} is possible. If \emph{chattering} occurs, the control \(u_i\) "rings" around zero until reaching \(t_1\), i.e. \(s_i\) has infinitely many root before entering the singular section.
				\end{itemize}
			% end
		% end

		\subsection{Application: Time-Minimal Robot Control} % 8.39, 8.40, 8.41
			\todo{Content}
		% end

		\subsection{Notes}
			\begin{itemize}
				\item If all degrees of freedom would become singular simultaneously, all adjunct variables would be zero (contradicting the maximum principle). Hence, this is not possible! For bang-bang control this means that at least one control input has to operate at its maximum or minimum.
				\item With the given properties, a numerically calculated solution can be checked for sanity (e.g. if all controls are singular on a section, the solution cannot be optimal).
				\item There are approaches to eliminate singular control beforehand so that only the number and order of switching points has to be calculated. But often a control problem can be found that necessarily has singular parts, so such methods cannot determine the optimal solution.
				\item The necessary conditions lead to a fully determined boundary value problem (BVP) for \(\vec{x}\) and \(\vec{\lambda}\) that can be solved using numerical methods (see~\autoref{sec:indirectMethods}), but the order of bang-bang and singular control has to be known.
			\end{itemize}
		% end
	% end

	\section{Value Function and Hamilton-Jacobi-Bellman Equation}
		There is a useful interpretation of the adjunct variables \(\vec{\lambda}\)! Note that by varying the initial conditions \(t_0\) and \( \vec{x}(t_0) = \vec{x}_0 \), different trajectories \(\vec{x}^\ast\), \(\vec{u}^\ast\), \(\vec{\lambda}^\ast\) are generated by solving the optimal control problem. The \emph{Value Function} expresses the value (of the objective) for a given initial condition \( (t_0, \vec{x}_0) \):
		\begin{align*}
			V(t_0, \vec{x}_0) & = \min_{\vec{u} \,\in\, U} \, \Bigg(\! \phi\big(\vec{x}(t_f), t_f\big) + \int_{t_0}^{t_f}\! L\big(\vec{x}(\tau), \vec{u}(\tau)\big) \dd{\tau} \!\Bigg) \\
			V(t_f, \vec{x})   & = \phi(\vec{x}, t_f)
		\end{align*}

		The \emph{Hamilton-Jacobi-Bellman Equation} is a partial differential equation for the value function:
		\begin{align*}
			-\pdv{V(t, \vec{x})}{t} & = \min_{\tilde{\vec{u}}} \, \Bigg(\! L(t, \vec{x}, \tilde{\vec{u}}) + \bigg( \pdv{V(t, \vec{x})}{\vec{x}} \bigg)^T \vec{f}(t, \vec{x}, \tilde{\vec{u}}) \!\Bigg) \\
			V(t_f, \vec{x})         & = \phi(\vec{x}, t_f)
		\end{align*}

		If \( V(t, \vec{x}) \) is a solution of the Hamilton-Jacobi-Bellman equation, then the minimization
		\begin{align*}
			\min_{\tilde{\vec{u}}} \, \Bigg(\! L(t, \vec{x}, \tilde{\vec{u}}) + \bigg( \pdv{V(t, \vec{x})}{\vec{x}} \bigg)^T \vec{f}(t, \vec{x}, \tilde{\vec{u}}) \!\Bigg)
		\end{align*}
		generates the optimal control \( \vec{u}^\ast \) of the basis problem with \( t_0 \leq t \leq t_f \) with the adjunct variables:
		\begin{align*}
			\vec{\lambda}^\ast(t) = \pdv{V\big(t, \vec{x}^\ast(t)\big)}{\vec{x}}
		\end{align*}
		Hence, the adjunct variables are the gradient of the minimal objective value w.r.t. the state.

		\subsection{Derivation} % 8.46, 8.47, 8.48, 8.49, 8.50, 8.51
			\todo{Content}
		% end

		\subsection{Notes}
			\begin{itemize}
				\item If \( \lambda_i^\ast(t) \equiv 0 \) for \( t \in [t_1, t_2] \), the value of the objective does not depend on \( x_i^\ast(t) \) for the same interval.
				\item The value function contains all information needed for the optimal feedback control \( \vec{u}^\ast(t, \vec{x}) \). If the value function would be available, the optimal control could be calculate for arbitrary initial conditions. But most of the time it is not available\dots
				\item It is nearly impossible to solve the HJB equation numerically (!) for relevant problems, even without constraints.
			\end{itemize}
		% end
	% end

	\section{Constraints}
		It is possible to add state and control constraints to the optimal control problem in the form
		\begin{align*}
			\vec{g}\big(\vec{x}(t), \vec{u}(t)\big) \geq \vec{0}
		\end{align*}
		where the inequality is element-wise. Such a constraint is called a \emph{control constraint} if \(\vec{u}\) appears explicitly in \(\vec{g}\), i.e. \( \pdv{\vec{g}}{\vec{u}} \neq \mat{0} \).

		\subsection{Mixed Inequality Constraints}
			Mixed inequality constraints are both dependent on the state and the control:
			\begin{align*}
				\vec{g}\big(\vec{x}(t), \vec{u}(t)\big) \geq \vec{0},\quad \vec{g} : \R^{n_x} \times \R^{n_u} \to \R^{n_g},\quad t \in [0, t_f]
			\end{align*}
			This constraint is added to the Hamiltonian with multipliers \( \vec{\eta} \) yielding the augmented Hamiltonian, similar to the Lagrangian of static optimization:
			\begin{align*}
				H(\vec{x}, \vec{u}, \vec{\lambda}, \vec{\eta}) = L(\vec{x}, \vec{u}) + \lambda^T \vec{f}(\vec{x}, \vec{u}) + \vec{\eta}^T \vec{g}(\vec{x}, \vec{u})
			\end{align*}
			Analogous to the normal Hamiltonian, the augmented Hamiltonian is called \emph{regular} along the optimal solution \( \vec{x}^\ast(t) \), \( \vec{\lambda}^\ast(t) \), \( \vec{\eta}^\ast(t) \) iff \( H\big( \vec{x}^\ast(t), \vec{u}, \vec{\lambda}^\ast(t), \vec{\eta}^(t) \big) \) has a unique minimum \( \vec{u} = \vec{u}^\ast(t) \) for all \( t \in [0, t_f] \).

			\subsubsection{Necessary Conditions (Maximum Principle)}
				Let the problem be autonomous, \(\vec{f}\), \(\vec{g}\), \(\phi\) be continuously differentiable, \(U\) the set of feasible optimal controls and let the inequality constraint \( \vec{g} \geq \vec{0} \) have an active component \( g_i \) with \( g_i(t) = 0 \) for \( t \in [t_1, t_2] \), i.e. let it be active on \( [t_1, t_2] \).

				If not all boundary conditions are given, they have to be determined analogous to\autoref{subsec:boundary}.

				Then the necessary optimality conditions can be defined analogous to the ones of the basis problem: Iff \( \vec{x}^\ast \), \( \vec{u}^\ast \), \( \vec{\lambda}^\ast \), \( \vec{\eta}^\ast \) are optimal, the canonical differential equations
				\begin{align*}
					\dot{\vec{x}} = \pdv{H}{\vec{\lambda}} \qquad \dot{\vec{\lambda}} = -\pdv{H}{\vec{x}}
				\end{align*}
				are fulfilled and
				\begin{align*}
					\eta_j
					\begin{cases*}
						= 0    & iff \( g_j\big(\vec{x}(t), \vec{u}(t)\big) > 0 \) \\
						\leq 0 & iff \( g_j\big(\vec{x}(t), \vec{u}(t)\big) = 0 \)
					\end{cases*}
				\end{align*}
				which is, for the special constraint \(g_i\), equivalent to:
				\begin{align*}
					\eta_j
					\begin{cases*}
						= 0    & \( t \in [0, t_1) \cup (t_2, t_f] \) \\
						\leq 0 & \( t \in [t_1, t_2] \)
					\end{cases*}
				\end{align*}
				Also, the following condition have to be fulfilled on the switching points of the constraints:
				\begin{align*}
					\lim\limits_{\delta \to 0^+} \vec{\lambda}(t_\Delta + \delta) = \bigg(\! \lim\limits_{\delta \to 0^-} \vec{\lambda}(t_\Delta + \delta) \!\bigg) - \nu_\Delta \cdot \pdv{g_i\big(\vec{x}(t_\Delta), \vec{u}(t_\Delta)\big)}{\vec{x}}
				\end{align*}
				Where \( \eta_\Delta = \const \leq 0 \) and \( t_\Delta \) is the entry/exit point, i.e. \( \Delta = 1 \) or \( \Delta = 2 \). This implies that the adjunct variables are in general not continuous at the switching points.

				The optimal control than has to minimize the augmented Hamiltonian over \(U\):
				\begin{align}
					H\big(\vec{x}^\ast(t), \vec{u}^\ast(t), \vec{\lambda}^\ast(t), \vec{\eta}^\ast(t)\big) = \min_{\tilde{\vec{u}} \,\in\, U} \, H\big(\vec{x}^\ast(t), \tilde{\vec{u}}, \vec{\lambda}^\ast(t), \vec{\eta}^\ast(t)\big),\quad \forall t \in [0, t_f]  \label{eq:constrainedMax}
				\end{align}

				Along the active constraint,
				\begin{align*}
					g_i\big(\vec{x}(t), \vec{u}(t)\big) = 0 \quad\text{and}\quad \pdv{g_i(\vec{x}, \vec{u})}{u_j} \neq 0
				\end{align*}
				yields a condition for determining \(u_j\) along the active constraint. All other controls as well as \(u_j\) outside of the active section have to fulfill~\eqref{eq:constrainedMax} and the Legendre-Clebsch condition (iff \(H\) is regular) or the corresponding bang-bang/singular control, respectively.

				As the problem is autonomous, the Hamiltonian is constant sectionally constant w.r.t. time \(t\) and it can change when constraints become active/inactive.
			% end
		% end

		\subsection{State Inequality Constraints}
			Simple state inequality constraints
			\begin{align*}
				\vec{g}\big(\vec{x}(t)\big) \geq \vec{0}
			\end{align*}
			that does not contain \(\vec{u}\), but at least one \(x_i\) are handled like a mixture of mixed constraints and singular control.

			Let a single constraint \( g_i \) be active in the interval \( [t_1, t_2] \), i.e. \( g_i\big(\vec{x}(t), \vec{u}(t)\big) = 0 \) for \( t \in [t_1, t_2] \). Then also the time derivative has to vanish as well as the second-order time derivative\footnote{Let \( g_i^{(0)} \coloneqq g_i \).}:
			\begin{align*}
				g_i^{(m_g)} = \dv{t} g_i^{(m_g - 1)}\big(\vec{x}(t)\big) \equiv 0
			\end{align*}
			Let \(m_g\) be the smallest number such that \( g_i^{(m_g)} \) explicitly contains the control \(u_j\), then
			\begin{align*}
				g^{(m_g)}(\vec{x}, \vec{u}) = 0
			\end{align*}
			yields an equation for determining \( u_j \) in the interval \( [t_1, t_2] \) and \(m_g\) is called the \emph{order} of the state constraint. Hence, a single active constraints determined a single control variable!

			\subsubsection{Augmented Hamiltonian}
				Let \( \vec{g}(\vec{x}) = g(\vec{x}) \), i.e. only a single state constraint.

				Mathematically, the active constraint \( g\big(\vec{x}(t)\big) = 0 \) for \( t \in [t_1, t_2] \) is equivalent to
				\begin{align*}
					g^{(k)}\big(\vec{x}(t), \vec{u}(t)\big) = 0,\quad t \in [t_1, t_2]
				\end{align*}
				for every \( k = \dotsrange{1}{m_g} \). Hence, there are multiple, equivalent formulations of the maximum principle for active state constraints by viewing at the corresponding augmented Hamiltonian, e.g.:
				\begin{align*}
					H^0(\vec{x}, \vec{u}, \vec{\lambda}^0, \eta^0)         & = L(\vec{x}, \vec{u}) + \big(\vec{\lambda}^0\big)^T \vec{f}(\vec{x}, \vec{u}) + \eta^0 g^{(0)}(\vec{x})           \\
					                                                       & \,\,\,\vdots                                                                                                      \\
					H^k(\vec{x}, \vec{u}, \vec{\lambda}^k, \eta^k)         & = L(\vec{x}, \vec{u}) + \big(\vec{\lambda}^k\big)^T \vec{f}(\vec{x}, \vec{u}) + \eta^k g^{(k)}(\vec{x})           \\
					                                                       & \,\,\,\vdots                                                                                                      \\
					H^k(\vec{x}, \vec{u}, \vec{\lambda}^{m_g}, \eta^{m_g}) & = L(\vec{x}, \vec{u}) + \big(\vec{\lambda}^{m_g}\big)^T \vec{f}(\vec{x}, \vec{u}) + \eta^{m_g} g^{(m_g)}(\vec{x})
				\end{align*}
			% end

			\subsubsection{Maximum Principle}
				As in the previous section, a single state constraint \(g\) is assumed.

				If not all boundary conditions are given, they have to be determined analogous to\autoref{subsec:boundary}.

				The maximum principle for state constraints is similar to the one for mixed constraints. Let \(g\) be active in the interval \( [t_1, t_2] \). Iff \( \vec{x}^\ast \), \( \vec{u}^\ast \), \( \vec{\lambda}^{k\ast} \), \( \vec{\eta}^{k\ast} \) are optimal, the canonical differential equations
				\begin{align*}
					\dot{\vec{x}} = \pdv{H^k}{\vec{\lambda}} \qquad \dot{\vec{\lambda}}^k = -\pdv{H^k}{\vec{x}}
				\end{align*}
				are fulfilled and
				\begin{align*}
					\eta
					\begin{cases*}
						= 0    & iff \( g\big(\vec{x}(t), \vec{u}(t)\big) > 0 \) \\
						\leq 0 & iff \( g\big(\vec{x}(t), \vec{u}(t)\big) = 0 \)
					\end{cases*}
					\quad\iff\quad
					\eta
					\begin{cases*}
						= 0    & \( t \in [0, t_1) \cup (t_2, t_f] \) \\
						\leq 0 & \( t \in [t_1, t_2] \)
					\end{cases*}
				\end{align*}
				Additionally \(\eta^k\) have to fulfill the sign conditions,
				\begin{align*}
					(-1)^j \dv[j]{t} \eta^k(t) = \eta^{k - j}(t) \leq 0,\quad j = \dotsrange{1}{k}
				\end{align*}
				and the final conditions
				\begin{align*}
					\lim\limits_{\delta \to 0^-} \dv[j]{t} \eta^k(t_2 + \delta) = 0,\quad j = \dotsrange{0}{k - 2} \text{ if } k \geq 2
				\end{align*}
				For \( k = \dotsrange{1}{m_g} \) the active constraints have to fulfill the following at the entry point:
				\begin{align*}
					\lim\limits_{\delta \to 0^+} \vec{\lambda}^k(t_1 + \delta) = \bigg(\! \lim\limits_{\delta \to 0^-} \vec{\lambda}^k(t_1 + \delta) \!\bigg) - \sum_{j = 1}^{k} \beta^j \cdot \pdv{g^{(j - 1)}\big(\vec{x}(t_1)\big)}{\vec{x}}
				\end{align*}
				with \( \beta^j \leq 0 \). And on the exit point the condition
				\begin{align*}
					\lim\limits_{\delta \to 0^+} \vec{\lambda}^k(t_2 + \delta) = \lim\limits_{\delta \to 0^-} \vec{\lambda}^k(t_2 + \delta)
				\end{align*}
				has to be fulfilled for \( k = \dotsrange{1}{m_g} \). That is, the adjunct variables of the \(x_i\) that appear in \( g^{(j - 1)} \) are discontinuous at the entry point and continuous at the exit point of the active constraint.

				The optimal control than has to minimize the augmented Hamiltonian over \(U\):
				\begin{align*}
					H\big(\vec{x}^\ast(t), \vec{u}^\ast(t), \vec{\lambda}^\ast(t), \vec{\eta}^\ast(t)\big) = \min_{\tilde{\vec{u}} \,\in\, U} \, H\big(\vec{x}^\ast(t), \tilde{\vec{u}}, \vec{\lambda}^\ast(t), \vec{\eta}^\ast(t)\big),\quad \forall t \in [0, t_f]
				\end{align*}

				As the problem is autonomous, the Hamiltonian is constant sectionally constant w.r.t. time \(t\) and it can change when constraints become active/inactive.

				To get back and forth in different formulations for \(k\), the following recursion can be defined. For \( m_g \geq 1 \):
				\begin{align*}
					\eta^1(t) & =
					\begin{cases*}
						\nu_2 + \int_{t}^{t_2} \eta^0(\tau) \dd{\tau} & iff \( t \in [t_1, t_2] \) \\
						0                                             & else
					\end{cases*} \\
					\beta^1   & = \nu_1 + \eta^1(t_1)
				\end{align*}
				And for \( m_g \geq 2 \) and \( k \geq 2 \):
				\begin{align*}
					\eta^k(t)          & =
					\begin{cases*}
						\int_{t}^{t_2} \eta^{k - 1}(\tau) \dd{\tau} & iff \( t \in [t_1, t_2] \) \\
						0                                           & else
					\end{cases*}                                                    \\
					\beta^k            & = \eta^k(t_1)                                                                                          \\
					\vec{\lambda}^k(t) & = \vec{\lambda}^0(t) - \sum_{j = 1}^{k} \eta^j(t) \cdot \pdv{g^{(j - 1)}\big(\vec{x}(t)\big)}{\vec{x}}
				\end{align*}
				Where \( \nu_1, \nu_2 \leq 0 \) are given by
				\begin{align*}
					\lim\limits_{\delta \to 0^+} \vec{\lambda}^0(t_\Delta + \delta) = \bigg(\! \lim\limits_{\delta \to 0^-} \vec{\lambda}^0(t_\Delta + \delta) \!\bigg) - \nu_\Delta \cdot \pdv{g_i\big(\vec{x}(t_\Delta), \vec{u}(t_\Delta)\big)}{\vec{x}}
				\end{align*}
				for \( \Delta = 1 \) and \( \Delta = 2 \).
			% end

			\subsubsection{Notes}
				\begin{itemize}
					\item There are three different types of active constraints possible:
						\begin{enumerate}
							\item Contact point \\ The states control directly "into" the infeasible region and have to do an immediate turn when having contact with the constraint (like bumping a car into a wall and bouncing off).
							\item Touch point \\ The states just merely touch the border of the infeasible region and no changes have to be made to the control (like a a go-around with merely touching the ground).
							\item Boundary arc \\ The states have to remain a while on the border (the "active section") until going away again (like sliding along the edge on an ice rink).
						\end{enumerate}
					\item If the Hamiltonian is regular, there are multiple possibilities shown in~\autoref{tab:activeConstraints}.
					\item Typically when working with the necessary conditions directly (analytically), \(H^0\) or \(H^{m_g}\) are used.
					\item The conditions and relations for \(H^k\) are primarily useful to analyze for the solutions can be transformed one into another and are as such equivalent.
					\item The solutions for \(\vec{x}\) and \(\vec{u}\) are the same for all \(H^k\), but the \(\vec{\lambda}\) and \(\vec{\eta}\) might be different on the active sections (and only on these).
				\end{itemize}

				\begin{table}
					\centering
					\begin{tabular}{c|cc}
						Order \(m_g\) & Touch Point & Boundary Arc \\ \hline
						\(1\)         & no          & yes          \\
						even          & yes         & yes          \\
						odd           & yes         & no
					\end{tabular}
					\caption{Possibilities for active state-only constraints in optimal control given the order of the constraint.}
					\label{tab:activeConstraints}
				\end{table}
			% end
		% end

		\subsection{Examples} % 8.75
			\todo{Content}

			\subsubsection{Optimal Robot Control} % 8.76
				\todo{Content}
			% end

			\subsubsection{Energy Minimization Problem} % 8.77, 8.78, 8.79, 8.80, 8.81, 8.82, 8.83, 8.84, 8.85, 8.86
				\todo{Content}
			% end
		% end

		\subsection{Summary}
			\begin{itemize}
				\item The conditions derivable from the maximum principle for yields a multi-point boundary value problem for \(\vec{x}^\ast\) and \(\vec{\lambda}^\ast\).
				\item The interior switching points \( t_{S, i} \) result of:
					\begin{itemize}
						\item Switching points at bang-bang and singular control.
						\item The entry and exit into/of active sections of mixed or state constraints.
						\item Other constraints at interior time steps.
						\item A dynamic that is only defined piecewise.
					\end{itemize}
				\item The switching structure (i.e. the number of order of switching points) in normally unknown!
				\item The switching structure has to be determined otherwise, e.g. by solving the unconstrained problem and successively tighten the constraints (continuation/homotopy technique).
				\item Other things like discontinuities in the system dynamics and state constraints at interior points are not covered here\dots
				\item Optimal control is hell more complex than it seems in this summary!
			\end{itemize}
		% end
	% end
% end

\chapter{Calculating Optimal Trajectories}
	This chapter covers the numerical calculation of optimal trajectories exploiting the theoretical results of~\autoref{c:optimalControl}.

	\section{First Computation Methods}
		\subsection{Dynamic Programming}
			The technique of \emph{dynamic programming} was introduced by Richard Bellman for numerically solving optimal control problems:
			\begin{itemize}
				\item Start from the final state \(\vec{x}_f\) at \(t_f\), the discretized Bellman Equation is used to successively iterate forward in time until \( t = 0 \).
				\item The effort raises exponentially with the dimensions \(n_x\) of \(\vec{x}\) (the "curse of dimensionality").
				\item A lot of current research is based on dynamic programming, especially in the field of reinforcement learning.
			\end{itemize}
		% end

		\subsection{Gradient Methods (Min-H Methods)}
			\begin{enumerate}
				\item Starting with an approximation for \( \vec{x} \), \( \vec{\lambda} \), \( 0 \leq t \leq t_f \), calculate an approximation for \( \vec{u} \) by minimizing the Hamiltonian at discrete time steps \(t_i\), e.g. using gradient methods.
				\item Use this approximation for \(\vec{u}\) to solve the dynamics numerically using forward integration and calculate the adjunct variables using backward iteration.
				\item Repeat with the new approximations for \(\vec{x}\), \(\vec{\lambda}\).
			\end{enumerate}
		% end
	% end

	\section{Indirect Methods}
		\label{sec:indirectMethods}

		\emph{Indirect methods} are based on the necessary conditions and try to solve the arising boundary value problem.
		\begin{itemize}
			\item Advantages:
				\begin{itemize}
					\item As it depends on the necessary conditions, the found solution is likely to be optimal.
					\item By using highly precise integration methods, the solution can be determined with a high precision. This is especially useful, e.g. for satellite missions.
					\item Increasingly powerful tools like automatic differentiation ease this method.
				\end{itemize}
			\item Disadvantages:
				\begin{itemize}
					\item A lot of expert knowledge is needed to derive the optimality conditions and formulate the multi-point boundary value problem.
					\item The explicit derivation of the necessary conditions can be really costly.
					\item The correct, optimal switching structure is not known a-prior.
					\item Numerical methods need initial approximations for \(\vec{x}\) and \(\vec{\lambda}\) which are, especially for \(\vec{\lambda}\), hard to get.
				\end{itemize}
		\end{itemize}
	% end

	\section{Direct Methods}
		\emph{Direct methods} avoid the explicit formulation of the necessary conditions by discretizing the control and possible the state variables. They transform the optimal control problem to a nonlinear optimization problem and employ methods of the nonlinear optimization.
		\begin{itemize}
			\item The user does not have to handle adjunct variables, switching structures, \dots
			\item The efficiency of the method highly depends on the discretization and the employed nonlinear optimization problem.
			\item Progress in nonlinear optimization (especially SQP) yield highly efficient direct methods.
			\item Most commonly used.
		\end{itemize}

		All of the following assumes an optimal control problem given as:
		\begin{align*}
			\min_{\vec{u}}           & \, J[\vec{u}] = \phi\big(\vec{x}(t_0), \vec{x}(t_f), t_f\big) \\
			\mathrm{subject~to}\quad & \,
			\begin{alignedat}[t]{2}
				\dot{\vec{x}}(t)                        & = \vec{f}\big(\vec{x}(t), \vec{u}(t)\big),\quad t \in [t_0, t_f]  \\
				x_i(0)                                  & = x_{i, 0},\quad i \in I_0 \subseteq \{\, \dotsrange{1}{n_x} \,\} \\
				x_j(t_f)                                & = x_{j, f},\quad j \in I_f \subseteq \{\, \dotsrange{1}{n_x} \,\} \\
				\vec{g}\big(\vec{x}(t)\big)             & \geq \vec{0}                                                      \\
				\vec{g}\big(\vec{x}(t), \vec{u}(t)\big) & \geq \vec{0}
			\end{alignedat}
		\end{align*}
		The biggest difference is that this problem does not have a Lagrangian term, but as the Lagrangian can be transformed to a Mayer term, this is no loss of generality.

		\subsection{Direct Collocation Methods}
			\emph{Direct collocation methods} approximate the control state variables \(\vec{u}\), \(\vec{x}\) using piecewise polynomial functions.

			First of all, the interval \( [t_0, t_f] \) is split into \(n_s\) segments \( [t_i, t_{i + 1}] \) where
			\begin{align*}
				t_i = t_0 + \tau_i \cdot (t_f - t_0) \quad\text{with}\quad 0 = \tau_1 < \tau_2 < \cdots < \tau_{n_s + 1} = 1
			\end{align*}
			with all \(\tau_i\) given. The resulting segmented space is called the \emph{mesh}. In every of these segments, \(\vec{u}\) and \(\vec{x}\) and approximated using a polynomial. As \(\vec{u}\) is part of the integrand for \(\vec{x}\), it is reasonable to choose a higher polynomial degree for \(\vec{x}\) than \(\vec{u}\).

			Also let \( t_{i + 1/2} \coloneqq t_i + h_i / 2 \) with \( h_i \coloneqq t_{i + 1} - t_i \).

			\subsubsection{First Discretization, Constant Control} % 9.12, 9.13, 9.14
				A first idea is to approximate the control using constant functions
				\begin{align*}
					\tilde{u}_i(t) = u_i(t_{j + 1/2}),\quad t_j \leq t < t_{j + 1},\quad j = \dotsrange{1}{n_s},\quad i = \dotsrange{1}{n_u}
				\end{align*}
				and to approximate the state using linear functions:
				\begin{align*}
					\tilde{x}_i(t) = x_i(t_j) + \frac{t - t_j}{h_j} \big( x_i(t_{j + 1}) - x_i(t_j) \big),\quad t_j \leq t < t_{j + 1},\quad j = \dotsrange{1}{n_s},\quad i = \dotsrange{1}{n_x}
				\end{align*}
				The optimization variables are given as the vector
				\begin{align*}
					\vec{p}
					 & = \begin{bmatrix} \vec{x}(t_1) & \vec{u}(t_{1 + 1/2}) & \vec{x}(t_2) & \vec{u}(t_{2 + 1/2}) & \cdots & \vec{x}(t_{n_s}) & \vec{u}(t_{n_s + 1/2}) & \vec{x}(t_{n_s + 1}) & t_f \end{bmatrix}^T \\
					 & = \begin{bmatrix} \vec{p}_1^x & \vec{p}_1^u & \vec{p}_2^x  & \vec{p}_2^u & \cdots & \vec{p}_{n_s}^x & \vec{p}_{n_s}^u & \vec{p}_{n_s + 1}^x & t_f \end{bmatrix}^T
				\end{align*}
				where the final time \(t_f\) might be left out if it is given. The second row specifies a shorthand notation for the parameter of the approximation of \(\vec{x}\)/\(\vec{u}\). The approximations shall now be determined subject to:
				\begin{itemize}
					\item Minimize the objective functional.
					\item Fulfill the differential equations and inequality constraints at the center of a segment.
					\item Comply with the initial and final conditions.
				\end{itemize}

				This yields the following finite-dimensional nonlinear optimization problem with \( n_p = n_s \cdot (n_x + n_u) + n_x + 1 \) optimization variables (or one less if the final time is given):
				\begin{align*}
					\min_{\vec{p} \,\in\, \R^{n_p}} & \, \varphi(\vec{p}),\quad \varphi(\vec{p}) = \phi\big(\vec{p}_1^x, \vec{p}_{n_s + 1}^x, t_f\big) \\
					\mathrm{subject~to}\quad        & \,
					\begin{alignedat}[t]{2}
						\vec{f}\big( \tilde{\vec{x}}(t_{j + 1/2}),\, \tilde{\vec{u}}(t_{j + 1/2}) \big) - \dot{\tilde{\vec{x}}}(t_{j + 1/2}) & = \vec{0},\quad j = \dotsrange{1}{n_s}                            \\
						\tilde{x}_i(t_0)                                                                                                     & = x_{i, 0},\quad i \in I_0 \subseteq \{\, \dotsrange{1}{n_x} \,\} \\
						\tilde{x}_j(t_f)                                                                                                     & = x_{j, f},\quad j \in I_f \subseteq \{\, \dotsrange{1}{n_x} \,\} \\
						\vec{g}\big( \tilde{\vec{x}}(t_{j + 1/2}), \tilde{\vec{u}}(t_{j + 1/2}) \big)                                        & \geq \vec{0},\quad j = \dotsrange{1}{n_s}
					\end{alignedat}
				\end{align*}
				With \( \tilde{\vec{u}}(t_{j + 1/2}) = \vec{p}_j^u \),\,\, \( \tilde{\vec{x}}(t_{j + 1/2}) = \frac{1}{2} \big( \vec{p}_j^x + \vec{p}_{j + 1}^x \big) \) and \( \dot{\tilde{\vec{x}}}(t_{j + 1/2}) = \frac{1}{h_j} \big( \vec{p}_{j + 1}^x - \vec{p}_j^x \big) \).

				But low approximations like this have to use lots of mesh points to get good approximations. Hence, approximations of higher degree might be useful.
			% end

			\subsubsection{Second Discretization, Linear Control}
				Approximate the control using linear functions:
				\begin{align*}
					\tilde{u}_i(t) = u_i(t_j) + \frac{t - t_j}{h_j} \big( u_i(t_{j + 1}) - u_i(t_j) \big),\quad t_j \leq t < t_{j + 1},\quad j = \dotsrange{1}{n_s},\quad i = \dotsrange{1}{n_u}
				\end{align*}
				And approximate the state using linear functions:
				\begin{align*}
					\tilde{x}_i(t) = \sum_{k = 0}^{3} c_{i, j, k} \bigg( \frac{t - t_j}{h_j} \!\bigg)^k,\quad t_j \leq t < t_{j + 1},\quad j = \dotsrange{1}{n_s},\quad i = \dotsrange{1}{n_u}
				\end{align*}
				Here, \( c_{i, j, k} \) is the \(k\)-th coefficient for the \(i\)-th component in the \(j\)-th segment \( [t_j, t_{j + 1}] \), yielding four unknown parameters per component and segment. One of the parameters is determined by requiring continuity an the left side of the segment. The other three parameters are determined by requiring the fulfilling of the ODEs at three time steps in the \(j\)-th segment:
				\begin{itemize}
					\item Gauss Points: \( t_{j + 1/2} - \sqrt{3/5} h_j \),\quad \( t_{j + 1/2} \),\quad \( t_{j + 1/2} + \sqrt{3/5} h_j \)
						\begin{itemize}
							\item Theoretically provide the best approximation.
							\item Do not provide differentiable transitions between the segments.
							\item Need \( 3n_s \) evaluations of the ODEs.
						\end{itemize}
					\item Lobatto Points: \( t_j \),\quad \( t_{j + 1/2} \),\quad \( t_{j + 1} \)
						\begin{itemize}
							\item Provide continuously differentiable transitions between the segments.
							\item Need \( 2n_s + 1 \) evaluations of the ODEs.
						\end{itemize}
				\end{itemize}
				When using Lobatto points, the NLP parameters per segment can be reduced from four to two, thus reducing the collocation constraints from three to one per segment. But the remaining constraints are more nonlinear\dots Hence, the dimension of the NLP is similar to the constraint approximation, bit given a much better approximation of the solution.

				By plugging the constraints/locations of the Lobatto points into the state approximation, this yields the following formulas for the coefficients:
				\begin{align*}
					\begin{bmatrix}
						c_{i, j, 0} \\
						c_{i, j, 1} \\
						c_{i, j, 2} \\
						c_{i, j, 3}
					\end{bmatrix}
					=
					\begin{bmatrix}
						1  & 0     & 0  & 0    \\
						0  & h_j   & 0  & 0    \\
						-3 & -2h_j & 3  & -h_j \\
						2  & h_j   & -2 & h_j
					\end{bmatrix}
					\cdot
					\begin{bmatrix}
						p_{i, j}^x         \\
						p_{i, j}^{\dot{x}} \\
						p_{i, j + 1}^{x}   \\
						p_{i, j + 1}^{\dot{x}}
					\end{bmatrix}
					=
					\begin{bmatrix}
						p_{i, j}^x                                                                                \\
						h_j p_{i, j}^{\dot{x}}                                                                    \\
						-3 p_{i, j}^x - 2h_j p_{i, j}^{\dot{x}} + 3 p_{i, j + 1}^{x} - h_j p_{i, j + 1}^{\dot{x}} \\
						2 p_{i, j}^x + h_j p_{i, j}^{\dot{x}} - 2 p_{i, j + 1}^{x} + h_j p_{i, j + 1}^{\dot{x}}
					\end{bmatrix}
				\end{align*}
				Where \( p_{i, j}^{\dot{x}} = \dot{x}_i(t_j) = f_i\big(\vec{x}(t_j), \vec{u}(t_j)\big) = f_i(\vec{p}_j^x, \vec{p}_j^u) \). Hence, all coefficients can be computed using only the parameter vector
				\begin{align*}
					\vec{p}
					 & = \begin{bmatrix} \vec{x}(t_1) & \vec{u}(t_{1 + 1/2}) & \vec{x}(t_2) & \vec{u}(t_{2 + 1/2}) & \cdots & \vec{x}(t_{n_s}) & \vec{u}(t_{n_s + 1/2}) & \vec{x}(t_{n_s + 1}) & \vec{u}(t_{n_s + 1 + 1/2}) & t_f \end{bmatrix}^T \\
					 & = \begin{bmatrix} \vec{p}_1^x & \vec{p}_1^u & \vec{p}_2^x  & \vec{p}_2^u & \cdots & \vec{p}_{n_s}^x & \vec{p}_{n_s}^u & \vec{p}_{n_s + 1}^x & \vec{p}_{n_s + 1}^u & t_f \end{bmatrix}^T
				\end{align*}
				with one parameter more than the constant control approximation. That is, \( n_p = (n_s + 1) (n_x + n_u) + 1 \) parameters (or one less if the final time is fixed). This yields the following nonlinear optimization problem:
				\begin{align*}
					\min_{\vec{p} \,\in\, \R^{n_p}} & \, \varphi(\vec{p}),\quad \varphi(\vec{p}) = \phi\big(\vec{p}_1^x, \vec{p}_{n_s + 1}^x, t_f\big) \\
					\mathrm{subject~to}\quad        & \,
					\begin{alignedat}[t]{2}
						\vec{f}\big( \tilde{\vec{x}}(t_{j + 1/2}),\, \tilde{\vec{u}}(t_{j + 1/2}) \big) - \dot{\tilde{\vec{x}}}(t_{j + 1/2}) & = \vec{0},\quad j = \dotsrange{1}{n_s}                            \\
						\tilde{x}_i(t_0)                                                                                                     & = x_{i, 0},\quad i \in I_0 \subseteq \{\, \dotsrange{1}{n_x} \,\} \\
						\tilde{x}_j(t_f)                                                                                                     & = x_{j, f},\quad j \in I_f \subseteq \{\, \dotsrange{1}{n_x} \,\} \\
						\vec{g}\big( \tilde{\vec{x}}(t_{j + 1/2}), \tilde{\vec{u}}(t_{j + 1/2}) \big)                                        & \geq \vec{0},\quad j = \dotsrange{1}{n_s}
					\end{alignedat}
				\end{align*}
				With \( \tilde{\vec{u}}(t_{j + 1/2}) = \vec{p}_j^u \),\,\, \( \tilde{\vec{x}}(t_{j + 1/2}) = \frac{1}{2} \big( \vec{p}_j^x + \vec{p}_{j + 1}^x \big) \) and \( \dot{\tilde{\vec{x}}}(t_{j + 1/2}) = \frac{1}{h_j} \big( \vec{p}_{j + 1}^x - \vec{p}_j^x \big) \).
			% end

			\subsubsection{Discretization of Inequality Constraints}
				In the NLP, the inequality constraints are evaluated at the points \( t_j \), i.e. the mesh points. But why not at other points, e.g. \( \vec{g}(t_{j + 1/2}) \)? As of the theoretical results, one control is determined by
				\begin{align*}
					g^{(m_g)}\big(\vec{x}(t), \vec{u}(t)\big) = \dv[m_g]{t} g\big(\vec{x}(t)\big) \equiv 0
				\end{align*}
				in an active segment \( t \in [t_1, t_2] \). Hence, for consistency, the number of degrees of freedom on a potential arc have to equal the number of degrees of freedom of the discretized control constraints. This is only ensured when the inequality constraints are evaluated at the mesh points \(t_j\).
			% end

			\subsubsection{Sparsity}
				In direct collocation methods, the resulting NLPs have sparse gradients and Jacobians. For example, the memory complexity of the full Jacobian is \( \mathcal{O}(n_s^2) \), while the non-zero elements rise linearly. By exploiting this sparse structure, the efficiency for NLP solvers can be highly enhanced!
			% end

			\subsubsection{Convergence Properties and Solution Validation}
				By studying the Lagrangian of the NLP, it is possible to show that a gradient method that is using the Lagrangian for determining the step size, the maximum principle is taken into account in a discretized matter. That way, using direct collocation methods, it is possible to compute approximations for the adjunct variables and the multipliers \(\vec{\eta}^0\). Hence, the theoretical optimality conditions can be validated afterwards!

				For autonomous problems, the Hamiltonian has to be sectionally constant which can be validated using the calculated approximations \( \tilde{\vec{a}}(t) \), \( \tilde{\vec{u}}(t) \), \( \tilde{\vec{\lambda}}^0(t) \), \( \tilde{\vec{\eta}}^0(t) \). Another thing that can be validates is whether the initial and final conditions for the adjunct variables are fulfilled.

				\paragraph{Derivation} % 9.30, 9.31, 9.32
					\todo{Content}
				% end
			% end

			\subsubsection{Choosing the Mesh Points}
				Direct collocation methods typically start with a rough grid and a rough approximation for \(\vec{x}^\ast\) and \(\vec{u}^\ast\) that is then successively adjusted until the solution is smooth enough. But how to do this mesh adjustments?
				\begin{itemize}
					\item Ideally: A segment-wise approximation error \( \big\lVert \tilde{\vec{x}}(t) - \vec{x}^\ast(t) \big\rVert \), \( \big\lVert \tilde{\vec{u}}(t) - \vec{u}^\ast(t) \big\rVert \).
					\item Alternatively: Use the error of in the ODE and inequality constraints: \( \big\lVert \vec{f}\big(\tilde{\vec{x}}(t), \tilde{\vec{u}}(t)\big) - \dot{\tilde{\vec{x}}}(t) \big\rVert \), \( \big\lVert \vec{g}_{\_}\big(\tilde{\vec{x}}(t), \tilde{\vec{u}}(t)\big) \big\rVert \) with \( g_{\_} \coloneqq \lvert g \rvert \) as the element-wise absolute value.
				\end{itemize}
				And refine the mesh until \( \lVert \cdot \rVert \leq \varepsilon_\mathrm{tol} \).

				\paragraph{Segment-Wise Error Estimation}
					Assuming \( \tilde{\vec{u}}(t) = \vec{u}^\ast(t) \), theories of collocation methods can be used to estimate the error
					\begin{align*}
						\big\lVert \tilde{\vec{x}}(t) - \vec{x}^\ast(t) \big\rVert
					\end{align*}
					where \( \vec{x}^\ast \) is the "real" solution of the BVP. But this method does not work well for direct collocation methods.
				% end

				\paragraph{Segment-Wise Error Estimation of Time-Continuous Constraints}
					Simple strategy that works good in practice: Check the fulfilling of the constraints at test points, e.g. for the second discretization:
					\begin{align*}
						\big\lVert \vec{f}\big( \tilde{\vec{x}}(t), \tilde{\vec{u}}(t) \big) - \dot{\tilde{\vec{x}}}(t) \big\rVert \quad \big\lVert \vec{g}_{\_}\big(\tilde{\vec{x}}(t), \tilde{\vec{u}}(t)\big) \big\rVert \quad\text{for}\quad t = t_{j + k/4} = t_j + \frac{k}{4} h_j,\quad k = 1, 2, 3
					\end{align*}
					Then successively split the segments with too big errors into smaller segments.
				% end

				\paragraph{Segment-Wise Estimation of Optimality Error}
					The previous error estimation methods only take the constraints into account, not that this is an optimal control problem. But it is also possible to approximate the "optimality error" using the calculated values for the adjunct variables (e.g. using quadrature rules):
					\begin{align*}
						L = \phi\big( \tilde{\vec{x}}(0), \tilde{\vec{x}}(t_f), t_f \big) + \sum_{j = 1}^{n_s} \underbrace{\int_{t_j}^{t_{j + 1}} \! \Big[ \tilde{\vec{\lambda}}^T\!(t) \cdot \Big(\! \vec{f}\big( \tilde{\vec{x}}(t), \tilde{\vec{u}}(t) \big) - \dot{\tilde{\vec{x}}}(t) \!\Big) + \tilde{\vec{\eta}}^T\!(t) \cdot \vec{g}\big( \tilde{\vec{x}}(t), \tilde{\vec{u}}(t) \big) \Big] \dd{t}}_\text{Optimality Error in the \(j\)-th Segment}
					\end{align*}
				% end
			% end

			\subsubsection{Implementation}
				The general approach to implement direct collocation methods is shown in\autoref{alg:directCollocation}.

				\begin{algorithm}  \DontPrintSemicolon
					\textbf{Initialization:} Choose a start grid \( \big( \tau_j^{(0)} \big)_{j = 1}^{n_s^{(0)} + 1} \) and initial values \( \vec{p}_j^{x, (0)} = \vec{x}(t_j) \), \( \vec{p}_j^{u, (0)} = \vec{u}(t_{j + 1/2}) \) \;
					\While{not converged (error in \(\dot{\vec{x}}\), maximum iterations or maximum mesh size)}{
					Adjust the mesh: \( \big( \tau_j^{(k)} \big)_{j = 1}^{n_s^{(k)} + 1} \) \;
					Solve the resulting NLP:
					\begin{align*}
						\min_{\vec{p} \,\in\, \R^{n_p}} & \, \varphi(\vec{p}) \\
						\mathrm{subject~to}\quad        & \,
						\begin{alignedat}[t]{2}
							\vec{a}(\vec{p}) & = \vec{0}    \\
							\vec{b}(\vec{p}) & \geq \vec{0}
						\end{alignedat}
					\end{align*} \;
					\( k \gets k + 1 \) \;
					}

					\caption{Direct Collocation Algorithm.}
					\label{alg:directCollocation}
				\end{algorithm}
			% end

			\subsubsection{Notes}
				\begin{itemize}
					\item For a rough mesh, the tolerances \( \varepsilon_{\mathrm{opt}} \) and \( \varepsilon_\mathrm{ft} \) do not have to be chosen too tight as the NLP solver might not ding a solution then.
					\item While successively refining the grid, the tolerances can be set to tighter values.
					\item Ideally, the most SQP iterations are needed for the first two meshes. For the high-dimensional NLPs for fine grids, only a few iterations are needed.
					\item System parameters can be optimized simultaneously by adding them to the variable vector \(\vec{p}\).
				\end{itemize}
			% end

			\subsubsection{Application: Optimal Robot Control} % 9.23, 9.24, 9.26, 9.27, 9.28, 9.34, 9.35
				\todo{Content}
			% end
		% end

		\subsection{Direct Shooting Methods}
			Similar to the direct collocation methods, shooting methods rely on a segmentation of the the interval \( [t_0, t_f] \) into \( n_s \) segments \( [t_j, t_{j + 1}] \) with \( n_s + 1 \) mesh point \( t_j \). Then the control \(\vec{u}\) is approximated per section, e.g. using constant or linear functions.

			Direct shooting methods then solve various initial value problems until one fulfills the boundary conditions. Informally speaking, multiple trajectories are "shot into the room" until one is feasible. More formally, the process is:
			\begin{enumerate}
				\item Divide the interval into \(n_s\) segments and approximate the control \(\vec{u}\), e.g. using a piecewise linear function \( \tilde{\vec{u}} \). Then parameterize \(\vec{u}\) by \( \vec{p} = \begin{bmatrix} \vec{u}(t_1) & \cdots & \vec{u}(t_f) & t_f \end{bmatrix}^T \in \R^{n_u \cdot (n_s + 1)} \).
				\item Simulate the movement by forward-integrating the system dynamics starting from the initial values and by using the approximated control \( \tilde{\vec{u}} \). This gives an approximation of the state, \( \tilde{\vec{x}} \).
				\item Calculate the objective and the violations of the constraints, especially the final constraints.
				\item Optimize \(\vec{p}\) such that the objective is minimized while fulfilling the constraints. Repeat.
			\end{enumerate}

			But the calculation of the NLP gradients and the Jacobians needs calculating the sensitivity matrix \( \pdv{\vec{x}(t; \vec{p})}{\vec{p}} \).
			\begin{itemize}
				\item Advantages:
					\begin{itemize}
						\item The resulting dimension of the NLPs are lot less than for direct collocation methods.
						\item In every iteration of the NLP solver, a solution of the ODEs is available.
					\end{itemize}
				\item Disadvantages:
					\begin{itemize}
						\item Solving the initial value problem may highly depend on the approximations \( \tilde{\vec{u}} \), \( \tilde{t}_f \). To increase the robustness, multiple shooting methods can be used.
						\item Approximations for the adjunct variables are not as natural as in direct collocation methods, but possible.
					\end{itemize}
			\end{itemize}
		% end
	% end

	\section{Notes}
		\begin{itemize}
			\item Some big disadvantages of indirect methods, that good approximations of the optimal state- and adjunct variables and the switching structure are needed, can be overcome by using a direct method first.
			\item The majority of optimal control problems are nowadays solved using direct methods (other approaches exist besides direct collocation and shooting methods).
			\item Important applications are for example, aerospace engineering, robots, vehicles, process engineering, economy, biology, ecology, \dots
			\item Direct collocation methods are also called "simultaneous simulation and optimization".
			\item Direct shooting methods are also called "iterative simulation and optimization".
				\begin{itemize}
					\item Two different algorithms are used for simulation and optimization.
					\item Highly efficient and stable calculation of the sensitivity matrix using special integration techniques.
				\end{itemize}
		\end{itemize}
	% end
% end

\chapter{Optimal Feedback Control}
	Applying the computed control trajectory directly as an open loop (feedforward) control as illustrated in~\autoref{fig:feedforwardControl} would cause growing divergence from the nominal state \( \vec{x}_d^\ast(t) \). This can be caused by e.g.:
	\begin{itemize}
		\item Errors in the model (complex models cannot be 100\% accurate).
		\item Noise: During the run of a dynamic process noise may affect the systems behavior.
	\end{itemize}
	Hence, the wanted end state might not be reached when just using feedforward control.

	\begin{figure}
		\centering
		\begin{tikzpicture}[every node/.style = { align = center }, block/.style = { draw, rectangle, minimum width = 2.5cm, minimum height = 1.5cm }]
			\node [block] (a) {Trajectory \\ Planning};
			\node [block, right = 2 of a] (b) {Open-Loop \\ Control};
			\node [block, right = 2 of b] (c) {Dynamic \\ Process};
			\node [right = 1 of c] (d) {\( \vec{x}_a(t) \)};

			\draw [->] (a) -- node[above]{\( \vec{x}_d^\ast(t) \)} node[below]{\( \vec{u}^\ast(t) \)} (b);
			\draw [->] (b) -- node[above]{\( \vec{u}^\ast(t) \)} (c);
			\draw [->] (c) -- (d);
		\end{tikzpicture}
		\caption{Feedforward Control}
		\label{fig:feedforwardControl}
	\end{figure}

	\section{Classical Feedback Control (Position Control)}
		It seems obvious to just use a classical position control for the nominal stater state trajectory \( \vec{x}^\ast(t) \) (a set point trajectory control) as illustrated in~\autoref{fig:feedbackControl}. But this causes "ringing" around the nominal trajectory, e.g. using a PID control law. Additionally, using a position control has more flaws:
		\begin{itemize}
			\item An individual control for each state component is merely possible.
			\item The quality of the position control depends on the desired states and the control parameters.
			\item Returning to the nominal trajectory using control laws is not the optimal trajectory from the disturbed state as the initial state to the final state!
			\item Also, using position control can cause violations of the constraints, e.g. a bang-bang control always operates on the border of the control constraints. Hence, returning to the nominal trajectory might cause overshooting this constraints.
		\end{itemize}

		\begin{figure}
			\centering
			\begin{tikzpicture}[every node/.style = { align = center }, block/.style = { draw, rectangle, minimum width = 2.5cm, minimum height = 1.5cm }]
				\node [block] (a) {Trajectory \\ Planning};
				\node [block, right = 2 of a] (b) {Control};
				\node [block, right = 2 of b] (c) {Dynamic \\ Process};
				\node [right = 1 of c] (d) {\( \vec{x}_a(t) \)};
				\node [draw, rectangle, minimum width = 2.5cm, below = 0.5 of c] (e) {Sensors, \\ Odometry};

				\draw [->] (a) -- node[above]{\( \vec{x}_d^\ast(t) \)} node[below]{\( \vec{u}^\ast(t) \)} (b);
				\draw [->] (b) -- node[above]{\( \vec{u}^\ast(t) \)} (c);
				\draw [->] (c) -- (d);
				\draw [->] (d) |- (e) -| (b);
			\end{tikzpicture}
			\caption{Feedback Control}
			\label{fig:feedbackControl}
		\end{figure}
	% end

	\section{Optimal Feedback Control}
		If the real trajectory at time step \( t_1 \) differs from the nominal state \( \vec{x}^\ast(t_1; \vec{x}_0) \) that was calculated starting from \( \vec{x}_0 \) it would be optimal to now following a new trajectory with the initial state \( \vec{x}_d(t_1) \). But how to calculate this new trajectory? These computations have to be done anytime. But data from the old trajectory can be used for faster calculations!

		These re-computations would not be necessary if the optimal control trajectory would not be calculated as a function of time \( \vec{u}^\ast(t) \) (as a feedforward control), but as a function of the initial state \( \vec{u}^\ast(\vec{x}_0) \), i.e. as a feedback control. Sadly, it is nearly impossible to calculate this function except for some special cases\dots
	% end

	\section{Linear Quadratic Regulator (LQR)}
		Linear systems with quadratic objective are the only system for which the optimal feedback control \( \vec{u}^\ast(\vec{x}_0) \) can be computed in closed form! A linear system with quadratic objective is given as
		\begin{align*}
			\min_{\vec{u}}           & \, J[\vec{u}],\quad J[\vec{u}] = \int_{t_0}^{t_f} \Big( \vec{x}^T\!(t) \mat{Q} \vec{x}(t) + \vec{u}^T\!(t) \mat{\Gamma} \vec{u}(t) \Big) \dd{t} \\
			\mathrm{subject~to}\quad & \,
			\begin{alignedat}{2}
				\dot{\vec{x}}(t) & = \mat{A}(t) \vec{x}(t) + \mat{B}(t) \vec{u}(t) \\
				\vec{x}(t)       & = \vec{x}_0                                     \\
				\vec{x}(t_f)     & \phantom{=}\,\,\, \text{free}
			\end{alignedat}
		\end{align*}
		with a symmetric and positive definite matrix \( \mat{Q} \) and a diagonal Matrix \( \mat{\Gamma} = \diag(\dotsrange{\gamma_1}{\gamma_{n_u}}) \). The optimal control law is then given as
		\begin{align*}
			\vec{u}(\vec{x}) = -\mat{\Gamma}^{-1} \mat{B}^T\!(t) \mat{P}(t) \vec{x}
		\end{align*}
		where the matrix \( \mat{P}(t) \) is found by solving the matrix-Riccati ODE
		\begin{align*}
			\dot{\mat{P}}(t) + \mat{Q} + \mat{A}^T\!(t) \mat{P}(t) + \mat{P}(t) \mat{A}(t) - \mat{P}(t) \mat{\Gamma}^{-1} \mat{B}(t) \mat{B}^T\!(t) \mat{P}(t) = \mat{O}
		\end{align*}
		with the boundary condition \( \mat{P}(t_f) = \mat{O} \).

		If the system is time invariant, i.e. \( \dot{\mat{A}} = \mat{O} \), \( \dot{\mat{B}} = \mat{O} \) or \( t_f = \infty \), it follows \( \dot{\mat{P}} = \mat{O} \) and hence \(\mat{P}\) is given by solving the algebraic matrix-Riccati equation:
		\begin{align*}
			\mat{Q} + \mat{A}^T \mat{P} + \mat{P} \mat{A} - \mat{P} \mat{\Gamma}^{-1} \mat{B} \mat{B}^T \mat{P} = \mat{O}
		\end{align*}
		As \(\mat{P}\) is symmetric, these are \( n_x \cdot (n_x + 1) / 2 \) equations to determine every element of \(\mat{P}\).

		These matrix-Riccati algebraic/differential equations can be solved efficiently using special numerical methods. LQR systems can approximate a lots of systems and are therefore more useful than it appears at first (e.g. active damping in cars or linearized inverted pendulum). In theory, LQR methods can be applied to any system using an infinite-dimensional nonlinear embedding ("measurement") which can be finitely approximated, still yielding good results. Another approach is iterative LQR (iLQR) that linearizes the system in every time step.

		\subsection{Derivation} % 10.15, 10.16, 10.17
			\todo{Content}
		% end
	% end

	\section{Neighboring Extremals}
		TO approximately the new trajectory \( \vec{x}^\ast\big(t; \vec{x}_a(t_1)\big) \), all information that have already been expended for calculating the trajectory \( \vec{x}^\ast(t; \vec{x}_0) \) can be used. These updates have to be be done continuously.

		\subsection{Indirect Methods}
			Indirect methods calculates a solution trajectory \( \vec{x}^\ast(t) \), \( \vec{u}^\ast(t) \), \( \vec{\lambda}^\ast(t) \), \( \vec{\eta}^\ast(t) \) as the numerical solution of the multi-point boundary value problem rising from the necessary optimality conditions. By Taylor-expanding the disturbed trajectory \( \vec{x}^\ast(t; \vec{x}_0 + \vec{\varepsilon}_0) \) around the nominal trajectory yields a linear-quadratic optimal control problem (called "accessory minimum problem") for
			\begin{align*}
				\delta\vec{x}(t),\quad \delta\vec{u}(t),\quad \delta\vec{\lambda}(t)
			\end{align*}
			Different variants of this are possible, e.g. the repeated correction method:
			\begin{enumerate}
				\item Calculate \( \vec{x}^\ast(t) \), \( \vec{u}^\ast(t) \), \( \vec{\lambda}^\ast(t) \), \( \vec{\eta}^\ast(t) \) using the multi-point BVP width multiple-shooting methods.
				\item Feedback schema for noisy nominal trajectory \( \vec{u}^\ast(t_0; \vec{x}_0 + \vec{\varepsilon}_0) = \vec{u}^\ast(t_0; \vec{x}_0) + \delta\vec{u}(t_0) \) containing the nominal control and a correction term.
				\item Calculate the control correction using: \( \Delta\vec{u}(t_0) = \mat{K}_1(t_0) \cdot \partial\vec{x}(t_0) + \mat{K}_2(t) \cdot \partial\vec{\lambda}(t_0) \)
				\item Apply this repeatedly for different time steps \( t_0 = t_1 \).
			\end{enumerate}

			\begin{itemize}
				\item Advantages:
					\begin{itemize}
						\item Fast, numerical calculation.
						\item First-order optimal trajectories.
						\item Can be used to correct noise in the model parameters of the ODE.
					\end{itemize}
				\item Disadvantages:
					\begin{itemize}
						\item Only locally applicable along a nominal trajectory (i.e. "little" noise that does not change the switching structure)
						\item Depends on the multi-point BVP of the necessary conditions. Hence, application is time consuming as the ODEs have to be formulated.
					\end{itemize}
			\end{itemize}
		% end

		\subsection{Direct Methods}
			Direct methods solve the problem by discretizing the problem, resulting in a NLP that contains the initial value \( \vec{x}(t_0) \) in its equality constraints. Studying the dependency on this parameters leads to sensitivity and stability analysis of nonlinear optimization problems.

			For direct collocation methods it can be shown that the linear-quadratic optimal control problem is analogous to a quadratic problem (QP) for calculating the corrections \( \vec{p}^\ast(\vec{x}_0 + \vec{\varepsilon}_0) = \vec{p}^\ast(\vec{x}_0) + \delta\vec{p} \).
		% end

		\subsection{Nonlinear Model Predictive Control (NMPC)}
			For \emph{nonlinear model predictive control}, a series of optimal control problems is solved for a varying time horizon \(P_j\). These methods are commonly used for chemical processes with slow reaction time and aerospace engineering, but not so often in fast environments like robot movements.
		% end
	% end

	\section{Numerical Synthesis of the Nonlinear Feedback Control}
		Direct collocation methods can compute optimal control problems with different initial conditions \( \vec{x}_0 \) pretty fast and robust. Using an appropriate "step size control", multiple neighboring trajectories can be computed kind of automatic.
		\begin{itemize}
			\item First approach for synthesis \( \vec{u}^\ast(\vec{x}) \): Calculate \( \vec{u}^\ast \) using the HJB equation where \( \vec{x}^\ast(t) \), \( \vec{\lambda}^\ast(t) \) are approximated using reference trajectories for \( \vec{x}_a(t_1) \).
			\item Second approach: Approximate \( \vec{u}^\ast(t) \), e.g. an approximation that has been trained on lots of open-loop optimal trajectories.
		\end{itemize}
	% end
% end

\chapter{Further Topics on Optimal Control}
	\section{Inverse Optimal Control}
		\emph{Inverse optimal control} approaches the following problem:
		\begin{itemize}
			\item \emph{Given} a dynamic process \( \dot{\vec{x}} = \vec{f}\big(\vec{x}(t), \vec{u}(t)\big) \) and measurements of a run \( \vec{x}_k = \vec{x}(t_k) + \vec{\varepsilon}_k \) that is in general noisy,
			\item \emph{Assume} the run was controlled optimally,
			\item \emph{Find} the objective function that was used.
		\end{itemize}

		One possible approach is the linear combination of multiple "basis" functionals \( J_k[\vec{u}] \), e.g. one for minimum energy and one for minimum time:
		\begin{align*}
			J^\ast[\vec{u}] = \sum_{k = 1}^{n_J} \omega_k J_k[\vec{u}]
		\end{align*}
		This yields a two-level optimization problem:
		\begin{itemize}
			\item "Outer" problem: Finite-dimensional, nonlinear optimization problem: \( \Phi(\vec{\omega}) = \sum_{k = 1}^{n_m} \lVert \vec{x}^\ast(t_k, \vec{\omega}) - \vec{x}_k \rVert_2^2 \)
			\item "Inner" problem: Constraints of the outer problem; Optimal control problem with solution \( \vec{x}^\ast \)
		\end{itemize}
		\begin{align*}
			\min_{\vec{u}}           & \, J^\ast[\vec{u}],\quad J^\ast[\vec{u}] = \sum_{k = 1}^{n_J} \omega_k J_k[\vec{u}] \\
			\mathrm{subject~to}\quad & \,
			\dot{\vec{x}}(t) = \vec{f}\big(\vec{x}(t), \vec{u}(t)\big)
		\end{align*}
	% end

	\section{Differential/Dynamic Games}
		\emph{Differential games} have ODEs for the complete state \( \vec{x} \) containing multiple ODEs for every player and controls \( \dotsrange{\vec{v}_1(t)}{\vec{v}_{n_p}(t)} \) for every placer. Additionally they might have state or control constraints. The players then might minimize or maximize one or more objective cooperative or non-cooperative.

		\subsection{Non-Cooperative Two-Player Zero-Sum Differential Games}
			A common class of differential games are \emph{non-cooperative two-player zero-sum differential games} where:
			\begin{itemize}
				\item \emph{Non-cooperative} means that one player tries to maximize the objective and the other tries to minimize it.
				\item \emph{Zero-sum} means that the loss of one player is the reward of the other and vice versa.
			\end{itemize}
			Necessary conditions for these games can be derived from the minimax principle similar to the maximum principle using the ODEs for both players, adjunct differential equations and a Hamiltonian.

			This also yields a multi-point BVP that can be solved numerically using indirect methods and it is also possible to use direct methods.

			These games can be used, e.g. to generate robust trajectories by letting one player "be the noise" or friction or similar that tries to destroy the optimal trajectory.

			\subsubsection{Example} % 11.7
				\todo{Content}
			% end
		% end
	% end

	\section{Learning Methods and Optimization}
		Learning methods (from machine learning) often rely on formulations of solving specific optimization problems. Hence, machine learning and optimization is highly coupled. The focus is a little bit different as most optimization methods try to find solutions as accurate as possible while ML algorithms try to generalize as best as possible to handle unseen scenarios.

		\subsection{Foundations}
			The underlying task often is to find a model \( \vec{f}_{\vec{\theta}}(\vec{x}) \) that has a specific input/output behavior that is enforced using a \emph{loss function}, e.g. the least squares loss function. These models are then optimized using basic gradient methods, gradient descent, without step size determination but using a small step size or an adaptive one (e.g. adam, adagrad, adadelta). A common function approximator are neural networks which use multiple \emph{layers}, \emph{activation functions} and \emph{weights} to approximate arbitrary functions (in fact, any function can be approximated using a two-layer neural net).

			Some open questions are:
			\begin{itemize}
				\item Why and when does gradient descent work and how fast?
				\item Why does not training not always cause overfitting (even when the number of parameters are much higher than the number of samples)?
				\item How to interpret the trained models (explainable AI)?
			\end{itemize}
		% end

		\subsection{Reinforcement Learning}
			\emph{Reinforcement learning} is highly related to optimal control in terms if a reward that is maximized by tweaking controls etc. However, optimal control is often discrete and models are often treated as stochastic models allowing reasoning about uncertainty. It is also possible to not have any model (e.g. in model-free reinforcement learning)! The model is then learned implicitly.

			\subsubsection{Reward}
				In the RL setting, it is possible to study both finite and infinite time horizons using discounted rewards ensuring that the sum of rewards converges.
			% end

			\subsubsection{Value Function}
				In RL, a policy \( \pi \) is searched which is commonly dependent on the state, not time: \( \pi = \pi(\vec{x}) \).

				The \emph{value function} describes the discounted reward starting from the current state:
				\begin{align*}
					V^\pi(\vec{x}_0) = \sum_{k = 0}^{\infty} \gamma^k r_k\big(\vec{x}_k, \pi(\vec{x}_k)\big)
				\end{align*}
				The \emph{state-action function} describes the discounted reward of the current state if a specific action is taken next and following the policy afterwards:
				\begin{align*}
					Q^\pi(\vec{x}_0, \vec{u}_0) = r_1(\vec{x}_0, \vec{u}_0) + \sum_{min}^{max}
				\end{align*}
			% end

			\subsubsection{First Approach}
				A first approach would be to learn the control directly by maximizing the reward \( J \) using gradient descent:
				\begin{align*}
					\grad_{\vec{\theta}}{J} = \pdv{J}{\pi_{\vec{\theta}}} \pdv{\pi_{\vec{\theta}}}{\vec{\theta}}
				\end{align*}
				But this requires a gradient of the objective\dots The approximation may have high variances. Additionally, new gradient is computed independently of old approximations. Hence, no learning happens.
			% end

			\subsubsection{Learning the Value Function}
				It is better to solve the optimization problem
				\begin{align*}
					\vec{u}^\ast(\vec{x}) = \arg\max_{\vec{u}} \, Q^\pi(\vec{x}, \vec{u})
				\end{align*}
				by approximating the value function \( V(\vec{x}_0) \) with a function \( V_{\vec{\theta}}(\vec{x}_0) \), e.g. with \emph{Temporal Difference Learning} (TD).

				This uses the Bellman equation
				\begin{align*}
					V(\vec{x}) = \max_{\vec{u}} \, r(\vec{x}, \vec{u}) + \gamma V(\vec{x}, \vec{u}) = \max_{\vec{u}} Q(\vec{x}, \vec{u})
				\end{align*}
				which is a discretized version of the H-J-B equation. Temporal difference learning measures the TD-error \( \delta_n = r(\vec{x}, \vec{u}) + V_{\vec{\theta}}(\vec{x}_n) - \gamma V_{\vec{\theta}}(\vec{x}_{n + 1}) \) and uses gradient descent to update the parameters \(\vec{\theta}\). An instance of this class of algorithms is Q-Learning.

				\begin{itemize}
					\item Advantages:
						\begin{itemize}
							\item Small variance in the approximations of the expected reward.
						\end{itemize}
					\item Disadvantages:
						\begin{itemize}
							\item In every state, an optimization problem has to be solved. The overhead can be reduced by discretizing the control and just trying out all possibilities.
							\item No guarantee of convergence (but this holds for every algorithm proposed in this course as it may guarantee convergence but convergence to a poor local minimum).
						\end{itemize}
				\end{itemize}
			% end

			\subsubsection{Actor-Critic}
				Another approach is to approximate the control and the value function at the same time:
				\begin{itemize}
					\item The \emph{actor} generates control \(\vec{u}\) for a given state \(\vec{x}\) and
					\item the \emph{critic} estimates the quality of the current control and learns the value functions that is used to tweak the control parameters.
				\end{itemize}
				This method combined the advantages of the previous approaches as it does not require to solve an optimization problem in each step and no control discretization. It also allows a more precise measurement of the gradients.
			% end

			\subsubsection{Notes}
				\begin{itemize}
					\item All proposed methods are based on an approximation of the value function or the control. In practice, neural networks are often used for these approximations.
					\item Stability proves have been made for systems with special structures (e.g. affine dynamics).
					\item Better convergence than actor-only methods.
					\item The approach can be extended to models with continuous time.
				\end{itemize}
			% end
		% end
	% end
% end
